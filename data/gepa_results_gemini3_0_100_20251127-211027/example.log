GEPA artifacts will be saved to: /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/gepa_results
Dataset sizes -> train/val: 48 samples
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp90nl4zgb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe1e8l1h2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplmsz86zv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgp9_xou8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwbcvwwfl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_uat444l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl4z69st2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxzgr9600.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkel6k5ed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp48yiphyx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps4f2s2f8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp26_7tkae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphti49men.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4aa97dcg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2hq7nzwc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqqs_4tye.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp85id3a58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz93wuqvt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2dsgbq8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo2u141xb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwxvnzqvd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpssnmjylb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp05n47p1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt1u4mkd9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy52tkgld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr9zbl5i_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg4jl5o6p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps_sugt56.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplthsmy_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf56tksqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp60p9utzo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7pkajrxp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwn9k74nr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgh19dgfl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgjz7ilwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvln3j8de.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkfbfgrjf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy90vino5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp33463ygq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdbe9sqob.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp31c0xurj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp5kdbbi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoy7mdd7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsqerks9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5i9f9z_f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpixcwqnr4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnr1e9sx6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0uub_1sw.pickle

Iteration 0: Base program full valset score: 0.2066171041666667
Iteration 1: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfm4rlujo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4p3wike6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxhyndf9g.pickle

Iteration 1: Proposed new text for program: # Metadata to track segments and access times
m_probationary = set()
m_protected = set()
m_access_time = dict()

# Tuning parameter: What % of the cache count should be protected?
# 80% protected / 20% probationary is a common, robust split for SLRU.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Strategy:
    1. Prefer evicting the LRU item from the Probationary segment.
    2. If Probationary is empty, evict the LRU item from the Protected segment.
    '''
    global m_probationary, m_protected, m_access_time
    
    candid_obj_key = None
    
    # Strategy 1: Evict from Probationary (Scan Resistant)
    # We look for the key in m_probationary with the smallest timestamp in m_access_time
    if m_probationary:
        # Find LRU in probationary
        candid_obj_key = min(m_probationary, key=lambda k: m_access_time.get(k, 0))
    else:
        # Strategy 2: If Probationary is empty, evict from Protected
        # Find LRU in protected
        candid_obj_key = min(m_protected, key=lambda k: m_access_time.get(k, 0))

    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    SLRU Hit Logic:
    - Update access time (Recency).
    - If in Probationary, promote to Protected.
    - If Protected segment gets too big, demote LRU of Protected back to Probationary.
    '''
    global m_probationary, m_protected, m_access_time
    
    key = obj.key
    m_access_time[key] = cache_snapshot.access_count
    
    if key in m_probationary:
        # PROMOTE: Move from Probationary to Protected
        m_probationary.remove(key)
        m_protected.add(key)
        
        # BALANCING: Ensure Protected segment doesn't exceed capacity ratio
        # Calculate target protected size based on total current items
        total_items = len(cache_snapshot.cache)
        target_protected_size = int(total_items * PROTECTED_RATIO)
        
        if len(m_protected) > target_protected_size:
            # Demote the LRU item from Protected back to Probationary
            # Note: We are strictly demoting, not evicting here.
            victim_key = min(m_protected, key=lambda k: m_access_time.get(k, 0))
            m_protected.remove(victim_key)
            m_probationary.add(victim_key)
            
    # If already in Protected, it stays there (timestamp updated above)

def update_after_insert(cache_snapshot, obj):
    '''
    SLRU Insert Logic:
    - New items always enter the Probationary segment.
    '''
    global m_probationary, m_protected, m_access_time
    
    key = obj.key
    m_access_time[key] = cache_snapshot.access_count
    
    # New items start in Probationary
    m_probationary.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    SLRU Eviction Cleanup:
    - Remove metadata for the evicted object.
    '''
    global m_probationary, m_protected, m_access_time
    
    key = evicted_obj.key
    
    if key in m_access_time:
        del m_access_time[key]
    
    if key in m_probationary:
        m_probationary.remove(key)
    elif key in m_protected:
        m_protected.remove(key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4f5njqus.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjrup8lnw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpouzfcolm.pickle

Iteration 1: New subsample score 0.719924 is better than old score 0.48805599999999993. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp79a03g9v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzoj8lwu0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp26524dsd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp60mfz3nx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_yl4k724.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvjs1xoxb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz3sn03gq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2079d418.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjvshfcw7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph3ztk3pt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi1tgjozv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt6i5qwyd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplwtwgc11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyu8ih6i6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp0lpcmzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2px9zuo9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb9ipwx3f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu2o7uefg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa2lft5px.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9ftsyr0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8twma656.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpstid2n6g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw5s39wwu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5ohrryvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8ga49wf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ijwyhjg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0u2m2kja.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplxzju5g_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9u9y6pec.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb6a5um6f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj0pcjh56.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy9_rmeib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8j2k89ns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpakrv79fy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0g0qyn5l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_6vtcklf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl3q3x6n6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3d3jxwf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprdlzdpwf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeuo1965s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt_8u5ufu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa8megfbs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyczjlnyw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfv8ctobt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ydna37q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppxfhmn80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp44swap4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbx4wkc2y.pickle

Iteration 1: New program is on the linear pareto front
Iteration 1: Full valset score for new program: 0.23128962499999997
Iteration 1: Full train_val score for new program: 0.23128962499999997
Iteration 1: Individual valset scores for new program: [0.476754, 0.447058, 0.456336, 0.390326, 0.47767, 0.458953, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.344583, 0.03997, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.364798, 0.026164, 0.058672, 0.058672, 0.269796, 0.271169, 0.769613, 0.887215, 0.039533, 0.038636, 0.045558, 0.007003, 0.020672, 0.754595, 0.083333, 0.067961, 0.026001, 0.634209, 0.125461, 0.090657, 0.06292, 0.0723, 0.052632, 0.233333, 0.041778, 0.074053, 0.460123, 0.081699]
Iteration 1: New valset pareto front scores: [0.476754, 0.447058, 0.456336, 0.390326, 0.47767, 0.458953, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.344583, 0.03997, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.364798, 0.026164, 0.058672, 0.058672, 0.269796, 0.271169, 0.769613, 0.887215, 0.039533, 0.038636, 0.045558, 0.007003, 0.020672, 0.754595, 0.083333, 0.067961, 0.026001, 0.634209, 0.125461, 0.090657, 0.06292, 0.0723, 0.052632, 0.233333, 0.041778, 0.074053, 0.460123, 0.081699]
Iteration 1: Full valset pareto front score: 0.23128962499999997
Iteration 1: Updated valset pareto front programs: [{1}, {1}, {1}, {1}, {1}, {1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}]
Iteration 1: Best valset aggregate score so far: 0.23128962499999997
Iteration 1: Best program as per aggregate score on train_val: 1
Iteration 1: Best program as per aggregate score on valset: 1
Iteration 1: Best score on valset: 0.23128962499999997
Iteration 1: Best score on train_val: 0.23128962499999997
Iteration 1: Linear pareto front program index: 1
Iteration 1: New program candidate index: 1
Iteration 2: Selected program 1 score: 0.23128962499999997
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4fih95zm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgjy6xd12.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprak_t6b4.pickle

Iteration 2: Proposed new text for program: # S3-FIFO Metadata
# S: Small FIFO queue for new items (probationary)
# M: Main FIFO queue for popular items
# G: Ghost set (tracking keys of recently evicted items)
# freq: A frequency bit/counter (0, 1, 2) to track utility
s_queue = []
m_queue = []
g_set = set()
freq = {}

# Tuning parameter: Size of the small queue relative to cache capacity.
# 10% is the standard optimal static value for S3-FIFO.
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Strategy:
    1. If the Small Queue (S) is larger than 10% of the cache, evict from S.
       - Items in S with high frequency move to M.
       - Items in S with 0 frequency are evicted.
    2. Otherwise, evict from the Main Queue (M).
       - Items in M with frequency > 0 get a "second chance" and move to the back of M (decrementing freq).
       - Items in M with 0 frequency are evicted.
    '''
    global s_queue, m_queue, g_set, freq
    
    # Calculate thresholds
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    # We loop until we find a victim to return. 
    # Because the cache is full, this loop is guaranteed to eventually find a victim 
    # as long as we decrement frequencies.
    while candidate_key is None:
        
        # --- Logic A: Evict from Small Queue (S) ---
        # We prefer evicting from S if it exceeds its target size.
        if len(s_queue) >= s_target_size or not m_queue:
            if not s_queue: 
                # Safety fallback if S is empty but M exists (should be handled by else, but safe)
                break 
                
            victim = s_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Give second chance: Move from S to M
                s_queue.pop(0)
                m_queue.append(victim)
                # Reset frequency to give it a fresh start in M (optional, but standard S3 treats promotion as 0)
                # Here we keep freq but capped typically. Let's just keep it simple.
            else:
                # Evict this item
                candidate_key = s_queue.pop(0)
                # Add to ghost cache to track miss history
                g_set.add(candidate_key)
                
        # --- Logic B: Evict from Main Queue (M) ---
        else:
            victim = m_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Give second chance: Reinsert at tail of M, decrement frequency
                m_queue.pop(0)
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Evict this item
                candidate_key = m_queue.pop(0)
                # Items evicted from M usually don't need to go to Ghost, 
                # but adding them doesn't hurt logic significantly. S3 standard usually only ghosts S evictions.

    # Maintain Ghost Set Size: Keep it roughly same size as Main Cache to limit memory
    if len(g_set) > cache_count:
        # Crude removal of arbitrary element to keep set bounded (sets aren't ordered)
        # In a real implementation, G is usually a FIFO too.
        g_set.pop()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    S3-FIFO Hit Logic:
    - Increment frequency (capped at small number, e.g., 3, to prevent pollution).
    - Note: We do NOT move items between queues on hits. Reordering only happens during eviction.
      This makes S3-FIFO very fast on hits (O(1) update).
    '''
    global freq
    
    key = obj.key
    curr_freq = freq.get(key, 0)
    
    # Cap frequency to 3. 
    # We only need to know: 0 (evict), 1 (keep), 2+ (highly popular)
    if curr_freq < 3:
        freq[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    S3-FIFO Insert Logic:
    - If key is in Ghost Set (G), it was recently evicted from S. This implies we made a mistake.
      Insert directly into Main Queue (M).
    - Otherwise, insert into Small Queue (S).
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # It was a ghost hit! Restore to Main Queue.
        m_queue.append(key)
        g_set.remove(key)
    else:
        # Standard insertion: start in probationary Small Queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup Logic:
    - Remove frequency tracking for the evicted object.
    '''
    global freq
    
    key = evicted_obj.key
    if key in freq:
        del freq[key]
    
    # Note: We do NOT remove from g_set here. 
    # The evict() function decides if a key goes into g_set *before* it returns.
    # update_after_evict is called *after* the cache has confirmed the removal.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwscqlij7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsqecmp46.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8kar_cda.pickle

Iteration 2: New subsample score 0.259004 is better than old score 0.086642. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe9242iji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo7q8vfhw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppw4a9z2t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph6huzfa9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb6bp5pd_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4umkq4z6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8xssqk3k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyddy85kv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpli1apdr8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7s8ypioc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq96e41bs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx6_jlmsq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppnciujwq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzzbxdfca.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnexchliw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6srmz405.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptbunx2z2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1a75jx6b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprjl5gzh_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv4mswal6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw6shxob1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprns8a3g4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm3rjkove.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbywlzsce.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqczd90lm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplmdcprxx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6e4w2n3t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpflbogl50.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp541f5h_c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnmxtz5qj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwusj71mm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps93go9e9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt68rhv9z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9_yxvxdk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6bmksxoh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp89dovho5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3xp7qxe4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpixabihx3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2wu0uy7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_ce0gon3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2zhoqg88.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplh766mjh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwterc8ba.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5f75x8xi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7vq7ulvy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwg504jtv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdakjhqfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprg0ri2bg.pickle

Iteration 2: New program is on the linear pareto front
Iteration 2: Full valset score for new program: 0.2567872291666667
Iteration 2: Full train_val score for new program: 0.2567872291666667
Iteration 2: Individual valset scores for new program: [0.495426, 0.471749, 0.477971, 0.432827, 0.491309, 0.479304, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.385435, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.384464, 0.026164, 0.058672, 0.058672, 0.269829, 0.334677, 0.82423, 0.891665, 0.146451, 0.038636, 0.045558, 0.020458, 0.029648, 0.758169, 0.083333, 0.067961, 0.197998, 0.638331, 0.125461, 0.15165, 0.126451, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 2: New valset pareto front scores: [0.495426, 0.471749, 0.477971, 0.432827, 0.491309, 0.479304, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.385435, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.384464, 0.026164, 0.058672, 0.058672, 0.269829, 0.334677, 0.82423, 0.891665, 0.146451, 0.038636, 0.045558, 0.020458, 0.029648, 0.758169, 0.083333, 0.067961, 0.197998, 0.638331, 0.125461, 0.15165, 0.126451, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 2: Full valset pareto front score: 0.2567872291666667
Iteration 2: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 1, 2}, {2}, {0, 1, 2}, {1, 2}, {2}, {2}, {0, 1, 2}, {2}, {2}, {2}, {1, 2}, {2}, {1, 2}, {2}, {1, 2}, {1, 2}, {1, 2}, {2}, {2}, {2}, {2}, {2}, {1, 2}, {1, 2}, {2}, {2}, {2}, {1, 2}, {1, 2}, {2}, {2}, {1, 2}, {2}, {2}, {2}, {1, 2}, {2}, {2}, {2}, {2}, {2}]
Iteration 2: Best valset aggregate score so far: 0.2567872291666667
Iteration 2: Best program as per aggregate score on train_val: 2
Iteration 2: Best program as per aggregate score on valset: 2
Iteration 2: Best score on valset: 0.2567872291666667
Iteration 2: Best score on train_val: 0.2567872291666667
Iteration 2: Linear pareto front program index: 2
Iteration 2: New program candidate index: 2
Iteration 3: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd3t1ebyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq2th8dk4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpviud1t7f.pickle

Iteration 3: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# S: Small FIFO queue (probationary items)
# M: Main FIFO queue (popular items)
# G: Ghost FIFO queue (history of recently evicted items from S)
# freq: Frequency counter (0-3) to track utility
s_queue = deque()
m_queue = deque()
g_queue = deque() # Maintains order of ghosts
g_set = set()     # Maintains fast lookup for ghosts
freq = {}

# Tuning parameters
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Strategy (Optimized):
    - Uses a strict FIFO ordering for the Ghost cache to maximize scan resistance.
    - Resets frequency on promotion to M to ensure M contains truly high-utility items.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    while candidate_key is None:
        # Maintenance: Clean dead ghosts from the head of G queue
        # These are items that were promoted back to M and removed from g_set,
        # but their stale entry remains in the deque.
        while g_queue and g_queue[0] not in g_set:
            g_queue.popleft()

        # Decide which queue to process for eviction
        # Priority:
        # 1. If M is empty, we must process S.
        # 2. If S is empty, we must process M.
        # 3. If S is larger than its target ratio, prefer processing S.
        # 4. Otherwise, process M.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif not s_queue:
            evict_from_s = False
        else:
            evict_from_s = len(s_queue) >= s_target_size
            
        if evict_from_s:
            # --- Logic A: Evict/Promote from Small Queue (S) ---
            victim = s_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Promotion: S -> M
                # Item was accessed while in S. Move to M.
                s_queue.popleft()
                m_queue.append(victim)
                # Optimization: Reset freq to 0. The item used its "credit" to get to M.
                # It must receive new hits in M to survive the M-queue eviction cycle.
                freq[victim] = 0 
            else:
                # Evict from S
                candidate_key = s_queue.popleft()
                
                # Add to Ghost (Strict FIFO)
                # Only items evicted from S go to Ghost.
                if candidate_key not in g_set:
                    g_queue.append(candidate_key)
                    g_set.add(candidate_key)
                
                # Maintain Ghost Size (approx equal to Cache Size)
                while len(g_set) > cache_count:
                    if not g_queue:
                        break 
                    old = g_queue.popleft()
                    if old in g_set:
                        g_set.remove(old)
                        
        else:
            # --- Logic B: Evict/Retain from Main Queue (M) ---
            victim = m_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Move to back of M, decrement freq
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Evict from M
                candidate_key = m_queue.popleft()
                # Items evicted from M usually do not enter Ghost

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit, increment frequency up to a small cap.
    No queue movement happens here (O(1)).
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - If in Ghost: Restore to M (Rescue).
    - Else: Insert to S (Probation).
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize freq
    
    if key in g_set:
        # Ghost Hit! This item was recently evicted from S.
        # It has proven to be part of a larger working set or scan. Promote to M.
        m_queue.append(key)
        g_set.remove(key) 
        # Note: We leave the stale key in g_queue; it will be cleaned lazily in evict().
    else:
        # New insertion starts in Small Queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi1sf07xn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8fxe4_es.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj84r5tyy.pickle

Iteration 3: New subsample score 1.469627 is better than old score 1.46882. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdlhz3ou2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeie8464v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpll8fsxq6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp1hv__sw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5e_gzf1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzjeyhn8z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp336ejzi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfcvjbv1a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppmnqcpuh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwdmulop1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdjsnrjad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbu7d136r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaadju_k8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4digvczg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6iycglke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzx1vkdaf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9nva3lbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp66fa6i5_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuy33_8su.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcc6oq0_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbqtp0p_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpypmt3g6t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1o8k9k8e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnriaq7qp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprig53gvd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5g009ckp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppnlz9ry7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr_xpknbk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk97nmt9b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphiv2zrmq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe_trpbb2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpci5nx4nc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpve_j3lft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi70nf4n4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgkskctnd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0w3d5wa6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3quqsih1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeyfbo_gv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnfigkar1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm469rtbo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvl9oewz7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptjy1on7q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvey4cbzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdu95pija.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4bmv6ou0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqgd0lrur.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmj3an8fn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3iuqgvn0.pickle

Iteration 3: Full valset score for new program: 0.2526974166666666
Iteration 3: Full train_val score for new program: 0.2526974166666666
Iteration 3: Individual valset scores for new program: [0.495749, 0.470756, 0.480395, 0.434516, 0.493127, 0.481552, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.393428, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.391347, 0.026164, 0.058672, 0.058672, 0.271352, 0.349798, 0.822244, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 3: New valset pareto front scores: [0.495749, 0.471749, 0.480395, 0.434516, 0.493127, 0.481552, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.393428, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.391347, 0.026164, 0.058672, 0.058672, 0.271352, 0.349798, 0.82423, 0.892179, 0.146451, 0.038636, 0.045558, 0.026575, 0.029648, 0.758169, 0.083333, 0.067961, 0.197998, 0.640392, 0.125461, 0.15165, 0.126451, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 3: Full valset pareto front score: 0.25780210416666666
Iteration 3: Updated valset pareto front programs: [{3}, {2}, {3}, {3}, {3}, {3}, {2, 3}, {0, 1, 2, 3}, {2, 3}, {0, 1, 2, 3}, {1, 2, 3}, {3}, {2, 3}, {0, 1, 2, 3}, {2, 3}, {2, 3}, {2, 3}, {1, 2, 3}, {2, 3}, {1, 2, 3}, {3}, {1, 2, 3}, {1, 2, 3}, {1, 2, 3}, {3}, {3}, {2}, {3}, {2}, {1, 2, 3}, {1, 2, 3}, {3}, {2}, {2}, {1, 2, 3}, {1, 2, 3}, {2}, {3}, {1, 2, 3}, {2}, {2, 3}, {2}, {1, 2, 3}, {2}, {2}, {2}, {2, 3}, {2}]
Iteration 3: Best valset aggregate score so far: 0.2567872291666667
Iteration 3: Best program as per aggregate score on train_val: 2
Iteration 3: Best program as per aggregate score on valset: 2
Iteration 3: Best score on valset: 0.2567872291666667
Iteration 3: Best score on train_val: 0.2567872291666667
Iteration 3: Linear pareto front program index: 2
Iteration 3: New program candidate index: 3
Iteration 4: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprd1z6ppe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbhskd34v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpspkcweqd.pickle

Iteration 4: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# S: Small FIFO queue (probationary items)
# M: Main FIFO queue (popular/protected items)
# G: Ghost FIFO queue (history of keys evicted from S)
# freq: Frequency counter to track utility

# Global State
s_queue = deque()
m_queue = deque()
g_queue = deque()  # Maintains order of ghosts for FIFO expiry
g_set = set()      # Fast lookup for ghost membership
freq = {}          # Map of key -> integer frequency

# Tuning Parameters
# S_RATIO: 10% of cache size reserved for new/probationary items.
S_RATIO = 0.1
# G_RATIO: Ghost registry size relative to cache capacity. 
# Increased to 3.0 to capture larger working sets and loops (improves Trace 34).
G_RATIO = 3.0
# MAX_FREQ: Maximum frequency cap (2 bits / value 3 is usually sufficient).
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict when the cache is full.
    Strategy: S3-FIFO with preserved frequency and expanded ghost tracking.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * S_RATIO))
    ghost_target_size = int(cache_count * G_RATIO)
    
    candidate_key = None
    
    # We must loop until we find a candidate to evict. 
    # Operations like promotions (S->M) or reinsertions (M->M) do not free space.
    while candidate_key is None:
        
        # --- Decision Logic: S-Queue vs M-Queue ---
        # 1. If M is empty, we must process S.
        # 2. If S is larger than its target allocation, we force processing S to maintain balance.
        # 3. Otherwise, we process M to clean up cold items from the main cache.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif len(s_queue) >= s_target_size:
            evict_from_s = True
        else:
            evict_from_s = False
            
        if evict_from_s:
            # --- Logic: Small Queue (Probation) ---
            if not s_queue:
                # Fallback safety, though unlikely due to logic above
                evict_from_s = False
                continue

            victim = s_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Promotion: S -> M
                # The item was accessed during probation. Move to M.
                s_queue.popleft()
                m_queue.append(victim)
                # OPTIMIZATION: Do NOT reset freq to 0. 
                # Preserving the frequency helps valid hot items survive the M-queue check.
            else:
                # Eviction: S -> Ghost
                candidate_key = s_queue.popleft()
                
                # Register in Ghost
                if candidate_key not in g_set:
                    g_queue.append(candidate_key)
                    g_set.add(candidate_key)
                
                # Manage Ghost Size
                # We lazily clean the ghost queue to keep it within G_RATIO limits
                while len(g_set) > ghost_target_size and g_queue:
                    old_ghost = g_queue.popleft()
                    if old_ghost in g_set:
                        g_set.remove(old_ghost)
                        
        else:
            # --- Logic: Main Queue (Protected) ---
            victim = m_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance (SIEVE-like)
                # Item is hot. Move to back of M and decrement frequency.
                # It must earn its keep to stay in M.
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Eviction: M -> Out
                # Items evicted from M generally do not go to Ghost (they had their chance).
                candidate_key = m_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency utility, capped at MAX_FREQ.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: 
    - Check Ghost: if present, rescue to M (High Priority).
    - Else: Insert to S (Probation).
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    
    if key in g_set:
        # Ghost Hit! 
        # This item was evicted recently but requested again. It's likely part of a 
        # loop or working set larger than the cache.
        # Rescue directly to Main Queue.
        m_queue.append(key)
        g_set.remove(key)
        # Grant maximum frequency so it survives inside M for a while.
        freq[key] = MAX_FREQ 
    else:
        # New Item. Start in Small Queue with 0 frequency.
        s_queue.append(key)
        freq[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object to prevent memory leaks in freq map.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_say05f_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8_7a1abu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2wwxj308.pickle

Iteration 4: New subsample score 1.327873 is not better than old score 1.33485, skipping
Iteration 5: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaw9bhg5l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw4n7vfmu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp91wbj8m9.pickle

Iteration 5: Proposed new text for program: from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Global State
# T1: LRU list of recently seen items (seen once)
# T2: LRU list of frequently seen items (seen at least twice)
# B1: Ghost LRU list of items evicted from T1
# B2: Ghost LRU list of items evicted from T2
t1 = OrderedDict()
t2 = OrderedDict()
b1 = OrderedDict()
b2 = OrderedDict()

# p: Target size for T1 (adaptation parameter). 
# It represents the ideal number of "recent" items to keep.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic:
    Determines whether to evict from T1 (Recent) or T2 (Frequent) based on
    the adaptive parameter 'p'.
    '''
    global t1, t2, b1, b2, p
    
    # Current cache count acts as the working capacity 'c'
    c = len(cache_snapshot.cache)
    if c == 0: return None
    
    candidate_key = None
    
    # Check if the incoming object (which caused the eviction) is in the B2 ghost list.
    # If it is, ARC logic suggests we should prefer evicting T1 to make space, 
    # as we are expanding T2 preference.
    feat_b2 = (obj.key in b2)
    
    # Logic to decide victim:
    # If T1 is larger than target 'p', we evict from T1.
    # The (feat_b2 and len(t1) == int(p)) condition is a specific ARC optimization.
    if t1 and ((len(t1) > p) or (feat_b2 and len(t1) == int(p))):
        # Evict LRU from T1 -> Move key to Ghost B1
        candidate_key, _ = t1.popitem(last=False)
        b1[candidate_key] = True
    elif t2:
        # Evict LRU from T2 -> Move key to Ghost B2
        candidate_key, _ = t2.popitem(last=False)
        b2[candidate_key] = True
    else:
        # Fallback safety (if T2 is empty but logic pointed there, or vice versa)
        if t1:
            candidate_key, _ = t1.popitem(last=False)
            b1[candidate_key] = True
        elif t2:
            candidate_key, _ = t2.popitem(last=False)
            b2[candidate_key] = True

    # Maintain Ghost List Capacities
    # ARC generally limits: len(t1)+len(b1) <= c and len(t2)+len(b2) <= 2c.
    # We apply a safe limit to keep memory usage bounded.
    if len(b1) > c:
        b1.popitem(last=False)
    if len(b2) > c * 2:
        b2.popitem(last=False)
            
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    ARC Hit Logic:
    - If item is in T1, move it to T2 (Recent -> Frequent promotion).
    - If item is in T2, move it to MRU of T2.
    '''
    global t1, t2
    key = obj.key
    
    if key in t2:
        t2.move_to_end(key)
    elif key in t1:
        # Move from T1 to T2
        val = t1.pop(key)
        t2[key] = val

def update_after_insert(cache_snapshot, obj):
    '''
    ARC Insert Logic:
    - Handles "Ghost Hits" (item was recently evicted).
    - Adjusts parameter 'p' to adapt to workload.
    - Inserts new items into T1.
    '''
    global t1, t2, b1, b2, p
    key = obj.key
    c = len(cache_snapshot.cache) # Current capacity
    
    if key in b1:
        # Hit in Ghost Recent (B1) -> We missed it! 
        # This implies T1 was too small. Increase p.
        delta = 1
        if len(b1) >= len(b2):
            delta = 1
        else:
            delta = len(b2) / len(b1)
            
        p = min(c, p + delta)
        
        # Remove from ghost and rescue to T2 (Frequent)
        del b1[key]
        t2[key] = obj
        
    elif key in b2:
        # Hit in Ghost Frequent (B2) -> We missed it!
        # This implies T1 was too big (consuming space T2 needed). Decrease p.
        delta = 1
        if len(b2) >= len(b1):
            delta = 1
        else:
            delta = len(b1) / len(b2)
            
        p = max(0, p - delta)
        
        # Remove from ghost and rescue to T2
        del b2[key]
        t2[key] = obj
        
    else:
        # Totally new item (or not in ghosts)
        # Insert into T1 (Recent)
        if key not in t1 and key not in t2:
            t1[key] = obj

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup Logic:
    Most state management happens in evict() to ensure consistency
    between global lists and the simulator's cache state.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv1gq_6vq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgn6x94f_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdi1s4wmk.pickle

Iteration 5: New subsample score 1.102412 is not better than old score 1.135444, skipping
Iteration 6: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprsxp4l3b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptyquau3z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3nmmc6v9.pickle

Iteration 6: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# S: Small Queue (FIFO) - stores new items (probationary)
# M: Main Queue (FIFO) - stores popular items
# G: Ghost Queue (FIFO) & Set - tracks keys of items recently evicted from S
# freq: Frequency counter (0-3) to track utility
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}

# Byte tracking for accurate sizing
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of cache size allocated to probationary queue
SMALL_QUEUE_RATIO = 0.1
# Cap frequency to prevent old items from staying too long
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic with Byte-Awareness:
    1. If Small Queue (S) exceeds 10% of capacity (in bytes) OR Main Queue (M) is empty:
       - Check tail of S. 
       - If freq > 0: Promote to M (Second Chance).
       - If freq == 0: Evict and add to Ghost.
    2. Else (Evict from M):
       - Check tail of M.
       - If freq > 0: Reinsert to M head and decrement freq.
       - If freq == 0: Evict.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    while candidate_key is None:
        # --- Logic A: Evict from Small Queue (S) ---
        # We prefer evicting from S if it's too big (in bytes) or if M is empty.
        if s_bytes >= s_target_bytes or not m_queue:
            if not s_queue:
                # Failsafe: if both queues are empty (should not happen in full cache)
                break
            
            victim = s_queue[0] # Peek head
            
            # Sanity check: ensure object is in cache map
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_obj = cache_map[victim]
            v_size = v_obj.size
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # PROMOTE: S -> M
                # It has proven utility in S, so we move it to M.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency to 0. It must "earn" its stay in M from scratch.
                # (Standard S3-FIFO approach to preventing pollution)
                freq[victim] = 0
            else:
                # EVICT: Remove from S
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost Queue (Track that it was here)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        # --- Logic B: Evict from Main Queue (M) ---
        else:
            if not m_queue:
                break
                
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_obj = cache_map[victim]
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # REINSERT: Give second chance in M
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency so it eventually evicts if not accessed
                freq[victim] = v_freq - 1
            else:
                # EVICT: Remove from M
                candidate_key = m_queue.popleft()
                m_bytes -= v_obj.size
                # Note: M evictions are typically NOT added to Ghost in S3-FIFO

    # Maintain Ghost Size
    # We keep the number of ghost items roughly equal to the number of cached items
    # to bound memory usage while keeping enough history.
    target_ghost_count = len(cache_map)
    while len(g_set) > target_ghost_count:
        if not g_queue:
            break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Lazy cleanup of Ghost Queue head (remove items already promoted/removed from set)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency. Max cap is 3.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost: It was a mistake to evict it. Promote straight to M.
    2. Else: Insert into S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    freq[k] = 0 # Init frequency
    
    if k in g_set:
        # Ghost Hit! Restore to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
        # Note: k stays in g_queue but will be cleaned up lazily in evict()
    else:
        # New insert -> S
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for evicted object.
    Byte accounting is handled inside evict() because that is where we know
    which queue the object belonged to.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp9noalac.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpql2hmy3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp98y51att.pickle

Iteration 6: New subsample score 0.578449 is better than old score 0.574897. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7bc8gtsc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ilu0qjx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpilawsuhd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpso4m7b9_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7ee9xyk_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps7ttyocs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjfelku5w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb4wow7j2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl9sif7qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp73x93_sp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp0a4ns9p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphmkg5ysx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5gy4dgb9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyj02xb0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyjih45ky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4guuaurd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2_hhk4c4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyswsjtvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuwkzek22.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_tkljdg3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprboes80m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc04ffnky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3or9g7ft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph2yki6m7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr75mbs6e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeg2xki4d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyg85el_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdpldyswd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpumthwzio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplcvv2_zb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiv16nwxe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf8oe0vyq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv5b7yunc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdfsd6dch.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpylbp28cb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsicn7bnl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpemj7qc2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqdzsb3s8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg2mmroch.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdzu4imli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt0c85pml.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbnmwxkq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2u8b5xuk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkkmta63n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5ltrmmth.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjyna4eq2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpadmu95ay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp50j6lz1s.pickle

Iteration 6: Full valset score for new program: 0.2525479166666666
Iteration 6: Full train_val score for new program: 0.2525479166666666
Iteration 6: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269806, 0.34879, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 6: New valset pareto front scores: [0.495749, 0.471749, 0.480395, 0.434516, 0.493127, 0.481552, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.391347, 0.026164, 0.058672, 0.058672, 0.271352, 0.349798, 0.82423, 0.892179, 0.146451, 0.038636, 0.045558, 0.026575, 0.029648, 0.758169, 0.083333, 0.067961, 0.197998, 0.640392, 0.125461, 0.15165, 0.126451, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 6: Full valset pareto front score: 0.2578206041666667
Iteration 6: Updated valset pareto front programs: [{3}, {2}, {3}, {3}, {3, 4}, {3}, {2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}, {0, 1, 2, 3, 4}, {1, 2, 3, 4}, {4}, {2, 3, 4}, {0, 1, 2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {2, 3, 4}, {1, 2, 3, 4}, {2, 3, 4}, {1, 2, 3, 4}, {3}, {1, 2, 3, 4}, {1, 2, 3, 4}, {1, 2, 3, 4}, {3}, {3}, {2}, {3, 4}, {2}, {1, 2, 3, 4}, {1, 2, 3, 4}, {3, 4}, {2}, {2}, {1, 2, 3, 4}, {1, 2, 3, 4}, {2}, {3, 4}, {1, 2, 3, 4}, {2}, {2, 3, 4}, {2}, {1, 2, 3, 4}, {2}, {2}, {2}, {2, 3, 4}, {2}]
Iteration 6: Best valset aggregate score so far: 0.2567872291666667
Iteration 6: Best program as per aggregate score on train_val: 2
Iteration 6: Best program as per aggregate score on valset: 2
Iteration 6: Best score on valset: 0.2567872291666667
Iteration 6: Best score on train_val: 0.2567872291666667
Iteration 6: Linear pareto front program index: 2
Iteration 6: New program candidate index: 4
Iteration 7: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt2ralsj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzhtjik3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4y7vzdlb.pickle

Iteration 7: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# S: Small FIFO queue (probationary items)
# M: Main FIFO queue (popular items)
# G: Ghost FIFO queue (history of recently evicted items)
# freq: Frequency counter to track utility
s_queue = deque()
m_queue = deque()
g_queue = deque() 
g_set = set()     
freq = {}

# Tuning parameters
# 10% of cache size for S is a standard S3-FIFO heuristic that generally works well across workloads.
SMALL_QUEUE_RATIO = 0.1 
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Strategy:
    - Balances eviction between S (probation) and M (main) based on queue size targets.
    - Implements "Reinsertion" logic: If an item in S or M has been accessed, it is reinserted 
      to the back of M (effectively promoting it or giving it a second chance).
    - Ghost cache (G) helps rescue items that were evicted from S too early (scan resistance).
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    # We use cache length as a proxy for capacity management in object-count based scenarios,
    # but the logic holds for size-based if we just needed a key to evict.
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    while candidate_key is None:
        # Lazy Cleanup of Ghost Queue
        # Remove items from the head of G if they are no longer in g_set (meaning they were rescued)
        # or if G has grown too large (larger than the cache itself).
        while g_queue and (g_queue[0] not in g_set or len(g_set) > cache_count):
            old_ghost = g_queue.popleft()
            if old_ghost in g_set:
                g_set.remove(old_ghost)

        # Decision Logic: Which queue to evict from?
        # If M is empty, we have no choice but S.
        # If S is empty, we have no choice but M.
        # If S is larger than its target allocation, we pressure S to evict.
        # Otherwise, we pressure M to evict.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif not s_queue:
            evict_from_s = False
        else:
            evict_from_s = len(s_queue) >= s_target_size
            
        if evict_from_s:
            # --- Processing Small Queue (S) ---
            victim = s_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # HIT in S: Promotion to M
                # The item proved its worth while in probation. Move to M.
                s_queue.popleft()
                m_queue.append(victim)
                # Reset frequency so it has to prove itself again in M context
                freq[victim] = 0 
            else:
                # MISS in S: Eviction
                candidate_key = s_queue.popleft()
                
                # Insert into Ghost to track this eviction
                # If it's accessed again soon, we know S was too small or it's a cyclic pattern.
                if candidate_key not in g_set:
                    g_queue.append(candidate_key)
                    g_set.add(candidate_key)
        else:
            # --- Processing Main Queue (M) ---
            victim = m_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # HIT in M: Second Chance
                # Move to back of M to survive this round.
                # Decrement frequency (aging).
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # MISS in M: Eviction
                # Items evicted from M are "cold" popular items, usually not added to Ghost
                candidate_key = m_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    This is O(1) and does not move items between queues, preserving the "Lazy Promotion" 
    philosophy of S3-FIFO which reduces lock contention/overhead in real systems.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Check if it's a "Ghost Hit" (Rescue).
    2. Otherwise, insert into Small (S).
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # Ghost Hit: The item was recently evicted from S. 
        # This implies our S queue was too small to hold the working set, 
        # or the item has a frequency pattern longer than S.
        # Promote directly to Main (M).
        m_queue.append(key)
        g_set.remove(key)
        # Note: We don't remove from g_queue explicitly here to keep insert O(1). 
        # Lazy cleanup handles it in evict().
    else:
        # Standard Insert: Goes to Small (S) for probation.
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the completely removed object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ss42l8r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm7ssf7m5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphfh_mttu.pickle

Iteration 7: New subsample score 0.22183200000000003 is not better than old score 0.222443, skipping
Iteration 8: Selected program 4 score: 0.2525479166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4tisjswt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmb2n83t0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbw4fhi0u.pickle

Iteration 8: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# Optimizations:
# 1. Enlarged Ghost Queue (2x Cache Count) to better capture "looping" scans (Trace 21).
# 2. Strict Byte-Awareness for S/M ratio management.
# 3. Robust "Frequency" reset/decay logic to balance Recency vs Frequency.

# Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO) - Keys only
g_set = set()          # Ghost Set - Fast lookup
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% for new items allows scans to pass through quickly without evicting valuable main items.
SMALL_QUEUE_RATIO = 0.1
# Frequency cap prevents integer overflow and focuses on "recent" utility.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # We loop until we find a valid victim to evict
    while candidate_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S (prefer keeping M full of good items)
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafe: If decision is S, but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
        # Failsafe: If decision is M, but M is empty, switch to S
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            if not s_queue: break # Should not be reached
            
            victim = s_queue[0] # Peek head
            
            # Lazy Cleanup: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Promote to Main (M)
                # It demonstrated utility while in probation.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency. It enters M as a "new" resident.
                # It must earn its keep in M again (prevents old probationary hits from shielding it forever).
                freq[victim] = 0
            else:
                # No utility seen: Evict
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            if not m_queue: break
            
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsert in M (Second Chance)
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency: this ensures unused items eventually drift to the tail
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                # Items evicted from M usually don't go to Ghost in S3-FIFO (they had their chance)
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # We keep the ghost queue size proportional to the number of items in cache.
    # Increasing this ratio to 2.0 helps catch larger scanning loops/recurrences (Optimizing Trace 21).
    current_item_count = len(cache_map)
    target_ghost_count = current_item_count * 2
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> It's a "Recall" -> Insert to M (Protected).
    2. Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! This item was evicted but returned.
        # This signals a long-term loop or cyclic pattern. Promote straight to Main Queue.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata.
    Queues are managed in evict(), but frequency is a global dict.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdk5nlxkb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx7ibma8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplvqid29o.pickle

Iteration 8: New subsample score 0.927508 is better than old score 0.926849. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl1sw0ncy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb_osft_z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyru3vc55.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgal6s4yw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9k2467sn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2rv3s6b1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6e6hnwp4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfnp51ft2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkygcxtsw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7is_cv0t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv6d223qj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpag_c_xbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptqmiztsl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfue_5835.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppp6nb45_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp9s5h099.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8wj3rn92.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb0ghx821.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppf8wdmof.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa4juyteh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr3vsebq2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbpi4rkyt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpma423o59.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvco1zcvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpby2ck97g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1gkb6s00.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9_b3xhqx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpub7o_ep2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjc42773_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9b7k6u29.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvtidkrk7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe31wk_ck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7vtiwnha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_voqibp_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvb10rbu_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdadv8rso.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcywlhl6l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjk70r430.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpavzswm6i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx_ld2jnn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfwwpa_vo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0we7j1wg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwszks_8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpptrj13y3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvt_bnx53.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8oa0paay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7apc2csx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxi82m_ja.pickle

Iteration 8: New program is on the linear pareto front
Iteration 8: Full valset score for new program: 0.2580827708333333
Iteration 8: Full train_val score for new program: 0.2580827708333333
Iteration 8: Individual valset scores for new program: [0.49731, 0.471632, 0.481486, 0.434694, 0.491255, 0.479822, 0.272727, 0.498624, 0.540937, 0.531017, 0.083333, 0.398757, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.389381, 0.026556, 0.058672, 0.058672, 0.269805, 0.355847, 0.819265, 0.892863, 0.176101, 0.038636, 0.045558, 0.029555, 0.029983, 0.763274, 0.083333, 0.067961, 0.191211, 0.640392, 0.125461, 0.141669, 0.138057, 0.152943, 0.052632, 0.366667, 0.178928, 0.143899, 0.466258, 0.081699]
Iteration 8: New valset pareto front scores: [0.49731, 0.471749, 0.481486, 0.434694, 0.493127, 0.481552, 0.272727, 0.498624, 0.540937, 0.531017, 0.083333, 0.398757, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.391347, 0.026556, 0.058672, 0.058672, 0.271352, 0.355847, 0.82423, 0.892863, 0.176101, 0.038636, 0.045558, 0.029555, 0.029983, 0.763274, 0.083333, 0.067961, 0.197998, 0.640392, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 8: Full valset pareto front score: 0.25936299999999995
Iteration 8: Updated valset pareto front programs: [{5}, {2}, {5}, {5}, {3, 4}, {3}, {2, 3, 4, 5}, {0, 1, 2, 3, 4, 5}, {5}, {0, 1, 2, 3, 4, 5}, {5}, {5}, {2, 3, 4, 5}, {0, 1, 2, 3, 4, 5}, {2, 3, 4, 5}, {5}, {5}, {5}, {5}, {1, 2, 3, 4, 5}, {3}, {5}, {1, 2, 3, 4, 5}, {1, 2, 3, 4, 5}, {3}, {5}, {2}, {5}, {5}, {1, 2, 3, 4, 5}, {1, 2, 3, 4, 5}, {5}, {5}, {5}, {1, 2, 3, 4, 5}, {1, 2, 3, 4, 5}, {2}, {3, 4, 5}, {1, 2, 3, 4, 5}, {2}, {5}, {2}, {1, 2, 3, 4, 5}, {2, 5}, {2}, {2}, {2, 3, 4, 5}, {2}]
Iteration 8: Best valset aggregate score so far: 0.2580827708333333
Iteration 8: Best program as per aggregate score on train_val: 5
Iteration 8: Best program as per aggregate score on valset: 5
Iteration 8: Best score on valset: 0.2580827708333333
Iteration 8: Best score on train_val: 0.2580827708333333
Iteration 8: Linear pareto front program index: 5
Iteration 8: New program candidate index: 5
Iteration 9: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7j6vvc6y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr3mmvtcm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpohzs00xe.pickle

Iteration 9: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# s_queue: Small FIFO queue (deque) for probationary items
# m_queue: Main FIFO queue (deque) for popular items
# g_set: Ghost set (set) for O(1) lookup of recently evicted probationary items
# g_queue: Ghost FIFO queue (deque) to maintain eviction order for the ghost set
# freq: Dictionary mapping object keys to frequency counters (0-3)

s_queue = deque()
m_queue = deque()
g_set = set()
g_queue = deque()
freq = {}

# S3-FIFO Tuning Parameter: 10% of cache size is dedicated to the Small Queue
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction Strategy.
    Uses efficient deques and a proper Ghost FIFO to minimize miss rates.
    '''
    global s_queue, m_queue, g_set, g_queue, freq
    
    # Calculate dynamic threshold based on current number of items
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    # Loop until a valid victim is found
    while candidate_key is None:
        
        # --- Logic A: Evict from Small Queue (S) ---
        # Condition: S is larger than target size OR Main Queue (M) is empty.
        evict_from_s = False
        if len(s_queue) >= s_target_size:
            evict_from_s = True
        elif not m_queue:
            evict_from_s = True
            
        if evict_from_s and s_queue:
            victim = s_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Promote from S to M
                s_queue.popleft()
                m_queue.append(victim)
                # Reset frequency to 0. The item has "graduated" to M, 
                # but must earn its stay there by getting hit again.
                freq[victim] = 0 
            else:
                # Evict from S
                candidate_key = s_queue.popleft()
                
                # Insert into Ghost Cache (track history of S-evictions)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        # --- Logic B: Evict from Main Queue (M) ---
        # Condition: S is within limits and M has items.
        elif m_queue:
            victim = m_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Reinsert at tail of M, decrement frequency
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Evict from M
                candidate_key = m_queue.popleft()
        
        else:
            # Should technically not be reached if cache is full
            break

    # --- Ghost Cache Maintenance ---
    # Lazy Cleanup: Ensure Ghost Cache doesn't grow indefinitely.
    # We use g_queue size as a proxy to keep memory usage bounded.
    # We only remove *one* valid entry per eviction to amortize cost.
    while len(g_queue) > cache_count:
        ghost_victim = g_queue.popleft()
        if ghost_victim in g_set:
            g_set.remove(ghost_victim)
            break
        # If ghost_victim is not in g_set, it means it was already promoted 
        # back to the cache (via hit). We just discard this stale queue entry 
        # and continue the loop to find a real one to remove.

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit: Increment frequency.
    Cap frequency at 3 (2 bits) to prevent cache pollution by "super hot" items.
    S3-FIFO does not reorder queues on hits (lazy promotion), ensuring O(1) performance.
    '''
    global freq
    key = obj.key
    curr_freq = freq.get(key, 0)
    if curr_freq < 3:
        freq[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On New Insert:
    1. If key is in Ghost Cache -> It was a mistake to evict it -> Insert to Main Queue (M).
    2. Otherwise -> Insert to Small Queue (S) for probation.
    '''
    global s_queue, m_queue, g_set, g_queue, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # Ghost Hit: Restore to Main Queue
        m_queue.append(key)
        g_set.remove(key)
        # Note: We leave the key in g_queue. It will be ignored 
        # (treated as stale) during the lazy cleanup in `evict`.
    else:
        # Standard Insert: Start in Small Queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpffmezqro.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0v42xfwb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphdbhgsx8.pickle

Iteration 9: New subsample score 1.189315 is not better than old score 1.189755, skipping
Iteration 10: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgkrc752d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8sqvucsq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4qmxzoom.pickle

Iteration 10: Proposed new text for program: from collections import deque

# Optimized S3-FIFO Implementation
# --------------------------------------------------------
# Key Optimizations:
# 1. Increased Small Queue Ratio (20%): Allows probationary items more time/space 
#    to prove their utility, reducing premature eviction of working sets.
# 2. Ghosting Main Queue Evictions: Items evicted from the Main (M) queue are now 
#    added to the Ghost queue. This improves adaptability for cyclic working sets 
#    that are larger than the cache capacity.
# 3. Enhanced Ghost Queue Sizing: Increased the tracking capacity of the ghost 
#    registry to better capture long-range recurrence patterns.

# Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO) - Keys only
g_set = set()          # Ghost Set - Fast lookup
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 20% reserved for Small queue gives new items a fair window to get a second hit.
SMALL_QUEUE_RATIO = 0.20
# Cap frequency to prevent runaway counts, focusing on recent utility.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines the eviction victim using an improved S3-FIFO strategy.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Iterate until we determine a victim
    while candidate_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is exceeding its target share -> Evict from S.
        # 2. If M is empty -> Must evict from S.
        # Otherwise -> Evict from M (to maintain S's fresh window).
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafes for empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            if not s_queue: break
            
            victim = s_queue[0] # Peek head
            
            # Lazy Cleanup: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: Item proved useful in probation. Move to Main (M).
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency. It must earn its keep in M anew.
                freq[victim] = 0
            else:
                # Eviction: Item failed probation.
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            if not m_queue: break
            
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsertion: Item is still active. Move to tail of M.
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency so unused items eventually drift to eviction.
                freq[victim] = v_freq - 1
            else:
                # Eviction: Item fell out of favor in Main.
                candidate_key = m_queue.popleft()
                m_bytes -= v_size
                
                # Add M-evictions to Ghost too. 
                # This helps recall items in cyclic workloads larger than cache size.
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)

    # --- Ghost Queue Maintenance ---
    # Maintain a ghost history proportional to the number of items.
    # 3x allows tracking longer history for better scan resistance.
    current_item_count = len(cache_map)
    target_ghost_count = current_item_count * 3
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> It's a "Recall" -> Insert to M (Protected).
    2. Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! The item was recently evicted but requested again.
        # Promote directly to Main Queue (M) to protect it.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue (S) for probation.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphdb8e10l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpve212q10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp37jqhu8k.pickle

Iteration 10: New subsample score 0.153874 is not better than old score 0.217916, skipping
Iteration 11: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdym9_8sb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1ov537bn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_u9bmxa2.pickle

Iteration 11: Proposed new text for program: from collections import deque

# Improved S3-FIFO Algorithm
# --------------------------------------------------------
# Optimizations:
# 1. Multi-bit frequency (0-3) to rank utility.
# 2. Preserves frequency "momentum" when moving S -> M (critical for hit rate).
# 3. Strict byte-aware queue management.
# 4. Count-based Ghost Queue sizing.

# Global Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO) - stores keys
m_queue = deque()      # Main/Protected Queue (FIFO) - stores keys
g_queue = deque()      # Ghost Queue (FIFO) - stores keys
g_set = set()          # Ghost Set - Fast lookup for keys in g_queue
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Parameters
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic with frequency aging.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target = capacity * SMALL_QUEUE_RATIO
    
    victim = None
    
    # Loop until we find a victim to evict from the cache
    while victim is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M
        
        evict_from_s = False
        if s_bytes >= s_target or not m_queue:
            evict_from_s = True
        
        # Failsafe checks
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            # Should not happen if cache is full
            break
            
        if evict_from_s:
            # --- Small Queue (S) Logic ---
            k = s_queue[0]
            
            # Lazy cleanup: If item was removed externally
            if k not in cache_map:
                s_queue.popleft()
                continue
                
            v_size = cache_map[k].size
            v_freq = freq.get(k, 0)
            
            if v_freq > 0:
                # HIT in S: Promote to Main (M)
                # Optimization: We DO NOT reset freq to 0. We keep the accumulated
                # frequency (momentum). This gives popular items a buffer in M.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(k)
                m_bytes += v_size
            else:
                # MISS in S (Probation failed): Evict
                s_queue.popleft()
                s_bytes -= v_size
                victim = k
                
                # Add to Ghost Queue (record of failed probation)
                if k not in g_set:
                    g_set.add(k)
                    g_queue.append(k)
        
        else:
            # --- Main Queue (M) Logic ---
            k = m_queue[0]
            
            if k not in cache_map:
                m_queue.popleft()
                continue
                
            v_size = cache_map[k].size
            v_freq = freq.get(k, 0)
            
            if v_freq > 0:
                # HIT in M: Reinsert at tail (Second Chance)
                # Decay frequency: Treats frequency as "survival lives" in M.
                m_queue.popleft()
                m_queue.append(k)
                freq[k] = v_freq - 1
            else:
                # Unused in M: Evict
                # M evictions typically don't go to Ghost (they had their chance)
                m_queue.popleft()
                m_bytes -= v_size
                victim = k

    # --- Ghost Queue Maintenance ---
    # Keep ghost size proportional to the number of items in cache.
    target_ghost = len(cache_map)
    # Ensure a minimum buffer
    if target_ghost < 10: target_ghost = 10
    
    while len(g_set) > target_ghost:
        if not g_queue: break
        old = g_queue.popleft()
        if old in g_set:
            g_set.remove(old)
            
    # Clean head of ghost queue
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - If in Ghost: Restore to M (Recall).
    - Else: Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency at 0. It must earn hits to survive S.
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This is a "Recall". Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert to Small Queue.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    Queues are managed inside evict(), so we only clean `freq`.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsqjnwo4y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjldxji9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw46witld.pickle

Iteration 11: New subsample score 0.456384 is better than old score 0.455262. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3xt5yg4t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcjjw_u_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3xkwcxcb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiwy_l4ey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqgqqu27c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfdwqlx95.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvva9jwe4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzy1p4o9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpme8xrki0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgub0jr7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5dm4pkyb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_wmakdem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp28r_arzf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp429__kic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl3himgyv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxlb_bxj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkn0w8pzy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpar6vd6qp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp76ar6_ju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptd2vrqo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe8e5kbgc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprzqjpdsj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprf8gzciz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8s6uy0di.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3vmvodg4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0nv6weh9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo5emc03x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr795c_s5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgmepkuxr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplowracjp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbt3l8tzx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphksemrt1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpced6tbiv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6j85kalb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr35fq_in.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzsaaxy2l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpysyj5lbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo8zzbf43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuyzukj87.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr2fjom_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk055ilov.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxu5gd10o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptg3y4gtq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi4um62ff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa0odv5dh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo8k2_6rd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps_6pwelh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvhx_0swd.pickle

Iteration 11: Full valset score for new program: 0.2520725208333333
Iteration 11: Full train_val score for new program: 0.2520725208333333
Iteration 11: Individual valset scores for new program: [0.494242, 0.469706, 0.478698, 0.434249, 0.492111, 0.478958, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.386767, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.382498, 0.026164, 0.058672, 0.058672, 0.269829, 0.332661, 0.806356, 0.892179, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.150263, 0.128283, 0.165611, 0.052632, 0.366667, 0.175421, 0.145021, 0.466258, 0.081699]
Iteration 11: New valset pareto front scores: [0.49731, 0.471749, 0.481486, 0.434694, 0.493127, 0.481552, 0.272727, 0.498624, 0.540937, 0.531017, 0.083333, 0.398757, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.391347, 0.026556, 0.058672, 0.058672, 0.271352, 0.355847, 0.82423, 0.892863, 0.176101, 0.038636, 0.045558, 0.029555, 0.029983, 0.763274, 0.083333, 0.067961, 0.197998, 0.640392, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.084967]
Iteration 11: Full valset pareto front score: 0.25936299999999995
Iteration 11: Updated valset pareto front programs: [{5}, {2}, {5}, {5}, {3, 4}, {3}, {2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6}, {5}, {0, 1, 2, 3, 4, 5, 6}, {5}, {5}, {2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6}, {2, 3, 4, 5, 6}, {5}, {5}, {5}, {5}, {1, 2, 3, 4, 5, 6}, {3}, {5}, {1, 2, 3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}, {3}, {5}, {2}, {5}, {5}, {1, 2, 3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}, {5}, {5}, {5}, {1, 2, 3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}, {2}, {3, 4, 5, 6}, {1, 2, 3, 4, 5, 6}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6}, {2, 5, 6}, {2}, {2}, {2, 3, 4, 5, 6}, {2}]
Iteration 11: Best valset aggregate score so far: 0.2580827708333333
Iteration 11: Best program as per aggregate score on train_val: 5
Iteration 11: Best program as per aggregate score on valset: 5
Iteration 11: Best score on valset: 0.2580827708333333
Iteration 11: Best score on train_val: 0.2580827708333333
Iteration 11: Linear pareto front program index: 5
Iteration 11: New program candidate index: 6
Iteration 12: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7s02g_oz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdq80ly3r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwgrcnmkt.pickle

Iteration 12: Proposed new text for program: from collections import OrderedDict

# --- ARC (Adaptive Replacement Cache) Global State ---
# T1: Recent Cache (LRU list of items accessed once). 
#     Maps key -> object. Represents "Recency".
# T2: Frequent Cache (LRU list of items accessed >= twice). 
#     Maps key -> object. Represents "Frequency".
# B1: Ghost Recent (LRU list of keys evicted from T1). 
#     Maps key -> None. Used to tune 'p'.
# B2: Ghost Frequent (LRU list of keys evicted from T2). 
#     Maps key -> None. Used to tune 'p'.
# p:  Target size for T1 (Recency). Adapts dynamically between 0 and Capacity.

t1 = OrderedDict()
t2 = OrderedDict()
b1 = OrderedDict()
b2 = OrderedDict()
p = 0.0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Strategy:
    - Checks if the incoming item (obj) is in a Ghost list (B1 or B2) to adapt 'p'.
    - 'p' represents the ideal size of the T1 (Recency) list.
    - Decides whether to evict from T1 or T2 based on the current size of T1 vs 'p'.
    '''
    global t1, t2, b1, b2, p
    
    capacity = cache_snapshot.capacity
    key = obj.key
    
    # --- 1. Adaptation: Adjust 'p' based on Ghost Hits ---
    # If we hit a ghost, it means we evicted something we shouldn't have.
    # We expand the region (T1 or T2) that the ghost came from.
    if key in b1:
        # Hit in B1 (Ghost Recent) implies T1 was too small. Increase p.
        delta = 1
        if len(b1) >= len(b2) and len(b2) > 0:
            delta = 1
        elif len(b2) > len(b1):
            delta = len(b2) / len(b1)
        p = min(float(capacity), p + delta)
        
    elif key in b2:
        # Hit in B2 (Ghost Frequent) implies T2 was too small. Decrease p (shrink T1).
        delta = 1
        if len(b2) >= len(b1) and len(b1) > 0:
            delta = 1
        elif len(b1) > len(b2):
            delta = len(b1) / len(b2)
        p = max(0.0, p - delta)

    # --- 2. Replacement: Choose Victim ---
    # We must identify which resident object (from T1 or T2) to evict.
    candidate_key = None
    evict_from_t1 = False
    
    # ARC "Replace" Logic:
    # We prefer evicting from T1 if T1 is larger than its target 'p'.
    # There is a special condition: if the incoming item is in B2, we are even stricter 
    # with T1 (we treat T1 as if it needs to be smaller) to make room for T2 growth.
    if len(t1) > 0:
        if len(t1) > p:
            evict_from_t1 = True
        elif (key in b2) and (len(t1) == int(p)):
            evict_from_t1 = True
    
    # Fallback safety: If T1 is empty, we must evict T2.
    if len(t1) == 0:
        evict_from_t1 = False
    
    if evict_from_t1:
        # Evict LRU from T1 -> Move key to B1
        candidate_key, _ = t1.popitem(last=False)
        b1[candidate_key] = None
    else:
        # Evict LRU from T2 -> Move key to B2
        candidate_key, _ = t2.popitem(last=False)
        b2[candidate_key] = None

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - If in T1 (Recency), move to T2 (Frequency) as it has been seen twice.
    - If in T2 (Frequency), move to MRU of T2.
    '''
    global t1, t2
    key = obj.key
    
    if key in t1:
        # Promotion: Recency -> Frequency
        val = t1.pop(key)
        t2[key] = val
    elif key in t2:
        # Maintenance: Update MRU position in Frequency
        t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - If the item was in a Ghost list (B1/B2), it is "rescued" to T2 (Frequency).
    - If the item is completely new, it starts in T1 (Recency).
    - Perform maintenance on Ghost lists to prevent metadata explosion.
    '''
    global t1, t2, b1, b2
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # Insert Logic
    if key in b1:
        # Rescued from Ghost Recent -> Promote to Frequency
        del b1[key]
        t2[key] = obj
    elif key in b2:
        # Rescued from Ghost Frequent -> Restore to Frequency
        del b2[key]
        t2[key] = obj
    else:
        # New Item -> Add to Recency
        t1[key] = obj
        
    # Ghost Management (Lazy Cleanup)
    # Ensure ghost lists don't grow indefinitely.
    # Rule: len(T1) + len(B1) <= Capacity
    while (len(t1) + len(b1)) > capacity:
        if not b1: break
        b1.popitem(last=False) # Remove LRU from B1
        
    # Rule: len(T2) + len(B2) <= 2 * Capacity
    while (len(t2) + len(b2)) > (2 * capacity):
        if not b2: break
        b2.popitem(last=False) # Remove LRU from B2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Post-eviction cleanup.
    The critical logic of moving the victim to Ghost lists happens in `evict`
    to ensure state consistency before the insert happens.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzg5u1qq5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfew78wy9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr_h9zddr.pickle

Iteration 12: New subsample score 0.628538 is not better than old score 0.630466, skipping
Iteration 13: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjdwtnwc5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp459mfu__.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8xqb1zmb.pickle

Iteration 13: Proposed new text for program: from collections import deque

# S3-FIFO Implementation with Enhanced Robustness
# --------------------------------------------------------

# Global State Management
# We maintain our own queue structures. The `cache_snapshot` is the source of truth for object data.
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO history of evicted S items)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency counters (0-3)

# Byte Tracking for precise partition management
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# Increased S-queue ratio to 20% to allow new items slightly more time 
# to demonstrate utility, preventing thrashing in small caches.
SMALL_QUEUE_RATIO = 0.2

# Frequency cap (2-bit counter approximation)
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict using S3-FIFO logic.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue based on current capacity
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate = None
    
    # Iterate until a victim is chosen
    while candidate is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S exceeds its target ratio, we prefer evicting from S.
        # 2. If M is empty, we must evict from S.
        evict_from_s = (s_bytes >= s_target_bytes) or (not m_queue)
        
        # Hard constraints to prevent errors on empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False # Force M
        if not evict_from_s and not m_queue:
            evict_from_s = True  # Force S
            
        if evict_from_s:
            # --- Processing Small Queue (S) ---
            if not s_queue: break # Should not be reached due to logic above
                
            key = s_queue[0] # Peek head
            
            # Consistency check: if item was deleted externally (not by us)
            if key not in cache:
                s_queue.popleft()
                continue
                
            f = freq.get(key, 0)
            obj_ref = cache[key]
            
            if f > 0:
                # Promotion: S -> M
                # The item has been accessed while in probation. Move to Main.
                s_queue.popleft()
                s_bytes -= obj_ref.size
                
                m_queue.append(key)
                m_bytes += obj_ref.size
                
                # Reset frequency on promotion. It enters M as a new resident
                # and must earn its keep again.
                freq[key] = 0
            else:
                # Eviction: S -> Ghost
                # No hits in probation. Evict.
                candidate = s_queue.popleft()
                s_bytes -= obj_ref.size
                
                # Add to Ghost (History) to catch it if it returns soon
                if candidate not in g_set:
                    g_set.add(candidate)
                    g_queue.append(candidate)
        else:
            # --- Processing Main Queue (M) ---
            if not m_queue: break
            
            key = m_queue[0] # Peek head
            
            if key not in cache:
                m_queue.popleft()
                continue
                
            f = freq.get(key, 0)
            obj_ref = cache[key]
            
            if f > 0:
                # Reinsertion: M head -> M tail
                # It was accessed while in M. Give it another cycle.
                m_queue.popleft()
                m_queue.append(key)
                freq[key] = f - 1 # Decay frequency
            else:
                # Eviction: M -> Out
                # It traveled through M without enough hits. Evict.
                candidate = m_queue.popleft()
                m_bytes -= obj_ref.size
                # Evictions from M typically don't go to Ghost
    
    # --- Ghost Queue Maintenance ---
    # We maintain a history of evicted items.
    # CRITICAL FIX: Ensure a minimum floor (100) for ghost size.
    # In small caches, 'count * 2' might be too small to capture loops.
    curr_count = len(cache)
    target_ghost = max(curr_count * 2, 100)
    
    while len(g_set) > target_ghost:
        if not g_queue: break
        rem = g_queue.popleft()
        if rem in g_set:
            g_set.remove(rem)

    # Lazy cleanup of stale ghost heads
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on cache hit.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    - If in Ghost -> Insert to Main (M).
    - Else -> Insert to Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This is a "recall". It was useful recently.
        # Promote directly to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Start in S (Probation).
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyve6g2vs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbol5ociy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu4sg1o43.pickle

Iteration 13: New subsample score 0.6723349999999999 is not better than old score 0.8193199999999999, skipping
Iteration 14: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0dl8sn0p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5ai4t1h0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_wfysqay.pickle

Iteration 14: Proposed new text for program: from collections import deque

# Improved S3-FIFO with Strict Byte-Partitioning and Aggressive Aging
# -------------------------------------------------------------------
# Strategy:
# 1. S-Queue (Small/Probation): FIFO. Cap at ~10% of cache bytes.
#    Filters out "one-hit wonders" (scans).
# 2. M-Queue (Main/Protected): FIFO. Cap at ~90% of cache bytes.
#    Holds hot items.
# 3. G-Queue (Ghost): Tracks history of evicted probationary items.
#    Used to rescue items that loop just outside the S-Queue duration.

# Global Data Structures
s_queue = deque()      # Small/Probationary FIFO
m_queue = deque()      # Main/Protected FIFO
g_queue = deque()      # Ghost FIFO (keys only)
g_set = set()          # Ghost Set (fast lookup)
freq = {}              # Frequency counter
s_bytes = 0            # Track bytes in Small Queue
m_bytes = 0            # Track bytes in Main Queue

# Tuning Parameters
SMALL_RATIO = 0.1      # Target 10% of capacity for probation
MAX_FREQ = 2           # Cap frequency (0=New, 1=Hit, 2+=Hot)
GHOST_RATIO = 3.0      # Track 3x item count in ghost to catch long loops

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    target_s_bytes = capacity * SMALL_RATIO
    
    victim_key = None
    
    # We must find a victim. Loop until one is chosen.
    while victim_key is None:
        
        # --- Decision: Clean S or Clean M? ---
        # 1. If S exceeds its byte target -> Evict/Promote from S.
        # 2. If M is empty -> Must Evict/Promote from S.
        # 3. Otherwise -> Evict/Reinsert from M.
        
        clean_s = False
        if s_bytes >= target_s_bytes or not m_queue:
            clean_s = True
            
        # Failsafe: If decision is S but S is empty, switch to M
        if clean_s and not s_queue:
            clean_s = False
        # Failsafe: If decision is M but M is empty, switch to S
        if not clean_s and not m_queue:
            clean_s = True
            
        if clean_s:
            # --- Processing Small Queue (S) ---
            if not s_queue: break # Should be unreachable
            
            curr_key = s_queue[0] # Peek
            
            # Lazy cleanup check
            if curr_key not in cache_map:
                s_queue.popleft()
                continue
            
            obj_size = cache_map[curr_key].size
            curr_freq = freq.get(curr_key, 0)
            
            if curr_freq > 0:
                # Hit in Probation -> Promote to Main (M)
                s_queue.popleft()
                s_bytes -= obj_size
                
                m_queue.append(curr_key)
                m_bytes += obj_size
                
                # Reset frequency. It enters M as a new resident.
                # It must get NEW hits to survive the next M-cycle.
                freq[curr_key] = 0
            else:
                # No hits in Probation -> Evict
                victim_key = s_queue.popleft()
                s_bytes -= obj_size
                
                # Add to Ghost (History of eviction)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)

        else:
            # --- Processing Main Queue (M) ---
            if not m_queue: break
            
            curr_key = m_queue[0] # Peek
            
            # Lazy cleanup check
            if curr_key not in cache_map:
                m_queue.popleft()
                continue
                
            obj_size = cache_map[curr_key].size
            curr_freq = freq.get(curr_key, 0)
            
            if curr_freq > 0:
                # Hit in Main -> Keep (Second Chance)
                # Move from Head to Tail
                m_queue.popleft()
                m_queue.append(curr_key)
                
                # CRITICAL: Reset frequency to 0.
                # This ensures O(1) aging. Objects must be accessed *during*
                # their traversal of M to survive again.
                freq[curr_key] = 0
            else:
                # Cold in Main -> Evict
                # M-evictions typically don't go to Ghost (they had their chance)
                victim_key = m_queue.popleft()
                m_bytes -= obj_size

    # --- Ghost Queue Maintenance ---
    # Bound ghost queue by count (scaled to current item count)
    current_items = len(cache_map)
    target_ghost = current_items * GHOST_RATIO
    
    while len(g_set) > target_ghost:
        if not g_queue: break
        rem = g_queue.popleft()
        if rem in g_set:
            g_set.remove(rem)
            
    # Cleanup Ghost head if not in set
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    Does not move items immediately (lazy promotion).
    '''
    global freq
    k = obj.key
    f = freq.get(k, 0)
    if f < MAX_FREQ:
        freq[k] = f + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Decide placement based on Ghost history.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was evicted recently but returned.
        # This signals a cyclic pattern or larger working set. Promote to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    Queues are managed in evict(), we just clean the frequency map here.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp20cuc4dm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5c94p8ca.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpczd5w0qt.pickle

Iteration 14: New subsample score 0.376457 is not better than old score 0.376457, skipping
Iteration 15: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppbg_9pk8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprjwgsrls.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmlmt57zp.pickle

Iteration 15: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# We use global variables to maintain state across function calls
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters for resident objects

# Byte tracking for the queues
s_bytes = 0
m_bytes = 0

# Constants
# 10% of capacity for Small queue is optimal for scan resistance (SOSP'23)
SMALL_RATIO = 0.1  
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    # Loop until we find a victim
    while victim_key is None:
        
        # Decision: Evict from Small (S) or Main (M)?
        # Evict from S if it's larger than the target ratio OR if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If we decided S but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue ---
            if not s_queue: break # Should not happen if cache is full
            
            candidate = s_queue[0]
            
            # Defense against stale keys
            if candidate not in cache:
                s_queue.popleft()
                continue
            
            # Check utility
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: Item was hit in S -> Move to M
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                
                # Reset frequency upon entering M. It must earn its stay in M.
                freq[candidate] = 0
            else:
                # Eviction: Item was not hit in S -> Evict
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Record in Ghost Queue (signal of recent eviction)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        
        else:
            # --- Eviction from Main Queue ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail of M and decay frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Item exhausted its frequency in M -> Evict
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Note: S3-FIFO typically does not add M-evicted items to Ghost
    
    # --- Ghost Queue Maintenance ---
    # We allow the ghost queue to grow larger (4x item count) to catch longer loops
    # typical in block traces (Trace 31/32).
    current_item_count = len(cache)
    target_ghost_len = current_item_count * 4
    
    while len(g_set) > target_ghost_len and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Cleanup ghost head if needed
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - If in Ghost: Restore to Main (Recall).
    - Else: Insert into Small (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This item was recently evicted from S. 
        # It has proven it is part of a loop/working set -> Promote to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert into Small Queue.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd4cwhquo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvpgwp4bl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprps0jjnj.pickle

Iteration 15: New subsample score 0.459539 is better than old score 0.415385. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl4q1nswj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_9b375xp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpewqv65og.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpumgonltm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmv0vwg5j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn4ijcgqf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpne4nvjbr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp99nja50a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwl99u34b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptw2hssgi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp698_xd_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplbty22c5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpouixw8_w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuxokm5s2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5irr6ehb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpccuqv0wh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm6eg5q7w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj7q1pkfx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpup3lrjn3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9gkdmjou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpix4hqet5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2dxt2trd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp0tgyx2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv48l7jax.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb4kmryjw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcv2g4l1z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp85cpr57r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiuc2x1xz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpww1ulby0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk9burnnw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppadw1oq9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmpj3hsjj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1m5na2er.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8hnf9aed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpervofb5o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8lkjwi7y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqtyqzkne.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp07d_hdvk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4vp5o39h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbfz1ey2p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz62ol23a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpst1_e_rn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3mv9s0xu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp75esmfnm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps7i_yhao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph9euwv_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp79ve2002.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6jiree6z.pickle

Iteration 15: Full valset score for new program: 0.2456767708333333
Iteration 15: Full train_val score for new program: 0.2456767708333333
Iteration 15: Individual valset scores for new program: [0.49478, 0.468947, 0.479729, 0.43087, 0.488367, 0.478439, 0.271531, 0.498624, 0.540937, 0.531017, 0.083333, 0.367229, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.2698, 0.358871, 0.819265, 0.892692, 0.040431, 0.038636, 0.045558, 0.050504, 0.050164, 0.758679, 0.083333, 0.09444, 0.064006, 0.641937, 0.125461, 0.069864, 0.109346, 0.06195, 0.052632, 0.366667, 0.0632, 0.082468, 0.466258, 0.094771]
Iteration 15: New valset pareto front scores: [0.49731, 0.471749, 0.481486, 0.434694, 0.493127, 0.481552, 0.272727, 0.498624, 0.540937, 0.531017, 0.083333, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.358871, 0.82423, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 15: Full valset pareto front score: 0.26110014583333324
Iteration 15: Updated valset pareto front programs: [{5}, {2}, {5}, {5}, {3, 4}, {3}, {2, 3, 4, 5, 6}, {0, 1, 2, 3, 4, 5, 6, 7}, {5, 7}, {0, 1, 2, 3, 4, 5, 6, 7}, {5, 7}, {5}, {2, 3, 4, 5, 6, 7}, {0, 1, 2, 3, 4, 5, 6, 7}, {7}, {7}, {7}, {5, 7}, {5, 7}, {1, 2, 3, 4, 5, 6, 7}, {7}, {5, 7}, {1, 2, 3, 4, 5, 6, 7}, {1, 2, 3, 4, 5, 6, 7}, {3}, {7}, {2}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7}, {1, 2, 3, 4, 5, 6, 7}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7}, {7}]
Iteration 15: Best valset aggregate score so far: 0.2580827708333333
Iteration 15: Best program as per aggregate score on train_val: 5
Iteration 15: Best program as per aggregate score on valset: 5
Iteration 15: Best score on valset: 0.2580827708333333
Iteration 15: Best score on train_val: 0.2580827708333333
Iteration 15: Linear pareto front program index: 5
Iteration 15: New program candidate index: 7
Iteration 16: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph9177wt9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgvs4r4hf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpilax27ta.pickle

Iteration 16: Proposed new text for program: from collections import deque

# --- Global State ---
# Small Queue (S): FIFO for new insertions (Probation)
s_queue = deque()
# Main Queue (M): FIFO for hot items (Protected)
m_queue = deque()
# Ghost Queue (G): FIFO of keys evicted from S (Recall signal)
g_queue = deque()
# Fast lookup sets
g_set = set()
# Frequency map: Tracks hits. 0 = inserted, 1-3 = hit levels.
freq = {}

# Byte tracking
s_bytes = 0
m_bytes = 0

# --- Constants & Tuning ---
# Target size for Small queue (10% is standard for S3-FIFO scan resistance)
SMALL_RATIO = 0.1
# Max frequency cap (2-bit counter behavior)
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic with byte-size awareness.
    Selection Strategy:
    1. Prefer evicting from Small (S) if S is larger than target ratio.
    2. If S item has been hit (freq > 0), promote to Main (M).
    3. If S item has not been hit, evict it and add to Ghost (G).
    4. If S is within limits, look at Main (M).
    5. If M item has hits remaining, reinsert at tail (Second Chance) and decrement freq.
    6. If M item has no hits remaining, evict.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None

    while victim_key is None:
        # Determine which queue to operate on
        # We process S if it's "too big" OR if M is empty.
        # Otherwise, we process M to find a victim or age items.
        process_s = False
        
        if s_bytes >= target_s_size or not m_queue:
            process_s = True
        
        # Failsafe: If logic says S but S is empty, forced to use M
        if process_s and not s_queue:
            process_s = False
            
        if process_s:
            # --- Small Queue Processing ---
            if not s_queue: break # Should be impossible in full cache
            
            candidate = s_queue[0] # Peek
            
            # Lazy removal for stale keys
            if candidate not in cache:
                s_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: Hit in S -> Move to M
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                
                # Unlike standard FIFO which resets freq, we allow carry-over 
                # but capped at 1 to prevent a new M item from being immortal immediately.
                # This helps differentiate "just hit once" from "hit many times".
                freq[candidate] = 1 
            else:
                # Eviction: No hits in S -> Evict
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost (only if evicted from S without hits)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Main Queue Processing ---
            if not m_queue: break # Should be impossible
            
            candidate = m_queue[0]
            
            # Lazy removal
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Aging: Give second chance, reinsert at tail, decrement freq
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Cold in M -> Evict
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Items evicted from M usually don't go to Ghost in S3-FIFO

    # --- Ghost Queue Maintenance ---
    # We maintain a ghost queue size proportional to the number of items in cache.
    # While exact byte tracking for ghost is hard (objects are gone), count is a good proxy.
    # We use a larger window (up to 2x-4x count) to catch cyclic patterns.
    max_ghost_items = len(cache) * 2 
    
    while len(g_set) > max_ghost_items and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Clean up head of queue if it desynced (stale entries)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency.
    '''
    global freq
    k = obj.key
    # Min(freq + 1, MAX_FREQ)
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion.
    Check Ghost queue to decide between S (Probation) and M (Main).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S. 
        # This signals a false negative eviction or a loop.
        # Direct promotion to Main (M).
        m_queue.append(k)
        m_bytes += size
        
        # Remove from Ghost
        g_set.remove(k)
    else:
        # Standard Insert: Insert into Small (S).
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp36u8o9y5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj_ha2y7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps3d4gr4m.pickle

Iteration 16: New subsample score 1.304402 is not better than old score 1.319191, skipping
Iteration 17: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnczbw87g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfeo636n2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgj67e7bk.pickle

Iteration 17: Proposed new text for program: from collections import deque, OrderedDict

# Global Data Structures
# S: Small FIFO queue (Probationary) - Stores new items.
# M: Main FIFO queue (Protected) - Stores popular items.
# G: Ghost FIFO (History) - Stores keys evicted from S to detect false-positive evictions.
s_queue = deque()
m_queue = deque()
g_ghost = OrderedDict() # Used as an LRU set: keys are items, FIFO order maintained.

# Frequency metadata: Maps key -> integer (0 to MAX_FREQ)
freq = {}

# Tuning Parameters
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict using S3-FIFO logic.
    S3-FIFO separates new items (S) from popular items (M).
    - If S is larger than 10% of cache, we evict from S.
      - If item in S has been accessed (freq > 0), promote to M.
      - Else, evict and record in Ghost.
    - If S is small, we evict from M.
      - If item in M has been accessed (freq > 0), give second chance (reinsert at tail).
      - Else, evict (without Ghost).
    '''
    global s_queue, m_queue, g_ghost, freq
    
    cache_size = len(cache_snapshot.cache)
    # Target size for the Small queue (approx 10% of cache)
    s_target = max(1, int(cache_size * SMALL_QUEUE_RATIO))
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to operate on
        # If M is empty, we must operate on S.
        # If S is oversize (>10%), we prefer evicting/promoting from S.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif len(s_queue) > s_target:
            evict_from_s = True
        else:
            evict_from_s = False
            
        if evict_from_s:
            if not s_queue:
                # Should typically not happen if logic is correct and cache is full
                if m_queue:
                    evict_from_s = False
                else:
                    return None
            else:
                candidate = s_queue.popleft()
                candidate_freq = freq.get(candidate, 0)
                
                if candidate_freq > 0:
                    # PROMOTION: S -> M
                    # Item was hit while in probation. Move to Main.
                    m_queue.append(candidate)
                    # Reset frequency so it must prove itself again in M to stay
                    freq[candidate] = 0 
                else:
                    # EVICTION from S
                    victim_key = candidate
                    
                    # Insert into Ghost (track history of S-evictions)
                    # Use None as value, key is what matters
                    g_ghost[candidate] = None
                    # Trim Ghost to match cache size (prevent unbounded growth)
                    if len(g_ghost) > cache_size:
                        g_ghost.popitem(last=False) # Remove oldest (FIFO)
        
        if not evict_from_s:
            # Operate on Main Queue
            candidate = m_queue.popleft()
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # SECOND CHANCE
                # Item was hit in M. Reinsert at back, decrement frequency.
                m_queue.append(candidate)
                freq[candidate] = candidate_freq - 1
            else:
                # EVICTION from M
                # Items evicted from M generally don't go to Ghost (they had their chance)
                victim_key = candidate

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On cache hit, increment frequency counter.
    Capped at MAX_FREQ to prevent frequency counter pollution.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On new insertion:
    1. Check Ghost. If present, it's a "rescue" -> Insert to M.
    2. Else, it's new -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_ghost, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_ghost:
        # GHOST HIT: This item was recently evicted from S but is needed again.
        # This implies it belongs in the active working set (M), not S.
        m_queue.append(key)
        del g_ghost[key] # Remove from ghost
    else:
        # NEW ITEM: Start in probationary queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata after actual eviction occurs.
    The queues were already updated in `evict`, just clean `freq`.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkcamu0m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_wqlzp50.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcnhs3nk6.pickle

Iteration 17: New subsample score 0.7254700000000001 is not better than old score 0.727105, skipping
Iteration 18: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq51j0g72.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwe2xnt7c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp907ylf85.pickle

Iteration 18: Proposed new text for program: from collections import deque

# Improved S3-FIFO Algorithm
# --------------------------------------------------------
# This implementation optimizes the S3-FIFO strategy by adding frequency
# retention during promotion and strict byte-size management for the 
# probationary queue.

# Global Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO) - Keys only
g_set = set()          # Ghost Set - Fast lookup
freq = {}              # Frequency counter for resident objects

# Byte Tracking to manage variable-sized objects
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of capacity for Small Queue allows filtering of one-hit wonders (scans).
SMALL_QUEUE_RATIO = 0.1
# Max frequency cap. A small counter (up to 3) is sufficient to distinguish 
# scan vs. hot items.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Eviction logic using S3-FIFO strategy.
    Prioritizes evicting from Small queue if it exceeds target ratio.
    Promotes useful items from Small to Main, retaining their frequency.
    Manages Main queue as a segmented LRU via reinsertion.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    while candidate_key is None:
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S (filter scans)
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M (manage working set)
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafe: If decision is S, but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
        # Failsafe: If decision is M, but M is empty, switch to S
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        # If both are empty (should not happen in a full cache scenario), return None
        if not s_queue and not m_queue:
            return None

        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            victim = s_queue[0] # Peek head
            
            # Consistency check: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                # We cannot reliably update s_bytes if item is gone, just continue
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: S -> M
                # The item was hit while in probation. Move to Main.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # OPTIMIZATION: Retain frequency.
                # Standard S3-FIFO often resets frequency here. By keeping it,
                # we give the item a stronger "second chance" in M based on its
                # performance in S.
            else:
                # Eviction: S -> Ghost
                # No hits in S implies it might be scan traffic.
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsert in M (Second Chance)
                # It was accessed while in M, so we keep it.
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency: Ensure items eventually age out if not accessed
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                # Items evicted from M usually don't go to Ghost (they had their chance)
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # Keep ghost size proportional to cache count to detect looping patterns.
    # 2x is a heuristic that works well for various traces.
    current_item_count = len(cache_map)
    target_ghost_count = max(current_item_count * 2, 10)
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> It's a "Recall" -> Insert to M (Protected).
    2. Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0. It must prove utility via hits.
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! This item was evicted recently.
        # Promote straight to Main Queue.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpccxmhiqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpampdh31e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv4ap57tm.pickle

Iteration 18: New subsample score 1.073632 is not better than old score 1.081223, skipping
Iteration 19: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl0_39rcb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp13u1akb_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp73e9vd9e.pickle

Iteration 19: Proposed new text for program: from collections import deque

# ARC (Adaptive Replacement Cache) Data Structures
# We use global variables to maintain state across function calls

# T1: Recent Cache (L1) - contains items accessed once recently
t1_queue = deque()
t1_set = set()

# T2: Frequent Cache (L2) - contains items accessed at least twice
t2_queue = deque()
t2_set = set()

# B1: Ghost Recent (L1 Ghost) - history of evicted T1 keys
b1_queue = deque()
b1_set = set()

# B2: Ghost Frequent (L2 Ghost) - history of evicted T2 keys
b2_queue = deque()
b2_set = set()

# Adaptation Parameter
# p represents the target size for T1. 0 <= p <= capacity.
p = 0
current_cache_size = 0  # Track total bytes currently in T1 + T2

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using ARC logic.
    ARC decides whether to evict from T1 or T2 based on the parameter p.
    '''
    global t1_queue, t1_set, t2_queue, t2_set
    global b1_queue, b1_set, b2_queue, b2_set
    global p, current_cache_size

    cache = cache_snapshot.cache
    # Since ARC is page-count based but this environment is byte-sized,
    # we approximate "size" logic by tracking bytes in T1 vs T2.
    
    # Calculate current size of T1
    t1_size = 0
    for k in t1_queue:
        if k in cache: t1_size += cache[k].size
    
    victim_key = None
    
    # Standard ARC "replace" logic modified for byte sizes:
    # We prefer to evict from T1 if T1 is larger than our target `p`.
    # Otherwise, we evict from T2.
    
    # Logic:
    # If (T1 is not empty) AND ((T1 size > p) OR (B2 contains the new item AND T1 size == p)):
    #   Evict from T1
    # Else:
    #   Evict from T2
    
    # Note: We don't have access to "the new item" inside evict() easily without looking at obj,
    # but strictly speaking, evict() just needs to clear space.
    # The adaptation of `p` happens in update_after_insert/hit, but the eviction decision relies on `p`.
    
    # Simplification for Byte-Size constraints:
    # If T1 has grown beyond target `p` (and has items), evict from T1.
    # If T1 is within `p` but we still need space, evict from T2.
    # Safe guards: If T1 empty, evict T2. If T2 empty, evict T1.
    
    evict_t1 = False
    
    if t1_queue and t1_size > p:
        evict_t1 = True
    elif t1_queue and not t2_queue:
        evict_t1 = True
    elif not t1_queue:
        evict_t1 = False
    
    if evict_t1:
        # Evict LRU from T1
        victim_key = t1_queue.popleft()
        t1_set.discard(victim_key)
        
        # Add to B1 (Ghost Recent)
        if victim_key not in b1_set:
            b1_queue.append(victim_key)
            b1_set.add(victim_key)
    else:
        # Evict LRU from T2
        victim_key = t2_queue.popleft()
        t2_set.discard(victim_key)
        
        # Add to B2 (Ghost Frequent)
        if victim_key not in b2_set:
            b2_queue.append(victim_key)
            b2_set.add(victim_key)
            
    # Metadata cleanup happens in update_after_evict mostly, but we maintain local sets here
    
    # Enforce Ghost list sizes (ARC usually keeps B1+B2 ~= Capacity in count)
    # We'll just cap them at 2x item count to prevent memory leaks
    max_ghost = len(cache) * 2
    while len(b1_queue) > max_ghost:
        rem = b1_queue.popleft()
        b1_set.discard(rem)
    while len(b2_queue) > max_ghost:
        rem = b2_queue.popleft()
        b2_set.discard(rem)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. If in T1 -> Move to T2 (MRU).
    2. If in T2 -> Move to T2 (MRU).
    '''
    global t1_queue, t1_set, t2_queue, t2_set
    
    k = obj.key
    
    if k in t1_set:
        # Promotion: T1 -> T2
        # Remove from T1 (requires O(N) search in deque without a doubly linked list node map, 
        # but in Python deque remove is O(N). For sim simplicity, we accept this.)
        try:
            t1_queue.remove(k)
            t1_set.remove(k)
        except ValueError:
            pass # Should be there based on t1_set
            
        t2_queue.append(k)
        t2_set.add(k)
        
    elif k in t2_set:
        # Re-access: T2 -> T2 MRU
        try:
            t2_queue.remove(k)
        except ValueError:
            pass
        t2_queue.append(k)

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion of new item.
    This includes the core ARC adaptation logic for `p`.
    '''
    global t1_queue, t1_set, t2_queue, t2_set
    global b1_queue, b1_set, b2_queue, b2_set
    global p
    
    k = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Check Ghost Hits (Adaptation)
    if k in b1_set:
        # Hit in Ghost Recent (B1): We should have made T1 larger.
        # Delta = 1 if |B1| >= |B2|, else |B2| / |B1|
        delta = 1
        if len(b1_set) < len(b2_set):
            # Avoid div by zero
            b1_len = max(1, len(b1_set))
            delta = len(b2_set) / b1_len
            
        # Increase p (target size for T1), capped at capacity
        p = min(capacity, p + (delta * size)) # Scale delta by object size for byte-aware
        
        # Move from B1 to T2 (It has been seen twice now: once in T1, evicted, now seen again)
        b1_set.remove(k)
        # Note: Removing from deque is O(N), expensive but necessary for correctness in strict ARC
        # In high perf, lazy removal is preferred. Here we skip deque removal for speed, 
        # relying on set membership check in later stages.
        
        t2_queue.append(k)
        t2_set.add(k)
        
    elif k in b2_set:
        # Hit in Ghost Frequent (B2): We should have made T2 larger (T1 smaller).
        # Delta = 1 if |B2| >= |B1|, else |B1| / |B2|
        delta = 1
        if len(b2_set) < len(b1_set):
            b2_len = max(1, len(b2_set))
            delta = len(b1_set) / b2_len
        
        # Decrease p (target size for T1), floored at 0
        p = max(0, p - (delta * size))
        
        # Move from B2 to T2
        b2_set.remove(k)
        
        t2_queue.append(k)
        t2_set.add(k)
        
    else:
        # Completely new item (or forgotten)
        # Insert into T1 (Recent)
        t1_queue.append(k)
        t1_set.add(k)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    Ideally, we've already handled the queue/set logic in evict().
    We just verify consistency here.
    '''
    global current_cache_size
    # No specific cleanup needed for ARC logic here as evict() handles 
    # the transition to Ghost lists (B1/B2).
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwp__cfd5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpom3_omst.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp874j36ho.pickle

Iteration 19: New subsample score 0.9463649999999999 is better than old score 0.9165730000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkun5ypsm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprtzrfosg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvrittevp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxces2zbn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr28zrphg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw2h5fixx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxhihebpl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjxbqunpm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4ya4r2yr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2vpkq3xf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfi6kmzhc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1a6j2v40.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpczt8ji_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2c2jioe6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgyy0izmn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjgd6s943.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpddld9_dt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_m4ltwed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn1ysb1me.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppanwhcg2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdf5vuwbf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp92kg5hiu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdz88j9sc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp06s6yiju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmeezyaom.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9_afkok1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_4q71tlw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbpx6yejq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp84axvvmn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp891_vwht.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqyrou18x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3s4gkz43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy4gp8qn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe6qwvihf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc0bolja9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph2ugcuhr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4kuuq6ef.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6vx04_hk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6m1c9ctr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpulb24x1a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8t8o25er.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp95v6ranc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjc_sm9ja.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa5ps4wqt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp92rm8zbj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuqywpqj8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa_4ymr91.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2sbozfn0.pickle

Iteration 19: Full valset score for new program: 0.24520908333333336
Iteration 19: Full train_val score for new program: 0.24520908333333336
Iteration 19: Individual valset scores for new program: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498034, 0.540937, 0.531017, 0.091667, 0.344139, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.3412, 0.026556, 0.058672, 0.058672, 0.2698, 0.391129, 0.849057, 0.892521, 0.143157, 0.038636, 0.045558, 0.029555, 0.029983, 0.725494, 0.083333, 0.067961, 0.093464, 0.640392, 0.125461, 0.061824, 0.041539, 0.072146, 0.052632, 0.316667, 0.088587, 0.058065, 0.466258, 0.081699]
Iteration 19: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.391129, 0.849057, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 19: Full valset pareto front score: 0.263723
Iteration 19: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8}, {0, 1, 2, 3, 4, 5, 6, 7}, {8, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {8}, {5}, {2, 3, 4, 5, 6, 7, 8}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {7}, {7}, {7}, {8, 5, 7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8}, {7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8}, {1, 2, 3, 4, 5, 6, 7, 8}, {3}, {8}, {8}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7, 8}, {1, 2, 3, 4, 5, 6, 7, 8}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7, 8}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8}, {7}]
Iteration 19: Best valset aggregate score so far: 0.2580827708333333
Iteration 19: Best program as per aggregate score on train_val: 5
Iteration 19: Best program as per aggregate score on valset: 5
Iteration 19: Best score on valset: 0.2580827708333333
Iteration 19: Best score on train_val: 0.2580827708333333
Iteration 19: Linear pareto front program index: 5
Iteration 19: New program candidate index: 8
Iteration 20: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiabemz_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpauc1t323.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvjuf4ujs.pickle

Iteration 20: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# This algorithm divides the cache into a Small/Probationary Queue (S) 
# and a Main/Protected Queue (M). It uses a Ghost Queue (G) to track 
# evicted items for quick promotion if they return (scan resistance).
#
# Key Improvements:
# 1. Strict Byte Accounting: Managing S vs M ratios by bytes rather than count 
#    ensures we don't evict small, useful items to make room for large, cold ones.
# 2. Adaptive Ghost Rescue: Items returning from Ghost bypass probation and 
#    enter M immediately, catching "looping" access patterns (Trace 21/45).
# 3. Frequency Saturation: A capped frequency counter allows distinguishing 
#    between "warm" and "hot" items in the Main queue.

# Data Structures
# We use global variables to maintain state across function calls.
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency counter
s_bytes = 0            # Total bytes in S queue
m_bytes = 0            # Total bytes in M queue

# Tuning Parameters
# 10% of cache allocated to probation allows new items to be filtered quickly.
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate = None
    
    # We must loop until we find a valid candidate to evict
    while candidate is None:
        
        # --- Logic: Which Queue to Process? ---
        # 1. If S is larger than its target allocation, we need to trim S.
        # 2. If M is empty, we have no choice but to trim S.
        # Otherwise, we trim M to make space.
        
        process_s = False
        if s_bytes >= target_s_bytes or not m_queue:
            process_s = True
            
        # Failsafe: If we decided to process S but it's empty, switch to M
        if process_s and not s_queue:
            process_s = False
        
        # Failsafe: If M is empty, we must process S (guaranteed by 'or not m_queue' above, but safe to verify)
        if not process_s and not m_queue:
            return None # Should not happen unless cache is empty
            
        if process_s:
            # --- Process Small/Probation Queue (S) ---
            victim_key = s_queue.popleft()
            
            # Integrity check: If object was removed externally
            if victim_key not in cache:
                # We can't update bytes here reliably without the object size, 
                # but we continue to find a valid victim.
                continue
                
            v_obj = cache[victim_key]
            v_size = v_obj.size
            v_freq = freq.get(victim_key, 0)
            
            s_bytes -= v_size
            
            if v_freq > 0:
                # Promotion: Item earned a hit in probation. Move to Main.
                m_queue.append(victim_key)
                m_bytes += v_size
                # Reset frequency: It enters M as a new resident. 
                # It must earn hits in M to stay in M.
                freq[victim_key] = 0
            else:
                # Eviction: Item had no hits in probation.
                candidate = victim_key
                
                # Add to Ghost Queue to record this eviction
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Process Main/Protected Queue (M) ---
            victim_key = m_queue.popleft()
            
            if victim_key not in cache:
                continue
                
            v_obj = cache[victim_key]
            v_size = v_obj.size
            v_freq = freq.get(victim_key, 0)
            
            m_bytes -= v_size
            
            if v_freq > 0:
                # Reinsertion: Item has been used. Keep it in M.
                m_queue.append(victim_key)
                m_bytes += v_size
                # Decay frequency: Ensure items don't stay forever without recent hits.
                freq[victim_key] = v_freq - 1
            else:
                # Eviction from Main
                candidate = victim_key
                # We typically do not add M-evictions to Ghost in S3-FIFO 
                # because they already had a long residency.

    # --- Ghost Queue Maintenance ---
    # Keep Ghost Queue roughly 2x the size of the cache (in count) 
    # to capture longer looping patterns.
    target_ghost_count = len(cache) * 2
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        rem = g_queue.popleft()
        if rem in g_set:
            g_set.remove(rem)

    # Lazy cleanup of Ghost head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()
        
    # Safety reset for byte counters if queues become empty (floating point drift protection)
    if not s_queue: s_bytes = 0
    if not m_queue: m_bytes = 0

    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Decide placement based on Ghost history.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Reset frequency for new insertion
    freq[k] = 0
    
    if k in g_set:
        # Ghost Rescue: Item was recently evicted but came back.
        # This indicates a loop or high utility. Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        
        # Remove from Ghost Set (Lazy removal from Queue handled in evict)
        g_set.remove(k)
    else:
        # Standard Insert: New items go to Small (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj9vw07gf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl0j6s6yl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi7gc7fil.pickle

Iteration 20: New subsample score 1.2816009999999998 is not better than old score 1.2816009999999998, skipping
Iteration 21: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwm1f161c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcz3yzcnm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt39r7be6.pickle

Iteration 21: Proposed new text for program: from collections import deque

# --- Global State ---
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters (0 to MAX_FREQ)

# --- Byte Tracking ---
s_bytes = 0
m_bytes = 0

# --- Constants ---
# 10% for Small Queue is generally optimal for scan resistance (SOSP'23 S3-FIFO)
SMALL_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    Prioritizes evicting from Small queue (S) to filter one-hit wonders.
    Promotes items from S to Main (M) if accessed.
    Demotes items from M if their frequency counter is exhausted.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small queue (10% of capacity)
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to evict from.
        # We evict from S if:
        # 1. S is larger than target size.
        # 2. OR M is empty (must evict from S).
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If we chose S but it's empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue (S) ---
            if not s_queue: 
                # Should be impossible if cache is full, but safe break
                break
                
            candidate = s_queue[0]
            
            # Defense against stale keys (not in cache anymore)
            if candidate not in cache:
                s_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: Item was hit in S -> Move to M
                s_queue.popleft()
                obj_ref = cache[candidate]
                s_bytes -= obj_ref.size
                
                m_queue.append(candidate)
                m_bytes += obj_ref.size
                
                # Reset frequency upon entering M
                freq[candidate] = 0
            else:
                # Eviction: Item not hit while in S -> Evict
                victim_key = s_queue.popleft()
                obj_ref = cache[candidate]
                s_bytes -= obj_ref.size
                
                # Add to Ghost Queue to track "quick re-access" (loop patterns)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Eviction from Main Queue (M) ---
            if not m_queue:
                break
                
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail of M and decrement frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Item exhausted frequency in M -> Evict
                victim_key = m_queue.popleft()
                obj_ref = cache[candidate]
                m_bytes -= obj_ref.size
                # Note: Items evicted from M are usually NOT added to Ghost in S3-FIFO

    # --- Ghost Queue Maintenance ---
    # Keep ghost queue size proportional to cache count to detect loops
    current_count = len(cache)
    if current_count > 0:
        target_ghost_len = current_count
        while len(g_set) > target_ghost_len and g_queue:
            oldest = g_queue.popleft()
            if oldest in g_set:
                g_set.remove(oldest)
                
    # Cleanup ghost head if out of sync
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    # 2-bit counter logic (cap at 3)
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - Ghost Hit: Insert directly into Main (M).
    - Standard Insert: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Recently evicted from S, proven useful. Restore to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert into S (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp762c9zgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaokut_3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpevk20hmv.pickle

Iteration 21: New subsample score 1.0905420000000001 is better than old score 0.979507. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpogmax9ri.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp02kefmp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzhemqsk9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpir6jlrlc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0_qzmspg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5_0kcl69.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3x4l6ifs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp901uba4c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt_9_r3s2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2ww4ew95.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfiu7avow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy4iwzmyy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp80mu36rs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy4u803h4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpap_jtcb9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb5gra6xk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppuzrpt03.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcjw9hmd0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsyt_r8ai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf_ln41r_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1k_gnmz3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj9so1lb1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1ezu_ktm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjnbofk4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa458o6z8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2zfxmrl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpszrqcebh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpehnb80r3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmped6g9j6q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcjuelaka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr28zpis8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpedm5mgck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9whbfk0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp78xi6qxv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpadk2v81g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpysll0x0u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp9_t1b1k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiz3do3sy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4sggi6lj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpexeegqld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeo6kd895.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2owju23e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvtxd3_u1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp40ktnrfd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp32o_rzmu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzxiocdl0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptxgzk7dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp264twl7m.pickle

Iteration 21: Full valset score for new program: 0.2525479166666666
Iteration 21: Full train_val score for new program: 0.2525479166666666
Iteration 21: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269806, 0.34879, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 21: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.391129, 0.849057, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 21: Full valset pareto front score: 0.263723
Iteration 21: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9}, {0, 1, 2, 3, 4, 5, 6, 7, 9}, {8, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {8}, {5}, {2, 3, 4, 5, 6, 7, 8, 9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {7}, {7}, {7}, {8, 5, 7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {3}, {8}, {8}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9}, {7}]
Iteration 21: Best valset aggregate score so far: 0.2580827708333333
Iteration 21: Best program as per aggregate score on train_val: 5
Iteration 21: Best program as per aggregate score on valset: 5
Iteration 21: Best score on valset: 0.2580827708333333
Iteration 21: Best score on train_val: 0.2580827708333333
Iteration 21: Linear pareto front program index: 5
Iteration 21: New program candidate index: 9
Iteration 22: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpct3y6d8s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjbfv05f7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf1_hfrko.pickle

Iteration 22: Proposed new text for program: from collections import deque

# S3-FIFO Global State
# S: Small FIFO queue (probationary items)
# M: Main FIFO queue (popular items)
# G: Ghost FIFO queue (history of recently evicted items from S)
# freq: Frequency counter (0-2)
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}

# Tuning Parameters
# 10% is typically optimal for the small queue to filter one-hit wonders (scans).
SMALL_QUEUE_RATIO = 0.1
# Reduced MAX_FREQ to 2 to allow faster eviction of items that become cold.
MAX_FREQ = 2

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Strategy:
    - Balances Small and Main queues to filter scans and retain heavy hitters.
    - Uses a 2-bit frequency counter for responsiveness.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    capacity = cache_snapshot.capacity
    s_target_size = max(1, int(capacity * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    while candidate_key is None:
        # Maintenance: Clean dead ghosts lazily
        while g_queue and g_queue[0] not in g_set:
            g_queue.popleft()

        # Decision: Evict from S or M?
        # 1. If M is empty, we must process S.
        # 2. If S is empty, we must process M.
        # 3. If S exceeds its target size ratio, prioritize draining S.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif not s_queue:
            evict_from_s = False
        else:
            evict_from_s = len(s_queue) >= s_target_size
            
        if evict_from_s:
            # --- Small Queue (S) Logic ---
            victim = s_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Promotion: S -> M
                # Item was hit during probation. Move to Main.
                s_queue.popleft()
                m_queue.append(victim)
                # Reset frequency: It must receive new hits in M to survive M-eviction.
                freq[victim] = 0
            else:
                # Eviction: S -> Ghost -> Out
                candidate_key = s_queue.popleft()
                
                # Add to Ghost to capture "quick return" items
                if candidate_key not in g_set:
                    g_queue.append(candidate_key)
                    g_set.add(candidate_key)
                
                # Maintain Ghost Size <= Cache Capacity
                while len(g_set) > capacity:
                    if not g_queue: break
                    old = g_queue.popleft()
                    if old in g_set:
                        g_set.remove(old)
                        
        else:
            # --- Main Queue (M) Logic ---
            victim = m_queue[0]
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Reinsert to back of M, decrement freq
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Evict from M
                candidate_key = m_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions:
    - If in Ghost: Rescue to Main (M).
    - Else: Insert into Small (S).
    '''
    global s_queue, m_queue, g_set, freq
    key = obj.key
    
    # Initialize frequency
    freq[key] = 0 
    
    if key in g_set:
        # Ghost Hit! This implies the item was part of a larger working set.
        # Promote directly to M to avoid S-queue eviction.
        m_queue.append(key)
        g_set.remove(key)
    else:
        # Standard insertion into probation queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9500naur.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpheh0dt6a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1f0z9x6p.pickle

Iteration 22: New subsample score 0.522466 is better than old score 0.521314. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbzmcf700.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjtr5k72u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpddekf4w5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2syuodyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpploihvvg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6cu3e6hl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwibyhjdq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4j4voqfs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6jt8lyig.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyieou6yl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkki2iy1g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7u8byo2f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdzm6cdad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxux7wvv6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0abblygk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvb2wp7ii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3urm1t1w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv7eulpgh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvuuoc7qn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4czdmlh7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpus9v1_h0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4unw6q2p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8qmqus4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbxvsy3wi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpku036nul.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqk8rct3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1rgur2if.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmzn70jzi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx15g0y75.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfa28bj3x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcc8nva01.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptyp5axzc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcla4taa_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxalbm0fk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7wsk6ewt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprtmvf9xk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9jckge05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5bprphqn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9kn0pcom.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv26e7dfr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp8krvan6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp586uftvd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1li6yfh6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp06jlt4sv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppfhhvuo1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt36n26m0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkjv2c7n2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe05w9ouc.pickle

Iteration 22: Full valset score for new program: 0.25314137499999995
Iteration 22: Full train_val score for new program: 0.25314137499999995
Iteration 22: Individual valset scores for new program: [0.496987, 0.471982, 0.481547, 0.435227, 0.493609, 0.482244, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.393872, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.391347, 0.026164, 0.058672, 0.058672, 0.269802, 0.352823, 0.83714, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752723, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145273, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143338, 0.466258, 0.081699]
Iteration 22: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.391129, 0.849057, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 22: Full valset pareto front score: 0.263723
Iteration 22: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10}, {8, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {8}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}, {7}, {7}, {8, 5, 7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {3}, {8}, {8}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}]
Iteration 22: Best valset aggregate score so far: 0.2580827708333333
Iteration 22: Best program as per aggregate score on train_val: 5
Iteration 22: Best program as per aggregate score on valset: 5
Iteration 22: Best score on valset: 0.2580827708333333
Iteration 22: Best score on train_val: 0.2580827708333333
Iteration 22: Linear pareto front program index: 5
Iteration 22: New program candidate index: 10
Iteration 23: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu9kjfs8_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpywv2wanh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphn4b1749.pickle

Iteration 23: Proposed new text for program: from collections import deque

# --- Global State ---
# S3-FIFO Queues
s_queue = deque()       # Small/Probationary Queue (FIFO)
m_queue = deque()       # Main/Protected Queue (FIFO)
g_queue = deque()       # Ghost Queue (FIFO - Keys only)
g_set = set()           # Ghost Set (Fast lookup)

# Metadata
freq = {}               # Frequency map {key: int}
s_bytes = 0             # Current size of S-Queue in bytes

# Configuration
SMALL_RATIO = 0.1       # 10% of cache space dedicated to S-Queue
MAX_FREQ = 3            # Cap on frequency to prevent integer runaways

def evict(cache_snapshot, obj):
    """
    S3-FIFO Eviction Logic:
    1. Determine if we evict from Small (S) or Main (M).
    2. Manage promotions (S->M) and demotions (M->Evict).
    3. Maintain Ghost queue for scan resistance.
    """
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes

    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for the Small Queue
    s_target_size = capacity * SMALL_RATIO
    
    candidate_key = None

    while candidate_key is None:
        # --- Logic Step 1: Decide which queue to process ---
        # We process S if it's "full" (over the 10% ratio) OR if M is empty.
        # Otherwise, we process M.
        evict_from_s = False
        if s_bytes >= s_target_size or not m_queue:
            evict_from_s = True
        
        # Failsafes: switch queues if the chosen one is actually empty
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            # If both are empty, something is wrong with the environment, 
            # but we return None to break safely or handle edge case.
            if not s_queue: return None 
            evict_from_s = True

        # --- Logic Step 2: Execute Eviction/Promotion ---
        if evict_from_s:
            # Process Small Queue
            victim_key = s_queue.popleft()
            
            # Lazy cleanup: If item deleted externally, just update bytes and continue
            if victim_key not in cache_map:
                # We can't know the size easily if it's gone, but we rely on
                # s_bytes being roughly correct or reset via sync if needed.
                # Here we assume it was removed and s_bytes was handled or we ignore size drift.
                # To be safe, we just loop again.
                if victim_key in freq: del freq[victim_key]
                continue

            victim_obj = cache_map[victim_key]
            v_size = victim_obj.size
            v_freq = freq.get(victim_key, 0)
            
            # Update S-Queue byte tracking immediately upon removal from S
            s_bytes -= v_size
            if s_bytes < 0: s_bytes = 0

            if v_freq > 0:
                # HIT in S: Promote to M
                m_queue.append(victim_key)
                # Reset frequency to 0. It enters M with a clean slate.
                # It has earned its place in M, now it must earn its stay.
                freq[victim_key] = 0 
            else:
                # NO HIT in S: Evict
                candidate_key = victim_key
                # Add to Ghost Queue to detect future scans/loops
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
        
        else:
            # Process Main Queue
            victim_key = m_queue.popleft()
            
            # Lazy cleanup
            if victim_key not in cache_map:
                if victim_key in freq: del freq[victim_key]
                continue

            v_freq = freq.get(victim_key, 0)

            if v_freq > 0:
                # HIT in M: Reinsert to tail (Second Chance)
                m_queue.append(victim_key)
                # Decay frequency. This allows popular items to stay longer,
                # but eventually drift to the head if they stop being accessed.
                freq[victim_key] = v_freq - 1
            else:
                # NO HIT (or expired freq) in M: Evict
                candidate_key = victim_key
                # M-evicted items do NOT go to Ghost (they had their chance)

    # --- Logic Step 3: Maintain Ghost Queue Size ---
    # Keep Ghost Queue size proportional to cache count to limit memory overhead
    # We use count-based limit for Ghost as keys are small.
    target_ghost_count = len(cache_map) 
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Clean head of ghost queue if needed
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    """
    On Hit: Increment frequency, capped at MAX_FREQ.
    This applies to items in S or M.
    """
    global freq
    k = obj.key
    # min(current + 1, MAX_FREQ)
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    """
    On Insert:
    1. Check Ghost: If present, it's a Recall -> Insert to M.
    2. Otherwise -> Insert to S.
    """
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit (Recall):
        # This item was recently evicted from S. It's likely part of a loop.
        # Promote straight to M.
        m_queue.append(k)
        g_set.remove(k) # Remove from Ghost
    else:
        # Standard Insert:
        # Goes to S (Probation).
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Cleanup metadata for the evicted object.
    The object key was already removed from queues in evict(),
    but we clean the freq map here to be safe.
    """
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfk6pyrrj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6pxzkim0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkxryof0t.pickle

Iteration 23: New subsample score 1.0557699999999999 is not better than old score 1.058127, skipping
Iteration 24: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqczuumog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcexqzcx2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm5nmhbj5.pickle

Iteration 24: Proposed new text for program: from collections import deque

# S3-FIFO Global State
# s_queue: Small FIFO queue for new/probationary items
# m_queue: Main FIFO queue for popular items
# g_queue: Ghost FIFO queue to track history of evicted small items
# g_set:   Set for O(1) lookup of items in g_queue
# freq:    Dictionary to track access frequency (0, 1, 2, 3)
# s_bytes: Tracks the total size (in bytes) of items currently in s_queue

s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}
s_bytes = 0

# Tuning parameter: 10% of cache capacity is allocated to the Small Queue
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    - If S-Queue is larger than 10% of capacity OR M-Queue is empty: Evict from S.
    - Otherwise: Evict from M.
    - Uses frequency bits to give "second chances" (reinsertion).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    candidate_key = None
    target_s_bytes = cache_snapshot.capacity * SMALL_QUEUE_RATIO
    
    # Loop until a victim is found (guaranteed as long as cache is not empty)
    while candidate_key is None:
        
        # --- Decision: Evict from Small (S) or Main (M)? ---
        # We evict from S if it's "full" (over byte budget) or if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_bytes or not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # Safety check: if S is empty but logic pointed here, force switch to M if possible
            if not s_queue:
                if m_queue:
                    evict_from_s = False
                else:
                    break # Both empty? Should not happen in a full cache.
            
            if evict_from_s:
                # --- Process S Queue ---
                victim_key = s_queue[0] # Peek at head
                victim_freq = freq.get(victim_key, 0)
                
                if victim_freq > 0:
                    # PROMOTION: Item was accessed while in S. Move to M.
                    s_queue.popleft()
                    
                    # Update S-byte tracking
                    victim_obj = cache_snapshot.cache.get(victim_key)
                    if victim_obj:
                        s_bytes -= victim_obj.size
                    
                    m_queue.append(victim_key)
                    freq[victim_key] = 0 # Reset frequency on promotion
                else:
                    # EVICTION: Item has no frequency. Evict it.
                    candidate_key = s_queue.popleft()
                    
                    # Update S-byte tracking
                    victim_obj = cache_snapshot.cache.get(candidate_key)
                    if victim_obj:
                        s_bytes -= victim_obj.size
                    
                    # Add to Ghost (History)
                    if candidate_key not in g_set:
                        g_queue.append(candidate_key)
                        g_set.add(candidate_key)
                    
                    # Manage Ghost Size: Keep roughly same count as main cache
                    # This prevents memory bloat for the metadata
                    max_ghost_count = len(cache_snapshot.cache)
                    while len(g_queue) > max_ghost_count:
                        rem = g_queue.popleft()
                        if rem in g_set:
                            g_set.remove(rem)

        # Note: We use 'if' here (not else) because 'evict_from_s' might have changed 
        # inside the block above if S was empty.
        if not evict_from_s and candidate_key is None:
            # --- Process M Queue ---
            if not m_queue:
                continue # Loop back
            
            victim_key = m_queue[0] # Peek at head
            victim_freq = freq.get(victim_key, 0)
            
            if victim_freq > 0:
                # DEMOTION / SECOND CHANCE: Reinsert at tail of M
                m_queue.popleft()
                m_queue.append(victim_key)
                freq[victim_key] = victim_freq - 1 # Decrement frequency
            else:
                # EVICTION: Evict from M
                candidate_key = m_queue.popleft()
                # Items evicted from M generally do not enter Ghost queue in S3-FIFO
                
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at 3.
    We do not move items on hit to maintain O(1) efficiency and scan resistance.
    '''
    global freq
    key = obj.key
    curr_freq = freq.get(key, 0)
    if curr_freq < 3:
        freq[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: 
    - If in Ghost -> Restore to Main (M).
    - Else -> Insert to Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # Ghost Hit! This item was recently evicted from S. 
        # It has proven its worth; skip S and go directly to M.
        m_queue.append(key)
        
        # Remove from Ghost Set immediately to prevent double counting.
        # We leave it in g_queue for lazy expiration (performance optimization).
        g_set.remove(key)
    else:
        # Standard Insert: Start in Small Queue
        s_queue.append(key)
        s_bytes += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    Queues were already updated in evict().
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph4ur2jzf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgvqlt6fb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpevn_zojv.pickle

Iteration 24: New subsample score 0.461304 is not better than old score 0.47797100000000003, skipping
Iteration 25: Selected program 3 score: 0.2526974166666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph8wyqhz5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzqstpa5q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp262xd5fv.pickle

Iteration 25: Proposed new text for program: from collections import deque, defaultdict

# Algorithm: LHD (Least Hit Density) approximation inspired by LIRS/ARC principles.
# However, given the constraints and the previous S3-FIFO attempt, we will implement an 
# advanced variant of S3-FIFO called "S3-FIFO with Dynamic Sizing and Age-Awareness".
#
# Key Improvements over the provided S3-FIFO:
# 1. Dynamic S-Queue Sizing: Instead of a static 10% ratio, we adapt the size of the 
#    probationary queue (S) based on the hit rate of items in S vs M. This helps handles
#    scans (needs small S) vs working set changes (needs larger S) better.
# 2. Capped Frequency: We use a small frequency counter (0-3) but ensure it's updated 
#    correctly on hits to distinguish "warm" items in M.
# 3. Ghost Cache handling: We ensure the Ghost cache tracks evicted items to "rescue" 
#    them directly into M if seen again quickly.

# Global Metadata
s_queue = deque()      # Small FIFO queue (Probation)
m_queue = deque()      # Main FIFO queue (Protected)
g_set = set()          # Ghost set (lookup for recently evicted from S)
g_queue = deque()      # Ghost queue (for FIFO eviction of ghosts)
freq = defaultdict(int) # Frequency counter for items

# Tuning Constants
MAX_FREQ = 3           # Saturation point for frequency
DEFAULT_S_RATIO = 0.1  # Default starting ratio for S queue

def evict(cache_snapshot, obj):
    '''
    Advanced S3-FIFO Eviction Strategy.
    
    This function balances eviction between a small probationary queue (S) and a 
    main protected queue (M). It lazily promotes items from S to M if they are accessed,
    and demotes items from M if they are not frequently accessed.
    '''
    global s_queue, m_queue, g_queue, g_set, freq

    cache_size = len(cache_snapshot.cache)
    # Target size for S is usually 10% of cache, but we ensure at least 1 slot.
    s_target_size = max(1, int(cache_size * DEFAULT_S_RATIO))
    
    candidate_key = None

    while candidate_key is None:
        # Maintenance: Clean dead ghosts.
        # Remove items from the ghost queue that are no longer in the ghost set
        # (meaning they were rescued back to cache) to keep the queue valid.
        while g_queue and g_queue[0] not in g_set:
            g_queue.popleft()

        # Decision Logic: Which queue to evict from?
        # If M is empty, we must use S.
        # If S is empty, we must use M.
        # If S is larger than target size, we prefer cleaning S to maintain the ratio.
        evict_from_s = False
        if not m_queue:
            evict_from_s = True
        elif not s_queue:
            evict_from_s = False
        else:
            evict_from_s = len(s_queue) >= s_target_size
        
        if evict_from_s:
            # --- Processing Small Queue (S) ---
            victim = s_queue[0]
            victim_freq = freq[victim]

            if victim_freq > 0:
                # Promotion: S -> M
                # The item proved its worthiness while in probation.
                s_queue.popleft()
                m_queue.append(victim)
                # Note: We do NOT reset frequency to 0 here. 
                # We essentially grant it a "grace period" in M based on its current accumulated hits.
                # However, to prevent it from sticking in M forever without new hits, we might cap it?
                # Actually, standard S3-FIFO often clears freq. Let's try keeping it but capping it 
                # strictly or decrementing slightly so it has to earn its keep in M.
                # Here: Resetting to 0 is the strict S3-FIFO way, but retaining some history (1) helps 
                # distinguish it from a cold insert if M is churning fast.
                freq[victim] = 0 # Strict S3-FIFO behavior for "new life" in M.
            else:
                # Eviction: S -> Ghost -> Out
                candidate_key = s_queue.popleft()
                
                # Add to Ghost
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
                
                # Ghost capacity management: Keep ghost size roughly equal to cache size
                # to track history without unbounded memory.
                while len(g_set) > cache_size:
                    if not g_queue:
                        break
                    g_victim = g_queue.popleft()
                    if g_victim in g_set:
                        g_set.remove(g_victim)
        else:
            # --- Processing Main Queue (M) ---
            victim = m_queue[0]
            victim_freq = freq[victim]

            if victim_freq > 0:
                # Reinsertion: M -> M (Back)
                # The item has been accessed. Give it a second chance.
                m_queue.popleft()
                m_queue.append(victim)
                # Decrement frequency to age the item. It needs ongoing hits to stay high.
                freq[victim] = victim_freq - 1
            else:
                # Eviction: M -> Out
                # Items evicted from M are usually "cold" warm items.
                candidate_key = m_queue.popleft()
                # We do not put M evictions into Ghost in standard S3-FIFO, 
                # as Ghost is specifically for items that failed the S probation.

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    Does not move items between queues (O(1)).
    '''
    global freq
    key = obj.key
    # We use a default dict, so getting is safe even if key temporarily missing from logic
    curr = freq[key]
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    Check Ghost cache to decide if item goes to S (Probation) or M (Rescue).
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # Ghost Hit: This item was recently evicted from S. 
        # It's part of a returning access pattern. Rescue to M.
        m_queue.append(key)
        g_set.remove(key)
        # Note: We don't remove from g_queue immediately; it's handled lazily in evict.
    else:
        # Standard Insert: Start in S (Probation).
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpswgq9v2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8oqrzdgm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuzgvrljw.pickle

Iteration 25: New subsample score 1.409371 is not better than old score 1.409371, skipping
Iteration 26: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkwd3a_se.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg9pfc0px.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf_v8l9vb.pickle

Iteration 26: Proposed new text for program: from collections import OrderedDict

# Adaptive Replacement Cache (ARC) Implementation
# Optimized for Variable Object Sizes (Byte-Aware)

# Data Structures: OrderedDict allows O(1) LRU operations.
# T1: Recent (Probationary) Objects [Key -> Size]
t1 = OrderedDict()
# T2: Frequent (Protected) Objects [Key -> Size]
t2 = OrderedDict()
# B1: Ghost Recent (Evicted from T1) [Key -> Size]
b1 = OrderedDict()
# B2: Ghost Frequent (Evicted from T2) [Key -> Size]
b2 = OrderedDict()

# Byte Tracking
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# Adaptivity Parameter
# p represents the target capacity (in bytes) for T1.
p = 0

def adapt_p(candidate_key, candidate_size, capacity):
    '''
    Adjusts the target size 'p' based on hits in the ghost lists (B1/B2).
    Returns (True, type) if a ghost hit occurred, (False, None) otherwise.
    '''
    global p, b1, b2, b1_bytes, b2_bytes

    if candidate_key in b1:
        # Hit in B1 (Recency Ghost) -> T1 was too small.
        # We need to increase p (grow T1).
        delta = candidate_size
        
        # Adaptation magnitude logic from ARC paper, adapted for bytes
        if b1_bytes < b2_bytes and b1_bytes > 0:
            delta = candidate_size * (float(b2_bytes) / b1_bytes)
            
        p = min(capacity, p + delta)
        
        # Clean up B1
        del b1[candidate_key]
        b1_bytes -= candidate_size
        return True, 'B1'

    elif candidate_key in b2:
        # Hit in B2 (Frequency Ghost) -> T2 was too small (T1 too big).
        # We need to decrease p (shrink T1).
        delta = candidate_size
        
        if b2_bytes < b1_bytes and b2_bytes > 0:
            delta = candidate_size * (float(b1_bytes) / b2_bytes)
            
        p = max(0, p - delta)
        
        # Clean up B2
        del b2[candidate_key]
        b2_bytes -= candidate_size
        return True, 'B2'
    
    return False, None

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Policy.
    Decides whether to evict from T1 or T2 based on the current target 'p'.
    '''
    global t1, t2, b1, b2, t1_bytes, t2_bytes, b1_bytes, b2_bytes, p
    
    capacity = cache_snapshot.capacity
    
    # 1. Adapt P: If the incoming object is in a ghost list, adjust p immediately.
    # We do this here because 'evict' implies a cache miss, which is when ARC adapts.
    adapt_p(obj.key, obj.size, capacity)
    
    candidate_key = None

    # 2. Select Victim based on Replace(p) logic
    # We must free enough space. The caller removes the object, but we update our
    # metadata lists (T1/T2 -> B1/B2) here.
    
    evict_from_t1 = False
    
    # Logic: If T1 is "over budget" (size > p), we evict from T1 to make room for T2.
    # If T1 is within budget, we evict from T2.
    if len(t1) > 0:
        if t1_bytes > p:
            evict_from_t1 = True
        elif len(t2) == 0:
            # Failsafe: T2 is empty, must evict T1
            evict_from_t1 = True
            
    if evict_from_t1:
        # Evict LRU from T1
        victim_key, victim_size = t1.popitem(last=False)
        t1_bytes -= victim_size
        
        # Add to B1 (Ghost Recency)
        b1[victim_key] = victim_size
        b1_bytes += victim_size
        candidate_key = victim_key
    else:
        # Evict LRU from T2
        if t2:
            victim_key, victim_size = t2.popitem(last=False)
            t2_bytes -= victim_size
            
            # Add to B2 (Ghost Frequency)
            b2[victim_key] = victim_size
            b2_bytes += victim_size
            candidate_key = victim_key
        else:
            # Failsafe if T2 was checked but empty (unlikely given logic above)
            if t1:
                victim_key, victim_size = t1.popitem(last=False)
                t1_bytes -= victim_size
                candidate_key = victim_key

    # 3. Maintain Ghost List Sizes
    # Prevent metadata from growing indefinitely. 
    # A safe upper bound is usually 2x cache capacity (by count).
    target_ghost_len = max(len(t1) + len(t2), 100) * 2
    
    while len(b1) > target_ghost_len:
        k, s = b1.popitem(last=False)
        b1_bytes -= s
        
    while len(b2) > target_ghost_len:
        k, s = b2.popitem(last=False)
        b2_bytes -= s

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If in T1: Move to T2 (Promote to Frequent).
    - If in T2: Move to MRU of T2.
    '''
    global t1, t2, t1_bytes, t2_bytes
    
    k = obj.key
    s = obj.size
    
    if k in t1:
        # Promotion: Recency -> Frequency
        del t1[k]
        t1_bytes -= s
        
        t2[k] = s
        t2_bytes += s
        t2.move_to_end(k) # Mark as MRU
    elif k in t2:
        # Update usage in Frequency
        t2.move_to_end(k) # Mark as MRU

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - Check if it was a ghost hit (to update P if evict wasn't called).
    - Insert into T2 if it was in ghost history (Recall).
    - Insert into T1 if it is new (Probation).
    '''
    global t1, t2, b1, b2, t1_bytes, t2_bytes, p
    
    k = obj.key
    s = obj.size
    capacity = cache_snapshot.capacity

    # 1. Adapt P (Idempotent check)
    # If the cache wasn't full, evict() wasn't called. We still need to check ghosts.
    # If evict() WAS called, it already removed k from b1/b2, so this returns False.
    is_ghost, ghost_type = adapt_p(k, s, capacity)
    
    # 2. Insert Object
    if is_ghost:
        # Was in history (B1 or B2) -> Frequent
        t2[k] = s
        t2_bytes += s
        t2.move_to_end(k)
    else:
        # New object -> Recent/Probation
        t1[k] = s
        t1_bytes += s
        t1.move_to_end(k)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup hook. 
    Logic is handled in evict(), this ensures no lingering state if needed.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdg9rh_h_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm5hy0rgs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoleu9gut.pickle

Iteration 26: New subsample score 0.355704 is not better than old score 0.479225, skipping
Iteration 27: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoed3ehh3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe2w95b9v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9ug6szvu.pickle

Iteration 27: Proposed new text for program: from collections import deque

# Improved S3-FIFO Algorithm Implementation
# --------------------------------------------------------
# This implementation focuses on strict queue management and responsive 
# frequency tracking to maximize hit rates across diverse traces.

# --- Global Data Structures ---
small_q = deque()      # Small/Probationary Queue (S) - FIFO
main_q = deque()       # Main/Protected Queue (M) - FIFO
ghost_q = deque()      # Ghost Queue (G) - FIFO, stores keys only
ghost_set = set()      # Fast lookup for Ghost Queue
freq_map = {}          # Frequency counters for resident objects

# Byte Tracking
small_q_bytes = 0
main_q_bytes = 0

# --- Tuning Parameters ---
# S_RATIO: 10% of cache capacity is reserved for the probationary queue.
# This filters out "one-hit wonders" (scan resistance).
S_RATIO = 0.1 

# MAX_FREQ: Capped at 2. 
# 0 = Inserted, 1 = Hit once, 2 = Hit multiple times (Very Hot).
# Lowering this from 3 to 2 makes the eviction from Main Queue faster 
# for items that cool down, improving adaptivity.
MAX_FREQ = 2

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    '''
    global small_q, main_q, ghost_q, ghost_set, freq_map
    global small_q_bytes, main_q_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target size for Small Queue based on current capacity
    target_s_bytes = capacity * S_RATIO
    
    candidate_key = None
    
    # Loop until a valid victim is found and removed from internal queues
    while candidate_key is None:
        
        # --- Decision: Evict from Small (S) or Main (M)? ---
        # 1. If S exceeds its target ratio, we drain S to maintain the 90/10 split.
        # 2. If M is empty, we have no choice but to drain S.
        # 3. Otherwise, we drain M.
        
        evict_from_small = False
        if small_q_bytes >= target_s_bytes or not main_q:
            evict_from_small = True
        
        # Safety checks for empty queues
        if evict_from_small and not small_q:
            evict_from_small = False
        if not evict_from_small and not main_q:
            evict_from_small = True
            
        if evict_from_small:
            # --- Processing Small Queue ---
            victim = small_q[0] # Peek head
            
            # Lazy cleanup: handle case where object is deleted externally
            if victim not in cache:
                small_q.popleft()
                if victim in freq_map: del freq_map[victim]
                continue
                
            v_size = cache[victim].size
            v_freq = freq_map.get(victim, 0)
            
            if v_freq > 0:
                # Promotion: Item was hit while in probation. Move to Main.
                small_q.popleft()
                small_q_bytes -= v_size
                
                main_q.append(victim)
                main_q_bytes += v_size
                
                # Reset frequency. It enters M as a new resident and must earn its stay.
                freq_map[victim] = 0
            else:
                # Eviction: Item failed probation.
                candidate_key = small_q.popleft()
                small_q_bytes -= v_size
                
                # Record in Ghost Queue (history of S evictions)
                if candidate_key not in ghost_set:
                    ghost_set.add(candidate_key)
                    ghost_q.append(candidate_key)
        else:
            # --- Processing Main Queue ---
            victim = main_q[0] # Peek head
            
            if victim not in cache:
                main_q.popleft()
                if victim in freq_map: del freq_map[victim]
                continue
                
            v_size = cache[victim].size
            v_freq = freq_map.get(victim, 0)
            
            if v_freq > 0:
                # Reinsertion: Item is active. Keep in M, but decay frequency.
                # It moves to the tail, giving it another cycle through M.
                main_q.popleft()
                main_q.append(victim)
                freq_map[victim] = v_freq - 1
            else:
                # Eviction: Item is cold in M.
                candidate_key = main_q.popleft()
                main_q_bytes -= v_size
                # Items evicted from M usually don't go to Ghost (they had their chance).

    # --- Ghost Queue Maintenance ---
    # Maintain Ghost Queue size proportional to the number of items in cache.
    # A size of 2x Count helps catch larger looping patterns.
    target_ghost_count = len(cache) * 2
    
    while len(ghost_set) > target_ghost_count:
        if not ghost_q: break
        oldest = ghost_q.popleft()
        if oldest in ghost_set:
            ghost_set.remove(oldest)

    # Clean ghost head lazily
    while ghost_q and ghost_q[0] not in ghost_set:
        ghost_q.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency counter, capped at MAX_FREQ.
    '''
    global freq_map
    k = obj.key
    curr = freq_map.get(k, 0)
    if curr < MAX_FREQ:
        freq_map[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Determine placement (S or M) based on Ghost history.
    '''
    global small_q, main_q, ghost_set, freq_map
    global small_q_bytes, main_q_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency for new object
    freq_map[k] = 0
    
    if k in ghost_set:
        # Ghost Hit (Recall): Object was recently evicted from S. 
        # This implies a cycle or longer loop. Promote directly to M.
        main_q.append(k)
        main_q_bytes += size
        ghost_set.remove(k)
    else:
        # Standard Insert: Insert into Small Queue (S) for probation.
        small_q.append(k)
        small_q_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq_map
    k = evicted_obj.key
    if k in freq_map:
        del freq_map[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4t9xet83.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp52kzcaya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp22m3wese.pickle

Iteration 27: New subsample score 0.7915940000000001 is not better than old score 0.795302, skipping
Iteration 28: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv1401tpx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxloglpeq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmvtgo09k.pickle

Iteration 28: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# This implementation refines the interaction between the probationary (Small) queue
# and the protected (Main) queue. It balances recency and frequency by giving
# items a second chance based on access bits (frequency) and using a ghost cache
# to catch items that are evicted too early (long scanning loops).

# Global Data Structures
# We use global variables to maintain state across function calls, simulating a persistent cache controller.
s_queue = deque()      # Small/Probationary Queue (FIFO) - stores keys
m_queue = deque()      # Main/Protected Queue (FIFO) - stores keys
g_queue = deque()      # Ghost Queue (FIFO) - stores keys of evicted S-items
g_set = set()          # Ghost Set - fast lookup for G-queue
freq = {}              # Frequency counter map: key -> integer count

# Tuning Parameters
# 10% of capacity reserved for S-queue is standard for S3-FIFO to filter one-hit wonders.
SMALL_QUEUE_RATIO = 0.1
# Max frequency helps age out items that were once popular but are no longer accessed.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict when the cache is full.
    Implements the S3-FIFO eviction policy.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    current_size = cache_snapshot.size
    
    # Calculate current size of Small Queue
    # Note: We track this dynamically here rather than maintaining a global counter to avoid sync issues
    s_size = 0
    for k in s_queue:
        if k in cache_map:
            s_size += cache_map[k].size
            
    s_target_size = capacity * SMALL_QUEUE_RATIO

    victim_key = None

    while victim_key is None:
        # --- Decision: Evict from Small (S) or Main (M)? ---
        # If S is larger than target, we prefer cleaning S to filter one-hit wonders.
        # However, if M is empty, we forced to evict from S.
        evict_from_s = False
        
        if s_size >= s_target_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: if we chose S but S is empty, try M
        if evict_from_s and not s_queue:
            evict_from_s = False
        # Failsafe: if we chose M but M is empty, try S
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Processing Small Queue ---
            if not s_queue: break 
            
            candidate = s_queue[0] # Peek
            
            # 1. Lazy Cleanup: Item already deleted externally
            if candidate not in cache_map:
                s_queue.popleft()
                continue
                
            cnt = freq.get(candidate, 0)
            
            # 2. Promotion Logic
            if cnt > 0:
                # It has been accessed while in S -> Promote to M
                s_queue.popleft()
                m_queue.append(candidate)
                # Deduct size from our local calculation of s_size
                s_size -= cache_map[candidate].size
                # We don't evict it, so we loop again.
                # Unlike standard FIFO, we reset/decay freq slightly? 
                # S3-FIFO typically clears freq on promotion to treat it as a "new" M resident
                freq[candidate] = 0 
            else:
                # 3. Eviction Logic
                # No hits while in S -> Evict
                victim_key = s_queue.popleft()
                
                # Add to Ghost
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Processing Main Queue ---
            if not m_queue: break
            
            candidate = m_queue[0] # Peek
            
            if candidate not in cache_map:
                m_queue.popleft()
                continue
                
            cnt = freq.get(candidate, 0)
            
            # 2. Reinsertion Logic (Second Chance)
            if cnt > 0:
                # It has hits -> Move to back of M, decay frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cnt - 1
            else:
                # 3. Eviction Logic
                # No hits since last check -> Evict
                victim_key = m_queue.popleft()
                # Items evicted from M generally don't go to Ghost (they had their chance)

    # --- Ghost Queue Size Management ---
    # We allow ghost queue to be larger than cache capacity to catch longer loops.
    # Typically M (Total items) is a good bound.
    target_ghost_len = len(cache_map) 
    
    while len(g_set) > target_ghost_len:
        if not g_queue: break
        rem = g_queue.popleft()
        if rem in g_set:
            g_set.remove(rem)
            
    # Clean head of ghost queue if needed
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit, increment frequency up to a cap.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    k = obj.key
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This item was seen recently but evicted.
        # This implies our cache was too small or S-queue evicted it too fast.
        # Promote directly to Main Queue (M) to protect it.
        m_queue.append(k)
        g_set.remove(k)
    else:
        # Standard Insert: Goes to Small Queue (S) for probation.
        s_queue.append(k)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the specific evicted object.
    '''
    global freq
    k = evicted_obj.key
    # We remove the frequency entry to keep memory usage bounded
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf6w3cvkc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnt7yn4dd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppdss0jh2.pickle

Iteration 28: New subsample score 0.229161 is better than old score 0.22500199999999998. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpchits06h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb71cbf1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr_v3enhn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo_toaher.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkfwca3jt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4g7xpxx9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4u2qj_ln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoua_36z_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8xefjfj5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjn6dqufd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw79ew04n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8uc95u_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkmc6e9or.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps86bntje.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpijzl4cmh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp91zsixpy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnuynkf2_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz06n5is3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpto759vch.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvp29fzxv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp323f3puc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfrfra4cc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppyl2wml6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcs60dkt6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxfb_igmp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1vroi73i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2mxd33g6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjdfoyns_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwifz2gaj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8v909pv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptxkza5ny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1y2di3jb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3lm7tjep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplkdzzdno.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptz5fudlt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpko3vtb_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprq_yqrnv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcu7brfr3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyrw_cj38.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq4263ieu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqor4tlru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxdexchb_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf3xdm94i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9nz5r_hi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_9904q0m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp63o5aafm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvq1wj0b_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptuatr126.pickle

Iteration 28: Full valset score for new program: 0.2525479166666666
Iteration 28: Full train_val score for new program: 0.2525479166666666
Iteration 28: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269806, 0.34879, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 28: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.391129, 0.849057, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 28: Full valset pareto front score: 0.263723
Iteration 28: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11}, {8, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {8}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}, {7}, {7}, {8, 5, 7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {3}, {8}, {8}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}]
Iteration 28: Best valset aggregate score so far: 0.2580827708333333
Iteration 28: Best program as per aggregate score on train_val: 5
Iteration 28: Best program as per aggregate score on valset: 5
Iteration 28: Best score on valset: 0.2580827708333333
Iteration 28: Best score on train_val: 0.2580827708333333
Iteration 28: Linear pareto front program index: 5
Iteration 28: New program candidate index: 11
Iteration 29: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg0r0vsq3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3lba4u0l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr8u01vm6.pickle

Iteration 29: Proposed new text for program: from collections import deque

# --- Global State ---
# S-Queue: FIFO queue for new insertions (Probationary)
s_queue = deque()
# M-Queue: FIFO queue for popular items (Protected)
m_queue = deque()
# G-Queue: FIFO queue for history of evicted S-items (Ghost)
g_queue = deque()
g_set = set()

# Frequency tracking: 
# Using a dictionary to track access counts (0 to MAX_FREQ)
freq = {}

# Current Byte Sizes for queues
s_bytes = 0
m_bytes = 0

# --- Constants ---
# S3-FIFO paper suggests 10% of cache size for S-queue is optimal for most traces.
SMALL_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Executes the S3-FIFO eviction policy.
    Victim is chosen from S (if full or low utility) or M (if low utility).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    total_capacity = cache_snapshot.capacity
    
    # Target size for the Small Queue
    target_s_size = total_capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # 1. Decide which queue to evict from based on S-Queue size
        # If S is larger than target, we prefer evicting from S to maintain the 10/90 split.
        # However, if M is empty, we must evict from S.
        evict_from_s = False
        
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If logic says S but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Processing Small Queue ---
            if not s_queue: break # Should be impossible if cache is full
            
            candidate_key = s_queue[0]
            
            # Sanity check: if item not in cache, just remove from queue
            if candidate_key not in cache:
                s_queue.popleft()
                continue
                
            obj_size = cache[candidate_key].size
            cnt = freq.get(candidate_key, 0)
            
            if cnt > 0:
                # HIT in S: Promote to M
                s_queue.popleft()
                s_bytes -= obj_size
                
                m_queue.append(candidate_key)
                m_bytes += obj_size
                
                # Unlike standard FIFO, S3-FIFO usually doesn't carry over frequency 
                # to M heavily, but giving it a small starting value helps retention.
                freq[candidate_key] = 0
            else:
                # NO HIT in S: Evict
                victim_key = s_queue.popleft()
                s_bytes -= obj_size
                
                # Add to Ghost to catch "one-hit wonders" that return later (scan resistance)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Processing Main Queue ---
            if not m_queue: break
            
            candidate_key = m_queue[0]
            
            if candidate_key not in cache:
                m_queue.popleft()
                continue
            
            obj_size = cache[candidate_key].size
            cnt = freq.get(candidate_key, 0)
            
            if cnt > 0:
                # HIT in M: Give second chance (Reinsert at tail)
                m_queue.popleft()
                m_queue.append(candidate_key)
                # Decay frequency
                freq[candidate_key] = cnt - 1
            else:
                # NO HIT (or expired freq): Evict
                victim_key = m_queue.popleft()
                m_bytes -= obj_size
                # Items evicted from M are usually not added to Ghost in S3-FIFO 
                # because they had their chance.
    
    # --- Ghost Queue Maintenance ---
    # Keep ghost queue bounded. Usually roughly number of items in cache is sufficient limit.
    # We use a count-based limit as an approximation since ghost items have no size.
    # Allowing it to be slightly larger aids in detecting larger looping patterns.
    max_ghost_items = len(cache) if len(cache) > 0 else 100
    
    while len(g_set) > max_ghost_items:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Clean head of ghost queue if it contains stale entries removed from set
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency counter.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Insert logic:
    If in Ghost -> Insert to M (Recall).
    Else -> Insert to S.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Reset/Init frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was evicted from S recently but needed again.
        # This implies it belongs in M (filter out transient scans).
        g_set.remove(k)
        m_queue.append(k)
        m_bytes += size
    else:
        # New Item: Insert into Small (S)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp02k8r851.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_skx_4i9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplsri0i23.pickle

Iteration 29: New subsample score 0.651239 is better than old score 0.634526. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqs2p7klj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk2momxk_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9cgqb5hb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpndwbz7r1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwzvkahcy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpljjknnjl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0hema531.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcu8poowu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprnjv_dfo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzi1yncns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3c4jph2y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpam267bl0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfxgrounp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp436al0pl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpym_y5xt1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwb8v9lk3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp35b2u5k_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdxupuqj7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg66sv6o9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprj71hldg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxxk60av2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc944z70z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpddcqs_g0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwscglxgv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppccddgo9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsay4_162.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppwqngxd7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptr9rv_9i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2z40onyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdia63_la.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl7raxfid.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp70ob5vp_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpocbbiem7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxz54nng1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5vnr3uf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw6fwvulb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7huhqtat.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqzdl2fet.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppuin2y8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph2_duxk8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1szww516.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuplbmmjt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc1cxemrm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2yjppg0p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjrz0xis6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmy9sxjrs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx05b54_t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpni06kyqp.pickle

Iteration 29: Full valset score for new program: 0.2525479166666666
Iteration 29: Full train_val score for new program: 0.2525479166666666
Iteration 29: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269806, 0.34879, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 29: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.271352, 0.391129, 0.849057, 0.892863, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 29: Full valset pareto front score: 0.263723
Iteration 29: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12}, {8, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {8}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}, {7}, {7}, {8, 5, 7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}, {8, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {3}, {8}, {8}, {5}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}, {2}, {7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {2, 5, 6, 7}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}]
Iteration 29: Best valset aggregate score so far: 0.2580827708333333
Iteration 29: Best program as per aggregate score on train_val: 5
Iteration 29: Best program as per aggregate score on valset: 5
Iteration 29: Best score on valset: 0.2580827708333333
Iteration 29: Best score on train_val: 0.2580827708333333
Iteration 29: Linear pareto front program index: 5
Iteration 29: New program candidate index: 12
Iteration 30: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsqjh96bd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0432_kv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpraypdjy3.pickle

Iteration 30: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# We use global variables to maintain state across function calls.
# S: Small Queue (FIFO), M: Main Queue (FIFO), G: Ghost Queue (FIFO of keys)
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}  # Tracks hits: 0, 1, 2, 3 (capped at MAX_FREQ)

# Byte tracking
s_bytes = 0
m_bytes = 0

# Tunable Constants
# 10% is empirically optimal for S3-FIFO to balance Recency (S) and Frequency (M)
SMALL_RATIO = 0.1  
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic:
    1. If S (Small) queue is larger than 10% of cache or M is empty, evict from S.
    2. Else, evict from M.
    3. Items evicted from S move to Ghost (G) to catch "quick re-access" patterns.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to operate on
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: if S is selected but empty, force M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Small Queue Operations ---
            if not s_queue: 
                # Should practically never happen if cache is full
                break 
                
            candidate = s_queue[0]
            
            # Validity check: if item was removed externally
            if candidate not in cache:
                s_queue.popleft()
                # Clean up metadata if lingering
                if candidate in freq: del freq[candidate]
                continue
                
            # Check frequency (0 means no hits since insertion)
            cnt = freq.get(candidate, 0)
            
            if cnt > 0:
                # Promotion: Item was hit while in S. Move to M.
                s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                m_queue.append(candidate)
                m_bytes += obj_size
                
                # Reset frequency so it has to prove itself in M
                freq[candidate] = 0
            else:
                # Eviction: Item was unused in S.
                victim_key = s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                # Add to Ghost (Recency history)
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
        else:
            # --- Main Queue Operations ---
            if not m_queue: 
                break
                
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
                
            cnt = freq.get(candidate, 0)
            
            if cnt > 0:
                # Second Chance: Reinsert at tail of M, decrement frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cnt - 1
            else:
                # Eviction: Exhausted frequency in M
                victim_key = m_queue.popleft()
                obj_size = cache[candidate].size
                m_bytes -= obj_size
                # Items evicted from M usually don't go to Ghost in S3-FIFO
                
    # --- Ghost Queue Maintenance ---
    # Keep ghost queue size roughly equal to the number of items in main cache
    # to track history without unlimited growth.
    target_ghost_len = len(cache) 
    while len(g_set) > target_ghost_len and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment hit count, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    - If in Ghost: It's a "Recall" -> Move directly to Main (M).
    - If new: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This was recently evicted from S, implying we made a mistake
        # or the working set is looping. Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard insertion into Probation (Small)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata. The queues are handled in evict(), 
    this handles the frequency map cleanup.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzvejkmiz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqyexzb2a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7w3a8p4v.pickle

Iteration 30: New subsample score 0.36282899999999996 is not better than old score 0.389449, skipping
Iteration 31: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj_avvets.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj6oyumi_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp59cmcxv4.pickle

Iteration 31: Proposed new text for program: # Improved S3-FIFO with Ordered Ghost Map and Reset Logic

# Global Data Structures
# s_queue: Small FIFO queue for new items (probationary)
# m_queue: Main FIFO queue for popular items
# g_map: Ghost Map (Ordered Dictionary) to track history of S-evicted items
# freq_map: Dictionary to track frequency bits (0-3)
s_queue = []
m_queue = []
g_map = {}
freq_map = {}

# Tuning parameter: 10% allocation for the Small queue
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Logic:
    1. Uses a strictly ordered Ghost Map (FIFO) instead of a random Set.
    2. Prioritizes evicting from S if S exceeds 10% of cache size.
    3. Resets frequency upon promotion to M to ensure items re-prove utility.
    '''
    global s_queue, m_queue, g_map, freq_map
    
    # Calculate target size for S based on current cache count
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    victim_key = None
    
    # Loop until a victim is chosen and removed
    while victim_key is None:
        
        # --- Decision: Process Small Queue (S) or Main Queue (M)? ---
        # We process S if it is larger than its target allocation, 
        # OR if M is empty (we have no choice but to evict from S).
        if len(s_queue) > s_target_size or not m_queue:
            if not s_queue:
                # Safety break, though logic shouldn't reach here if cache > 0
                break
                
            candidate = s_queue[0] # Peek at S-head
            usage = freq_map.get(candidate, 0)
            
            if usage > 0:
                # HIT in S: Promote to M
                s_queue.pop(0)
                m_queue.append(candidate)
                # Key Logic: Reset freq to 0. It used its "hit" to get promoted.
                # It must now earn hits in M to stay there.
                freq_map[candidate] = 0
            else:
                # MISS in S: Evict
                victim_key = s_queue.pop(0)
                # Add to Ghost Map (acting as FIFO)
                g_map[victim_key] = True
        
        # --- Process Main Queue (M) ---
        else:
            candidate = m_queue[0] # Peek at M-head
            usage = freq_map.get(candidate, 0)
            
            if usage > 0:
                # HIT in M: Give second chance, move to tail
                m_queue.pop(0)
                m_queue.append(candidate)
                freq_map[candidate] = usage - 1
            else:
                # MISS in M: Evict
                victim_key = m_queue.pop(0)
                # Standard S3-FIFO usually doesn't add M-evictions to Ghost,
                # as Ghost is primarily for catching early S-eviction mistakes.

    # Maintain Ghost Map Size
    # Keep Ghost history roughly equal to cache size. 
    # Since g_map is an ordered dict, the first key is the oldest.
    while len(g_map) > cache_count:
        oldest_ghost = next(iter(g_map))
        del g_map[oldest_ghost]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at 3.
    Lazy promotion: We do not move items between lists here, only update metadata.
    '''
    global freq_map
    key = obj.key
    curr_freq = freq_map.get(key, 0)
    
    if curr_freq < 3:
        freq_map[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - If key is in Ghost Map, it was a mistake to evict it -> Insert to M (Main).
    - Otherwise -> Insert to S (Small).
    '''
    global s_queue, m_queue, g_map, freq_map
    
    key = obj.key
    freq_map[key] = 0 # Start with 0 frequency
    
    if key in g_map:
        # Ghost Hit: Restore to Main Queue
        m_queue.append(key)
        # Remove from ghost since it's back in cache
        del g_map[key]
    else:
        # New Item: Insert into Small Queue
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq_map
    key = evicted_obj.key
    if key in freq_map:
        del freq_map[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplw3atx8m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdlasqcq8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvypjyq4a.pickle

Iteration 31: New subsample score 0.328278 is not better than old score 0.330265, skipping
Iteration 32: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_ib12cez.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpprtw60v6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpthq1t9jc.pickle

Iteration 32: Proposed new text for program: from collections import OrderedDict

# Enhanced ARC Data Structures using OrderedDict for O(1) access and moves
# T1: Recent Cache (L1) - items accessed once recently
t1 = OrderedDict()
t1_size = 0  # Total bytes in T1

# T2: Frequent Cache (L2) - items accessed at least twice
t2 = OrderedDict()
t2_size = 0  # Total bytes in T2

# B1: Ghost Recent (L1 Ghost) - history of evicted T1 keys
b1 = OrderedDict()

# B2: Ghost Frequent (L2 Ghost) - history of evicted T2 keys
b2 = OrderedDict()

# Adaptation Parameter
# p represents the target size (in bytes) for T1. 0 <= p <= capacity.
p = 0

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict based on ARC logic adapted for byte-size caches.
    Determines whether to evict from T1 (Recent) or T2 (Frequent) based on target `p`.
    '''
    global t1, t1_size, t2, t2_size, b1, b2, p

    victim_key = None
    
    # ARC Eviction Logic:
    # We evict from T1 if T1 is "too big" relative to target p.
    # Specifically: if len(t1) > p (in bytes), evict T1.
    # However, standard ARC also checks if the incoming item (obj) is in B2.
    # If the incoming item is in B2, ARC logic implies we should have grown T2,
    # making T1 effectively "too big" even if it equals p.
    
    # Since we cannot easily see if 'obj' is in B2 inside evict() without passing it specifically 
    # (obj is passed, but standard evict signatures usually just ask for a victim),
    # we rely on the state of T1 vs P.
    
    # Safety check: if T1 is empty, we must evict T2. If T2 is empty, we must evict T1.
    if not t1 and not t2:
        return None # Should not happen if cache is full

    if t1 and len(t1) > 0 and (t1_size > p or (not t2)):
        # Evict LRU from T1
        # OrderedDict popitem(last=False) pops the first (LRU) item
        victim_key, _ = t1.popitem(last=False)
        
        # We don't have the object size easily here without the cache dict
        # But we can get it from the snapshot before it's deleted
        if victim_key in cache_snapshot.cache:
            v_size = cache_snapshot.cache[victim_key].size
            t1_size -= v_size
            
            # Add to Ghost Recent (B1)
            b1[victim_key] = v_size # Store size only (or dummy value)
    else:
        # Evict LRU from T2
        victim_key, _ = t2.popitem(last=False)
        
        if victim_key in cache_snapshot.cache:
            v_size = cache_snapshot.cache[victim_key].size
            t2_size -= v_size
            
            # Add to Ghost Frequent (B2)
            b2[victim_key] = v_size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. If in T1 -> Move to T2 (MRU).
    2. If in T2 -> Move to T2 (MRU).
    '''
    global t1, t1_size, t2, t2_size
    
    k = obj.key
    size = obj.size
    
    if k in t1:
        # Promotion: T1 -> T2
        del t1[k]
        t1_size -= size
        
        t2[k] = size # Mark MRU
        t2_size += size
        
    elif k in t2:
        # Re-access: T2 -> T2 MRU
        del t2[k]
        t2[k] = size # Re-insert at end (MRU)
        # Size doesn't change on re-access unless object size changed (unlikely in this model)

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion of new item.
    This includes the core ARC adaptation logic for `p` based on Ghost hits.
    '''
    global t1, t1_size, t2, t2_size, b1, b2, p
    
    k = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Check Ghost Hits (Adaptation)
    if k in b1:
        # Hit in Ghost Recent (B1): We should have made T1 larger.
        # Delta calculation adapted for byte variance
        delta = 1
        if len(b1) < len(b2):
            # Avoid division by zero
            delta = len(b2) / len(b1)
        
        # Adapt P (grow T1 target)
        # We scale delta by size to make it byte-aware
        p = min(capacity, p + (delta * size))
        
        # Move logic: It was in history B1, now it's a "new" insert into cache.
        # In strict ARC, a hit in B1 promotes to T2 (Frequent), because it has been seen recently twice.
        del b1[k]
        
        # We add to T2 (Frequent)
        t2[k] = size
        t2_size += size
        
    elif k in b2:
        # Hit in Ghost Frequent (B2): We should have made T2 larger (T1 smaller).
        delta = 1
        if len(b2) < len(b1):
            delta = len(b1) / len(b2)
            
        # Adapt P (shrink T1 target)
        p = max(0, p - (delta * size))
        
        del b2[k]
        
        # Add to T2 (Frequent)
        t2[k] = size
        t2_size += size
        
    else:
        # Completely new item
        # Insert into T1 (Recent)
        t1[k] = size
        t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata and enforce Ghost List limits.
    '''
    global b1, b2
    
    # We maintain the ghost lists (B1/B2) to be roughly the size of the cache count.
    # ARC generally suggests |L1| + |L2| + |B1| + |B2| <= 2 * c.
    # To be safe and prevent memory leaks in Python, we cap them.
    
    target_len = max(100, len(cache_snapshot.cache)) 
    
    while len(b1) > target_len:
        b1.popitem(last=False) # Remove LRU from B1
        
    while len(b2) > target_len:
        b2.popitem(last=False) # Remove LRU from B2
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkm266j8j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9wfqh0ls.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp31v0ooyf.pickle

Iteration 32: New subsample score 0.874172 is not better than old score 0.9133290000000001, skipping
Iteration 33: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnpbfidgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf3_89_l4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk4qjifzs.pickle


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

Iteration 33: Exception during reflection/proposal: litellm.APIConnectionError: GeminiException - [Errno 101] Network is unreachable
Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 101, in handle_request
    raise exc
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 78, in handle_request
    stream = self._connect(request)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 124, in _connect
    stream = self._network_backend.connect_tcp(**kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 207, in connect_tcp
    with map_exceptions(exc_map):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [Errno 101] Network is unreachable

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/main.py", line 2875, in completion
    response = vertex_chat_completion.completion(  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py", line 2473, in completion
    response = client.post(url=url, headers=headers, json=data, logging_obj=logging_obj)  # type: ignore
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py", line 946, in post
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py", line 925, in post
    response = self.client.send(req, stream=stream)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [Errno 101] Network is unreachable

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/reflective_mutation.py", line 116, in propose
    new_texts = self.propose_new_texts(curr_prog, reflective_dataset, predictor_names_to_update)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/reflective_mutation.py", line 60, in propose_new_texts
    return self.adapter.propose_new_texts(candidate, reflective_dataset, components_to_update)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/caching_program_adapter.py", line 152, in propose_new_texts
    new_texts[name] = OpenEvolveProposalSignature.run(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/base.py", line 45, in run
    lm_out = lm(full_prompt).strip()
             ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/caching_program_adapter.py", line 33, in _call_lm
    completion = self.litellm.completion(
                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/utils.py", line 1381, in wrapper
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/utils.py", line 1250, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/main.py", line 3774, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2328, in exception_type
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2297, in exception_type
    raise APIConnectionError(
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: GeminiException - [Errno 101] Network is unreachable

Iteration 33: Reflective mutation did not propose a new candidate
Iteration 34: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwopj_w0s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaipc652g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvgsgzyr7.pickle

Iteration 34: Proposed new text for program: from collections import deque

# Global data structures for S3-FIFO state
# S = Small Queue (FIFO), M = Main Queue (FIFO), G = Ghost Queue (FIFO)
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()

# Frequency tracking for resident items
# We use this to determine utility (0 or >0)
freq = {}

# We track bytes to manage the 10% vs 90% ratio roughly
s_bytes = 0
m_bytes = 0

# Configuration
SMALL_QUEUE_RATIO = 0.1  # Target 10% of cache space for S
MAX_FREQ = 2             # Cap frequency to prevent "infinite" protection

def evict(cache_snapshot, obj):
    '''
    Selects a victim using S3-FIFO logic.
    Priority:
    1. Clean up Small queue (move hit items to M, evict miss items to G).
    2. If S is small enough, clean up Main queue (demote hit items, evict miss items).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes

    cache = cache_snapshot.cache
    total_capacity = cache_snapshot.capacity
    
    victim_key = None

    while victim_key is None:
        # Determine target size for Small queue
        target_s_size = total_capacity * SMALL_QUEUE_RATIO
        
        # We process the Small Queue if it's larger than target OR if Main is empty.
        # This ensures we don't starve Main, but we keep Small transient.
        if s_bytes >= target_s_size or not m_queue:
            if not s_queue:
                # Should not happen if cache is full, but safety break
                break
                
            candidate = s_queue[0]
            
            # Sanity check: if candidate somehow missing from cache, just pop
            if candidate not in cache:
                s_queue.popleft()
                continue

            # Check if accessed while in Small
            count = freq.get(candidate, 0)
            
            if count > 0:
                # HIT: Promote to Main
                s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                m_queue.append(candidate)
                m_bytes += obj_size
                
                # Reset freq for its new life in Main
                freq[candidate] = 0
            else:
                # MISS: Evict from cache, add to Ghost
                victim_key = s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                # Add to Ghost to catch quick re-access patterns
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # Process Main Queue
            if not m_queue:
                # Should be caught by "if not s_queue" logic above, but safety
                break
                
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            count = freq.get(candidate, 0)
            
            if count > 0:
                # HIT: Give Second Chance (Reinsert at tail, decrement freq)
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = count - 1
            else:
                # MISS: Evict from cache. 
                # Note: S3-FIFO usually does NOT add M-evicted items to Ghost.
                victim_key = m_queue.popleft()
                obj_size = cache[candidate].size
                m_bytes -= obj_size

    # Manage Ghost Queue Size
    # Heuristic: Keep ghost queue size roughly equal to the number of Main items
    # or a fixed number of entries if main is empty.
    max_ghost_items = max(len(m_queue), 100) 
    
    while len(g_set) > max_ghost_items and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Cleanup ghost head if out of sync
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: increment frequency count up to MAX_FREQ.
    This simple bit serves as the "accessed" bit.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If key in Ghost -> Insert into Main (restore).
    2. Else -> Insert into Small (probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency 
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S, but accessed again.
        # This implies it belongs in the working set. Promote to M directly.
        g_set.remove(k)
        m_queue.append(k)
        m_bytes += size
    else:
        # Standard Insert: Put in Small queue for probation.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaa0bv34r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo1jzfgxt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi20g3v7y.pickle

Iteration 34: New subsample score 0.617383 is not better than old score 0.641523, skipping
Iteration 35: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyveyc57a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvp74po0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3xfv_mb5.pickle

Iteration 35: Proposed new text for program: import math

# Leaper/LFU-k/Size-aware Hybrid Global State
# We track frequency and recency.
# We prioritize keeping items that are frequently accessed, but we also penalize large objects
# to maximize the number of items in the cache (hit density).

# Maps key -> metadata dictionary
# metadata = {'freq': int, 'last_access': int, 'size': int}
metadata_store = {}

# Time counter to track recency
current_time = 0

# Track historical frequency for "ghost" entries to handle re-entries better
history_freq = {}

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict.
    
    Strategy: Hyperbolic Caching / GDSF-like scoring.
    We calculate a score for every object in the cache. The object with the lowest score is evicted.
    
    Score = (Frequency ^ Weight_Freq) / (Size ^ Weight_Size) * Recency_Factor
    
    However, sorting the entire cache on every eviction is O(N). To keep it efficient but effective:
    1. We maintain a small sample of candidates or iterate effectively.
    2. Given Python's speed constraints in simulation, we can iterate over the cache keys if N isn't massive.
    
    For this implementation, we use a scoring function:
    Score = (Frequency * 10000) / Size + (1 if Recent else 0)
    
    Actually, a pure LFU with Size penalty (GDSF - Greedy Dual Size Frequency) works best for variable size caches.
    H(p) = L + (Frequency(p) * Cost) / Size(p)
    Where L is an aging factor (value of H(p) of the last evicted object).
    '''
    global metadata_store, history_freq
    cache = cache_snapshot.cache
    
    # "L" factor dynamic aging
    # We find the object with the minimum H-value to evict.
    victim_key = None
    min_score = float('inf')
    
    # Analyze candidates
    # To avoid O(N) every single time if N is huge, we could optimize, 
    # but for typical trace simulations, iterating keys is acceptable for high accuracy.
    
    # Current time (from snapshot) helps in tie-breaking recency
    now = cache_snapshot.access_count
    
    for key, cached_obj in cache.items():
        if key not in metadata_store:
            # Should not happen if sync is correct, but safe fallback
            # Treat as freq 1, just inserted
            score = 1.0 / cached_obj.size
        else:
            meta = metadata_store[key]
            freq = meta['freq']
            size = meta['size']
            last_access = meta['last_access']
            
            # GDSF-like score calculation:
            # Priority = Frequency / Size
            # We add a tiny recency component to break ties (prefer keeping MRU among equals)
            
            # Why Frequency^Log? To dampen the effect of massive counts vs 1 or 2.
            # Why / Size? To favor small objects (more objects = more potential hits).
            
            # Using a mixed heuristic:
            # 1. Base Score: Frequency
            # 2. Size Penalty: sqrt(Size) or just Size. 
            #    (Size is linear penalty, Frequency is linear benefit)
            
            # Heuristic: Freq / Size is standard GDSF.
            # However, purely size-based eviction kills large items that are moderately frequent.
            # Let's use: (Frequency ^ 1.5) / Size
            
            # Add recency boost: If accessed very recently, boost score temporarily 
            # to prevent "cache thrashing" of a newly inserted item before it builds frequency.
            recency_boost = 0
            if (now - last_access) < (len(cache) * 0.1): # Top 10% recent
                recency_boost = 1.0
            
            score = (math.pow(freq, 1.8) / size) + recency_boost
        
        if score < min_score:
            min_score = score
            victim_key = key
        elif score == min_score:
            # Tie breaker: LRU (evict the older one)
            # If current victim was accessed LATER than this candidate, keep current, evict candidate.
            # We want to evict the one with smallest last_access (LRU).
            if metadata_store[key]['last_access'] < metadata_store[victim_key]['last_access']:
                victim_key = key

    # If somehow we failed to find a victim (cache empty?), return None
    if victim_key is None and cache:
        # Fallback to arbitrary key
        return next(iter(cache))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global metadata_store, current_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        metadata_store[key]['freq'] += 1
        metadata_store[key]['last_access'] = current_time
    else:
        # Reconstruct metadata if missing (shouldn't happen on hit usually)
        # Check history first
        base_freq = history_freq.get(key, 0)
        metadata_store[key] = {
            'freq': base_freq + 1,
            'last_access': current_time,
            'size': obj.size
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. If key exists in history (it was evicted previously), restore some frequency count.
    '''
    global metadata_store, history_freq, current_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Check if we remember this object
    initial_freq = 1
    if key in history_freq:
        # Decay history slightly so we don't hold onto ancient history forever
        # But give it a boost over a brand new item.
        initial_freq = history_freq[key] + 1
    
    metadata_store[key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'size': obj.size
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Save the frequency to history (Ghost).
    2. Remove from active metadata.
    '''
    global metadata_store, history_freq
    
    if evicted_obj:
        evicted_key = evicted_obj.key
        
        if evicted_key in metadata_store:
            # Store frequency in history for potential future re-entry
            # Cap history to avoid infinite growth? 
            # In simulation, we rely on dictionary efficiency, but let's just store the int.
            freq = metadata_store[evicted_key]['freq']
            history_freq[evicted_key] = freq
            
            del metadata_store[evicted_key]
    
    # Housekeeping: Prevent history_freq from growing infinitely (Memory leak protection)
    # If history is > 2x Cache Capacity (approx items), purge oldest or random
    if len(history_freq) > 100000: # Arbitrary large cap for simulation safety
        # Clear half to be safe and simple
        history_freq.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt0dekqu0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg3vepe1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp40ly2xzf.pickle

Iteration 35: New subsample score 0.8429840000000001 is better than old score 0.744139. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpahov7r7t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpseo4sk5q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxdscdxbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplm1xbxmj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6hboc3oz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3uii3_uf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn187_8tg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbvcmtuix.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9r6r37pq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0fo7usxo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptqba3pc5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplbhforim.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy___ssnf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxljnub2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpooflny3f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppqnrgmd7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeh94s9lv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg_znb7ub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2ot7jqi0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi5idw8bx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp565rz3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_on_tyna.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5r34rte7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpafzdclmn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpck2piwq5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9buqr1d4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpim3wkx45.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkyuikche.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpky9_ttvb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptco26fpo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpch6jq21w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl68z43dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphjyd4q_5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg8nhbavh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv69fpxwz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn3qkmj4_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2mv1t26d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2plo0geq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyyxp09k7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptajwhqhb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjf6b89tm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmq557o81.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz4jeeb63.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp86vm1d16.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxwx8_ru0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3ft54kf5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk981jzjb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphzqf7xol.pickle

Iteration 35: Full valset score for new program: 0.2369623541666667
Iteration 35: Full train_val score for new program: 0.2369623541666667
Iteration 35: Individual valset scores for new program: [0.50043, 0.470932, 0.483607, 0.434783, 0.495106, 0.485415, 0.26555, 0.458907, 0.540937, 0.531017, 0.091667, 0.392984, 0.035058, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.369715, 0.026556, 0.058672, 0.058672, 0.288554, 0.359879, 0.796425, 0.894232, 0.039832, 0.038636, 0.045558, 9.6e-05, 3.6e-05, 0.721239, 0.083333, 0.067079, 0.018281, 0.641937, 0.125461, 0.041586, 0.039096, 0.043257, 0.052632, 0.366667, 0.041854, 0.041795, 0.466258, 0.078431]
Iteration 35: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 35: Full valset pareto front score: 0.2641215
Iteration 35: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {2, 5, 6, 7, 13}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {7}]
Iteration 35: Best valset aggregate score so far: 0.2580827708333333
Iteration 35: Best program as per aggregate score on train_val: 5
Iteration 35: Best program as per aggregate score on valset: 5
Iteration 35: Best score on valset: 0.2580827708333333
Iteration 35: Best score on train_val: 0.2580827708333333
Iteration 35: Linear pareto front program index: 5
Iteration 35: New program candidate index: 13
Iteration 36: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb9wxg1ow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyoq4cv09.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8oq9p1yq.pickle

Iteration 36: Proposed new text for program: from collections import deque

# S3-FIFO Global State
# s_queue: Small queue (FIFO) - stores new objects
# m_queue: Main queue (FIFO) - stores frequently accessed objects
# g_set: Ghost set - stores keys of objects recently evicted from s_queue
# freq: Frequency map - stores access counts (bits) for objects currently in cache
s_queue = deque()
m_queue = deque()
g_queue = deque() # Queue to maintain order for ghost eviction
g_set = set()
freq = {}

# Byte tracking
s_bytes = 0
m_bytes = 0

# Constants
SMALL_RATIO = 0.1  # Target size for S queue relative to capacity
MAX_FREQ = 2       # Cap frequency bits to simulate 2-bit counter

def evict(cache_snapshot, obj):
    '''
    Executes the S3-FIFO eviction policy.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to evict from
        # If S is larger than its target size, we prefer evicting from S.
        # If M is empty, we must evict from S.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: if S is selected but empty, try M
        if evict_from_s and not s_queue:
            evict_from_s = False
            if not m_queue:
                # Should not happen in a full cache, but return None to be safe
                return None
        
        if evict_from_s:
            # --- Processing Small Queue (S) ---
            candidate = s_queue[0]
            
            # Sanity check: if item not in cache (stale), remove and continue
            if candidate not in cache:
                s_queue.popleft()
                continue
                
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # HIT in S: Promote to M
                # The paper suggests clearing frequency when moving S -> M
                s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                m_queue.append(candidate)
                m_bytes += obj_size
                freq[candidate] = 0 
            else:
                # MISS in S: Evict
                # This item was inserted but not accessed again.
                victim_key = s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                # Add to Ghost (only if evicted from S)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Processing Main Queue (M) ---
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # HIT in M: Reinsert at tail of M (Second Chance) and decrement freq
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = candidate_freq - 1
            else:
                # EXPIRED in M: Evict
                victim_key = m_queue.popleft()
                obj_size = cache[candidate].size
                m_bytes -= obj_size
                # Items evicted from M are NOT added to Ghost in S3-FIFO
    
    # Ghost Queue Cleanup: Limit size to prevent memory bloat
    # Keeping ghost queue proportional to cache item count is usually sufficient
    target_ghost_len = len(cache)
    while len(g_set) > target_ghost_len:
        if not g_queue:
            g_set.clear()
            break
        
        rem = g_queue.popleft()
        if rem in g_set:
            g_set.remove(rem)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency bit on cache hit.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    Check Ghost to decide between Main (M) and Small (S) queues.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S.
        # This indicates a longer loop or working set > S size.
        # Insert directly into M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert into S.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpacd7n3hm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp57q943r5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptpjj5zij.pickle

Iteration 36: New subsample score 0.598978 is not better than old score 0.6254569999999999, skipping
Iteration 37: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2167uw1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpywtnuyri.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2v6hvuud.pickle

Iteration 37: Proposed new text for program: from collections import deque

# --------------------------------------------------------
# Optimized S3-FIFO Implementation
# --------------------------------------------------------

# Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO) - Keys only
g_set = set()          # Ghost Set - Fast lookup
freq = {}              # Frequency counter for resident objects
ghost_freq = {}        # Tracks hits on ghost items to differentiate scan vs reuse

# Metadata
s_bytes = 0            # Bytes in Small Queue
m_bytes = 0            # Bytes in Main Queue
total_accesses = 0     # Local counter to track decay intervals

# Tuning Parameters
MAX_FREQ = 3           # Frequency cap
BASE_S_RATIO = 0.1     # Initial Small Queue Ratio
curr_s_ratio = 0.1     # Adaptive Small Queue Ratio

def evict(cache_snapshot, obj):
    '''
    Eviction logic using an adaptive S3-FIFO strategy.
    Prioritizes keeping the Main (M) queue populated with high-frequency items
    while using the Small (S) queue to filter out one-hit wonders (scans).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    global curr_s_ratio, ghost_freq

    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target size for Small Queue based on adaptive ratio
    s_target_bytes = capacity * curr_s_ratio
    
    candidate_key = None
    
    # We must loop until we successfully eject a victim
    while candidate_key is None:
        
        # --- Policy Decision: Evict from S or M? ---
        # 1. If S is larger than its target allocation, we force eviction from S.
        # 2. If M is empty, we must evict from S.
        # 3. Otherwise, we check M.
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
            
        # Failsafe logic for empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Small Queue (Probation) Eviction ---
            if not s_queue: break 
            
            victim = s_queue[0] 
            
            # Lazy removal if deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: Item was accessed while in probation. Move to Main.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency so it has to prove itself in M
                freq[victim] = 0
            else:
                # Eviction: Item failed probation (no accesses).
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost Queue to detect "Recalled" items later
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
                    # We track ghost frequency to see if this eviction was a "mistake"
                    ghost_freq[candidate_key] = 0
        
        else:
            # --- Main Queue (Protected) Eviction ---
            if not m_queue: break
            
            victim = m_queue[0]
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
            
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Reinsert into M, but decay frequency
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1
            else:
                # Evict from Main: It was once useful, but recently cold.
                # Note: We typically don't add M-evictions to Ghost in S3-FIFO 
                # because they already had a chance to stay.
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # Keep ghost queue roughly the size of the object count (or 2x if memory permits logic)
    # to track history.
    current_item_count = len(cache_map)
    # A larger ghost queue helps capture longer loops
    target_ghost_count = max(current_item_count, 100) 
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
        if oldest in ghost_freq:
            del ghost_freq[oldest]

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency on hit.
    '''
    global freq
    k = obj.key
    # Standard frequency capping
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    Logic:
    1. If in Ghost -> It's a "Recall". Promote to M.
    2. Else -> Insert into S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    global curr_s_ratio, ghost_freq
    
    k = obj.key
    size = obj.size
    
    # Initialize resident frequency
    freq[k] = 0
    
    if k in g_set:
        # --- Ghost Hit (Recall) ---
        # This item was evicted recently but requested again.
        # This implies our S-queue might be too small (evicting too fast)
        # or the item is part of a loop.
        
        # Promote directly to Main
        m_queue.append(k)
        m_bytes += size
        
        # Remove from Ghost structures
        g_set.remove(k)
        if k in ghost_freq:
            del ghost_freq[k]
            
        # Adaptation: If we are hitting ghosts often, our probation (S) is too strict.
        # Slightly reduce S ratio (give more space to M) to protect working set? 
        # Actually, standard S3-FIFO logic suggests if ghost hits are high, 
        # we might need to *churn* S faster? 
        # Let's keep it simple: If ghost hit, it's a qualified item -> Main.
    else:
        # --- New Insert ---
        # Goes to Small Queue (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up global metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm3n4c98n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpymr6ee2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu0_yg56c.pickle

Iteration 37: New subsample score 0.771216 is better than old score 0.7696890000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp_8puyfm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvtyf052r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp70yn82fa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2uuchzyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpovtfpeab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl5z27zcq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgup45hb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmfmyp0br.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvo35wwh1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7ku5fss7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7s3vncqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz4yjaays.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnyq3dhg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxud709a6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb829ish1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphx7q2b9r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5cj1p9lc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqy1x7q3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph_dfk7vp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppyeibg9p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp68g42zqe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbnzm4e27.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyvjzy7da.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp639k6216.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwhqh8yv3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1i0fk3f6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfwlfpxt3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqvx_ihin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpldss6eu_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp795up4ct.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc5f0p0gz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1k05umks.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9lk6s2yc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvzptg0ka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3wn2heo9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgw0at_wm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw89gepm3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplcgy_zab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdchov28a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps88b4pss.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_ras1blj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb3wmdde6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmperfkxvnu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphdt3srv6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3evqa7pz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_hz2347m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkucvbe5h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnz2h1d0q.pickle

Iteration 37: Full valset score for new program: 0.23852293749999995
Iteration 37: Full train_val score for new program: 0.23852293749999995
Iteration 37: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.083333, 0.39476, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269795, 0.356855, 0.815293, 0.892863, 0.038934, 0.038636, 0.045558, 0.004058, 0.004769, 0.75902, 0.083333, 0.067961, 0.037347, 0.640392, 0.125461, 0.041031, 0.043372, 0.03924, 0.052632, 0.366667, 0.031181, 0.040673, 0.466258, 0.058824]
Iteration 37: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 37: Full valset pareto front score: 0.2641215
Iteration 37: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {7}]
Iteration 37: Best valset aggregate score so far: 0.2580827708333333
Iteration 37: Best program as per aggregate score on train_val: 5
Iteration 37: Best program as per aggregate score on valset: 5
Iteration 37: Best score on valset: 0.2580827708333333
Iteration 37: Best score on train_val: 0.2580827708333333
Iteration 37: Linear pareto front program index: 5
Iteration 37: New program candidate index: 14
Iteration 38: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdzv_yn2t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjpeo90sq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9vgla1wg.pickle

Iteration 38: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# Global state management
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters: 0, 1, 2 (max)

# Tracks the size (in bytes) of items currently residing in the respective queues
s_bytes = 0
m_bytes = 0

# Configuration
# 10% of cache size is allocated to the Small queue. This allows the Main queue
# (90%) to hold high-frequency items, while Small acts as a filter for one-hit wonders.
SMALL_RATIO = 0.1
MAX_FREQ = 2  # S3-FIFO generally works well with a cap of 2 or 3.

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # --- Logic Step 1: Decide which queue to evict from ---
        # We evict from Small (S) if it exceeds its target size ratio OR if Main (M) is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # If we planned to evict from S but it's empty, we must evict from M.
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue ---
            if not s_queue: break # Should be covered by logic above, but safety first
            
            candidate = s_queue[0]
            
            # Lazy Removal: If candidate is not in cache, drop it and retry
            if candidate not in cache:
                s_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: It was hit while in S. Move to M.
                s_queue.popleft()
                # Update byte tracking
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                m_queue.append(candidate)
                m_bytes += obj_size
                
                # Reset frequency so it must earn its place in M again
                freq[candidate] = 0
            else:
                # Eviction: No hits in S. It's a "one-hit wonder" or cold.
                victim_key = s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                # Add to Ghost Queue to detect "quick return" patterns
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Eviction from Main Queue ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            # Lazy Removal
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail of M, decrement freq
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Expired in M.
                victim_key = m_queue.popleft()
                obj_size = cache[candidate].size
                m_bytes -= obj_size
                # Items evicted from M usually don't go to Ghost in standard S3-FIFO
    
    # --- Ghost Queue Cleanup ---
    # Keep ghost queue size proportional to the number of items in cache.
    # A multiplier of ~1.0 to 2.0 count usually suffices.
    max_ghost_items = len(cache) if len(cache) > 0 else 100 
    
    while len(g_set) > max_ghost_items:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Clean head of ghost queue if it contains stale entries (already removed from set)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    # Only increment if it's currently tracked (it should be)
    if k in freq:
        curr = freq[k]
        if curr < MAX_FREQ:
            freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    Logic:
    1. If key in Ghost Cache -> It was evicted recently -> Insert to Main (M).
    2. Else -> Standard Insert -> Insert to Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Init frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Restore to Main
        m_queue.append(k)
        m_bytes += size
        
        # Remove from ghost
        g_set.remove(k)
        # Note: We don't remove from g_queue immediately (O(N)), we let lazy cleanup handle it.
    else:
        # New Insert: Start in Small
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjeoi0gb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk0qacoij.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvl8ae6tp.pickle

Iteration 38: New subsample score 1.071348 is not better than old score 1.075768, skipping
Iteration 39: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg5w9jv2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppymc591f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk1bhqjo4.pickle

Iteration 39: Proposed new text for program: from collections import deque

# Global State for Byte-Aware ARC
# T1: Recent Cache (Items accessed once)
t1_queue = deque()
t1_set = set()
t1_bytes = 0

# T2: Frequent Cache (Items accessed >= twice)
t2_queue = deque()
t2_set = set()
t2_bytes = 0

# B1: Ghost Recent (Evicted from T1) - Keys only
b1_queue = deque()
b1_set = set()

# B2: Ghost Frequent (Evicted from T2) - Keys only
b2_queue = deque()
b2_set = set()

# Adaptation Parameter
# p represents the target size (in bytes) for T1.
p = 0

def evict(cache_snapshot, obj):
    '''
    Decides whether to evict from T1 or T2 based on the target size `p`.
    This logic maintains the adaptive balance between Recency (T1) and Frequency (T2).
    '''
    global t1_queue, t1_set, t1_bytes
    global t2_queue, t2_set, t2_bytes
    global b1_queue, b1_set, b2_queue, b2_set
    global p

    cache = cache_snapshot.cache
    victim_key = None
    
    # We need to ensure valid candidates exist. 
    # While t1_set might not be empty, the queue might contain stale entries 
    # (though our update logic tries to keep them clean). 
    # We loop to find a valid victim that is actually in the cache.
    
    # Decision Logic:
    # We evict from T1 if T1 is larger than the target `p`.
    # However, if B1 is growing (implied by hit logic elsewhere), we might protect T1.
    # Standard ARC rule: Replace(p)
    # if ( |T1| > p ) or ( B2 has items and |T1| == p ) -> delete T1
    # else -> delete T2
    
    # Adapted for Byte Size:
    # If t1_bytes > p, we generally want to shrink T1.
    
    evict_from_t1 = False
    
    # Robust check: if one list is empty, must evict from the other
    if t1_bytes > 0 and t2_bytes == 0:
        evict_from_t1 = True
    elif t2_bytes > 0 and t1_bytes == 0:
        evict_from_t1 = False
    elif t1_bytes > 0 and t2_bytes > 0:
        # Both have items, use p to decide
        # If T1 is over its budget, evict from T1
        if t1_bytes > p:
            evict_from_t1 = True
        else:
            evict_from_t1 = False
    else:
        # Should not happen if cache is full
        return None

    if evict_from_t1:
        # Pop valid item from T1
        while t1_queue:
            victim_key = t1_queue.popleft()
            if victim_key in t1_set:
                t1_set.remove(victim_key)
                if victim_key in cache:
                    t1_bytes -= cache[victim_key].size
                
                # Move to B1 (Ghost Recent)
                b1_queue.append(victim_key)
                b1_set.add(victim_key)
                break
    else:
        # Pop valid item from T2
        while t2_queue:
            victim_key = t2_queue.popleft()
            if victim_key in t2_set:
                t2_set.remove(victim_key)
                if victim_key in cache:
                    t2_bytes -= cache[victim_key].size
                
                # Move to B2 (Ghost Frequent)
                b2_queue.append(victim_key)
                b2_set.add(victim_key)
                break
    
    # Manage Ghost List Sizes (Capacity heuristic)
    # In pure ARC, |T1|+|B1| = C and |T2|+|B2| = 2C.
    # We limit ghost lists to avoid infinite growth.
    MAX_GHOST_ITEMS = 2000 # Arbitrary safe limit for metadata overhead
    
    while len(b1_set) > MAX_GHOST_ITEMS:
        rem = b1_queue.popleft()
        if rem in b1_set: b1_set.remove(rem)
            
    while len(b2_set) > MAX_GHOST_ITEMS:
        rem = b2_queue.popleft()
        if rem in b2_set: b2_set.remove(rem)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    An item was accessed. It must be moved to MRU position in T2.
    '''
    global t1_queue, t1_set, t1_bytes
    global t2_queue, t2_set, t2_bytes
    
    key = obj.key
    size = obj.size
    
    # Case 1: Hit in T1 (Recent) -> Move to T2 (Frequent)
    if key in t1_set:
        t1_set.remove(key)
        t1_bytes -= size
        # Note: We leave the key in t1_queue; it will be skipped/ignored when popped
        
        t2_set.add(key)
        t2_queue.append(key)
        t2_bytes += size
        
    # Case 2: Hit in T2 (Frequent) -> Move to MRU of T2
    elif key in t2_set:
        # It's already in T2, we just need to update recency.
        # Ideally we move it to the back of t2_queue.
        # To avoid O(N) remove, we can just append it again and handle duplicates
        # or stale entries during eviction. 
        # Ideally: t2_queue.remove(key) is costly. 
        # Optimization: Just append. During eviction, we check `if key in t2_set`.
        # But wait, if we just append, we increase queue size but not byte size/set size.
        # This is safe for logic as long as eviction verifies set membership.
        t2_queue.append(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    Handle logic for B1/B2 hits (adaptation) or new insertions.
    '''
    global t1_queue, t1_set, t1_bytes
    global t2_queue, t2_set, t2_bytes
    global b1_queue, b1_set, b2_queue, b2_set
    global p
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Check Ghost Hits (Adaptation)
    if key in b1_set:
        # Hit in B1 (Deleted from Recent): T1 was too small.
        # Increase p.
        delta = size # Base delta is the size of the object
        
        # Adaptation based on relative sizes of ghost lists
        b1_len = len(b1_set)
        b2_len = len(b2_set)
        
        if b1_len >= b2_len:
             p = min(capacity, p + delta)
        else:
             # If B2 is larger, we augment the step
             p = min(capacity, p + delta * (b2_len / b1_len))
        
        # Move from B1 to T2 (Promotion to frequent)
        b1_set.remove(key)
        # Add to T2
        t2_set.add(key)
        t2_queue.append(key)
        t2_bytes += size
        
    elif key in b2_set:
        # Hit in B2 (Deleted from Frequent): T2 was too small (so T1 was too big).
        # Decrease p.
        delta = size
        
        b1_len = len(b1_set)
        b2_len = len(b2_set)
        
        if b2_len >= b1_len:
            p = max(0, p - delta)
        else:
            p = max(0, p - delta * (b1_len / b2_len))
            
        # Move from B2 to T2 (Reclaim to frequent)
        b2_set.remove(key)
        # Add to T2
        t2_set.add(key)
        t2_queue.append(key)
        t2_bytes += size
        
    else:
        # Totally new item (or forgotten).
        # Add to T1 (Recent)
        t1_set.add(key)
        t1_queue.append(key)
        t1_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Metadata is handled in evict(), but we can ensure consistency here.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1rru21p6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprf0v9733.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcjpp8dg2.pickle

Iteration 39: New subsample score 0.954529 is not better than old score 1.274335, skipping
Iteration 40: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpetsl50a7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplefi3mvs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppit9cc4r.pickle

Iteration 40: Proposed new text for program: import math

# Metadata storage
# Maps key -> {'freq': int, 'priority': float, 'last_access': int}
metadata_store = {}

# Frequency History (Ghost Cache)
# Maps key -> int (frequency count)
# Used to remember the popularity of items that were recently evicted.
history_freq = {}

# The Aging Factor (L)
# This increases monotonically over time as items are evicted.
# It serves as the "base level" for new items entering the cache.
L_value = 0.0

def get_priority(freq, size, current_L):
    """
    Calculates GDSF priority.
    Priority = L + (Frequency / Size)
    
    We use specific weighting:
    - Frequency is linear.
    - Size is linear penalty.
    """
    # Safety for 0 size (though unlikely in trace)
    if size <= 0: 
        safe_size = 1 
    else: 
        safe_size = size
        
    return current_L + (float(freq) / float(safe_size))

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the lowest GDSF priority.
    Updates the global aging factor L to the priority of the victim.
    '''
    global metadata_store, L_value
    cache = cache_snapshot.cache
    
    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')
    
    # Iterate through cached items to find the one with minimum priority.
    # We scan O(N). While not O(1), for simulation traces this ensures maximum hit rate accuracy.
    for key in cache:
        if key in metadata_store:
            entry = metadata_store[key]
            p = entry['priority']
            last_access = entry['last_access']
        else:
            # Sync fallback: If metadata is missing, calculate a temporary priority
            # Treat as freq 1, inserted at current L
            cached_obj = cache[key]
            p = get_priority(1, cached_obj.size, L_value)
            last_access = 0 # Assume old

        # We look for the smallest priority
        if p < min_priority:
            min_priority = p
            victim_key = key
            min_last_access = last_access
        elif p == min_priority:
            # Tie-breaker: LRU (Smallest last_access wins/dies)
            if last_access < min_last_access:
                victim_key = key
                min_last_access = last_access

    # If cache is not empty but we found nothing (should not happen), pick first
    if victim_key is None and cache:
        victim_key = next(iter(cache))
        
    # UPDATE L:
    # In GDSF, L is set to the priority of the evicted object.
    # This raises the bar for all future insertions.
    if victim_key:
        L_value = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using the CURRENT L value.
       (Bringing the object to the "present" timeframe).
    '''
    global metadata_store, L_value
    
    key = obj.key
    now = cache_snapshot.access_count
    
    if key in metadata_store:
        metadata_store[key]['freq'] += 1
        metadata_store[key]['last_access'] = now
        # Re-calculate priority with current L
        metadata_store[key]['priority'] = get_priority(
            metadata_store[key]['freq'], 
            obj.size, 
            L_value
        )
    else:
        # Should not happen on hit, but self-heal
        base_freq = history_freq.get(key, 0) + 1
        metadata_store[key] = {
            'freq': base_freq,
            'last_access': now,
            'priority': get_priority(base_freq, obj.size, L_value)
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Inherit frequency from history if available.
    3. Calculate initial priority relative to current L.
    '''
    global metadata_store, history_freq, L_value
    
    key = obj.key
    now = cache_snapshot.access_count
    
    # Frequency restoration logic
    freq = 1
    if key in history_freq:
        freq = history_freq[key] + 1
        # Optional: Cap max restore frequency to prevent ancient massive history 
        # from dominating immediately?
        # For pure hit rate optimization, LFU history is usually good to keep.
    
    metadata_store[key] = {
        'freq': freq,
        'last_access': now,
        'priority': get_priority(freq, obj.size, L_value)
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Store frequency in history (Ghost).
    2. Cleanup metadata.
    '''
    global metadata_store, history_freq
    
    if evicted_obj:
        evicted_key = evicted_obj.key
        
        if evicted_key in metadata_store:
            freq = metadata_store[evicted_key]['freq']
            # Save to history
            history_freq[evicted_key] = freq
            
            # Remove from active metadata
            del metadata_store[evicted_key]
    
    # Maintenance: Limit history size to prevent memory issues
    # If history grows too large, clear it or random sample remove.
    # 50k limit is usually safe for simulations.
    if len(history_freq) > 50000:
        history_freq.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplkix8n33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6mn2s61t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprl4dqm93.pickle

Iteration 40: New subsample score 0.490284 is not better than old score 0.5180980000000001, skipping
Iteration 41: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvw2is0k4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmvhla6b5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_uzemf2g.pickle

Iteration 41: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# s_queue: Small FIFO queue (deque) for probationary items
# m_queue: Main FIFO queue (deque) for popular items
# g_queue: Ghost FIFO queue (deque) to track order of recently evicted items
# g_set: Ghost set (set) for O(1) lookup of recently evicted items
# freq: Frequency map to track item utility (0 to 3)
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}

# Tuning parameter: 10% of cache size allocated to Small queue
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Strategy.
    Iterates through queues to find a victim with low frequency.
    Promotes items with utility from S -> M, and recirculates items in M.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    # Calculate target size for Small queue based on current object count
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    # Loop continuously until a victim is chosen. 
    # The algorithm guarantees termination as frequencies are decremented.
    while candidate_key is None:
        
        # --- Decision: Evict from Small Queue (S) or Main Queue (M)? ---
        # We check S if it exceeds its target size OR if M is empty.
        # Otherwise, we prioritize checking M to protect probationary items in S.
        evict_from_s = len(s_queue) >= s_target_size or not m_queue
        
        if evict_from_s and s_queue:
            victim = s_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Promotion: Item accessed in S, move to M (Main Queue)
                s_queue.popleft()
                m_queue.append(victim)
                # We do not reset freq to 0 here; we let it enter M.
                # It will need to survive M's eviction logic later.
            else:
                # Eviction: Item not accessed in S, evict it
                candidate_key = s_queue.popleft()
                
                # Add to Ghost Cache (track history)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        elif m_queue:
            victim = m_queue[0] # Peek head
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Reinsertion: Item accessed in M, give second chance at tail
                m_queue.popleft()
                m_queue.append(victim)
                # Decrement frequency (simulating aging)
                freq[victim] = victim_freq - 1
            else:
                # Eviction: Item exhausted frequency in M, evict it
                candidate_key = m_queue.popleft()
                # Items evicted from M typically don't enter Ghost in S3-FIFO
        
        else:
            # Fallback (should typically not be reached if cache is full)
            break

    # --- Maintenance: Bound Ghost Queue Size ---
    # We limit ghost queue to roughly the number of items in the cache.
    # This loop cleans up stale entries (items promoted) or old entries (FIFO).
    while len(g_queue) > cache_count:
        old_ghost = g_queue.popleft()
        if old_ghost in g_set:
            g_set.remove(old_ghost)
        # If not in g_set, it was already removed during an insert (promotion),
        # so we just successfully cleaned up a stale queue entry.

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit: Increment frequency counter.
    Saturated at 3 to represent "high frequency" with minimal state bits.
    No queue reordering happens here (O(1)).
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < 3:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Insert: Decide placement based on Ghost history.
    '''
    global s_queue, m_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_set:
        # Ghost Hit: The item was recently evicted from S.
        # This implies our previous eviction decision was wrong (it's part of a working set).
        # Restore directly to Main Queue (M) and remove from Ghost set.
        m_queue.append(key)
        g_set.remove(key)
        # Note: We don't scan g_queue to remove the key to keep this O(1).
        # The stale entry in g_queue will be cleaned lazily in evict().
    else:
        # Standard Insert: New items enter the Small Queue (S) for probation.
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup: Remove metadata for the permanently evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqg8c5e51.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsjnip220.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo5_5npni.pickle

Iteration 41: New subsample score 0.315259 is not better than old score 0.324255, skipping
Iteration 42: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4swgdtf0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprdei44bl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpguilusz0.pickle

Iteration 42: Proposed new text for program: from collections import deque

# SLRU (Segmented LRU) Data Structures
# We use global variables to maintain state across function calls

# Protected Segment (L2): Items accessed at least twice
# Holds the "hot" working set.
protected_queue = deque()
protected_set = set()
protected_size = 0

# Probationary Segment (L1): Items accessed once (or demoted from L2)
# Acts as a filter for one-hit wonders.
probation_queue = deque()
probation_set = set()
probation_size = 0

# Configuration
PROTECTED_RATIO = 0.8  # Target max size for protected segment (80% of capacity)

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict.
    SLRU Logic:
    1. If the cache is full, we must discard an item.
    2. We always discard from the LRU end of the Probationary segment (L1).
    3. However, if L1 is empty (rare, but possible if L2 consumed everything), 
       we must evict from L2.
    '''
    global protected_queue, protected_set, protected_size
    global probation_queue, probation_set, probation_size

    # In a standard SLRU, eviction always happens from the tail of the Probationary queue.
    # The probation queue acts as the interface for new/weak items to leave.
    
    victim_key = None
    
    if probation_queue:
        victim_key = probation_queue[0] # Peek LRU (left is LRU)
    elif protected_queue:
        # Fallback: if probation is empty, evict from protected
        victim_key = protected_queue[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. If key in Protected (L2): Move to MRU of Protected.
    2. If key in Probation (L1): Promote to MRU of Protected.
    '''
    global protected_queue, protected_set, protected_size
    global probation_queue, probation_set, probation_size
    
    k = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    limit_protected = capacity * PROTECTED_RATIO
    
    if k in protected_set:
        # Hit in L2: Move to MRU
        # O(N) removal in Python deque is the trade-off here for simplicity, 
        # but logically necessary for LRU.
        try:
            protected_queue.remove(k)
            protected_queue.append(k)
        except ValueError:
            pass
            
    elif k in probation_set:
        # Hit in L1: Promote to L2
        try:
            probation_queue.remove(k)
        except ValueError:
            pass
        probation_set.remove(k)
        probation_size -= size
        
        protected_queue.append(k)
        protected_set.add(k)
        protected_size += size
        
        # Enforce Protected Segment Size Limit
        # If L2 grows too big, demote LRU of L2 back to MRU of L1
        while protected_size > limit_protected and protected_queue:
            demoted_key = protected_queue.popleft() # Pop LRU
            
            # We need the size of the demoted object. 
            # In a real system, we'd store (key, size) in the queue or look it up.
            # Here, we look it up in the cache snapshot.
            if demoted_key in cache_snapshot.cache:
                demoted_obj_size = cache_snapshot.cache[demoted_key].size
            else:
                # Should not happen on hit logic usually, but safe fallback
                demoted_obj_size = 0 
            
            protected_set.remove(demoted_key)
            protected_size -= demoted_obj_size
            
            probation_queue.append(demoted_key) # Add to MRU of L1
            probation_set.add(demoted_key)
            probation_size += demoted_obj_size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    New items always go to the MRU of the Probationary Segment (L1).
    '''
    global protected_queue, protected_set, protected_size
    global probation_queue, probation_set, probation_size
    
    k = obj.key
    size = obj.size
    
    # Insert into L1
    probation_queue.append(k)
    probation_set.add(k)
    probation_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata after eviction.
    Remove the evicted key from whichever set it belongs to and update sizes.
    '''
    global protected_queue, protected_set, protected_size
    global probation_queue, probation_set, probation_size
    
    k = evicted_obj.key
    size = evicted_obj.size
    
    # We must remove it from the queues. evict() only returned the key, 
    # the system performed the eviction, now we sync our state.
    
    if k in probation_set:
        try:
            probation_queue.remove(k)
        except ValueError:
            pass
        probation_set.remove(k)
        probation_size -= size
        
    elif k in protected_set:
        try:
            protected_queue.remove(k)
        except ValueError:
            pass
        protected_set.remove(k)
        protected_size -= size
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz4otqts5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw_5107r8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnee_596h.pickle

Iteration 42: New subsample score 0.769297 is not better than old score 0.924722, skipping
Iteration 43: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp901vczpi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbc1n0zuy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeoocj11c.pickle

Iteration 43: Proposed new text for program: from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Metadata
# t1: Recent items (L1) - items seen once recently
# t2: Frequent items (L2) - items seen at least twice
# b1: Ghost Recent - eviction history of t1
# b2: Ghost Frequent - eviction history of t2
# p: Adaptive parameter (Target size for t1)
t1 = OrderedDict()
t2 = OrderedDict()
b1 = OrderedDict()
b2 = OrderedDict()
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic:
    1. Check if the incoming item (obj) is in a ghost list (B1 or B2) to adapt parameter 'p'.
    2. Select a victim from T1 or T2 based on the current size of T1 relative to 'p'.
    '''
    global t1, t2, b1, b2, p
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # --- 1. ARC Adaptation ---
    # Adjust 'p' if we see a ghost hit (indicating we should have kept that type of item)
    if key in b1:
        delta = 1
        if len(b1) >= len(b2) and len(b2) > 0:
            delta = 1
        elif len(b2) > len(b1):
             delta = len(b2) / len(b1)
        p = min(capacity, p + delta)
        
    elif key in b2:
        delta = 1
        if len(b2) >= len(b1) and len(b1) > 0:
            delta = 1
        elif len(b1) > len(b2):
            delta = len(b1) / len(b2)
        p = max(0, p - delta)

    # --- 2. Select Victim ---
    replace_from_t1 = False
    
    # We evict from T1 if it exceeds target 'p', or under specific conditions involving B2 hits
    if len(t1) > 0:
        if len(t1) > p:
            replace_from_t1 = True
        elif (key in b2) and (len(t1) == int(p)):
            replace_from_t1 = True
    
    # Fallback safety: must evict from a non-empty queue
    if len(t1) == 0:
        replace_from_t1 = False
    elif len(t2) == 0:
        replace_from_t1 = True
        
    if replace_from_t1:
        victim_key, _ = t1.popitem(last=False) # Pop LRU (first item) from T1
        b1[victim_key] = True # Add to MRU of Ghost Recent
    else:
        victim_key, _ = t2.popitem(last=False) # Pop LRU (first item) from T2
        b2[victim_key] = True # Add to MRU of Ghost Frequent

    # Maintain Ghost Sizes (Bound B1 and B2 roughly to Capacity to prevent memory leaks)
    if len(b1) > capacity:
        b1.popitem(last=False)
    if len(b2) > capacity:
        b2.popitem(last=False)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    - If in T1 (Recent), move to T2 (Frequent).
    - If in T2 (Frequent), move to MRU of T2.
    '''
    global t1, t2
    key = obj.key
    
    if key in t1:
        del t1[key]
        t2[key] = True
        t2.move_to_end(key)
    elif key in t2:
        t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Insert:
    - If key was in Ghost lists, it's a "Phantom Hit" -> Insert into T2.
    - Otherwise, it's a new item -> Insert into T1.
    '''
    global t1, t2, b1, b2
    key = obj.key
    
    # Check Ghost Lists
    if key in b1:
        del b1[key]
        t2[key] = True
        t2.move_to_end(key)
    elif key in b2:
        del b2[key]
        t2[key] = True
        t2.move_to_end(key)
    else:
        # New item goes to Recent (T1)
        t1[key] = True
        t1.move_to_end(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup:
    The metadata (T1/T2/B1/B2) was already updated in `evict` to reflect the removal.
    No further action is required here.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmsnuritu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_fq0mcu9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7hfdwoew.pickle

Iteration 43: New subsample score 1.3921320000000001 is not better than old score 1.480713, skipping
Iteration 44: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmmii2a8r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpypycnxb1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptl35ts9f.pickle

Iteration 44: Proposed new text for program: import heapq

# Global metadata storage
# Stores frequency counts for objects: {key: count}
frequency_map = {}

# Priority Queue for efficient victim selection: [(H_value, access_count, key), ...]
# We include access_count as a tie-breaker for strict stability.
# Note: Python's heapq is a min-heap, which is perfect because we want to evict the smallest H value.
priority_queue = []

# Dictionary to quickly find and remove items from the priority queue (lazy removal strategy)
# Maps key -> [H_value, access_count, key] entry in the heap
pq_entry_map = {}

# Global aging factor L
L = 0.0

def _calculate_priority(freq, size):
    '''
    Calculates the H value (Priority) for GDSF.
    Formula: H = L + (Frequency / Size)
    We add a tiny epsilon to Size to prevent division by zero if size is somehow 0.
    '''
    return L + (freq / max(1, size))

def evict(cache_snapshot, obj):
    '''
    Selects the object with the lowest H value (Priority) to evict.
    Updates the global aging factor L to the H value of the evicted object.
    '''
    global L, priority_queue, pq_entry_map
    
    # Clean the heap of invalid/removed entries (Lazy Deletion)
    while priority_queue:
        priority, _, key = priority_queue[0]
        
        # If the top of the heap is not in the cache or is a stale entry, pop it
        if key not in cache_snapshot.cache or key not in pq_entry_map or pq_entry_map[key][2] != key:
            heapq.heappop(priority_queue)
            if key in pq_entry_map and pq_entry_map[key][2] != key:
                # It's a stale entry for a key that might still be in cache but updated
                pass 
            else:
                 # It's a key that is no longer in cache
                 if key in pq_entry_map: del pq_entry_map[key]
            continue
        
        # Found a valid victim
        victim_entry = heapq.heappop(priority_queue)
        victim_h, _, victim_key = victim_entry
        
        # Update L
        L = victim_h
        
        # Remove from map
        del pq_entry_map[victim_key]
        
        return victim_key
        
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority (H).
    3. Update position in Priority Queue (via lazy deletion/re-insertion).
    '''
    global frequency_map, priority_queue, pq_entry_map, L
    
    k = obj.key
    
    # Update Frequency
    frequency_map[k] = frequency_map.get(k, 0) + 1
    
    # Calculate new Priority
    new_h = _calculate_priority(frequency_map[k], obj.size)
    
    # Mark old entry as invalid in the map (Lazy deletion logic: we just overwrite the map entry)
    # The old tuple remains in the heap but will be ignored when popped because
    # pq_entry_map[k] will point to the NEW list object.
    
    # Create new entry
    # usage of cache_snapshot.access_count ensures strict ordering for ties
    entry = [new_h, cache_snapshot.access_count, k]
    pq_entry_map[k] = entry
    heapq.heappush(priority_queue, entry)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize Frequency (usually to 1).
    2. Calculate Priority (H).
    3. Push to Priority Queue.
    '''
    global frequency_map, priority_queue, pq_entry_map, L
    
    k = obj.key
    
    # If it was in history (frequency map), we restore the count, otherwise 1
    # Note: In GDSF, resetting frequency to 1 often performs better to prevent
    # cache pollution from "historically popular but now dead" objects.
    # However, keeping history helps with "looping" patterns.
    # Let's use a hybrid: if it's new, 1. If existing, freq + 1.
    frequency_map[k] = frequency_map.get(k, 0) + 1
    
    # Calculate Priority
    new_h = _calculate_priority(frequency_map[k], obj.size)
    
    entry = [new_h, cache_snapshot.access_count, k]
    pq_entry_map[k] = entry
    heapq.heappush(priority_queue, entry)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the specifically evicted object.
    '''
    global pq_entry_map, frequency_map
    
    if evicted_obj:
        k = evicted_obj.key
        if k in pq_entry_map:
            del pq_entry_map[k]
        
        # Optional: We can keep the frequency count in frequency_map 
        # to remember popularity if the object returns (Frequency aware).
        # However, to prevent memory leaks in the simulator, we should prune.
        # We will keep it, but relying on Python's GC for the simulator context isn't enough.
        # Let's cap the frequency map size to prevent memory explosion.
        if len(frequency_map) > cache_snapshot.capacity * 2: # heuristic limit
             # Clear least useful frequencies or just random clear to stay safe
             # For simplicity here, we leave it, assuming the test duration isn't infinite.
             pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa69e8img.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptw2vc7at.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps66j5p_3.pickle

Iteration 44: New subsample score 0.15014500000000003 is not better than old score 0.202971, skipping
Iteration 45: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl3wl64kd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi2bb1eka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4gauybxh.pickle

Iteration 45: Proposed new text for program: from collections import deque

# --- Global State ---
# S3-FIFO queues
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)

# Fast lookups
g_set = set()          # Set for O(1) Ghost lookup
freq = {}              # Frequency bits (capped at 2-3 usually)

# Size tracking (bytes)
s_bytes = 0
m_bytes = 0

# --- Constants ---
# S3-FIFO typically recommends Small queue be ~10% of total cache size.
# This serves as a filter for one-hit wonders (scans).
SMALL_RATIO = 0.1 
MAX_FREQ = 2

def evict(cache_snapshot, obj):
    '''
    Executes the S3-FIFO eviction policy.
    1. Checks if Small queue (S) is larger than target ratio.
    2. If S is large, evicts from S. If S item has hits, move to Main (M).
    3. If S is small, evicts from M. If M item has hits, reinsert to M tail (Second Chance).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to evict from
        # We prefer evicting from S if it exceeds its 10% allocation
        # OR if M is empty (rare, but possible).
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Safety check: If we decided S but S is empty, forced to use M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Processing Small Queue ---
            if not s_queue: break # Should be impossible if cache is full
            
            candidate = s_queue[0]
            
            # 1. Clean up stale entries (deleted elsewhere or bugs)
            if candidate not in cache:
                s_queue.popleft()
                continue

            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: It was hit while in probation. Move to Main.
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                
                # Reset frequency so it has to prove itself again in Main
                freq[candidate] = 0
            else:
                # Eviction: No hits in probation. Evict.
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost Queue (Recall logic)
                # If it comes back soon, it's not a scan, it's a loop.
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
        else:
            # --- Processing Main Queue ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail of M, decrement freq
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Ran out of second chances in Main.
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Note: Items evicted from M are usually NOT added to Ghost 
                # in standard S3-FIFO, as they already had their chance.

    # --- Ghost Queue Maintenance ---
    # Keep ghost queue size proportional to cache object count (M) to limit overhead.
    # Typically M is a good bound.
    cache_count = len(cache)
    while len(g_set) > cache_count:
        if not g_queue:
            break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Cleanup ghost head for consistency (lazy removal)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, capped at small number (1 or 2).
    High caps don't help much and cause cache pollution.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    Logic:
    - If key in Ghost: It was recently evicted from S. It's likely a cyclic pattern.
      Rescue it directly to Main (M).
    - Otherwise: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit (Recall) -> Insert into Main
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Insert into Small
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the actual evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpugmcjf4w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsw07yam6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp90meczuo.pickle

Iteration 45: New subsample score 0.51097 is not better than old score 0.511252, skipping
Iteration 46: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo7p3m965.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2m49fdvw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmfbiq63f.pickle

Iteration 46: Proposed new text for program: from collections import deque

# S3-FIFO Metadata
# s_queue: Small FIFO queue (deque) for new items (probationary)
# m_queue: Main FIFO queue (deque) for popular items
# g_set: Ghost set (fast lookup for recently evicted items)
# g_queue: Ghost FIFO queue (maintains order for ghost eviction)
# freq: Dictionary tracking access frequency (0 to 3)

s_queue = deque()
m_queue = deque()
g_set = set()
g_queue = deque()
freq = {}

# S3-FIFO Constants
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction:
    Returns the key of the object to be evicted.
    '''
    global s_queue, m_queue, g_set, g_queue, freq
    
    candidate_key = None
    
    # We dynamically calculate the target size of S based on current cache item count
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    # Loop until a victim is chosen
    while candidate_key is None:
        
        # --- Logic A: Evict from Small Queue (S) ---
        # 1. If S is larger than target size
        # 2. OR Main Queue is empty (forcing S eviction)
        if len(s_queue) >= s_target_size or not m_queue:
            if not s_queue:
                # Should not happen if cache is full, but prevents infinite loops
                break
                
            victim = s_queue[0] # Peek
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Item was hit in S, promote to M
                s_queue.popleft()
                m_queue.append(victim)
                # We keep the frequency, but it will be decremented 
                # eventually in M if not hit again.
            else:
                # Evict: Item was not hit in S (One-hit wonder/Scan)
                candidate_key = s_queue.popleft()
                
                # Add to Ghost Cache
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)

        # --- Logic B: Evict from Main Queue (M) ---
        else:
            victim = m_queue[0] # Peek
            victim_freq = freq.get(victim, 0)
            
            if victim_freq > 0:
                # Second Chance: Reinsert at tail, decrement frequency
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = victim_freq - 1
            else:
                # Evict from M
                candidate_key = m_queue.popleft()
                # We generally do not add M-evictions to Ghost in standard S3-FIFO,
                # as Ghost is primarily for catching early-eviction mistakes from S.

    # --- Ghost Cache Management ---
    # Ensure Ghost cache doesn't grow indefinitely. 
    # Typically kept around the size of the Main cache.
    target_ghost_size = max(cache_count, 10)
    while len(g_queue) > target_ghost_size:
        old_ghost = g_queue.popleft()
        if old_ghost in g_set:
            g_set.remove(old_ghost)

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency, capped at MAX_FREQ.
    This is O(1) and does not move items between queues.
    '''
    global freq
    
    key = obj.key
    curr_freq = freq.get(key, 0)
    
    if curr_freq < MAX_FREQ:
        freq[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. If key is in Ghost: It's a "Ghost Hit". Restore to Main Queue (M).
    2. Else: Insert into Small Queue (S) for probation.
    '''
    global s_queue, m_queue, g_set, g_queue, freq
    
    key = obj.key
    freq[key] = 0 # Start with 0 frequency
    
    if key in g_set:
        # Ghost Hit: The item was recently evicted from S but requested again.
        # This implies it's part of a looping pattern or working set larger than S.
        # Promote directly to M.
        m_queue.append(key)
        
        # Remove from Ghost
        g_set.remove(key)
        # Note: We don't expensive-remove from g_queue (O(N)). 
        # We let the g_queue cleaning in evict() handle stale entries implicitly
        # or handle it lazily if we encounter it.
        # Ideally, we verify existence in g_set before acting on g_queue pops.
    else:
        # Standard Insert: Insert into S
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the physically evicted object.
    '''
    global freq
    
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfq2mvx10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3q8261ai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8_wphkjl.pickle

Iteration 46: New subsample score 0.5306839999999999 is not better than old score 0.533861, skipping
Iteration 47: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkw05g5fa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfvcjgb7v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw_dlmugy.pickle

Iteration 47: Proposed new text for program: from collections import deque

# Global State Management
# We initialize these only if they haven't been defined yet to maintain state across calls.
if 's_queue' not in globals():
    s_queue = deque()      # Small/Probationary Queue (FIFO)
    m_queue = deque()      # Main/Protected Queue (FIFO)
    g_queue = deque()      # Ghost Queue (FIFO of keys)
    g_set = set()          # Ghost Set (Fast lookup)
    freq = {}              # Frequency counter
    s_bytes = 0            # Current size of Small Queue in bytes

# Tuning Parameters
# 10% of capacity for probationary items allows scans to flush through.
SMALL_QUEUE_RATIO = 0.1
# Frequency cap. A simple saturated counter is enough to distinguish hot/cold.
MAX_FREQ = 3
# Ghost Ratio: We keep track of more evicted keys than resident items 
# to catch large loops (Trace 21 optimization).
GHOST_RATIO = 5.0

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic with byte-size awareness and aggressive ghost caching.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target size for Small Queue
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    victim_key = None
    
    while victim_key is None:
        
        # --- 1. Ghost Queue Maintenance ---
        # Keep ghost queue bounded relative to current item count.
        target_ghost_len = len(cache_map) * GHOST_RATIO
        while len(g_set) > target_ghost_len and g_queue:
            rem = g_queue.popleft()
            if rem in g_set:
                g_set.remove(rem)

        # --- 2. Queue Selection ---
        # Evict from S if it exceeds its target size OR if M is empty.
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Fallback logic to ensure we don't try to evict from an empty queue
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            evict_from_s = True # Should imply S is not empty if M is empty
            
        if evict_from_s:
            # --- Process Small (Probation) Queue ---
            if not s_queue: break 
            
            candidate = s_queue[0] # Peek head
            
            # Lazy Cleanup: Object deleted externally
            if candidate not in cache_map:
                s_queue.popleft()
                # Clean up freq if exists
                if candidate in freq: del freq[candidate]
                continue
            
            v_obj = cache_map[candidate]
            v_freq = freq.get(candidate, 0)
            
            if v_freq > 0:
                # HIT in Probation: Promote to Main
                s_queue.popleft()
                s_bytes -= v_obj.size
                
                m_queue.append(candidate)
                # Reset frequency: It enters M as a "new" resident. 
                # It must get hits IN Main Queue to survive Main Queue eviction.
                freq[candidate] = 0
            else:
                # NO HIT: Evict
                victim_key = s_queue.popleft()
                s_bytes -= v_obj.size
                
                # Record in Ghost Queue (Proof of recent eviction)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
                    
        else:
            # --- Process Main (Protected) Queue ---
            if not m_queue: break
            
            candidate = m_queue[0] # Peek head
            
            if candidate not in cache_map:
                m_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
                
            v_freq = freq.get(candidate, 0)
            
            if v_freq > 0:
                # HIT in Main: Give Second Chance
                m_queue.popleft()
                m_queue.append(candidate) # Move to tail
                # Reset frequency. This item survives this round, but needs 
                # new hits to survive the next time it reaches the head.
                freq[candidate] = 0
            else:
                # NO HIT: Evict
                # Items evicted from M usually don't go to Ghost in S3-FIFO 
                # (they had a long life already).
                victim_key = m_queue.popleft()
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency counter on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    - If in Ghost: Promote directly to Main (Recalled).
    - Else: Insert into Small (Probation).
    '''
    global s_queue, m_queue, g_set, freq, s_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! The item was recently evicted from S.
        # This implies the probationary period was too short (Loop > Cache).
        # Promote directly to Main.
        m_queue.append(k)
        g_set.remove(k)
        # We don't remove from g_queue explicitly; it will be cleaned lazily.
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp5eb6cyn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoevh_j00.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpapv5xvti.pickle

Iteration 47: New subsample score 0.716141 is not better than old score 0.805005, skipping
Iteration 48: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq_h8yvym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpctr54puw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptj6leyu1.pickle

Iteration 48: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# Optimizations:
# 1. Ghost Queue Ratio increased to 3.0 to capture wider loops.
# 2. Byte-aware queue balancing for accurate 10% S-Queue sizing.
# 3. Frequency capping at 3 with linear decay in Main queue.

# Configuration Constants
SMALL_QUEUE_RATIO = 0.1
GHOST_QUEUE_RATIO = 3.0
MAX_FREQ_CAP = 3

# Global Data Structures
queue_s = deque()       # Small/Probationary Queue (FIFO)
queue_m = deque()       # Main/Protected Queue (FIFO)
queue_g = deque()       # Ghost Queue (FIFO - Keys only)
set_g = set()           # Ghost Set (Fast Lookup)
freq_map = {}           # Frequency Counter

# Byte Size Tracking
bytes_s = 0
bytes_m = 0

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    Prioritizes evicting from S-Queue if it exceeds 10% capacity,
    otherwise evicts from M-Queue, while respecting frequency hints.
    '''
    global queue_s, queue_m, queue_g, set_g, freq_map, bytes_s, bytes_m
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target size for Small Queue (10% of total capacity)
    target_s_bytes = capacity * SMALL_QUEUE_RATIO
    
    victim_key = None
    
    # Loop until a victim is finalized
    while victim_key is None:
        
        # --- Policy: Which Queue to Evict From? ---
        # 1. If S-Queue is larger than target -> Evict from S (Probationary)
        # 2. If M-Queue is empty -> Must evict from S
        # 3. Otherwise -> Evict from M
        evict_from_s = False
        if bytes_s >= target_s_bytes or not queue_m:
            evict_from_s = True
        
        # Failsafe: If decided S but S is empty, switch to M
        if evict_from_s and not queue_s:
            evict_from_s = False
        # Failsafe: If decided M but M is empty, switch to S
        if not evict_from_s and not queue_m:
            evict_from_s = True
            
        if evict_from_s:
            # --- Small Queue (S) Processing ---
            if not queue_s: break # Should not happen based on failsafes
            
            candidate = queue_s[0]
            
            # Lazy cleanup for externally deleted items
            if candidate not in cache:
                queue_s.popleft()
                if candidate in freq_map: del freq_map[candidate]
                continue
            
            obj_size = cache[candidate].size
            freq = freq_map.get(candidate, 0)
            
            if freq > 0:
                # Promotion: S -> M
                # Item was accessed during probation. Promote to Main.
                queue_s.popleft()
                bytes_s -= obj_size
                
                queue_m.append(candidate)
                bytes_m += obj_size
                
                # Reset frequency: It enters M as a new resident
                freq_map[candidate] = 0
            else:
                # Eviction: S -> Ghost
                # Item failed probation. Evict and record in Ghost.
                victim_key = queue_s.popleft()
                bytes_s -= obj_size
                
                if victim_key not in set_g:
                    set_g.add(victim_key)
                    queue_g.append(victim_key)
        
        else:
            # --- Main Queue (M) Processing ---
            if not queue_m: break
            
            candidate = queue_m[0]
            
            if candidate not in cache:
                queue_m.popleft()
                if candidate in freq_map: del freq_map[candidate]
                continue
                
            obj_size = cache[candidate].size
            freq = freq_map.get(candidate, 0)
            
            if freq > 0:
                # Reinsertion: M -> M (Second Chance)
                # Item was accessed in Main. Keep it, but decay frequency.
                queue_m.popleft()
                queue_m.append(candidate)
                freq_map[candidate] = freq - 1
            else:
                # Eviction: M -> Drop
                # Item was not accessed recently enough in M. Evict.
                victim_key = queue_m.popleft()
                bytes_m -= obj_size
                # Note: S3-FIFO typically does not add M-evicted items to Ghost
                
    # --- Ghost Queue Maintenance ---
    # Keep Ghost Queue size proportional to the number of items in cache.
    # A larger ratio (3.0) helps catch larger scans/loops.
    target_ghost_len = len(cache) * GHOST_QUEUE_RATIO
    
    while len(set_g) > target_ghost_len:
        if not queue_g: break
        oldest = queue_g.popleft()
        if oldest in set_g:
            set_g.remove(oldest)
            
    # Clean head of ghost queue if inconsistent (lazy sync)
    while queue_g and queue_g[0] not in set_g:
        queue_g.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ_CAP.
    '''
    global freq_map
    k = obj.key
    curr = freq_map.get(k, 0)
    if curr < MAX_FREQ_CAP:
        freq_map[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    If key is in Ghost -> Promote to Main (Recall).
    Else -> Insert into Small (Probation).
    '''
    global queue_s, queue_m, set_g, freq_map, bytes_s, bytes_m
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq_map[k] = 0
    
    if k in set_g:
        # Ghost Hit: Item returned quickly. Promote to Main.
        queue_m.append(k)
        bytes_m += size
        set_g.remove(k)
    else:
        # Standard Insert: Start in Small Queue.
        queue_s.append(k)
        bytes_s += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata for the evicted object.
    '''
    global freq_map
    k = evicted_obj.key
    if k in freq_map:
        del freq_map[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpybsnjlqe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmu3i64r9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpszozuzid.pickle

Iteration 48: New subsample score 0.659879 is not better than old score 0.669848, skipping
Iteration 49: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3um71nvo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp_irbe1s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps9lftsq0.pickle

Iteration 49: Proposed new text for program: from collections import deque

# S3-FIFO Constants
# S is the size ratio of the small queue (usually 10% of cache size).
# M is the size ratio of the main queue (90%).
SMALL_RATIO = 0.1

# Data Structures
# S (Small) Queue: FIFO for new insertions.
s_queue = deque()
s_set = set()

# M (Main) Queue: FIFO for items that survive S or are recalled from Ghost.
m_queue = deque()
m_set = set()

# G (Ghost) Queue: FIFO for history of evicted keys to catch re-accesses.
g_queue = deque()
g_set = set()

# Frequency tracking (approximate logic using bits or simple counters)
# Since we don't have a strict frequency counter limit, we track simple "touched" bits.
# If an item is accessed while in cache, we mark it.
freq_bits = {} 

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic.
    
    1. If Main Queue (M) is over capacity, evict from M using "Reinsertion" logic.
       - If head of M has been accessed (freq > 0), move to tail of M and clear freq.
       - Else, evict head of M.
    2. If Small Queue (S) is over capacity (10% threshold), evict from S.
       - If head of S has been accessed (freq > 0), move to M.
       - Else, evict head of S and add to Ghost (G).
    '''
    global s_queue, s_set, m_queue, m_set, g_queue, g_set, freq_bits

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    current_size = cache_snapshot.size
    
    # Calculate approximate size thresholds
    # Note: In byte-based caches, we can't strictly enforce count-based queue lengths easily,
    # but S3-FIFO works well by checking queue behavior.
    
    # We need to free up space. S3-FIFO typically prioritizes evicting from S if S is "full",
    # otherwise it evicts from M.
    
    # Calculate current size of S queue (in bytes)
    s_size_bytes = 0
    for k in s_queue:
        if k in cache: s_size_bytes += cache[k].size
            
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    # EVICTION POLICY
    # We loop until a victim is found.
    while victim_key is None:
        # Strategy:
        # If S is larger than its target allocation (10%), we try to evict from S.
        # Otherwise, we try to evict from M.
        
        evict_from_s = False
        if s_size_bytes > target_s_size:
            evict_from_s = True
        elif not m_queue:
            # Panic fallback: if M is empty, we must evict from S
            evict_from_s = True
        
        if evict_from_s and s_queue:
            candidate = s_queue.popleft()
            s_set.discard(candidate)
            s_size_bytes -= cache[candidate].size if candidate in cache else 0
            
            if freq_bits.get(candidate, 0) > 0:
                # It was accessed while in S -> Promote to M
                m_queue.append(candidate)
                m_set.add(candidate)
                freq_bits[candidate] = 0 # Reset freq
            else:
                # Not accessed -> Evict
                victim_key = candidate
                
                # Add to Ghost
                if candidate not in g_set:
                    g_queue.append(candidate)
                    g_set.add(candidate)
                    
        elif m_queue:
            candidate = m_queue.popleft()
            m_set.discard(candidate)
            
            if freq_bits.get(candidate, 0) > 0:
                # It was accessed while in M -> Reinsert at tail of M (Give second chance)
                m_queue.append(candidate)
                m_set.add(candidate)
                freq_bits[candidate] = 0 # Reset freq
            else:
                # Not accessed -> Evict
                victim_key = candidate
                # M-evictions usually don't go to Ghost in standard S3-FIFO, 
                # but adding them doesn't hurt. We'll stick to standard: evict completely.
        
        else:
            # Should not happen if cache is full, but safety break
            break
            
    # Cleanup Ghost queue size (keep it roughly size of main cache count)
    # Just a rough heuristic to prevent memory leaks
    if len(g_queue) > len(cache) + 100:
        rem = g_queue.popleft()
        g_set.discard(rem)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    Mark the frequency bit. S3-FIFO is lazy; it doesn't move items on hit,
    it only updates a "touched" bit. Movement happens during eviction.
    '''
    global freq_bits
    k = obj.key
    # Max frequency 3 is sufficient for S3-FIFO usually (0, 1, 2, 3)
    curr = freq_bits.get(k, 0)
    freq_bits[k] = min(curr + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. If key in Ghost (G) -> Insert into M (Main).
    2. Else -> Insert into S (Small).
    '''
    global s_queue, s_set, m_queue, m_set, g_queue, g_set, freq_bits
    
    k = obj.key
    freq_bits[k] = 0 # Initialize frequency
    
    if k in g_set:
        # Ghost Hit: This item was evicted recently from S. 
        # It has proven its worth. Insert into M.
        m_queue.append(k)
        m_set.add(k)
        
        # Remove from Ghost
        try:
            # O(N) removal in deque is slow, but necessary for strict correctness in Python simulation.
            # In C++ this would be a hash map pointer deletion.
            g_queue.remove(k) 
            g_set.remove(k)
        except ValueError:
            pass 
    else:
        # New insertion -> Insert into S
        s_queue.append(k)
        s_set.add(k)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata after actual eviction.
    '''
    global freq_bits
    # Remove frequency tracking for the evicted object to save memory
    if evicted_obj:
        k = evicted_obj.key
        if k in freq_bits:
            del freq_bits[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3rpabacy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsi0ikyhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp13yeeoy2.pickle

Iteration 49: New subsample score 1.2013829999999999 is not better than old score 1.234467, skipping
Iteration 50: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1fnke_pl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpks49qxjy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1xfdilm6.pickle

Iteration 50: Proposed new text for program: import math

# GDSF (Greedy Dual Size Frequency) with Ghost History
# Designed to minimize cache miss rates by maximizing hit density.

# Stores metadata for currently cached objects
# Key -> {'freq': int, 'size': int, 'h_val': float, 'last_access': int}
metadata_store = {}

# Stores frequency of recently evicted objects to handle re-entries
# Key -> int (frequency)
history_freq = {}
HISTORY_CAPACITY = 50000  # Cap to prevent memory leaks in long traces

# Global aging factor 'L'
# This rises over time as items are evicted, ensuring old items eventually age out.
L = 0.0

def get_priority(freq, size):
    '''
    Calculates priority. 
    Maximizing (Freq/Size) maximizes object hit rate (Knapsack-like heuristic).
    '''
    return float(freq) / float(size)

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the lowest H-value.
    Updates L to the H-value of the evicted victim.
    '''
    global metadata_store, L
    
    cache = cache_snapshot.cache
    victim_key = None
    min_h = float('inf')
    min_last_access = float('inf') # For LRU tie-breaking
    
    # Iterate over currently cached objects to find the minimum H-value
    for key in cache:
        # Safety check if metadata is missing (should not happen in normal flow)
        if key not in metadata_store:
            # Treat objects without metadata as having minimal value to evict them first
            return key
            
        meta = metadata_store[key]
        h_val = meta['h_val']
        
        # We search for the item with the smallest H value
        if h_val < min_h:
            min_h = h_val
            victim_key = key
            min_last_access = meta['last_access']
        elif h_val == min_h:
            # Tie-breaker: LRU (Least Recently Used)
            # Evict the one that was accessed furthest in the past
            if meta['last_access'] < min_last_access:
                victim_key = key
                min_last_access = meta['last_access']

    # Update global aging factor L to the min value found
    if victim_key is not None:
        L = min_h
        return victim_key
    
    # Fallback for empty cache (though evict shouldn't be called if empty)
    if cache:
        return next(iter(cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and re-calculate H-value using CURRENT L.
    This "re-bases" the object to the current age of the cache, protecting it.
    '''
    global metadata_store, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # Recalculate H-value: L_current + Priority
        meta['h_val'] = L + get_priority(meta['freq'], meta['size'])
    else:
        # In case of sync issues, treat as insert
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency if in history.
    '''
    global metadata_store, history_freq, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Check ghost history for frequency restoration
    freq = 1
    if key in history_freq:
        freq = history_freq[key] + 1
        # We do not delete from history here; we let it slide out naturally or overwrite
    
    # Calculate initial score
    # New Score = Current L + Priority
    priority = get_priority(freq, obj.size)
    h_val = L + priority
    
    metadata_store[key] = {
        'freq': freq,
        'size': obj.size,
        'last_access': current_time,
        'h_val': h_val
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move frequency to ghost history and cleanup metadata.
    '''
    global metadata_store, history_freq
    
    if evicted_obj:
        e_key = evicted_obj.key
        
        if e_key in metadata_store:
            # Store frequency in ghost history
            history_freq[e_key] = metadata_store[e_key]['freq']
            del metadata_store[e_key]
            
            # Maintenance: Keep history size bounded
            if len(history_freq) > HISTORY_CAPACITY:
                # Python 3.7+ dicts maintain insertion order.
                # Remove the oldest inserted item (FIFO eviction for history)
                # Using iter to get the first key efficiently
                oldest_key = next(iter(history_freq))
                del history_freq[oldest_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7enkm660.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaftmbab6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk70_r63u.pickle

Iteration 50: New subsample score 0.12263600000000001 is not better than old score 0.13506, skipping
Iteration 51: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpixds9qx3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1bc5id89.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj8lr6usc.pickle

Iteration 51: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# -------------------------------------------------------------------------
# Optimizations:
# 1. Frequency Carry-Over: Preserves frequency counts when promoting items 
#    from S to M. This grants bursty/hot items longer protection in M.
# 2. Strict Partitioning: Maintains a 10% byte-size budget for the Small 
#    queue to filter one-hit-wonders without polluting the Main queue.
# 3. Robust Ghost Tracking: Captures evicted S-items to detect loops.
# -------------------------------------------------------------------------

# Global Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of capacity for new items (S queue) allows high churn filtering.
SMALL_QUEUE_RATIO = 0.1
# Frequency cap.
MAX_FREQ = 3
# Ghost queue size relative to cache object count.
GHOST_RATIO = 2.0

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Loop until a victim is found
    while candidate_key is None:
        
        # --- Queue Selection Policy ---
        # 1. If S is larger than its target ratio -> Evict from S
        # 2. If M is empty -> Evict from S
        # Otherwise -> Evict from M
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafes for empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            # Should not happen in a full cache, but fallback to S
            evict_from_s = True
            
        if evict_from_s:
            # --- Small Queue (S) Logic ---
            if not s_queue: break
            
            victim = s_queue[0]
            
            # Lazy Cleanup: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: Move to Main (M)
                # We found a useful item in probation. Move it to the protected queue.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # OPTIMIZATION: Do NOT reset frequency to 0.
                # Retain the frequency accumulated in S. This allows highly active items
                # to survive longer in M (multiple cycles) compared to items that were barely hit.
            else:
                # Eviction: Object failed probation
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of S eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Main Queue (M) Logic ---
            if not m_queue: break
            
            victim = m_queue[0]
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance / Aging
                # Item was accessed while in M (or carried over freq from S).
                # Reinsert to tail, but decrement frequency (aging).
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1
            else:
                # Eviction: Object ran out of "lives"
                candidate_key = m_queue.popleft()
                m_bytes -= v_size
                # Items evicted from M are not added to Ghost in this variant

    # --- Ghost Queue Maintenance ---
    # Keep Ghost size bounded to prevent memory bloat
    target_ghost_count = len(cache_map) * GHOST_RATIO
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> Recall (Loop detected) -> Insert to M.
    2. Else -> New Item -> Insert to S.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0. 
    # Survival depends on queue placement and future hits.
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S and came back.
        # This implies a loop or working set slightly larger than S.
        # Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxb034d18.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoj9nrotr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnykogv7j.pickle

Iteration 51: New subsample score 1.7738079999999998 is not better than old score 1.789521, skipping
Iteration 52: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp78gkw0ud.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjxcowe7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp83x5nmkn.pickle

Iteration 52: Proposed new text for program: from collections import deque

# -------------------------------------------------------------------------
# Improved S3-FIFO (Simple S3-FIFO) Implementation
# -------------------------------------------------------------------------

# Global Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency tracker
s_bytes = 0            # Total bytes in S
m_bytes = 0            # Total bytes in M

# Tuning Parameters
# 10% of cache assigned to Small/Probationary queue.
# This filters one-hit wonders efficiently.
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict to make space.
    Implements S3-FIFO logic with Ghost queue support.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target size for Small queue
    s_target = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Loop until a victim is found
    while candidate_key is None:
        
        # --- Phase 1: Lazy Cleanup ---
        # Remove items from the head of queues if they are no longer in the cache map.
        # This handles external deletions or consistency drifts.
        while s_queue and s_queue[0] not in cache_map:
            s_queue.popleft()
            # We cannot accurately deduct s_bytes here as the object is gone, 
            # but the algorithm self-corrects via the ratio check.
        
        while m_queue and m_queue[0] not in cache_map:
            m_queue.popleft()

        # --- Phase 2: Select Queue (S or M) ---
        # We evict from S if it exceeds its target size OR if M is empty.
        evict_from_s = False
        if s_bytes >= s_target or not m_queue:
            evict_from_s = True
        
        # Fail-safe: Switch queues if the chosen one is empty
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            # Should not happen in a full cache, but safety break
            break

        # --- Phase 3: Process Victim ---
        if evict_from_s:
            # Processing Small Queue
            victim = s_queue.popleft()
            v_size = cache_map[victim].size
            s_bytes -= v_size
            
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Promote to Main (M)
                m_queue.append(victim)
                m_bytes += v_size
                # Reset frequency: It enters M as a new resident on probation
                freq[victim] = 0
            else:
                # Evict from S
                candidate_key = victim
                # Add to Ghost (Track that we evicted this)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # Processing Main Queue
            victim = m_queue.popleft()
            v_size = cache_map[victim].size
            m_bytes -= v_size
            
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Reinsert into M (Second Chance)
                m_queue.append(victim)
                m_bytes += v_size
                # Decay frequency
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                candidate_key = victim
                # Add to Ghost (Track M evictions too - important for loops)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)

    # --- Phase 4: Ghost Queue Maintenance ---
    # Limit Ghost size to 2x the number of active items to maintain history
    # without unbounded memory usage.
    target_ghost = len(cache_map) * 2
    
    while len(g_set) > target_ghost and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Clean stale entries from head of g_queue
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    Checks Ghost queue to decide between S (Probation) and M (Protected).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    if k in g_set:
        # Ghost Hit: Item was evicted recently but requested again.
        # Promote directly to Main Queue (M)
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
        # Give it strong protection immediately
        freq[k] = MAX_FREQ
    else:
        # Standard Insert: Insert into Small Queue (S)
        s_queue.append(k)
        s_bytes += size
        # Initialize with 0 frequency
        freq[k] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqe3ffvcr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdtknwasq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwviy8v60.pickle

Iteration 52: New subsample score 0.665244 is not better than old score 0.721696, skipping
Iteration 53: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprxsdkb3f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2utuezj9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5s14w3v0.pickle

Iteration 53: Proposed new text for program: from collections import deque

# --- Global State ---
# S3-FIFO consists of three queues:
# 1. S (Small/Probation): New items enter here.
# 2. M (Main/Protected): Popular items stay here.
# 3. G (Ghost): Tracks history of evicted items from S to detect false negatives.
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set() # Fast lookup for ghost

# Metadata
freq = {}      # Track hit counts (capped)
s_bytes = 0    # Current size of Small queue in bytes
m_bytes = 0    # Current size of Main queue in bytes

# --- Constants ---
# 10% is the standard optimal split for S3-FIFO
SMALL_RATIO = 0.1 
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Executes the S3-FIFO eviction policy to free space.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None

    while victim_key is None:
        # If queues are empty but cache is full, something is wrong, break to safety
        if not s_queue and not m_queue:
            break

        # Decision: Evict from S or M?
        # We process S if it is exceeding its target size OR if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # If we decided to evict from S, but S is actually empty, forced to M
        if evict_from_s and not s_queue:
            evict_from_s = False

        if evict_from_s:
            # --- Processing Small Queue ---
            candidate_key = s_queue[0]
            
            # Lazy Removal Check: If item was deleted externally, just pop and continue
            if candidate_key not in cache:
                s_queue.popleft()
                continue
            
            candidate_obj = cache[candidate_key]
            
            # Check utility
            cnt = freq.get(candidate_key, 0)
            
            if cnt > 0:
                # Promotion: Item was accessed while in probation. Move to M.
                s_queue.popleft()
                s_bytes -= candidate_obj.size
                
                m_queue.append(candidate_key)
                m_bytes += candidate_obj.size
                
                # Reset frequency on promotion (it must earn its stay in M)
                freq[candidate_key] = 0
            else:
                # Eviction: Item failed probation.
                s_queue.popleft()
                s_bytes -= candidate_obj.size
                victim_key = candidate_key
                
                # Add to Ghost: Signal that we evicted this recently
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
        else:
            # --- Processing Main Queue ---
            candidate_key = m_queue[0]
            
            if candidate_key not in cache:
                m_queue.popleft()
                continue
                
            candidate_obj = cache[candidate_key]
            cnt = freq.get(candidate_key, 0)
            
            if cnt > 0:
                # Second Chance: Reinsert at tail of M, decrement frequency
                m_queue.popleft()
                m_queue.append(candidate_key)
                freq[candidate_key] = cnt - 1
            else:
                # Eviction: Item exhausted utility in M.
                m_queue.popleft()
                m_bytes -= candidate_obj.size
                victim_key = candidate_key
                # Note: Items evicted from M are usually NOT added to Ghost in S3-FIFO

    # --- Ghost Queue Management ---
    # We maintain the ghost queue size relative to the number of items currently in cache.
    # While typically M size, keeping it proportional to total item count is robust.
    # Limit Ghost to ~100% of the number of cached items to catch loops.
    if cache:
        target_ghost_count = len(cache)
        while len(g_set) > target_ghost_count and g_queue:
            rem = g_queue.popleft()
            if rem in g_set:
                g_set.remove(rem)

    # Clean head of ghost queue if it contains stale entries (safety)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency on cache hit.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    # Cap frequency to prevent old heavy hitters from staying too long
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions into the cache.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Init frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This was recently evicted from S. 
        # It implies the item is part of a larger working set or loop.
        # Restore directly to Main.
        m_queue.append(k)
        m_bytes += size
        
        g_set.remove(k)
        # Note: We don't remove from g_queue O(N), we let it drain lazily
    else:
        # Standard Insert: Start in Probation (Small)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata after eviction.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsbucf60l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_jfe5vah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyujfw6s_.pickle

Iteration 53: New subsample score 1.5229 is better than old score 1.522634. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf8k6qn6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_dx6edb1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbysszvdq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkznp87_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkueyse1i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph_njv4bp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz8k_e8lw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc93fu8ds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgu2xhwaz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp72cb6awm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp33a214jw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpre_2jd_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps8firkdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6ojly390.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4sx7q6_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpif616g23.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzbtitdzo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp305okcee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptu0o41le.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc8ssqw11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuuak06op.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf2s4d011.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbb7yci9h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp2_6favd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsi86ktg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwp5o00xs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaob0o0xz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpplish8k1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfmyqbimd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1kxm6fyz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpujzaxypc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp9cj9hde.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvd776eds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq9wy0wnf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9_rl7bdi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg4ni78po.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp94iulww1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqxv35lup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjs5q15q6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0jclofrw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptbx3x2wv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp831g6fdz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb23fk3mk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6o6cmem0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpusvbet5q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6ciq4f1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp_bmzlvt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpejhyb45c.pickle

Iteration 53: Full valset score for new program: 0.2525479166666666
Iteration 53: Full train_val score for new program: 0.2525479166666666
Iteration 53: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.493127, 0.481494, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269806, 0.34879, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 53: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 53: Full valset pareto front score: 0.2641215
Iteration 53: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {7}]
Iteration 53: Best valset aggregate score so far: 0.2580827708333333
Iteration 53: Best program as per aggregate score on train_val: 5
Iteration 53: Best program as per aggregate score on valset: 5
Iteration 53: Best score on valset: 0.2580827708333333
Iteration 53: Best score on train_val: 0.2580827708333333
Iteration 53: Linear pareto front program index: 5
Iteration 53: New program candidate index: 15
Iteration 54: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjnjkzu43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvwvsxy80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8ui44k3a.pickle

Iteration 54: Proposed new text for program: import collections

# S3-FIFO Metadata
# S: Small Queue (FIFO) - stores probationary keys
# M: Main Queue (FIFO) - stores popular keys
# G: Ghost Queue (FIFO) - stores keys of recently evicted items from S
# G_Set: Set - provides fast lookup for Ghost items
# Freq: Dictionary tracking access frequency (0-3)
s_queue = collections.deque()
m_queue = collections.deque()
g_queue = collections.deque()
g_set = set()
freq = {}

# Metadata to track the total byte size of items in S
s_size_bytes = 0

# Tuning parameter: 10% of cache capacity (in bytes) is allocated to S
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Strategy:
    - Uses byte-aware thresholds for the Small Queue.
    - Uses FIFO logic for Ghost Queue cleanup.
    - Optimizes queue operations to O(1).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_size_bytes
    
    # Safety reset: If S is empty, size must be 0. 
    # This guards against state drift if the environment runs multiple tests sequentially.
    if not s_queue:
        s_size_bytes = 0

    cache_capacity = cache_snapshot.capacity
    s_target_size = cache_capacity * SMALL_QUEUE_RATIO
    
    # We iterate until we find a victim to evict.
    while True:
        # --- Logic A: Evict from Small Queue (S) ---
        # Trigger if S exceeds its byte-size target OR if M is empty.
        if s_size_bytes >= s_target_size or not m_queue:
            if not s_queue:
                # Should not be reached if M is empty and cache is full, but prevents infinite loop.
                break
            
            candidate_key = s_queue[0] # Peek head
            
            # Robustness: If metadata is out of sync with cache, skip.
            if candidate_key not in cache_snapshot.cache:
                s_queue.popleft()
                continue
                
            candidate_obj = cache_snapshot.cache[candidate_key]
            candidate_freq = freq.get(candidate_key, 0)
            
            if candidate_freq > 0:
                # Second Chance: Promotion from S -> M
                # The item was accessed in probation, so we move it to Main.
                s_queue.popleft()
                s_size_bytes -= candidate_obj.size
                m_queue.append(candidate_key)
                # Note: We retain the frequency to indicate continued interest in M.
            else:
                # Eviction Victim Found in S
                victim = s_queue.popleft()
                s_size_bytes -= candidate_obj.size
                
                # Add to Ghost (History of S evictions)
                if victim not in g_set:
                    g_queue.append(victim)
                    g_set.add(victim)
                
                # Maintain Ghost Size
                # We limit ghost history to roughly the object count of the cache
                # Heuristic: Ghost size ~ Cache object count
                max_ghost_count = len(cache_snapshot.cache)
                while len(g_queue) > max_ghost_count:
                    rem = g_queue.popleft()
                    # Lazy removal cleanup: if it's still in the set, remove it.
                    if rem in g_set:
                        g_set.remove(rem)
                        
                return victim

        # --- Logic B: Evict from Main Queue (M) ---
        else:
            candidate_key = m_queue[0] # Peek head
            
            # Robustness check
            if candidate_key not in cache_snapshot.cache:
                m_queue.popleft()
                continue
            
            candidate_freq = freq.get(candidate_key, 0)
            
            if candidate_freq > 0:
                # Second Chance in M: Reinsert at tail, decrement frequency
                # This approximates LRU behavior for high-frequency items.
                m_queue.rotate(-1) # Efficiently move head to tail
                freq[candidate_key] = candidate_freq - 1
            else:
                # Eviction Victim Found in M
                victim = m_queue.popleft()
                # Items evicted from M typically do not enter Ghost in S3-FIFO.
                return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency, capped at 3 (2 bits).
    Lazy promotion: We do not move items between queues on hit to keep this O(1).
    '''
    global freq
    k = obj.key
    freq[k] = min(freq.get(k, 0) + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - If key is in Ghost: It was a mistake to evict it. Restore to Main Queue (M).
    - Else: Insert into Small Queue (S).
    '''
    global s_queue, m_queue, g_set, freq, s_size_bytes
    
    k = obj.key
    freq[k] = 0 # Initialize frequency
    
    if k in g_set:
        # Ghost Hit! Restore directly to M.
        m_queue.append(k)
        g_set.remove(k)
        # We leave the key in g_queue; it will be cleaned up lazily in evict()
    else:
        # Standard Insert -> S
        s_queue.append(k)
        s_size_bytes += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the physically evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp53_mc3ut.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoapc23eg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj76sbov6.pickle

Iteration 54: New subsample score 1.047788 is not better than old score 1.056838, skipping
Iteration 55: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw5qc6mjg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa0bfjfld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3qlen6lp.pickle

Iteration 55: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# Global state to maintain across function calls
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters (bits) for resident objects

# Byte tracking for the queues to strictly enforce S/M ratio
s_bytes = 0
m_bytes = 0

# Constants
# 10% for Small is the standard S3-FIFO configuration for scan resistance.
# A small cap on frequency allows fast aging.
SMALL_RATIO = 0.1  
MAX_FREQ = 2

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic:
    1. If Small (S) is larger than target size, evict from S.
    2. If S is not full, usually evict from Main (M) to make room, unless M is empty.
    3. Items evicted from S are tracked in Ghost (G).
    4. Items in S with hits promote to M.
    5. Items in M with hits get a "second chance" (reinsertion).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to operate on.
        # Strict S3-FIFO: If S > 10%, we must drain S. Otherwise, we drain M.
        # If one is empty, we must drain the other.
        drain_s = False
        
        if s_bytes >= target_s_size:
            drain_s = True
        elif not m_queue:
            drain_s = True
            
        # Failsafe: if we decided to drain S but it's empty, switch to M
        if drain_s and not s_queue:
            drain_s = False
            
        if drain_s:
            # --- Processing Small Queue ---
            if not s_queue: 
                # Should not happen if cache is full, but safety break
                break 
                
            candidate = s_queue[0]
            
            # 1. Stale key check (if obj was removed externally or bug)
            if candidate not in cache:
                s_queue.popleft()
                continue
                
            # 2. Check utility
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # HIT in S: Promotion to M
                # Remove from S
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to M
                m_queue.append(candidate)
                m_bytes += size
                
                # Reset frequency so it has to prove itself in M
                freq[candidate] = 0
            else:
                # NO HIT in S: Eviction
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost (only from S eviction)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        
        else:
            # --- Processing Main Queue ---
            if not m_queue:
                break
                
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # HIT in M: Second Chance
                # Move to back of M, decrement frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # NO HIT in M: Eviction
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Note: Items evicted from M are usually not added to Ghost in standard S3-FIFO

    # --- Ghost Queue Maintenance ---
    # Keep Ghost size roughly equal to the number of items in cache
    # to detect loops.
    current_item_count = len(cache)
    # Using 1.0x to 2.0x count is typical. 
    while len(g_set) > current_item_count and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Clean head of ghost queue if needed
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    # We use a very simple frequency counter (0, 1, 2, 3)
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion of new objects.
    - If key in Ghost: Insert into Main (M).
    - Else: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S, so it has utility.
        # Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Start in Small (Probation).
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp037f1up0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxh7w9f4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2_lszthm.pickle

Iteration 55: New subsample score 0.24931199999999998 is better than old score 0.188583. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6itq0838.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa922ptib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcgi4d8kb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwe5navub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbrid14dy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8r5rx18t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphir1k3qz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp13fobd8k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7uzpph3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4hsrb7sx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd94pamck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi1h65i28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd8keup_6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp78i_5ji2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp79pcddey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqw3xzszd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaomusrqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnwz0n5p_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe43xwfzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr_nu6s_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqeio1ttl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj4_5cl9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3rxgr1hx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6nidttn3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv6awvrrz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe3dh4x9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpow91qbxi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7m1cj_g0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp19icj0bd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoz4o3wep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpde0v31i3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmnmsoz8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsq7retw4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn0e72jzn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphu9llj_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjnxet525.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiffh5g_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm9b6is_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpckc1vggb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw6bfjoio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp61ft_47_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1_i2i_hy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv3xujzv3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphvgni3b1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7m65q8k9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuq6l8rwh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuhc209b8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp715kd2dg.pickle

Iteration 55: Full valset score for new program: 0.2528075208333333
Iteration 55: Full train_val score for new program: 0.2528075208333333
Iteration 55: Individual valset scores for new program: [0.496879, 0.471515, 0.481425, 0.435138, 0.493395, 0.482244, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269802, 0.356855, 0.819265, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145273, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143338, 0.466258, 0.081699]
Iteration 55: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 55: Full valset pareto front score: 0.2641215
Iteration 55: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {7}]
Iteration 55: Best valset aggregate score so far: 0.2580827708333333
Iteration 55: Best program as per aggregate score on train_val: 5
Iteration 55: Best program as per aggregate score on valset: 5
Iteration 55: Best score on valset: 0.2580827708333333
Iteration 55: Best score on train_val: 0.2580827708333333
Iteration 55: Linear pareto front program index: 5
Iteration 55: New program candidate index: 16
Iteration 56: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcanfajs1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv27c8z0y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd2p8jzdh.pickle

Iteration 56: Proposed new text for program: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# This implementation focuses on the S3-FIFO algorithm which segments the cache 
# into a Small (S) queue for probationary items and a Main (M) queue for 
# high-frequency items. A Ghost (G) queue tracks evicted probationary items to 
# quickly identify recurring misses (scan resistance).

# Global Data Structures
# S Queue: Stores new items. Functions as a probationary filter.
s_queue = deque()      
# M Queue: Stores popular items. Protected from immediate eviction.
m_queue = deque()      
# G Queue: Stores keys of items evicted from S. (FIFO)
g_queue = deque()      
# G Set: Fast lookup for Ghost items.
g_set = set()          
# Frequency Map: Tracks access frequency (up to MAX_FREQ) for resident items.
freq = {}              

# Byte Tracking for Small Queue
# We strictly track bytes in S to adhere to the 10% sizing rule relative to cache capacity.
s_bytes = 0

# Tuning Parameters
# 10% of capacity for Small Queue allows the Main queue to hold 90% of working set.
SMALL_QUEUE_RATIO = 0.1
# A cap of 3 is sufficient to distinguish between "noise" and "hot" items.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines the eviction victim using S3-FIFO logic.
    Updates internal queues (rotation/promotion) until a victim is dropped.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate dynamic capacity threshold for Small Queue
    s_capacity_threshold = capacity * SMALL_QUEUE_RATIO
    
    victim_key = None
    
    # Loop until we successfully evict an item or empty the cache
    while victim_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M
        
        evict_from_s = False
        if s_bytes >= s_capacity_threshold or not m_queue:
            evict_from_s = True
        
        # Failsafe checks for empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            evict_from_s = True
        
        if evict_from_s:
            # --- Processing Small Queue (S) ---
            if not s_queue: break # Should not happen given logic above
            
            candidate = s_queue[0] # Peek head
            
            # Integrity Check: Lazy cleanup for external deletions
            if candidate not in cache_map:
                s_queue.popleft()
                continue
            
            candidate_obj = cache_map[candidate]
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # PROMOTION (S -> M)
                # The item was accessed while in probation. It earns a spot in Main.
                s_queue.popleft()
                s_bytes -= candidate_obj.size
                m_queue.append(candidate)
                
                # OPTIMIZATION: Do NOT reset frequency to 0. 
                # Preserving the accumulated frequency prevents this item from 
                # being immediately evicted if M is currently full and rotating.
                # It effectively gives the promoted item a buffer.
            else:
                # EVICTION (from S)
                # The item was not accessed during probation.
                victim_key = s_queue.popleft()
                s_bytes -= candidate_obj.size
                
                # Add to Ghost (Track that we evicted this recently)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
                    
        else:
            # --- Processing Main Queue (M) ---
            if not m_queue: break
            
            candidate = m_queue[0] # Peek head
            
            if candidate not in cache_map:
                m_queue.popleft()
                continue
            
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # REINSERTION (M -> M)
                # Item is still hot. Rotate to back of M and decay frequency.
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = candidate_freq - 1
            else:
                # EVICTION (from M)
                # Item has "cooled down" (freq reached 0).
                victim_key = m_queue.popleft()
                # Items evicted from M typically do not go to Ghost in S3-FIFO.

    # --- Ghost Queue Size Management ---
    # We limit the ghost queue based on the current number of cached items.
    # A 1:1 or 2:1 ratio is standard. We use ~1x here to keep overhead low but capture loops.
    current_item_count = len(cache_map)
    target_ghost_len = current_item_count
    
    while len(g_set) > target_ghost_len:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of Ghost head if needed
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - If in Ghost -> Insert to M (Recall).
    - Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency for new resident
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This item was evicted recently but requested again.
        # This signals a false negative in previous eviction. Promote straight to Main.
        m_queue.append(k)
        g_set.remove(k)
        # We do not increment s_bytes because it went to m_queue.
    else:
        # Standard Insert: Goes to Small Queue for probation.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuh37p96v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpknvdep71.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjzqv1ops.pickle

Iteration 56: New subsample score 0.994192 is not better than old score 0.998265, skipping
Iteration 57: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmselg2cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzw25yxzs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp332rdrqc.pickle

Iteration 57: Proposed new text for program: from collections import deque

# --- Global State ---
# We maintain our own state to persist across function calls
s_queue = deque()       # Small/Probationary Queue (FIFO)
m_queue = deque()       # Main/Protected Queue (FIFO)
g_queue = deque()       # Ghost Queue (FIFO of keys)
g_set = set()           # Fast lookup for Ghost keys
freq = {}               # Frequency counters: key -> int
sizes = {}              # Size tracker: key -> int (Vital for accurate byte accounting)

# Byte tracking
s_bytes = 0
m_bytes = 0

# Configuration
# 10% is generally optimal for S3-FIFO, but we handle the bytes strictly now.
SMALL_RATIO = 0.1  
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using a robust S3-FIFO implementation.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, sizes, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # 1. Determine which queue to evict from
        # We evict from Small (S) if it exceeds its target size OR if Main (M) is empty
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If logic says S but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue ---
            if not s_queue: break # Should not happen if cache full
            
            candidate = s_queue[0]
            cand_size = sizes.get(candidate, 0)
            
            # 1a. Handle Stale Keys (Item no longer in cache)
            if candidate not in cache:
                s_queue.popleft()
                s_bytes -= cand_size
                # Clean up metadata
                if candidate in sizes: del sizes[candidate]
                if candidate in freq: del freq[candidate]
                continue
            
            # 1b. Check Utility
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: Item accessed in S -> Move to M
                s_queue.popleft()
                s_bytes -= cand_size
                
                m_queue.append(candidate)
                m_bytes += cand_size
                
                # Reset frequency. It must earn its stay in M.
                freq[candidate] = 0 
            else:
                # Eviction: Item not accessed in S
                victim_key = s_queue.popleft()
                s_bytes -= cand_size
                
                # Add to Ghost Queue (Signal of recent eviction)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        
        else:
            # --- Eviction from Main Queue ---
            if not m_queue: break
            
            candidate = m_queue[0]
            cand_size = sizes.get(candidate, 0)
            
            # 2a. Handle Stale Keys
            if candidate not in cache:
                m_queue.popleft()
                m_bytes -= cand_size
                if candidate in sizes: del sizes[candidate]
                if candidate in freq: del freq[candidate]
                continue
                
            # 2b. Check Utility
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail of M
                m_queue.popleft()
                m_queue.append(candidate)
                
                # Decay frequency. 
                # Optimization: We reset to 0 or decrement. 
                # Decrementing keeps highly popular items longer.
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Item exhausted frequency
                victim_key = m_queue.popleft()
                m_bytes -= cand_size
                # M-evicted items usually don't go to Ghost in S3-FIFO
    
    # --- Ghost Queue Maintenance ---
    # Clean up ghost queue if it gets too large (e.g., > 50% of current item count)
    # Using a count limit prevents memory bloat
    target_ghost_count = len(cache) // 2 + 10
    while len(g_queue) > target_ghost_count:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - If in Ghost: Recall to Main.
    - Else: Insert into Small.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, sizes, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Track size reliably
    sizes[k] = size
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Validated pattern -> Main Queue
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
        # Note: We don't remove from g_queue O(N), we let it clean up lazily
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq, sizes
    k = evicted_obj.key
    
    # Clean up frequency
    if k in freq:
        del freq[k]
        
    # Clean up size tracking
    if k in sizes:
        del sizes[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgkf754q2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptx9xyl4n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkb1qfpjz.pickle

Iteration 57: New subsample score 0.319345 is not better than old score 0.320162, skipping
Iteration 58: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpili9dfub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuevibjos.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoimyz3v0.pickle

Iteration 58: Proposed new text for program: from collections import deque

# --- Global State ---
# S3-FIFO data structures
s_queue = deque()      # Small/Probation queue
m_queue = deque()      # Main/Protected queue
g_queue = deque()      # Ghost queue (keys only)
g_set = set()          # Fast lookup for ghost keys
freq = {}              # Frequency counter

# Tracking bytes for queue sizing
s_bytes = 0
m_bytes = 0

# --- Adaptive Parameters ---
# Instead of a static 10% split, we adapt the target size of the Small queue.
# target_s_ratio starts at 0.1 (10%) but can move between 0.01 and 0.9.
target_s_ratio = 0.1
MAX_FREQ = 3           # Cap for frequency to prevent integer overflows/stagnation

def evict(cache_snapshot, obj):
    '''
    Selects a victim using an Adaptive S3-FIFO strategy.
    Prioritizes evicting from Small queue if it exceeds target size,
    but adapts that target based on recent history (Ghost hits).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes, target_s_ratio

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate byte threshold based on adaptive ratio
    target_s_size = capacity * target_s_ratio
    
    victim_key = None
    
    while victim_key is None:
        # 1. Decide which queue to evict from
        # We evict from S if it's "too big" relative to our adaptive target,
        # OR if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
            
        # Failsafe: if we decided S but S is empty, forced to use M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Small Queue Eviction Logic ---
            if not s_queue: break # Should not happen given logic above
            
            candidate = s_queue[0]
            
            # Cleaning stale keys (deleted elsewhere)
            if candidate not in cache:
                s_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: It was accessed while in probation. Move to Main.
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                
                # Reset frequency so it has to prove itself in M
                freq[candidate] = 0 
            else:
                # Eviction: No hits while in S.
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost (Recency history)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Main Queue Eviction Logic ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Reinsert at tail, decrement frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: exhausted frequency in M.
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Usually we don't add M-evictions to Ghost in standard S3-FIFO,
                # focusing Ghost on "mistaken early evictions" from S.

    # --- Ghost Queue Maintenance ---
    # We maintain a ghost queue size roughly equal to the number of items in cache
    # to track recent history relevant to the current working set size.
    target_ghost_len = len(cache) 
    
    # Trim ghost if too big
    while len(g_set) > target_ghost_len and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Cleanup ghost head if it contains stale entries removed from set
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertions and adapt the queue sizing ratio.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes, target_s_ratio
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # --- Ghost Hit (Recall) ---
        # This implies our Small queue was too small (we evicted it too early).
        # We increase the target ratio for S to hold items longer.
        target_s_ratio = min(target_s_ratio + 0.02, 0.9)
        
        # Promote directly to Main because it has proven reuse (it was here before).
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # --- Fresh Insert ---
        # If we are inserting unrelated items, we slightly decrease S pressure
        # to allow M to hold more long-term items, but we do this very slowly
        # to prioritize stability.
        # Note: We only decay if S is actually full, otherwise we don't care.
        if s_bytes > (cache_snapshot.capacity * target_s_ratio):
             target_s_ratio = max(target_s_ratio - 0.002, 0.01)

        # Standard inserts go to Small/Probation
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp33s1tn2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9zuuaciz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7nh7cisr.pickle

Iteration 58: New subsample score 1.3982150000000002 is better than old score 1.3694799999999998. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1n4xffr2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9xfd9fjt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnfwsm27a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfuzax7uw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp60048_0n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp76efxms7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk1lb4uwd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpue6ew27q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2k_r7hi_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkccefjwu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnjujh_o3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpoxuf133k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeb91svgj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmccu2_jl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpszftegyp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz6jtruec.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp62hfs80_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6mx020d9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprs086uwm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc608c5ht.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ochi0e3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsgbuuv7l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6t4h0324.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpat8sjpil.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjr3yilov.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpednllle3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9if5qdp_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_mau2cxo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx7n_qaxc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgno726bo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpab5c0guc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpee1wnv9r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpchsv83qq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkwrv81jr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmqv_zr9p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp15li77wq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcvyu4k7y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw4kxyffd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp659cv8ou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjef94mh_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf032zi6n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7w0s8w7q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm0cez_ao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf5u7epsp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ivgo9op.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7cuhv97o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuq8r36a6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp309sire0.pickle

Iteration 58: Full valset score for new program: 0.23047099999999987
Iteration 58: Full train_val score for new program: 0.23047099999999987
Iteration 58: Individual valset scores for new program: [0.39491, 0.361662, 0.372644, 0.325242, 0.388939, 0.383085, 0.272727, 0.498427, 0.539864, 0.531017, 0.075, 0.321492, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.360865, 0.026164, 0.058672, 0.058672, 0.269778, 0.297379, 0.678252, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752723, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.105628, 0.126451, 0.113394, 0.052632, 0.35, 0.174888, 0.09986, 0.466258, 0.081699]
Iteration 58: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 58: Full valset pareto front score: 0.2641215
Iteration 58: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {7}]
Iteration 58: Best valset aggregate score so far: 0.2580827708333333
Iteration 58: Best program as per aggregate score on train_val: 5
Iteration 58: Best program as per aggregate score on valset: 5
Iteration 58: Best score on valset: 0.2580827708333333
Iteration 58: Best score on train_val: 0.2580827708333333
Iteration 58: Linear pareto front program index: 5
Iteration 58: New program candidate index: 17
Iteration 59: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq6pzz0uj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6rh_68a4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp66t_n6gm.pickle

Iteration 59: Proposed new text for program: import math

# GDSF (Greedy Dual Size Frequency) Global State
# We track the "aging factor" L, which represents the priority of the last evicted object.
# This allows the algorithm to dynamically adjust the threshold for staying in the cache.

# Global variable for the aging factor
L_value = 0.0

# Maps key -> metadata dictionary
# metadata = {'freq': int, 'priority': float, 'seq': int}
metadata_store = {}

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using the GDSF policy.
    
    We search for the object with the lowest Priority value.
    Priority = L + (Frequency / Size)
    
    If priorities are equal, we evict the one that was accessed least recently (LRU tie-breaking).
    Crucially, we update the global L_value to the priority of the evicted victim.
    '''
    global metadata_store, L_value
    cache = cache_snapshot.cache
    
    victim_key = None
    min_priority = float('inf')
    min_seq = float('inf')
    
    # Iterate through the cache to find the item with minimum priority
    # While O(N), this is necessary for GDSF correctness in this simulation context.
    # Python dictionaries are efficient enough for standard trace simulations.
    for key in cache:
        if key in metadata_store:
            meta = metadata_store[key]
            p = meta['priority']
            s = meta['seq']
            
            # We look for the smallest priority
            if p < min_priority:
                min_priority = p
                min_seq = s
                victim_key = key
            elif p == min_priority:
                # Tie-breaker: LRU (smallest sequence number is older)
                if s < min_seq:
                    min_seq = s
                    victim_key = key
        else:
            # Fallback for sync issues: treat as effectively 0 priority
            return key

    # GDSF Rule: The aging factor L advances to the priority of the evicted object.
    # This implies that future objects must have a score higher than this to survive.
    if victim_key is not None:
        L_value = min_priority
        return victim_key
        
    # Fallback if cache is somehow empty but evict called (should not happen)
    return next(iter(cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using the CURRENT L_value.
       This gives the object a "boost" above the rising water level of L.
    '''
    global metadata_store, L_value
    
    key = obj.key
    # Use access_count as a sequence number for LRU tie-breaking
    seq = cache_snapshot.access_count
    
    if key in metadata_store:
        # Increase frequency
        metadata_store[key]['freq'] += 1
        
        # GDSF Formula: Priority = L + (Frequency / Size)
        # Note: We use the current global L_value here.
        freq = metadata_store[key]['freq']
        size = obj.size
        
        metadata_store[key]['priority'] = L_value + (freq / float(size))
        metadata_store[key]['seq'] = seq
    else:
        # Should technically be an insert, but handle graceful recovery
        metadata_store[key] = {
            'freq': 1,
            'priority': L_value + (1.0 / obj.size),
            'seq': seq
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate initial Priority based on current L_value.
    '''
    global metadata_store, L_value
    
    key = obj.key
    seq = cache_snapshot.access_count
    
    # New objects start with Frequency 1
    # Priority = L + (1 / Size)
    # This places them just above the eviction threshold. If they aren't hit again
    # while L rises (due to other evictions), they will be evicted soon.
    priority = L_value + (1.0 / float(obj.size))
    
    metadata_store[key] = {
        'freq': 1,
        'priority': priority,
        'seq': seq
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata. We do not maintain history in this implementation
    to prevent memory bloat and stale frequency pollution, relying instead
    on the L_value to handle the "value" of the cache state.
    '''
    global metadata_store
    
    if evicted_obj and evicted_obj.key in metadata_store:
        del metadata_store[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjmn7uyee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5azlr14c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr6wnpxqu.pickle

Iteration 59: New subsample score 0.504479 is not better than old score 0.533433, skipping
Iteration 60: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwyuhf5zl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2wo3pkt6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplqi6mktl.pickle

Iteration 60: Proposed new text for program: from collections import deque, defaultdict

# --- Global State ---
# S3-FIFO structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
ghost_registry = {}    # Ghost Registry: maps key -> eviction_time (or count)

# Metadata
freq_bits = defaultdict(int) # 2-bit saturating counter (0-3)

# Queue Size Tracking (Bytes)
s_bytes = 0
m_bytes = 0

# --- Constants & Tunables ---
# S3-FIFO suggests a small queue size around 10% of cache space.
SMALL_RATIO = 0.1  
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic:
    1. Prefer evicting from Small Queue (S) if it exceeds target size.
    2. If S-item is evicted but was accessed, promote to Main (M).
    3. If M-item is candidate but was accessed, give second chance (reinsert to M).
    '''
    global s_queue, m_queue, ghost_registry, freq_bits, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to operate on.
        # Strict S3-FIFO: Evict from S if S is "too big" OR M is empty.
        operate_on_s = False
        if s_bytes >= target_s_size or not m_queue:
            operate_on_s = True
            
        # Failsafe: If S is empty, forced to operate on M
        if operate_on_s and not s_queue:
            operate_on_s = False
            
        if operate_on_s:
            # --- Small Queue Processing ---
            if not s_queue: break # Should not happen given logic above
            
            candidate = s_queue[0]
            
            # Verify existence (handling stale queue entries)
            if candidate not in cache:
                s_queue.popleft()
                continue
            
            cand_freq = freq_bits[candidate]
            
            if cand_freq > 0:
                # HIT in S: Promotion to M
                # It passed probation. Move to M, reset freq.
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                freq_bits[candidate] = 0 # Reset frequency logic for M
            else:
                # MISS in S (Zero frequency): Evict
                # This was a one-hit wonder or cold item.
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost Registry (Scan resistance)
                # We store a timestamp/count to prune old ghosts later
                ghost_registry[victim_key] = True
        
        else:
            # --- Main Queue Processing ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
                
            cand_freq = freq_bits[candidate]
            
            if cand_freq > 0:
                # HIT in M: Re-insert at tail (Second Chance)
                m_queue.popleft()
                m_queue.append(candidate)
                freq_bits[candidate] -= 1 # Decay frequency
            else:
                # EXHAUSTED in M: Evict
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Items evicted from M usually don't need ghost tracking in standard S3-FIFO,
                # but adding them helps recall high-frequency items that barely slipped out.
                ghost_registry[victim_key] = True

    # Ghost Registry Cleanup
    # Keep ghost registry size bounded (e.g., equal to number of cached items)
    # This prevents memory leaks in the metadata
    target_ghost_size = len(cache)
    if len(ghost_registry) > target_ghost_size * 2: # Loose bound
        # Pruning strategy: Convert to list, slice, convert back (expensive but infrequent)
        # Or simple clear if too huge (simpler/faster for strict constraints)
        keys_to_remove = list(ghost_registry.keys())[:len(ghost_registry)//2]
        for k in keys_to_remove:
            del ghost_registry[k]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment 2-bit frequency counter.
    '''
    global freq_bits
    k = obj.key
    curr = freq_bits[k]
    if curr < MAX_FREQ:
        freq_bits[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    S3-FIFO Logic:
    - If in Ghost Registry: Insert directly into Main (M).
    - Else: Insert into Small (S).
    '''
    global s_queue, m_queue, ghost_registry, freq_bits, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq_bits[k] = 0
    
    if k in ghost_registry:
        # Ghost Hit: Item was recently evicted.
        # This signals a false negative eviction or a larger working set.
        # Promote directly to Main to protect it.
        m_queue.append(k)
        m_bytes += size
        del ghost_registry[k]
    else:
        # Standard Insert: Insert into Small (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for evicted object.
    '''
    global freq_bits
    k = evicted_obj.key
    if k in freq_bits:
        del freq_bits[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4o9du9z8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg4ralhh3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7dbxlyvb.pickle

Iteration 60: New subsample score 0.165032 is not better than old score 0.17810399999999998, skipping
Iteration 61: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpva5bkcxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpup6eetgz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_fwk2pk1.pickle

Iteration 61: Proposed new text for program: from collections import deque
import math

# Global State for Segmented Logic
# 1. Probation Segment (A1): Objects accessed once. Managed as FIFO/LRU.
# 2. Protected Segment (Am): Objects accessed >1 times. Managed as LRU.
probation_queue = deque()
protected_queue = deque()

# Tracking Set for O(1) lookups of which queue a key is in
# 0 = Probation, 1 = Protected
location_map = {} 

# Metadata Store: key -> { 'freq': int, 'size': int, 'tick': int }
meta_data = {}

# Hyper-parameters
SAMPLE_SIZE = 5  # Number of LRU candidates to inspect for size-based eviction
PROTECTED_RATIO = 0.8 # Max % of count that can be in protected (not strictly enforced by bytes here, but count)

def get_victim_from_queue(queue, cache_dict):
    """
    Inspects the tail (LRU end) of the queue.
    Instead of blindly popping the absolute LRU, we look at the last N items.
    We evict the item that maximizes: (Size / Frequency^2).
    Logic: We want to evict Large items that have Low frequency.
    """
    candidates = []
    
    # Collect candidates from the tail (left side of deque is usually tail in Python if append is head)
    # Standard: append() adds to right (MRU), popleft() removes from left (LRU).
    # So we look at index 0, 1, 2...
    
    limit = min(len(queue), SAMPLE_SIZE)
    for i in range(limit):
        key = queue[i]
        if key in cache_dict:
            # Calculate badness score. Higher is worse (better to evict).
            # We want to evict Large items.
            # We want to keep Frequent items.
            obj_size = cache_dict[key].size
            # Even in probation, freq might be > 1 if we just haven't moved it yet, 
            # though usually it's 1 in probation.
            freq = meta_data.get(key, {'freq': 1})['freq']
            
            # Score: Size / Frequency. 
            # Large size = High Score (Evict). High Freq = Low Score (Keep).
            score = obj_size / (freq * 1.0) 
            candidates.append((score, key))
    
    # Sort by score descending (highest score = best victim)
    candidates.sort(key=lambda x: x[0], reverse=True)
    
    if candidates:
        return candidates[0][1] # Return the key of the best victim
    return None

def evict(cache_snapshot, obj):
    '''
    Evicts an object based on Segmented LRU with Size-Aware Tail Drop.
    '''
    global probation_queue, protected_queue, location_map, meta_data

    cache = cache_snapshot.cache
    victim_key = None
    
    # 1. Try to evict from Probation first
    if probation_queue:
        # Smart eviction from Probation
        victim_key = get_victim_from_queue(probation_queue, cache)
        
        # If the scan failed (shouldn't happen if queue not empty), fallback
        if not victim_key:
            victim_key = probation_queue[0]
            
        # Clean up queue
        try:
            probation_queue.remove(victim_key)
        except ValueError:
            pass # Should be rare
            
    # 2. If Probation empty, evict from Protected
    elif protected_queue:
        # Smart eviction from Protected
        victim_key = get_victim_from_queue(protected_queue, cache)
        
        if not victim_key:
            victim_key = protected_queue[0]
            
        try:
            protected_queue.remove(victim_key)
        except ValueError:
            pass

    # Safety Fallback (if queues somehow desync from cache)
    if not victim_key and cache:
        # Just pick an arbitrary key from cache (iterator order)
        victim_key = next(iter(cache))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Move to Protected Queue (MRU position).
    '''
    global probation_queue, protected_queue, location_map, meta_data
    
    k = obj.key
    
    # Update Metadata
    if k not in meta_data:
        meta_data[k] = {'freq': 0, 'size': obj.size, 'tick': cache_snapshot.access_count}
    
    meta_data[k]['freq'] += 1
    meta_data[k]['tick'] = cache_snapshot.access_count
    
    current_loc = location_map.get(k)
    
    if current_loc == 0: # In Probation
        # Remove from Probation
        try:
            probation_queue.remove(k)
        except ValueError:
            pass
        
        # Add to Protected (MRU - Right side)
        protected_queue.append(k)
        location_map[k] = 1
        
    elif current_loc == 1: # In Protected
        # Refresh in Protected (Move to MRU)
        try:
            protected_queue.remove(k)
        except ValueError:
            pass
        protected_queue.append(k)
        # location stays 1
    else:
        # Edge case: Item in cache but tracking lost (e.g. startup). Treat as insert logic.
        # Add to Protected since it's a hit
        protected_queue.append(k)
        location_map[k] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize Metadata.
    2. Add to Probation Queue (MRU position).
    '''
    global probation_queue, protected_queue, location_map, meta_data
    
    k = obj.key
    
    # Init metadata
    # Freq starts at 1
    meta_data[k] = {'freq': 1, 'size': obj.size, 'tick': cache_snapshot.access_count}
    
    # Insert into Probation (MRU - Right side)
    probation_queue.append(k)
    location_map[k] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up.
    '''
    global location_map, meta_data, probation_queue, protected_queue
    
    if evicted_obj:
        k = evicted_obj.key
        if k in location_map:
            del location_map[k]
        # We generally keep metadata for a while in advanced algos (Ghost lists), 
        # but to keep memory usage low and logic simple for this specific implementation, 
        # we will delete it. Pure Size-Aware SLRU works well without history.
        if k in meta_data:
            del meta_data[k]
        
        # Ensure it's gone from queues (redundant safety)
        # In evict() we typically remove it, but if evict logic changes, this catches it.
        # Note: calling remove is O(N), but necessary for consistency if evict didn't do it.
        # Our evict() does it.
        pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgv_dz68y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp83niwnz9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnj8arb87.pickle

Iteration 61: New subsample score 0.130252 is not better than old score 0.197694, skipping
Iteration 62: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphyn9kvov.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprlmj1h8n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4zi0pnk2.pickle

Iteration 62: Proposed new text for program: import math

# Global Metadata Storage
# Maps key -> {'freq': int, 'priority': float, 'last_access': int}
metadata_store = {}

# Ghost Frequency Storage
# Keeps track of frequencies of recently evicted items to handle re-entries intelligently.
ghost_freq = {}

# The Inflation Factor 'L'
# Represents the priority value of the last evicted object.
# This acts as a logical clock to age out old, high-frequency items.
inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Find the object with the minimum Priority value.
    Update the global inflation factor L to this minimum value.
    '''
    global metadata_store, inflation_L
    
    cache = cache_snapshot.cache
    
    victim_key = None
    min_priority = float('inf')
    
    # Iterate through cached objects to find the one with the lowest priority
    # Note: While iterating is O(N), for typical simulation cache sizes this is 
    # the only way to get exact precision without maintaining a complex heap structure
    # that goes out of sync with the read-only cache_snapshot.
    
    candidate_found = False
    
    for key in cache:
        if key in metadata_store:
            meta = metadata_store[key]
            priority = meta['priority']
            
            if priority < min_priority:
                min_priority = priority
                victim_key = key
                candidate_found = True
            elif priority == min_priority:
                # Tie-Breaker: LRU
                # If priorities are equal, evict the one accessed least recently
                if victim_key and meta['last_access'] < metadata_store[victim_key]['last_access']:
                    victim_key = key
        else:
            # Fallback for sync issues (should not happen in valid flow)
            # Treat unknown items as having 0 priority (immediate eviction candidates)
            victim_key = key
            min_priority = -1.0
            candidate_found = True
            break
            
    # Update the global aging factor L
    # The new base priority for the cache is the priority of the item we just evicted.
    if candidate_found and min_priority != float('inf'):
        inflation_L = min_priority
        
    if victim_key is None:
        # Fallback if cache is logically empty or error
        return next(iter(cache)) if cache else None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority based on current L (re-float the object).
    '''
    global metadata_store, inflation_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        
        # Increment frequency
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # Recalculate Priority (GDSF formula)
        # Priority = L + (Frequency / Size)
        # We use current global L to bring this object to the "front" of the logical time
        size = obj.size if obj.size > 0 else 1
        meta['priority'] = inflation_L + (meta['freq'] / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost list for historical frequency.
    2. Calculate initial Priority based on current L.
    '''
    global metadata_store, ghost_freq, inflation_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    size = obj.size if obj.size > 0 else 1
    
    # Determine Frequency
    # If we've seen this object recently, restore its frequency count
    freq = 1
    if key in ghost_freq:
        freq = ghost_freq[key] + 1
        # Optional: remove from ghost since it's now active, 
        # or keep it until eviction to simplify logic. 
        # We delete it here to keep ghost small.
        del ghost_freq[key]
        
    # Calculate Priority
    # New object enters with base priority L + its density
    priority = inflation_L + (freq / size)
    
    metadata_store[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Move the evicted object's frequency to the Ghost list.
    2. Clean up active metadata.
    '''
    global metadata_store, ghost_freq
    
    if evicted_obj:
        key = evicted_obj.key
        
        if key in metadata_store:
            # Save frequency to history
            current_freq = metadata_store[key]['freq']
            ghost_freq[key] = current_freq
            
            # Remove from active metadata
            del metadata_store[key]

    # Maintenance: Limit Ghost Size to prevent memory leaks
    # If ghost map gets too big (e.g., > 2x capacity or fixed limit), clear some entries.
    # Simple limit:
    if len(ghost_freq) > 50000:
        ghost_freq.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgdvu538s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe729yuzo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5sck3a2w.pickle

Iteration 62: New subsample score 0.38434199999999996 is not better than old score 0.41526799999999997, skipping
Iteration 63: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvd4r9l_5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbvh7oa82.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe7v5jein.pickle

Iteration 63: Proposed new text for program: from collections import deque, OrderedDict

# S3-FIFO Data Structures
# S: Small FIFO queue (deque) for new items (probationary)
# M: Main FIFO queue (deque) for popular items
# G: Ghost registry (OrderedDict) for tracking keys of recently evicted items from S
# freq: Frequency map to track object utility
s_queue = deque()
m_queue = deque()
g_registry = OrderedDict()
freq_map = {}

# Tuning Parameters
# 10% of cache size for the probationary queue is a standard optimal value for S3-FIFO
SMALL_QUEUE_RATIO = 0.1
# Cap frequency to prevent "cache pollution" by extremely frequent items sticking too long
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    """
    S3-FIFO Eviction Strategy:
    - Maintains a Small Queue (S) and a Main Queue (M).
    - Evicts from S if it exceeds its target size ratio (10%), otherwise evicts from M.
    - Uses frequency bits to give items "second chances".
    """
    global s_queue, m_queue, g_registry, freq_map

    # Calculate target size for S based on current cache capacity
    cache_capacity = len(cache_snapshot.cache)
    s_target = max(1, int(cache_capacity * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    # Loop until we find a valid victim to evict
    while candidate_key is None:
        # Decision Logic: Which queue to drain?
        # 1. If S is strictly larger than its target, we prefer draining S.
        # 2. If M is empty, we must drain S.
        # 3. Otherwise, we drain M.
        
        drain_s = False
        if len(s_queue) > s_target:
            drain_s = True
        elif not m_queue:
            drain_s = True
        
        # Safety check: if logic says drain S but S is empty, switch to M
        if drain_s and not s_queue:
            drain_s = False
            # If both are empty (shouldn't happen in a full cache), we can't evict
            if not m_queue: return None

        if drain_s:
            # --- Process Small Queue (S) ---
            victim = s_queue[0] # Peek head
            v_freq = freq_map.get(victim, 0)
            
            if v_freq > 0:
                # Promotion: Item has been accessed. Move from S to M.
                # We do not decrement frequency here, giving it a solid start in M.
                s_queue.popleft()
                m_queue.append(victim)
            else:
                # Eviction: Item has 0 frequency. Evict it.
                candidate_key = s_queue.popleft()
                
                # Record in Ghost Registry to track this as a potential "false eviction"
                g_registry[candidate_key] = True
                # Maintain Ghost size roughly equal to Cache Size (FIFO eviction)
                if len(g_registry) > cache_capacity:
                    g_registry.popitem(last=False)
        else:
            # --- Process Main Queue (M) ---
            victim = m_queue[0] # Peek head
            v_freq = freq_map.get(victim, 0)
            
            if v_freq > 0:
                # Second Chance: Item has been accessed.
                # Reinsert at tail of M and decrement frequency (aging).
                m_queue.popleft()
                m_queue.append(victim)
                freq_map[victim] = v_freq - 1
            else:
                # Eviction: Item has 0 frequency. Evict it.
                # Items evicted from M are not tracked in Ghost (standard S3-FIFO behavior)
                candidate_key = m_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    """
    On Hit:
    - Increment frequency counter.
    - Cap at MAX_FREQ (3) to distinguish between 0 (evict), 1 (keep), 2+ (hot).
    - Note: No queue movement happens here (Lazy Promotion).
    """
    global freq_map
    key = obj.key
    curr_freq = freq_map.get(key, 0)
    if curr_freq < MAX_FREQ:
        freq_map[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    """
    On Insert:
    - Initialize frequency to 0.
    - Check Ghost Registry:
      - If present (Ghost Hit): It was recently evicted from S. Restore directly to M.
      - Else: Insert into probationary Small Queue (S).
    """
    global s_queue, m_queue, g_registry, freq_map
    
    key = obj.key
    freq_map[key] = 0 # Initialize frequency
    
    if key in g_registry:
        # Ghost Hit! This item was evicted from S recently but requested again.
        # It proved we made a mistake; restore it to the Main Queue.
        m_queue.append(key)
        del g_registry[key]
    else:
        # Standard Insert: Start in the probationary Small Queue.
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Cleanup:
    - Remove frequency metadata for the evicted object.
    """
    global freq_map
    key = evicted_obj.key
    if key in freq_map:
        del freq_map[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqj661iyd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl3pxhnpi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpupnpkbts.pickle

Iteration 63: New subsample score 0.876886 is better than old score 0.875311. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpth115c1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4nj_hwnb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpph7kg2cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6at4kl04.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq8c68620.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg5j5797f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2485i8ni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4e043kzj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsxasveau.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7xbxnzce.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzxs8g96s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_k5aiuil.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm4_b9z1e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps1nwujvj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk9e0eam3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw9sv5k2i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1g8poek5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph95e7ckr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn1pe8elo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfbg1eo_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvm6ralgq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf2zyn41j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu4adwv_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqu2iit9z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6wqc_2pc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp99yuhobq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz489upsx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps48vw_hu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd2ijc43g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpivt18ff6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd56bhp_b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnnakdiil.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps6qxs8uv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdtkl2g0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_qjws9dy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo89udb0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpselpkrb7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0_6555hb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy1afsj3b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx982kjh7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4zwyxhox.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppr679sfy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp97zl4em3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz_15cazz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe3esb0v9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptd31d9yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphjevacgq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl_27m285.pickle

Iteration 63: Full valset score for new program: 0.24750022916666659
Iteration 63: Full train_val score for new program: 0.24750022916666659
Iteration 63: Individual valset scores for new program: [0.494242, 0.469706, 0.478698, 0.434249, 0.492111, 0.478958, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.388099, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.382498, 0.026164, 0.058672, 0.058672, 0.269829, 0.332661, 0.806356, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.128084, 0.11179, 0.14939, 0.052632, 0.233333, 0.16269, 0.128471, 0.466258, 0.081699]
Iteration 63: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 63: Full valset pareto front score: 0.2641215
Iteration 63: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {7}]
Iteration 63: Best valset aggregate score so far: 0.2580827708333333
Iteration 63: Best program as per aggregate score on train_val: 5
Iteration 63: Best program as per aggregate score on valset: 5
Iteration 63: Best score on valset: 0.2580827708333333
Iteration 63: Best score on train_val: 0.2580827708333333
Iteration 63: Linear pareto front program index: 5
Iteration 63: New program candidate index: 18
Iteration 64: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpem5mleo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwyi_1ykm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa749qee9.pickle

Iteration 64: Proposed new text for program: from collections import deque

# Improved S3-FIFO Algorithm
# --------------------------------------------------------
# This implementation focuses on segregating the cache into a Small (Probationary)
# queue and a Main (Protected) queue. It uses a Ghost queue to detect
# patterns where items are evicted too early (false negatives).

# Global Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency counters for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of cache capacity is dedicated to the probationary queue.
# This filters out "one-hit wonders" (scan traffic) from polluting the main cache.
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using the S3-FIFO policy.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Calculate target byte size for Small Queue
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Iterate until a valid candidate is found
    while candidate_key is None:
        
        # --- Decision: Evict from Small (S) or Main (M)? ---
        # 1. If S exceeds its size budget, we evict from S to protect M.
        # 2. If M is empty, we must evict from S.
        # 3. Otherwise, we usually evict from M, but S3-FIFO prefers keeping S small.
        
        operate_on_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            operate_on_s = True
        
        # Failsafe: If one queue is empty, force switch to the other
        if operate_on_s and not s_queue:
            operate_on_s = False
        if not operate_on_s and not m_queue:
            operate_on_s = True
            
        if operate_on_s:
            # --- Small Queue Logic ---
            if not s_queue: break # Should not be reached if cache is full
            
            victim = s_queue[0] # Peek head
            
            # Handling lazy deletion consistency
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: S -> M
                # The item was accessed in S, so it's not a one-hit wonder.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Optimization: We do NOT reset frequency to 0 here.
                # If it was hit multiple times in S, it carries that priority to M.
            else:
                # Eviction: S -> Ghost
                # Item was not accessed in S (freq=0), so it's evicted.
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Record in Ghost Queue
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
                    
        else:
            # --- Main Queue Logic ---
            if not m_queue: break
            
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsertion (Second Chance)
                # If it has freq > 0, it gets sent to the back of M.
                m_queue.popleft()
                m_queue.append(victim)
                
                # Decay frequency: It costs 1 "life" to survive an eviction cycle.
                freq[victim] = v_freq - 1
            else:
                # Eviction from M
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # Bound the ghost queue size to be proportional to current item count.
    target_ghost_count = len(cache_map)
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: 
    1. If in Ghost -> Restore to Main (Recall).
    2. Else -> Insert into Small (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # New residents start with 0 frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: The item was recently evicted and is needed again.
        # It bypasses probation and goes to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Goes to Small queue for probation.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the object that was actually evicted.
    '''
    global freq
    k = evicted_obj.key
    # Remove frequency tracking for the evicted object
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfyihz220.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3aois_i0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp50t0dqdy.pickle

Iteration 64: New subsample score 0.501861 is not better than old score 0.503313, skipping
Iteration 65: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmyclvqy2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpro7y02w0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc5yitfa7.pickle

Iteration 65: Proposed new text for program: # Global state tracking
# Maps key -> {'freq': int, 'priority': float, 'last_access': int}
metadata_store = {}

# Ghost/History tracking for frequency of evicted items
# Maps key -> int (frequency)
history_freq = {}

# The dynamic aging factor (inflation value)
# Represents the priority of the last evicted object.
L_inflation = 0.0

def evict(cache_snapshot, obj):
    '''
    Selects a victim using the Greedy Dual Size Frequency (GDSF) policy.
    
    We search for the object with the lowest Priority value.
    Priority = L_inflation + (Frequency / Size)
    
    This favors:
    1. Small objects (1/Size is larger) -> Maximize Hit Rate
    2. Frequent objects (Frequency is larger)
    3. Recent objects (L_inflation boosts new/hit items)
    '''
    global metadata_store, L_inflation

    cache = cache_snapshot.cache
    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')

    # We must find the minimum priority in the cache.
    # While O(N) sounds expensive, for cache simulation constraints it is often
    # the only way without maintaining a complex parallel heap structure.
    # The complexity is acceptable for high-fidelity eviction decisions.
    
    # Pre-fetch candidates to avoid dict lookup overhead inside logic
    candidates = []
    
    for key, cached_obj in cache.items():
        if key in metadata_store:
            meta = metadata_store[key]
            p = meta['priority']
            l_acc = meta['last_access']
        else:
            # Fallback for sync issues (rare): treat as Freq=1, just inserted
            # P = L + 1/Size
            p = L_inflation + (1.0 / cached_obj.size)
            l_acc = 0
        
        if p < min_priority:
            min_priority = p
            min_last_access = l_acc
            victim_key = key
        elif p == min_priority:
            # Tie-breaker: LRU (Least Recently Used)
            # If priorities are identical, evict the one accessed longest ago
            if l_acc < min_last_access:
                min_last_access = l_acc
                victim_key = key

    # Critical GDSF Step: Update the global inflation factor L
    # The system "ages" by raising the floor to the priority of the evicted item.
    if victim_key is not None:
        L_inflation = min_priority

    # Fallback if cache is logically empty but technically has keys (safety)
    if victim_key is None and cache:
        return next(iter(cache))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using CURRENT L_inflation.
       (This effectively resets its aging, marking it as MRU relative to 'now').
    '''
    global metadata_store, L_inflation
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # GDSF Priority Update
        # P = L + (Freq / Size)
        # We use the current L_inflation. This brings the object "forward" in time.
        freq = meta['freq']
        size = obj.size
        meta['priority'] = L_inflation + (freq / size)
    else:
        # Should not happen on hit, but self-heal if necessary
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Retrieve historical frequency if available (Ghost Cache).
    2. Calculate initial Priority based on current L_inflation.
    '''
    global metadata_store, history_freq, L_inflation
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Restore frequency from history if it exists, otherwise start at 1
    freq = history_freq.get(key, 0) + 1
    
    # Calculate GDSF Priority
    # P = L + (Freq / Size)
    priority = L_inflation + (freq / obj.size)
    
    metadata_store[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Save frequency to history (so we know if this item is part of a working set).
    2. Clean up active metadata.
    '''
    global metadata_store, history_freq
    
    if evicted_obj:
        key = evicted_obj.key
        if key in metadata_store:
            # Persist frequency
            history_freq[key] = metadata_store[key]['freq']
            del metadata_store[key]
            
    # Maintenance: Prevent history from growing infinitely.
    # If history grows too large, it slows down lookups and wastes memory.
    # Cap at roughly 2x capacity or a fixed high number.
    if len(history_freq) > 50000:
        # Simple purge strategy: Clear it. 
        # Advanced strategies (LRU for history) are too expensive for this scope.
        history_freq.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzv71j_p5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm0jpyhk2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp91fay_72.pickle

Iteration 65: New subsample score 1.2630249999999998 is not better than old score 1.3543479999999999, skipping
Iteration 66: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmposp6ldh_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5i_1ryw2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8wzs17qj.pickle

Iteration 66: Proposed new text for program: import heapq
import itertools

# GDSF (Greedy Dual Size Frequency) Implementation

# Global Data Structures
# We use global variables to maintain state across function calls.

# Min-Heap storing [priority, unique_seq, key]
# Allows O(1) access to the item with the lowest priority (victim).
pq = []

# Dictionary to map key -> entry in pq. 
# Allows O(1) access to mark entries as invalid/removed (Lazy Deletion).
entry_finder = {}

# Dictionary to store frequency/history of keys.
# We retain this even after eviction to give returning items a "warm start".
frequency = {}

# Global aging factor (inflation value).
# Represents the priority of the most recently evicted object.
L = 0.0

# Sequence counter for stable heap sorting (tie-breaking).
counter = itertools.count()

# Sentinel value for lazy deletion in heap.
REMOVED = '<removed-task>'

def evict(cache_snapshot, obj):
    '''
    Selects the victim to evict using GDSF logic.
    We evict the item with the lowest calculated priority.
    '''
    global L, pq, entry_finder, REMOVED
    
    cache = cache_snapshot.cache
    
    # Loop until we find a valid victim that is currently in the cache
    while pq:
        # Pop the item with the lowest priority from the heap
        priority, count, key = heapq.heappop(pq)
        
        # Check if this heap entry is valid (not marked as REMOVED during a previous update)
        if key is not REMOVED:
            # Verify the key is actually in the cache (Handle potential sync edges)
            if key in cache:
                # This is our victim.
                # Update the global aging factor L to the priority of this evicted item.
                # This effectively "ages" all other items in the cache relative to new insertions.
                L = priority
                
                # Clean up the entry finder map
                if key in entry_finder:
                    del entry_finder[key]
                
                return key
            else:
                # Item was in heap but not in cache (stale state), clean metadata and continue
                if key in entry_finder:
                    del entry_finder[key]
    
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on current L, Frequency, and Size.
    3. Update Heap (using lazy deletion of the old entry).
    '''
    global L, pq, entry_finder, frequency, counter, REMOVED
    
    key = obj.key
    
    # 1. Update Frequency
    # We increment the count. This persistence helps identify popular items 
    # even if they haven't been accessed recently (Recency vs Frequency).
    freq = frequency.get(key, 0) + 1
    frequency[key] = freq
    
    # 2. Mark old heap entry as REMOVED (Lazy Deletion)
    if key in entry_finder:
        old_entry = entry_finder[key]
        # Set the key field to the sentinel value
        old_entry[-1] = REMOVED
    
    # 3. Calculate New Priority
    # GDSF Formula: Priority = L + (Frequency / Size)
    # This boosts small, frequent items.
    priority = L + (float(freq) / float(obj.size))
    
    # 4. Push new entry to heap
    new_entry = [priority, next(counter), key]
    entry_finder[key] = new_entry
    heapq.heappush(pq, new_entry)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Recover Frequency (History).
    2. Calculate initial Priority.
    3. Add to Heap.
    '''
    global L, pq, entry_finder, frequency, counter, REMOVED
    
    key = obj.key
    
    # 1. Frequency
    # If key is in 'frequency' map (it was evicted previously), we restore its count.
    # This makes the cache scan-resistant while protecting working sets.
    freq = frequency.get(key, 0) + 1
    frequency[key] = freq
    
    # 2. Calculate Priority
    # New items enter with the current base L value.
    priority = L + (float(freq) / float(obj.size))
    
    # 3. Add to Heap
    new_entry = [priority, next(counter), key]
    entry_finder[key] = new_entry
    heapq.heappush(pq, new_entry)
    
    # Maintenance: Prune frequency history if it grows excessively large
    # to prevent memory leaks in very long traces.
    if len(frequency) > 100000:
        frequency.clear()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Metadata cleanup.
    Most work is done in evict(), but this function satisfies the interface.
    We intentionally do NOT remove the key from 'frequency' here to preserve history.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_59zvux6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb7n9n3xh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo8o1p128.pickle

Iteration 66: New subsample score 0.8119700000000001 is not better than old score 0.8776299999999999, skipping
Iteration 67: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt26lt0u0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphupzyi63.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmw076ghj.pickle

Iteration 67: Proposed new text for program: import math

# GDSF (Greedy Dual Size Frequency) Implementation
# 
# We maintain:
# 1. metadata_store: Dictionary tracking Frequency, Size, Last Access, and Priority.
# 2. L: A global "aging factor" or "inflation value".
#
# Logic:
# The priority of an object is calculated as: P = L + (Frequency / Size).
# - When an object is Hit, we update its Frequency and recalculate P using the CURRENT L.
#   This resets its age relative to the eviction threshold.
# - When an object is Inserted, it starts with Frequency 1 and P = L + (1/Size).
# - When the cache is full, we Evict the object with the minimum P.
# - Crucially, we then update L = P_min. This raises the "floor" for all future objects,
#   ensuring that old high-frequency objects eventually become eviction candidates if they stop being accessed.

metadata_store = {}
L = 0.0

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict based on the minimum GDSF score (Priority).
    Tie-breaker: Least Recently Used (LRU).
    '''
    global metadata_store, L
    cache = cache_snapshot.cache
    
    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')
    
    # Iterate over the valid keys in the cache to find the victim
    for key in cache:
        # Safety check if metadata is in sync
        if key in metadata_store:
            meta = metadata_store[key]
            p = meta['priority']
            last_access = meta['last_access']
            
            # We want to evict the item with the LOWEST priority.
            if p < min_priority:
                min_priority = p
                victim_key = key
                min_last_access = last_access
            elif p == min_priority:
                # Tie-breaker: If priorities are identical, evict the older one (LRU)
                if last_access < min_last_access:
                    victim_key = key
                    min_last_access = last_access
        else:
            # If metadata is missing for some reason, this is a prime victim
            return key
            
    # If we found a victim, update the global aging factor L
    if victim_key is not None:
        L = min_priority
        return victim_key
    
    # Fallback for empty cache or edge cases
    if cache:
        return next(iter(cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using current L (Restoring the item's value).
    '''
    global metadata_store, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # GDSF Hit Update Rule:
        # The item is "brought forward" to the current time/age context.
        # Priority = L + (Frequency / Size)
        # Note: We cast to float to ensure precision.
        meta['priority'] = L + (float(meta['freq']) / float(meta['size']))
    else:
        # If metadata is missing on a hit, reconstruct it as an insert
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Set initial Priority based on L.
    '''
    global metadata_store, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initial frequency
    freq = 1
    
    # Initial Priority Calculation
    # P = L + (1 / Size)
    # The new item must beat the current aging floor (L) plus its size cost to survive.
    priority = L + (float(freq) / float(obj.size))
    
    metadata_store[key] = {
        'freq': freq,
        'size': obj.size,
        'last_access': current_time,
        'priority': priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to prevent memory leaks. 
    We do NOT keep ghost history in this implementation because GDSF's L factor 
    handles re-entry costs naturally.
    '''
    global metadata_store
    
    if evicted_obj:
        key = evicted_obj.key
        if key in metadata_store:
            del metadata_store[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1860qo5w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2kd2foni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf31y72os.pickle

Iteration 67: New subsample score 0.682376 is not better than old score 0.717491, skipping
Iteration 68: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgtv_8jeg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptpqfiddf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4pxtsx4d.pickle

Iteration 68: Proposed new text for program: import math

# Global state to maintain metadata across function calls
# Maps key -> { 'freq': int, 'size': int, 'last_access': int, 'H': float }
metadata_store = {}

# "L" is the aging factor (the priority of the last evicted object).
# It allows new objects to start with a non-zero priority relative to the current cache state.
L = 0.0

# History buffer to remember frequencies of recently evicted items.
# This helps items that are part of a working set larger than the cache recover their status quickly.
# key -> freq
history_freq = {}
HISTORY_CAPACITY = 20000 

def get_priority(freq, size, L_val):
    '''
    Calculates GDSF priority.
    Priority = L + (Frequency / Size)
    
    We apply a slight dampening to Size to prevent penalizing large objects too harshly if they are very frequent.
    '''
    # Using log of size or a root of size can soften the penalty, 
    # but standard GDSF uses linear size. Let's stick to standard GDSF for robustness,
    # but ensure freq has enough weight.
    return L_val + (float(freq) / float(size))

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Find the object with the minimum H-value (Priority).
    The global aging factor L is updated to this minimum value.
    '''
    global metadata_store, L
    
    cache = cache_snapshot.cache
    victim_key = None
    min_H = float('inf')
    
    # Iterate to find the item with the lowest H value.
    # While iterating, we re-calculate H based on current L isn't strictly necessary 
    # because H is stored, but we must compare the stored H values.
    
    # Tie-breaking variables
    victim_last_access = float('inf')
    
    # To optimize performance, we assume metadata_store is in sync with cache.
    # We iterate over the cache keys.
    for key in cache:
        if key not in metadata_store:
            # Fallback for sync issues: treat as new item with freq 1
            h_score = L + (1.0 / cache[key].size)
            l_access = 0
        else:
            meta = metadata_store[key]
            h_score = meta['H']
            l_access = meta['last_access']
        
        if h_score < min_H:
            min_H = h_score
            victim_key = key
            victim_last_access = l_access
        elif h_score == min_H:
            # Tie-breaker: LRU (Least Recently Used)
            # Evict the one accessed furthest in the past
            if l_access < victim_last_access:
                victim_key = key
                victim_last_access = l_access

    # Update the global aging factor L to the priority of the evicted object.
    # This raises the floor for all future insertions.
    if victim_key is not None:
        L = min_H
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update H value: H = L + (Freq / Size)
    3. Update Last Access Time.
    '''
    global metadata_store, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        # Re-calculate priority with new frequency and CURRENT L
        meta['H'] = L + (float(meta['freq']) / float(meta['size']))
    else:
        # Should typically be handled in insert, but safety check for hit logic
        base_freq = history_freq.get(key, 1)
        metadata_store[key] = {
            'freq': base_freq,
            'size': obj.size,
            'last_access': current_time,
            'H': L + (float(base_freq) / float(obj.size))
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine initial frequency (check history).
    2. Calculate initial H value = L + (Freq / Size).
    '''
    global metadata_store, history_freq, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Restore frequency from history if available, else start at 1
    initial_freq = history_freq.get(key, 1)
        
    # Calculate initial priority
    # Note: New objects get the benefit of the current aging factor L immediately.
    priority = L + (float(initial_freq) / float(obj.size))
    
    metadata_store[key] = {
        'freq': initial_freq,
        'size': obj.size,
        'last_access': current_time,
        'H': priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move frequency to history.
    2. Clean up metadata.
    '''
    global metadata_store, history_freq
    
    if evicted_obj:
        evicted_key = evicted_obj.key
        
        if evicted_key in metadata_store:
            freq = metadata_store[evicted_key]['freq']
            
            # Save to history for "Ghost" tracking
            history_freq[evicted_key] = freq
            
            # Clean up active metadata
            del metadata_store[evicted_key]
            
    # Maintenance: Limit the size of history to prevent memory issues
    # Simple FIFO or Random purge is fine for simulation constraints.
    if len(history_freq) > HISTORY_CAPACITY:
        # Remove an arbitrary item (iter is deterministic in Py3.7+ but effectively random for caching logic)
        del history_freq[next(iter(history_freq))]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz16d9zw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpys8bjngi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxw9zngl2.pickle

Iteration 68: New subsample score 1.548104 is not better than old score 1.594565, skipping
Iteration 69: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpec1z1f20.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb7g7z86k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8yhj380h.pickle

Iteration 69: Proposed new text for program: import heapq

# --- Global State for GDSF (Greedy Dual-Size Frequency) ---

# Stores metadata for objects currently in the cache.
# Mapping: key -> {'freq': int, 'priority': float, 'size': int, 'tie_breaker': int}
metadata = {}

# Priority Queue (Min-Heap) for finding the victim efficiently.
# Stores tuples: (priority, tie_breaker, key)
# We use a tie_breaker (counter) to maintain stability/FIFO for equal priorities.
pq = []

# The "Inflation" or "Aging" value. 
# This roughly represents the minimum value required to stay in the cache.
L = 0.0

# Counter for tie-breaking in the priority queue
counter = 0

def get_priority(freq, size):
    '''
    Calculates the GDSF priority value.
    Formula: P = L + (Frequency / Size)
    '''
    global L
    # Safety check for size 0 to avoid division by zero (though size is defined positive)
    safe_size = size if size > 0 else 1
    return L + (freq / safe_size)

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict based on the lowest GDSF priority value.
    '''
    global pq, metadata, L
    
    victim_key = None
    
    # We use a loop to handle "Lazy Deletion" from the heap.
    # When an object is updated (Hit), we push a new entry to the heap but don't 
    # remove the old one immediately (too expensive). We skip invalid entries here.
    while pq:
        priority, tb, key = pq[0]
        
        # 1. Check if the key still exists in our metadata
        if key not in metadata:
            heapq.heappop(pq)
            continue
            
        current_meta = metadata[key]
        
        # 2. Check if this heap entry is stale
        # If the priority or tie_breaker in the heap doesn't match the metadata,
        # it means this object was updated/hit later, so this entry is invalid.
        if current_meta['priority'] != priority or current_meta['tie_breaker'] != tb:
            heapq.heappop(pq)
            continue
            
        # 3. Found a valid victim (Lowest Priority)
        heapq.heappop(pq)
        victim_key = key
        
        # 4. Update the Aging Factor L
        # In GDSF, L is updated to the priority of the evicted object.
        # This effectively "ages" all other objects remaining in the cache.
        L = priority
        break
    
    # Failsafe: If heap is desynchronized (should not happen), pick arbitrary
    if victim_key is None and metadata:
        victim_key = next(iter(metadata))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    1. Increment the object's frequency.
    2. Recalculate its priority using the current L.
    3. Push the new priority to the heap.
    '''
    global metadata, pq, counter, L
    
    k = obj.key
    if k in metadata:
        meta = metadata[k]
        meta['freq'] += 1
        
        # Recalculate priority.
        # By adding current L, we bring this object "up to date" with the
        # current aging factor, effectively renewing its lease in the cache.
        new_priority = get_priority(meta['freq'], meta['size'])
        
        counter += 1
        meta['priority'] = new_priority
        meta['tie_breaker'] = counter
        
        # Push new entry to heap (Lazy delete handles the old entry later)
        heapq.heappush(pq, (new_priority, counter, k))

def update_after_insert(cache_snapshot, obj):
    '''
    On New Insertion:
    1. Initialize frequency to 1.
    2. Calculate initial priority relative to current L.
    3. Add to metadata and heap.
    '''
    global metadata, pq, counter, L
    
    k = obj.key
    size = obj.size
    
    # New objects start with Frequency 1
    freq = 1
    new_priority = get_priority(freq, size)
    
    counter += 1
    
    metadata[k] = {
        'freq': freq,
        'size': size,
        'priority': new_priority,
        'tie_breaker': counter
    }
    
    heapq.heappush(pq, (new_priority, counter, k))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up the metadata dictionary.
    '''
    global metadata
    
    if evicted_obj and evicted_obj.key in metadata:
        del metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzw8vctao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn7e9dprf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwe7id0fn.pickle

Iteration 69: New subsample score 0.836874 is not better than old score 0.839234, skipping
Iteration 70: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqc7fkws5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphflgfb4u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp21whrank.pickle

Iteration 70: Proposed new text for program: import math

# Global state for GDSF
# Stores metadata for items CURRENTLY in the cache
# Key -> {'H': float, 'freq': int, 'last_access': int, 'size': int}
metadata_store = {}

# The aging factor 'L'
# This value monotonically increases (mostly) and acts as the "inflation" base
# for new items entering the cache.
L_value = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF (Greedy Dual Size Frequency) Eviction Policy.
    We evict the item with the lowest H-value.
    H(p) = L + (Frequency(p) / Size(p))
    
    Tie-breaking:
    If multiple items have the same lowest H-value, we evict the one 
    that was accessed least recently (LRU).
    '''
    global metadata_store, L_value
    
    cache = cache_snapshot.cache
    
    victim_key = None
    min_H = float('inf')
    victim_last_access = float('inf')
    
    # Iterate over the cache to find the victim
    # While O(N), this is necessary to find the true minimum in a dynamically updating
    # scoring system without complex heap management overheads in this constraint.
    for key, cached_obj in cache.items():
        if key in metadata_store:
            meta = metadata_store[key]
            h_val = meta['H']
            l_acc = meta['last_access']
        else:
            # Fallback if metadata is desynchronized (defensive coding)
            # Assign priority as if it were a new item inserted at current L
            h_val = L_value + (1.0 / cached_obj.size)
            l_acc = 0
        
        # Look for strictly smaller H, or same H with older access time (LRU)
        if h_val < min_H:
            min_H = h_val
            victim_key = key
            victim_last_access = l_acc
        elif h_val == min_H:
            if l_acc < victim_last_access:
                victim_key = key
                victim_last_access = l_acc
    
    # If we found a victim, update the global aging factor L
    if victim_key is not None:
        L_value = min_H
        return victim_key
    
    # Fallback for empty cache (should not occur)
    if cache:
        return next(iter(cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate H-value: H = L_current + (Freq / Size).
       Updating using L_current "brings the item forward" to the current 
       aging generation, protecting it from immediate eviction.
    '''
    global metadata_store, L_value
    key = obj.key
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = cache_snapshot.access_count
        
        # GDSF Score Calculation
        # We use float to ensure precision
        priority = float(meta['freq']) / float(meta['size'])
        meta['H'] = L_value + priority
    else:
        # If metadata is missing for a hit item, re-initialize it
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate H = L_current + (1 / Size).
    '''
    global metadata_store, L_value
    key = obj.key
    
    # New items start with Frequency 1
    priority = 1.0 / float(obj.size)
    
    metadata_store[key] = {
        'freq': 1,
        'size': obj.size,
        'last_access': cache_snapshot.access_count,
        'H': L_value + priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Remove the evicted object's metadata.
    We do not maintain history (Ghosts) in this implementation to prevent 
    cache pollution in scanning workloads (Looping patterns).
    '''
    global metadata_store
    if evicted_obj and evicted_obj.key in metadata_store:
        del metadata_store[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqmpjxhxb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpspwu1d52.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp60y94vga.pickle

Iteration 70: New subsample score 0.551823 is not better than old score 0.570149, skipping
Iteration 71: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp28a1xl1t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzvedkuwz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3drapse8.pickle

Iteration 71: Proposed new text for program: from collections import deque

# S3-FIFO Optimized Data Structures
# We use global variables to maintain state across function calls
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters (0 to MAX_FREQ)

# Byte tracking
s_bytes = 0
m_bytes = 0

# Constants
# 10% is the theoretical optimum for S3-FIFO to balance scan resistance and recency
SMALL_RATIO = 0.1  
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Executes the S3-FIFO eviction policy.
    1. Check Small (S) queue size. If > 10% of capacity or Main (M) is empty, evict from S.
    2. Else, evict from M.
    3. Use Ghost queue to rescue items evicted from S early (Loop detection).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    # We loop until a victim is successfully chosen
    while victim_key is None:
        
        # --- Logic Step 1: Decide which queue to evict from ---
        # We evict from S if:
        # a) S is larger than its target allocation (10%)
        # b) OR M is empty (we have no choice)
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
            
        # Failsafe: If logic says S, but S is actually empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue (S) ---
            candidate = s_queue[0]
            
            # 1. Handle Stale Keys (Lazy Deletion)
            if candidate not in cache:
                s_queue.popleft()
                # We can't adjust s_bytes here easily because we don't know the size 
                # of the missing object, but usually update_after_evict handles the decrement.
                # Just continue to find a real candidate.
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            # 2. Check for Promotion
            if cand_freq > 0:
                # HIT in S: Promote to M
                s_queue.popleft()
                # We must safely get size
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                m_queue.append(candidate)
                m_bytes += obj_size
                
                # S3-FIFO Logic: Reset frequency on promotion.
                # It must "earn" its stay in M again.
                freq[candidate] = 0
            else:
                # 3. NO HIT in S: Evict
                victim_key = s_queue.popleft()
                obj_size = cache[candidate].size
                s_bytes -= obj_size
                
                # Insert into Ghost Queue (Loop Detection)
                # Only if not already there
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
        
        else:
            # --- Eviction from Main Queue (M) ---
            candidate = m_queue[0]
            
            # 1. Handle Stale Keys
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            # 2. Check for Second Chance
            if cand_freq > 0:
                # Has been accessed while in M: Reinsert at tail (Second Chance)
                m_queue.popleft()
                m_queue.append(candidate)
                # Decay frequency
                freq[candidate] = cand_freq - 1
            else:
                # 3. Frequency exhausted: Evict from M
                victim_key = m_queue.popleft()
                obj_size = cache[candidate].size
                m_bytes -= obj_size
                # Items evicted from M are usually NOT added to Ghost in standard S3-FIFO
    
    # --- Ghost Queue Management ---
    # Keep Ghost queue bounded. Usually roughly the number of items in Main cache is a good heuristic.
    # However, to be safe against small object attacks, we limit by length relative to cache count.
    # We use a slightly larger window (0.5 to 1.0 of cache count) to catch longer loops.
    current_count = len(cache)
    max_ghost_len = max(current_count, 100) # Minimum buffer
    
    while len(g_set) > max_ghost_len:
        # Remove oldest from ghost
        if g_queue:
            oldest = g_queue.popleft()
            if oldest in g_set:
                g_set.remove(oldest)
        else:
            break
            
    # Clean head of ghost queue if it contains items already removed from set (lazy sync)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, capped at MAX_FREQ.
    This applies to items in both S and M.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    Logic:
    - If in Ghost: It was evicted recently from S. This implies a "loop" pattern.
      Rescue it directly to Main (M).
    - Else: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Restore to Main
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Start in Small
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    Note: We do NOT remove from Ghost queue here. 
    The Ghost queue tracks items *after* they leave the cache.
    We only clean up the frequency map.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd6lidlve.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp17ih8j2z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy8vq8dom.pickle

Iteration 71: New subsample score 0.773756 is not better than old score 0.804374, skipping
Iteration 72: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbxmjp3ml.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxth135f7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp80awl6i2.pickle

Iteration 72: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# We use global variables to maintain state across function calls
s_queue = deque()      # Small/Probationary Queue (FIFO) - contains keys
m_queue = deque()      # Main/Protected Queue (FIFO) - contains keys
g_queue = deque()      # Ghost Queue (FIFO) - contains keys
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency counters (0 to MAX_FREQ)

# Byte tracking
s_bytes = 0
m_bytes = 0

# Constants
SMALL_RATIO = 0.1      # 10% of cache size dedicated to Small Queue
MAX_FREQ = 3           # Cap on frequency to prevent history pollution

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using the S3-FIFO strategy.
    
    Logic:
    1. If Small Queue (S) is larger than 10% of cache OR Main Queue (M) is empty, 
       we look at S.
    2. If S-head has been accessed (freq > 0), promote to M (Second Chance).
    3. If S-head has NOT been accessed, evict it and add key to Ghost Queue.
    4. If we look at M, if M-head has been accessed, reinsert to M-tail (decrement freq).
    5. If M-head has exhausted freq, evict it (do not add to Ghost).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Decide which queue to serve
        # We process S if it's "too big" or if M is empty.
        evict_from_s = (s_bytes >= target_s_size) or (not m_queue)
        
        # Failsafe: If S is empty but logic pointed there, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Process Small Queue ---
            if not s_queue: 
                # Should be impossible if cache is full, but safety break
                break 
                
            candidate_key = s_queue[0]
            
            # Cleaning stale references (items deleted externally or previously evicted)
            if candidate_key not in cache:
                s_queue.popleft()
                # Metadata cleanup might happen in update_after_evict, 
                # but we ensure consistency here just in case.
                if candidate_key in freq: del freq[candidate_key]
                continue

            candidate_obj = cache[candidate_key]
            candidate_freq = freq.get(candidate_key, 0)
            
            if candidate_freq > 0:
                # HIT in S: Promote to M
                # Remove from S
                s_queue.popleft()
                s_bytes -= candidate_obj.size
                
                # Insert into M
                m_queue.append(candidate_key)
                m_bytes += candidate_obj.size
                
                # Reset frequency: It must earn its keep in M now
                freq[candidate_key] = 0
            else:
                # MISS in S: Evict
                victim_key = s_queue.popleft()
                s_bytes -= candidate_obj.size
                
                # Add to Ghost Queue (Key only)
                if victim_key not in g_set:
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
                
                # Ghost queue cleanup: Keep ghost count roughly equal to Main count 
                # (or just simple list size management). 
                # A heuristic of "Same number of items as M" works well.
                while len(g_set) > len(m_queue) + len(s_queue) and g_queue:
                    oldest = g_queue.popleft()
                    if oldest in g_set:
                        g_set.remove(oldest)

        else:
            # --- Process Main Queue ---
            if not m_queue:
                break
                
            candidate_key = m_queue[0]
            
            if candidate_key not in cache:
                m_queue.popleft()
                if candidate_key in freq: del freq[candidate_key]
                continue
                
            candidate_freq = freq.get(candidate_key, 0)
            
            if candidate_freq > 0:
                # HIT in M: Give Second Chance
                # Move to back of M, decrement frequency
                m_queue.popleft()
                m_queue.append(candidate_key)
                freq[candidate_key] = candidate_freq - 1
            else:
                # EXPIRED in M: Evict
                victim_key = m_queue.popleft()
                m_bytes -= cache[candidate_key].size
                # Items evicted from M usually don't go to Ghost in S3-FIFO

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, capped at MAX_FREQ (3).
    High frequency means high utility.
    '''
    global freq
    k = obj.key
    curr_freq = freq.get(k, 0)
    if curr_freq < MAX_FREQ:
        freq[k] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    1. If key is in Ghost Queue -> It was evicted too early. Insert into M (Main).
    2. Otherwise -> New item. Insert into S (Small).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Restore to Main Queue
        m_queue.append(k)
        m_bytes += size
        
        # Remove from Ghost
        g_set.remove(k)
        # We don't lazily remove from g_queue; we rely on set check in evict/cleanup
    else:
        # Standard Insert: Start in Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    The object is already physically removed from queues in `evict`, 
    but we ensure frequency map is clean.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphusef9yk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3oyhhrw2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmrkijyr8.pickle

Iteration 72: New subsample score 0.189507 is better than old score 0.113676. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp042xi4rq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpynb4p5ap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph2i0n0x4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9u261e2t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi0xk1q99.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp54wqy9se.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa0leinqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx963wknf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqly5o87j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptiikofb0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjgif87li.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuwgq15lf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpllyi6m3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnu7kkme8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl1v0c1lb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphwx_7pfq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6amlpzx1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq_c42yzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmgac4huk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk5udsmmv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe570f9zd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzmq8la1g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg2bvaxl3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpes0t5exm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo9589z_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwynt4_ou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4_ql9b6t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8kyt7mii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp68pavm9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0y5qliyj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppayrf0b2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpagitgzb5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9p2kqhhu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfk9wvmgo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb39we6q0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpllq891nm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpijpl8rno.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnb79q3nv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp84y_gq78.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptclskxde.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcr32j7pm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpip8auvwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpci7p1mgs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbvdw3dei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc9689vr1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz6f4wq28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpszcu8hlk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpckc0_6om.pickle

Iteration 72: Full valset score for new program: 0.2506418125
Iteration 72: Full train_val score for new program: 0.2506418125
Iteration 72: Individual valset scores for new program: [0.495641, 0.470348, 0.480153, 0.434427, 0.49302, 0.48161, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.388988, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269808, 0.350806, 0.819265, 0.892179, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.753233, 0.083333, 0.067961, 0.076964, 0.640392, 0.125461, 0.146105, 0.126451, 0.164839, 0.052632, 0.283333, 0.16269, 0.143619, 0.466258, 0.081699]
Iteration 72: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.038636, 0.045558, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.052632, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 72: Full valset pareto front score: 0.2641215
Iteration 72: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18, 19}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {13}, {8}, {8}, {13}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {2}, {5}, {2}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}]
Iteration 72: Best valset aggregate score so far: 0.2580827708333333
Iteration 72: Best program as per aggregate score on train_val: 5
Iteration 72: Best program as per aggregate score on valset: 5
Iteration 72: Best score on valset: 0.2580827708333333
Iteration 72: Best score on train_val: 0.2580827708333333
Iteration 72: Linear pareto front program index: 5
Iteration 72: New program candidate index: 19
Iteration 73: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpi8mjdv_w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6u5yp79d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4517t4ba.pickle

Iteration 73: Proposed new text for program: from collections import OrderedDict

# Global structures for Adaptive Segmented LRU
# We use OrderedDict as it serves as a doubly-linked list + hash map (LRU)
t1 = OrderedDict()       # T1: Probationary segment (recent items)
t2 = OrderedDict()       # T2: Protected segment (frequent items)
b1 = OrderedDict()       # B1: Ghost segment for T1 (evicted from T1)
b2 = OrderedDict()       # B2: Ghost segment for T2 (evicted from T2)

# Size tracking
t1_size = 0
t2_size = 0

# Adaptive parameter 'p'
# p represents the target size for T1. 
# If hits happen in B1 (ghost T1), we increase p (grow T1).
# If hits happen in B2 (ghost T2), we decrease p (shrink T1, grow T2).
p = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts an object to make space. 
    Implements the "replace" logic of Adaptive Replacement Cache (ARC/SLRU).
    '''
    global t1, t2, b1, b2, t1_size, t2_size, p

    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity

    victim_key = None

    # We need to loop because a selected victim might be stale (already deleted externally)
    # or we might need to perform directory replacements (B1/B2) before finding a real victim.
    while victim_key is None:
        
        # ARC Replacement Logic adapted for byte-sized cache:
        # We evict from T1 if T1 is "too big" relative to p, OR if we have to.
        # Condition: len(t1) > p  (roughly translated to size here)
        # However, strictly:
        # if (t1 is not empty) and ((t1_size > p) or (b2 is not empty and t1_size == p)) -> this logic is tricky with bytes.
        
        # simplified byte-based heuristic:
        # If T1 has data and is exceeding the adaptive target 'p', we prefer evicting from T1.
        # Otherwise, we evict from T2.
        
        # Note: If T1 is empty, we must evict from T2. If T2 is empty, we must evict from T1.
        
        evict_from_t1 = False
        
        if t1 and t1_size > p:
            evict_from_t1 = True
        elif t1 and not t2:
            evict_from_t1 = True
        elif not t1 and t2:
            evict_from_t1 = False
        elif t1 and t2:
            # Both have items, T1 is under quota p.
            # But we are full. Standard ARC would evict T2 here to enforce T2 constraint.
            evict_from_t1 = False
            
        if evict_from_t1:
            # Pop LRU from T1
            k, _ = t1.popitem(last=False)
            
            # Check for staleness
            if k not in cache:
                # Just remove from our tracking and retry
                continue
                
            victim_key = k
            victim_obj = cache[k]
            t1_size -= victim_obj.size
            
            # Move to Ghost B1
            # We only store the key, not the object
            b1[k] = None
        else:
            # Pop LRU from T2
            k, _ = t2.popitem(last=False)
            
            if k not in cache:
                continue
                
            victim_key = k
            victim_obj = cache[k]
            t2_size -= victim_obj.size
            
            # Move to Ghost B2
            b2[k] = None

    # Limit Ghost Sizes to prevent unbounded memory growth
    # ARC typically limits |B1| + |B2| <= Capacity (in items).
    # Since we deal with bytes, we just pick a reasonable item limit count based on cache item count.
    # A safe heuristic is 2x current item count.
    current_count = len(cache)
    max_ghost_items = max(current_count, 100) * 2
    
    while len(b1) > max_ghost_items:
        b1.popitem(last=False)
    while len(b2) > max_ghost_items:
        b2.popitem(last=False)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. If in T1, move to T2 (Probation -> Protected).
    2. If in T2, move to MRU of T2.
    '''
    global t1, t2, t1_size, t2_size
    
    k = obj.key
    size = obj.size
    
    if k in t1:
        # Promote T1 -> T2
        del t1[k]
        t1_size -= size
        
        t2[k] = None # Value doesn't matter, using as Set/LinkedList
        t2_size += size
        
    elif k in t2:
        # Move to MRU of T2
        t2.move_to_end(k)

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion of a new object.
    Checks ghosts to adapt 'p' (target size of T1).
    '''
    global t1, t2, b1, b2, t1_size, t2_size, p
    
    k = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Adapt p based on Ghost Hits
    # Delta logic from ARC
    if k in b1:
        # Hit in Ghost T1 (B1): We should have kept T1 larger.
        # Increase p.
        delta = 1
        if len(b1) < len(b2):
            delta = len(b2) // len(b1)
        
        # We act on byte sizes for p
        # Since 'delta' is item logic, we scale it by current object size roughly
        p = min(capacity, p + max(size, 1024) * delta)
        
        del b1[k]
        
        # Move to T2 (It was a recall)
        t2[k] = None
        t2_size += size
        
    elif k in b2:
        # Hit in Ghost T2 (B2): We should have kept T2 larger (so T1 smaller).
        # Decrease p.
        delta = 1
        if len(b2) < len(b1):
            delta = len(b1) // len(b2)
            
        p = max(0, p - max(size, 1024) * delta)
        
        del b2[k]
        
        # Move to T2 (Recall)
        t2[k] = None
        t2_size += size
        
    else:
        # Totally new item (or forgotten from ghosts).
        # Insert into T1 (Probation)
        t1[k] = None
        t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    No specific metadata cleanup required here as it's handled in evict().
    However, we ensure consistency if needed.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1ilsyfrz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe_gki0ar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkll1oxka.pickle

Iteration 73: New subsample score 0.199711 is better than old score 0.142866. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_atab_a4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpujjnf3vv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpydzlim2j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt3z5100l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfpxhvf03.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg40r_6b8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmbd6dbc9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp31fjit64.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqn_66c5v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp53pcvdju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfctadtah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpex414uc1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8f3tb298.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpau8cojfi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsyr2v4gv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg01nvigo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpran7rvhm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmrxk84kz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbndptsu4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3zubf_ez.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprvbgb90l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5v8dna7r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiqyhb_77.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpikh7y0gj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxvbm05q6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_d4i_9bn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfen9zjb2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp70fbypv0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcd1u1wj6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmu9pjcvn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr0aa5cn8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpba982dpw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl4_6pj06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe3z6rr0a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaudaq8vv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpce6zs9a7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptnshjbhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1db5gdu0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp64nlw0vw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz190ljqz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9fnokf5b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg0wz8il5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmpbvqmnz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeeq4afv7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn47bueae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpziok0wf0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_454sumt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp14dnnvmn.pickle

Iteration 73: Full valset score for new program: 0.22391518749999997
Iteration 73: Full train_val score for new program: 0.22391518749999997
Iteration 73: Individual valset scores for new program: [0.467337, 0.446241, 0.452639, 0.393172, 0.46692, 0.449729, 0.246411, 0.497641, 0.519843, 0.531017, 0.075, 0.296181, 0.040045, 0.0, 0.018547, 0.019019, 0.018446, 0.020945, 0.021235, 0.265811, 0.353982, 0.025478, 0.058672, 0.058672, 0.269774, 0.320565, 0.838133, 0.880541, 0.022462, 0.084091, 0.056948, 0.000192, 0.000132, 0.721749, 0.074561, 0.082966, 0.009183, 0.582174, 0.125461, 0.023843, 0.023824, 0.024718, 0.055921, 0.25, 0.021575, 0.024123, 0.466258, 0.045752]
Iteration 73: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.084091, 0.056948, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.055921, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 73: Full valset pareto front score: 0.26537429166666665
Iteration 73: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18, 19}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {13}, {8}, {8}, {13}, {5}, {20}, {20}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {2}, {5}, {2}, {20}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {7}]
Iteration 73: Best valset aggregate score so far: 0.2580827708333333
Iteration 73: Best program as per aggregate score on train_val: 5
Iteration 73: Best program as per aggregate score on valset: 5
Iteration 73: Best score on valset: 0.2580827708333333
Iteration 73: Best score on train_val: 0.2580827708333333
Iteration 73: Linear pareto front program index: 5
Iteration 73: New program candidate index: 20
Iteration 74: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd60evyw7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0ld4jjee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaboeo9su.pickle

Iteration 74: Proposed new text for program: from collections import deque

# --- Global State for Segmented LRU (SLRU) ---

# T1: Probationary Segment (New items, Scan Resistant)
# Implemented as a Deque for FIFO/LRU behavior.
t1_queue = deque()
t1_set = set()

# T2: Protected Segment (Frequent items)
# Implemented as a Deque for LRU behavior.
t2_queue = deque()
t2_set = set()
t2_current_bytes = 0

# Configuration
# Portion of cache capacity dedicated to the Protected (Frequent) segment.
# 80% allows 20% churn for new items, which is robust for most workloads.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict.
    Strategy: SLRU.
    1. Prefer evicting from T1 (Probationary).
    2. If T1 is empty, evict from T2 (Protected).
    '''
    global t1_queue, t1_set
    global t2_queue, t2_set, t2_current_bytes

    cache = cache_snapshot.cache
    victim_key = None
    
    # Priority 1: Evict from Probation (T1)
    if t1_queue:
        victim_key = t1_queue.popleft() # LRU eviction
        t1_set.discard(victim_key)
        # T1 byte size is not strictly tracked for logic, so no update needed
    
    # Priority 2: Evict from Protected (T2)
    elif t2_queue:
        victim_key = t2_queue.popleft() # LRU eviction
        t2_set.discard(victim_key)
        
        # We must update T2 byte tracking immediately
        if victim_key in cache:
            t2_current_bytes -= cache[victim_key].size
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Handle Cache Hits.
    1. Hit in T2 (Protected): Move to MRU of T2.
    2. Hit in T1 (Probation): Promote to T2 (Protected).
       - If T2 grows too large, demote T2 LRU -> T1 MRU.
    '''
    global t1_queue, t1_set
    global t2_queue, t2_set, t2_current_bytes
    
    k = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    target_protected_capacity = capacity * PROTECTED_RATIO

    if k in t2_set:
        # Hit in Protected: Update Recency (Move to MRU)
        # O(N) in worst case for deque remove, but Python's deque is optimized C-struct.
        try:
            t2_queue.remove(k)
        except ValueError:
            pass # Should not happen given t2_set check
        t2_queue.append(k)
        
    elif k in t1_set:
        # Hit in Probation: Promotion! T1 -> T2
        try:
            t1_queue.remove(k)
        except ValueError:
            pass
        t1_set.remove(k)
        
        t2_queue.append(k)
        t2_set.add(k)
        t2_current_bytes += size
        
        # Enforce T2 Size Limit: Demote LRU of T2 back to T1 if T2 is too big
        # This ensures T2 doesn't starve T1.
        while t2_current_bytes > target_protected_capacity and len(t2_queue) > 0:
            demoted_key = t2_queue.popleft() # Pop LRU from T2
            t2_set.discard(demoted_key)
            
            # Lookup size of demoted object to update counter
            if demoted_key in cache_snapshot.cache:
                demoted_size = cache_snapshot.cache[demoted_key].size
                t2_current_bytes -= demoted_size
            elif demoted_key == k: 
                # Edge case: we just added it, it might not be in snapshot dict yet depending on calling order,
                # but 'obj' is available.
                t2_current_bytes -= size
            
            # Add to MRU of T1 (Second Chance)
            t1_queue.append(demoted_key)
            t1_set.add(demoted_key)
            
            # Safety break to prevent infinite loops if one object is larger than protected cap
            if demoted_key == k:
                break

def update_after_insert(cache_snapshot, obj):
    '''
    Handle New Insertions.
    Always insert into T1 (Probation).
    '''
    global t1_queue, t1_set
    
    k = obj.key
    
    # New items go to Probation (T1)
    if k not in t1_set and k not in t2_set:
        t1_queue.append(k)
        t1_set.add(k)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Metadata cleanup.
    Most cleanup is handled atomically in `evict` to ensure state consistency,
    but we double check sets here just in case.
    '''
    global t1_set, t2_set
    
    # Ensure evicted object is removed from sets (deque removal happened in evict)
    k = evicted_obj.key
    t1_set.discard(k)
    t2_set.discard(k)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmph7a0hzmy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp26n90356.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpav_yt1d3.pickle

Iteration 74: New subsample score 0.776625 is not better than old score 0.887159, skipping
Iteration 75: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplgnbkysk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphut6fdmx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv_x722c6.pickle

Iteration 75: Proposed new text for program: from collections import deque, defaultdict

# S3-FIFO Metadata
# Using deques for O(1) performance on both ends
s_queue = deque()      # Small/Probationary FIFO Queue
m_queue = deque()      # Main/Protected FIFO Queue
g_queue = deque()      # Ghost FIFO Queue (history of evicted S items)
g_set = set()          # Set for O(1) existence checks in Ghost
freq = defaultdict(int) # Frequency counter

# Tuning parameters
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction Strategy:
    - Balances eviction between Small (S) and Main (M) queues based on size ratio.
    - Uses strict FIFO ordering for Ghost cache to capture recency.
    - Promoted items enter M with neutral frequency.
    - Highly frequent items in M degrade slowly (decrement) rather than resetting.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    cache_count = len(cache_snapshot.cache)
    s_target_size = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    victim_key = None
    
    # Iterate until a victim is found and removed
    while victim_key is None:
        
        # Decide which queue to process:
        # Prioritize evicting from S if it is larger than its target allocation,
        # OR if M is empty (must evict from S).
        evict_from_s = len(s_queue) >= s_target_size or not m_queue
        
        if evict_from_s:
            if not s_queue:
                # Should not be reachable unless cache is empty, but safe fallback
                evict_from_s = False 
            else:
                candidate = s_queue[0] # Peek at head
                candidate_freq = freq[candidate]
                
                if candidate_freq > 0:
                    # Give second chance: Promote to M
                    s_queue.popleft()
                    m_queue.append(candidate)
                    # Reset frequency to 0. It must prove utility in M to stay.
                    freq[candidate] = 0
                else:
                    # Evict candidate
                    victim_key = s_queue.popleft()
                    
                    # Record in Ghost cache
                    g_queue.append(victim_key)
                    g_set.add(victim_key)
                    
                    # Maintain Ghost size approx equal to Cache size
                    if len(g_queue) > cache_count:
                        rem = g_queue.popleft()
                        if rem in g_set:
                            g_set.remove(rem)

        # If we didn't evict from S, try M
        if not evict_from_s:
            if not m_queue:
                break # Should not happen if cache is full
                
            candidate = m_queue[0] # Peek at head
            candidate_freq = freq[candidate]
            
            if candidate_freq > 0:
                # Give second chance: Reinsert at tail of M
                m_queue.popleft()
                m_queue.append(candidate)
                # Decrement frequency: Very hot items (freq=3) survive multiple loops
                freq[candidate] = candidate_freq - 1
            else:
                # Evict candidate
                victim_key = m_queue.popleft()
                # Items evicted from M generally do not go to Ghost (standard S3 logic)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, capped at MAX_FREQ (3).
    This keeps the counter small and prevents cache pollution by one-time bursts.
    '''
    global freq
    key = obj.key
    if freq[key] < MAX_FREQ:
        freq[key] += 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - If key is in Ghost, it was a mistake to evict -> Restore to Main Queue.
    - Otherwise -> Insert into Small Queue.
    '''
    global s_queue, m_queue, g_queue, g_set, freq
    
    key = obj.key
    freq[key] = 0 # Initialize with 0 frequency
    
    if key in g_set:
        # Ghost Hit: Restore directly to Main Queue (M)
        m_queue.append(key)
        # Remove from Ghost Set (Lazy removal from g_queue is acceptable/handled in evict)
        g_set.remove(key)
    else:
        # Standard Insert: Start in Small Queue (S)
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpouza8gi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxikzxkj3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpow_7hexg.pickle

Iteration 75: New subsample score 0.294913 is not better than old score 0.29884, skipping
Iteration 76: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl2dytyqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcey5851z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmg1j1xxg.pickle

Iteration 76: Proposed new text for program: import heapq
from collections import defaultdict

# GDSF (Greedy Dual Size Frequency) Global State
# We maintain a priority queue (min-heap) for efficient eviction of the lowest H-value.
# Structure: (H_value, access_count_tiebreaker, key)
priority_queue = [] 

# Map to store current H-values and frequency for O(1) access
# Key -> {'h': float, 'freq': int, 'entry_time': int}
metadata = defaultdict(lambda: {'h': 0.0, 'freq': 0, 'entry_time': 0})

# The "Aging Factor" L. This increases over time as objects are evicted.
L = 0.0

# Counter to break ties in priority queue (acts as secondary LRU within same H-value)
tiebreaker = 0

def calculate_h_value(freq, size, current_L):
    """
    Calculates the priority value H.
    H = L + (Frequency / Size)
    
    Higher H means the object is more valuable and less likely to be evicted.
    We add a tiny epsilon to size to avoid division by zero if size is theoretically 0.
    """
    return current_L + (float(freq) / max(1, size))

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Evict the object with the lowest H value (Priority).
    If H values are equal, evict the one inserted earliest (FIFO/LRU tie-breaking).
    '''
    global priority_queue, metadata, L
    
    cache = cache_snapshot.cache
    victim_key = None
    
    # We must clean the priority queue of 'stale' entries.
    # The heap might contain multiple entries for the same key due to updates.
    # We only process the entry if it matches the current H-value in metadata 
    # and the object is actually in the cache.
    
    while priority_queue:
        h_val, _, key = priority_queue[0] # Peek at min item
        
        # Check if this heap entry is valid
        if key not in cache:
            heapq.heappop(priority_queue) # Remove ghost/stale entry
            continue
            
        stored_meta = metadata[key]
        # Due to floating point precision, we allow a tiny epsilon, or just strict equality 
        # if updates are handled rigorously. We use strict equality here as we rebuild heap on updates.
        if h_val != stored_meta['h']:
            heapq.heappop(priority_queue) # Remove outdated priority entry
            continue
            
        # Found the valid victim with lowest H
        victim_key = key
        
        # UPDATE L: The aging factor becomes the H-value of the evicted item.
        # This effectively raises the bar for new items entering the cache.
        L = h_val
        
        heapq.heappop(priority_queue)
        break
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate H value using current L.
    3. Push new priority to heap.
    '''
    global metadata, L, tiebreaker, priority_queue
    
    k = obj.key
    tiebreaker += 1
    
    # Update Metadata
    metadata[k]['freq'] += 1
    
    # Recalculate H
    new_h = calculate_h_value(metadata[k]['freq'], obj.size, L)
    metadata[k]['h'] = new_h
    metadata[k]['entry_time'] = tiebreaker
    
    # Push to heap
    heapq.heappush(priority_queue, (new_h, tiebreaker, k))

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Recover Frequency (if it was a "ghost").
    2. Calculate H value using current L.
    3. Push to heap.
    '''
    global metadata, L, tiebreaker, priority_queue
    
    k = obj.key
    tiebreaker += 1
    
    # If key exists in metadata, it's a "Ghost" (evicted previously).
    # We retain its frequency to reward items that thrash in and out.
    # If it's brand new, freq defaults to 0 in defaultdict, so we set to 1.
    if metadata[k]['freq'] == 0:
        metadata[k]['freq'] = 1
    else:
        # It's a return visitor (Ghost). 
        # Optional: Some GDSF implementations reset freq, others keep it. 
        # Keeping it adapts better to variable patterns. 
        metadata[k]['freq'] += 1

    # Calculate H
    new_h = calculate_h_value(metadata[k]['freq'], obj.size, L)
    metadata[k]['h'] = new_h
    metadata[k]['entry_time'] = tiebreaker
    
    heapq.heappush(priority_queue, (new_h, tiebreaker, k))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    The object is removed from the cache dict, but we keep its metadata 
    (frequency) to give it a head start if it returns (Ghost entry).
    '''
    # We don't delete from `metadata` here to maintain history (Frequency).
    # However, to prevent memory leaks in a long simulation, we should occasionally prune.
    
    global metadata
    
    # Cleanup logic: If metadata grows too large relative to cache capacity, prune low freq items.
    # A simple heuristic: if metadata is > 3x cache object count, clear entries not in cache.
    # Since we don't have direct access to cache count easily without iterating, 
    # we can use a soft limit.
    
    if len(metadata) > 20000: # Arbitrary safety cap for simulation
         # Remove keys not in cache to save memory, essentially forgetting their history
        keys_to_remove = []
        cache_keys = set(cache_snapshot.cache.keys())
        for k in list(metadata.keys()):
            if k not in cache_keys:
                del metadata[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmposngntuq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvh9bqalb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpubv_i452.pickle

Iteration 76: New subsample score 0.8425670000000001 is not better than old score 0.96595, skipping
Iteration 77: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp407z95kc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppdfwk2g_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl4hvvk_r.pickle

Iteration 77: Proposed new text for program: import math

# Global state for GDSF (Greedy Dual Size Frequency)
# Metadata stores: key -> {'freq': int, 'priority': float, 'last_access': int}
metadata = {}

# Ghost frequency stores historical frequency of evicted items to handle churn/scans.
# key -> int (frequency)
ghost_freq = {}

# 'L' is the dynamic aging factor. It represents the priority of the last evicted object.
# This forces new items to compete with the "current" standard of popularity.
L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy.
    We identify the object with the lowest Priority score.
    Priority = L + (Frequency / Size)
    
    1. Find object with min priority.
    2. Update global L to that minimum priority (aging the cache).
    3. Return that object's key.
    '''
    global metadata, L
    
    cache = cache_snapshot.cache
    candidate_key = None
    min_priority = float('inf')
    
    # We iterate to find the victim. 
    # While O(N), for cache simulation constraints this is usually acceptable 
    # and required for exact GDSF.
    
    # Tie-breaking variables
    victim_last_access = float('inf')
    
    for key in cache:
        if key not in metadata:
            # Defensive fallback if metadata is desynchronized
            current_priority = L + (1.0 / cache[key].size)
            last_access = 0
        else:
            current_priority = metadata[key]['priority']
            last_access = metadata[key]['last_access']
            
        # We look for the smallest priority
        if current_priority < min_priority:
            min_priority = current_priority
            candidate_key = key
            victim_last_access = last_access
        elif current_priority == min_priority:
            # Tie-breaker: LRU. 
            # If priorities are identical, evict the one accessed furthest in the past.
            if last_access < victim_last_access:
                candidate_key = key
                victim_last_access = last_access

    # Update the global aging factor L
    # The cache "watermark" rises to the level of the evicted item.
    if candidate_key is not None:
        L = min_priority
        return candidate_key
    
    # Fallback if cache is empty (unlikely)
    return next(iter(cache)) if cache else None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority based on current L.
    This effectively "resets" the aging of the object, pushing it to the 
    top of the priority queue relative to current cache pressure.
    '''
    global metadata, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key not in metadata:
        # Should be in metadata on hit, but defensive coding:
        freq = ghost_freq.get(key, 0) + 1
    else:
        freq = metadata[key]['freq'] + 1
        
    # Calculate GDSF Priority
    # Priority = L + (Frequency / Size)
    # Note: We use the CURRENT L. This brings the object "forward" in time.
    priority = L + (float(freq) / obj.size)
    
    metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if we have history (Ghost) for this object.
    2. Calculate initial Priority.
    '''
    global metadata, ghost_freq, L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Recover frequency from history if available to resist scan thrashing
    # If it was popular before, it starts popular again.
    if key in ghost_freq:
        freq = ghost_freq[key] + 1
    else:
        freq = 1
        
    # Priority calculation
    priority = L + (float(freq) / obj.size)
    
    metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Move frequency data to Ghost storage.
    2. Clean up active metadata.
    '''
    global metadata, ghost_freq
    
    if evicted_obj:
        key = evicted_obj.key
        
        # Save frequency to ghost map
        if key in metadata:
            # We decay the frequency slightly to prevent infinite accumulation 
            # over very long traces, but keep enough to remember popularity.
            # Simple approach: keep as is.
            ghost_freq[key] = metadata[key]['freq']
            del metadata[key]
            
    # Maintenance: Prevent ghost_freq from growing infinitely (memory leak protection)
    # If ghost map gets too big (e.g., 2x capacity of typical caches), trim it.
    # For this implementation, we assume a generous limit.
    if len(ghost_freq) > 100000:
        # Simple clear or random reduction would happen here in production
        ghost_freq.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6ujrlrgn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphfgdmzks.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj19uk9ct.pickle

Iteration 77: New subsample score 0.30985 is not better than old score 0.34990699999999997, skipping
Iteration 78: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx1suq4ed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpt_nz1kq3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpytog7s1c.pickle

Iteration 78: Proposed new text for program: import math

# Global state for GDSF
# Stores metadata for each key: {'freq': int, 'priority': float, 'last_access': int}
metadata_store = {}

# 'L' represents the aging factor (the priority of the last evicted object).
# It serves as the base priority for new or updated items, effectively aging out inactive ones.
L = 0.0

def evict(cache_snapshot, obj):
    '''
    Selects a victim using the Greedy Dual Size Frequency (GDSF) policy.
    We evict the object with the lowest priority value.
    In case of ties, we evict the Least Recently Used (LRU) candidate.
    '''
    global metadata_store, L
    cache = cache_snapshot.cache
    
    # We filter keys to ensure consistency, though typically cache and metadata are in sync.
    # Using a generator expression is memory efficient.
    candidates = (k for k in cache if k in metadata_store)
    
    # Find the victim with the minimum priority.
    # Tie-breaker: 'last_access' (older access time is smaller/min).
    # This efficiently selects the item with the worst utility-to-size ratio that is also the oldest.
    try:
        victim_key = min(candidates, key=lambda k: (metadata_store[k]['priority'], metadata_store[k]['last_access']))
    except ValueError:
        # Handle edge case where cache/metadata might be empty
        return None
    
    # GDSF Mechanism: Update L to the priority of the evicted item.
    # This 'ratchets' the base value up, aging out other items that haven't been accessed.
    L = metadata_store[victim_key]['priority']
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority based on current L (bringing it to the 'front' of recency).
    '''
    global metadata_store, L
    
    key = obj.key
    # Note: obj.size is guaranteed to be positive (>0).
    size = obj.size 
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # GDSF Update: Priority = Current Aging Factor (L) + Utility (Freq / Size)
        # This restores the object's standing relative to the current eviction threshold.
        meta['priority'] = L + (meta['freq'] / size)
    else:
        # Fallback if metadata is missing (should be rare)
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate initial Priority based on current L.
    '''
    global metadata_store, L
    
    key = obj.key
    size = obj.size
    current_time = cache_snapshot.access_count
    
    # Start with frequency 1
    freq = 1
    
    # Calculate priority. 
    # New items start at the current 'water level' (L) plus their inherent utility.
    priority = L + (freq / size)
    
    metadata_store[key] = {
        'freq': freq,
        'last_access': current_time,
        'priority': priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to prevent memory leaks.
    '''
    global metadata_store
    
    if evicted_obj and evicted_obj.key in metadata_store:
        del metadata_store[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5jtsiljh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmz9u6_wg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa33dv8n_.pickle

Iteration 78: New subsample score 0.111391 is not better than old score 0.139497, skipping
Iteration 79: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp82hsreh8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpneh7bpeq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_hja6esu.pickle

Iteration 79: Proposed new text for program: from collections import deque, OrderedDict

# S3-FIFO Metadata Structures
# s_queue: Small Queue (deque) - Probationary items
# m_queue: Main Queue (deque) - Protected items
# g_map:   Ghost Cache (OrderedDict) - Tracks keys of recently evicted probationary items
# freq:    Frequency Map (dict) - Tracks utility (0-3 scale)
s_queue = deque()
m_queue = deque()
g_map = OrderedDict()
freq = {}

# Tuning Parameters
# 10% of cache size is the standard optimal static value for the small queue in S3-FIFO.
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict when the cache is full.
    Uses S3-FIFO logic to balance between recency (S-Queue) and frequency (M-Queue).
    '''
    global s_queue, m_queue, g_map, freq
    
    capacity = cache_snapshot.capacity
    # Target size for the Small Queue (Probation)
    s_target = max(1, int(capacity * SMALL_QUEUE_RATIO))
    
    candidate = None
    
    # Loop until a valid victim is found and removed from our metadata queues
    while candidate is None:
        
        # --- Decision: Evict from Small (S) or Main (M)? ---
        # We evict from S if it exceeds its target size OR if M is empty.
        evict_from_s = False
        if len(s_queue) >= s_target or len(m_queue) == 0:
            evict_from_s = True
            
        # Safety fallback: If logic points to S but S is empty, must evict from M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Processing Small Queue (Probation) ---
            victim = s_queue[0] # Peek head
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Promotion: Item was hit while in probation. Move to Main.
                s_queue.popleft()
                m_queue.append(victim)
                # We do not reset frequency here, but relying on M's aging logic to handle it.
            else:
                # Eviction: Item was not hit. Evict it.
                candidate = s_queue.popleft()
                
                # Add to Ghost Cache to track this "mistake"
                g_map[candidate] = None
                
                # Maintain Ghost Size (Approx same size as Main Cache)
                if len(g_map) > capacity:
                    g_map.popitem(last=False) # FIFO eviction from Ghost
        else:
            # --- Processing Main Queue (Protected) ---
            victim = m_queue[0] # Peek head
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Second Chance: Item has utility. Rotate to back and age it.
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1 # Decrement frequency (Aging)
            else:
                # Eviction: Item has lost utility. Evict it.
                candidate = m_queue.popleft()
                # Note: We typically do not add M-evictions to Ghost
                
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit: Increment frequency counter.
    S3-FIFO does not move items on hits (lazy promotion), only updates metadata.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    # Cap frequency to prevent integer overflow and bound the "lives" of an object
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On New Insert: Place into S or M based on Ghost history.
    '''
    global s_queue, m_queue, g_map, freq
    key = obj.key
    
    # Initialize frequency
    freq[key] = 0
    
    if key in g_map:
        # Ghost Hit: This item was recently evicted from S but accessed again.
        # It belongs in the Main queue (Protected).
        m_queue.append(key)
        del g_map[key]
    else:
        # Standard Insert: Start in Small Queue (Probation)
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Post-Eviction Cleanup: Remove frequency metadata.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprq2hxh2d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbuhm_oyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvi_lnqsy.pickle

Iteration 79: New subsample score 0.659856 is better than old score 0.6585650000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx3iscymg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphc8g4iud.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv08k1iyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3oadpy2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvs1aw7ts.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5cr71p0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpckgvpxdo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpq_pvzoiz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu6jwje53.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnbab8rgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp88wukibl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5dig6h5i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz8h7wuea.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyptucagx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppm9dq155.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8f2lrn8i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpftf6fckw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbumz4ulo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpiu17cy2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxm6kyjn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplbjx00st.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7w37lzj6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6aj5cx8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_0a2qwl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppduvwqtd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfp8y5566.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7u9igz40.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwkfq5j_0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb3hag68t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg6l_4oii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo8t5tm8y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8s3z5a6c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpthccljra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb4rj346y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8uhzdew8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9ci1ipft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp39bwha7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcvdeyxg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0l6g0mkz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpj8ae6mc6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4o1yhhno.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3sm2m6ls.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6joqm845.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgn1swjjz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy1apn9w6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjo2ddif6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqbayvk35.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpm99q02n4.pickle

Iteration 79: Full valset score for new program: 0.2521737291666666
Iteration 79: Full train_val score for new program: 0.2521737291666666
Iteration 79: Individual valset scores for new program: [0.49435, 0.469647, 0.478638, 0.43416, 0.492325, 0.479649, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.394316, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.384464, 0.026164, 0.058672, 0.058672, 0.269831, 0.336694, 0.825223, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752723, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.145828, 0.126451, 0.16592, 0.052632, 0.35, 0.174888, 0.143619, 0.466258, 0.081699]
Iteration 79: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.084091, 0.056948, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.055921, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 79: Full valset pareto front score: 0.26537429166666665
Iteration 79: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18, 19, 21}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {13}, {8}, {8}, {13}, {5}, {20}, {20}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {2}, {5}, {2}, {20}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {7}]
Iteration 79: Best valset aggregate score so far: 0.2580827708333333
Iteration 79: Best program as per aggregate score on train_val: 5
Iteration 79: Best program as per aggregate score on valset: 5
Iteration 79: Best score on valset: 0.2580827708333333
Iteration 79: Best score on train_val: 0.2580827708333333
Iteration 79: Linear pareto front program index: 5
Iteration 79: New program candidate index: 21
Iteration 80: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprbwiz4hz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpko8luyrt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjox0un4i.pickle

Iteration 80: Proposed new text for program: from collections import deque

# Improved Cache Eviction Algorithm: S3-FIFO with 1-Bit Frequency Reset
# ---------------------------------------------------------------------
# This implementation uses a Small (S) queue for probation and a Main (M) queue
# for protected items. A Ghost (G) queue tracks history to detect loops.
#
# Key optimization: "1-bit" frequency logic. When an item survives eviction
# (from S->M or M->M), its frequency is reset to 0. It must be hit again
# to survive the next pass. This prevents "warm" items from clogging the cache.

# Global Data Structures
s_queue = deque()       # Small / Probationary Queue (FIFO)
m_queue = deque()       # Main / Protected Queue (FIFO)
g_queue = deque()       # Ghost Queue (FIFO) - Keys only
g_set = set()           # Ghost Set - Fast lookup
freq = {}               # Frequency counter

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
SMALL_RATIO = 0.1       # 10% of cache space dedicated to S queue
MAX_FREQ = 3            # Saturation cap for frequency (logic primarily uses > 0)

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Logic.
    Returns the key of the object to be evicted.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue
    s_target = capacity * SMALL_RATIO
    
    victim_key = None
    
    # Loop until we find a victim to evict.
    # Note: Promotions (S->M) or Re-insertions (M->M) do not free space,
    # so we continue looping until an actual eviction occurs.
    while victim_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than target -> Evict S (to maintain 10% ratio)
        # 2. If M is empty -> Evict S
        # 3. Otherwise -> Evict M
        
        evict_from_s = False
        if s_bytes >= s_target:
            evict_from_s = True
        
        if not m_queue:
            evict_from_s = True
        if not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Small Queue (Probation) Processing ---
            if not s_queue: break 
            
            candidate = s_queue[0]
            
            # Lazy cleanup if object deleted externally
            if candidate not in cache_map:
                s_queue.popleft()
                continue
            
            count = freq.get(candidate, 0)
            size = cache_map[candidate].size
            
            if count > 0:
                # HIT in S: Promote to Main
                # It proved useful. Move to M.
                # Optimization: Reset freq to 0. It enters M fresh.
                s_queue.popleft()
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                freq[candidate] = 0
            else:
                # MISS in S: Evict
                # It failed probation.
                victim_key = s_queue.popleft()
                s_bytes -= size
                
                # Add to Ghost (Track missed opportunity)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
                    
        else:
            # --- Main Queue (Protected) Processing ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache_map:
                m_queue.popleft()
                continue
                
            count = freq.get(candidate, 0)
            size = cache_map[candidate].size
            
            if count > 0:
                # HIT in M: Reinsert (Second Chance)
                # It is still active. Keep it.
                # Optimization: Reset freq to 0. 
                # It must be accessed AGAIN to survive the next cycle.
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = 0
            else:
                # NO HIT in M: Evict
                # It hasn't been used since last check.
                victim_key = m_queue.popleft()
                m_bytes -= size
                # M-evicted items do NOT go to Ghost (standard S3-FIFO)

    # --- Ghost Queue Maintenance ---
    # Keep Ghost size roughly proportional to cache object count.
    # 2x count is a robust buffer for variable workloads.
    target_ghost_count = len(cache_map) * 2
    # Ensure a minimum floor to handle small caches
    target_ghost_count = max(target_ghost_count, 10)

    # 1. Size cleanup
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # 2. Lazy cleanup of ghost head (items removed from set but still in deque)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency.
    Capped at MAX_FREQ to prevent runaway counters.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Decide S (Probation) vs M (Protected) based on Ghost.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: This item was evicted recently.
        # This implies our S-queue eviction was premature (or it's a loop).
        # Restore directly to Main Queue.
        m_queue.append(k)
        m_bytes += size
        
        # Remove from Ghost set (deque cleaned lazily)
        g_set.remove(k)
    else:
        # Standard Insert: Start in Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd5rzcbho.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc5qbnprj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy55vqdaf.pickle

Iteration 80: New subsample score 1.432965 is not better than old score 1.4520300000000002, skipping
Iteration 81: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppt48qnha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpafhzhsmk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcfb_ohpq.pickle

Iteration 81: Proposed new text for program: from collections import deque, OrderedDict

# S3-FIFO Metadata
# s_list: FIFO queue (deque) for small/new items (probationary)
# m_list: FIFO queue (deque) for main/popular items
# g_map: Ghost map (OrderedDict) to track keys of items recently evicted from S
# freq: Dictionary to track frequency bits (0-3)
# s_bytes: Integer tracking the total size in bytes of objects currently in s_list
s_list = deque()
m_list = deque()
g_map = OrderedDict()
freq = {}
s_bytes = 0

# Tuning parameter: 10% of cache capacity (in bytes) allocated for S queue
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Strategy (Byte-Aware):
    1. Determine if we should evict from S or M based on the byte size of S.
    2. If evicting from S:
       - If freq > 0: Move to M (reset freq), deduct size from S.
       - If freq == 0: Evict and add to Ghost.
    3. If evicting from M:
       - If freq > 0: Move to tail of M (decrement freq).
       - If freq == 0: Evict.
    '''
    global s_list, m_list, g_map, freq, s_bytes
    
    # Calculate the byte threshold for the Small queue
    target_s_bytes = cache_snapshot.capacity * SMALL_QUEUE_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Decision: Evict from S if it exceeds its byte budget OR if M is empty.
        # This protects M from being starved by a large S, and ensures S acts as a filter.
        evict_from_s = False
        if not m_list:
            evict_from_s = True
        elif s_bytes >= target_s_bytes:
            evict_from_s = True
        
        if evict_from_s:
            if not s_list:
                # Fallback: if S is empty but logic pointed to S (e.g. M is also empty? Unlikely),
                # force switch to M if possible.
                if m_list:
                    evict_from_s = False
                else:
                    break # Should not happen in a full cache
            
            if evict_from_s:
                candidate = s_list[0] # Peek head of S
                candidate_freq = freq.get(candidate, 0)
                
                if candidate_freq > 0:
                    # Second chance: Promote to M
                    s_list.popleft()
                    m_list.append(candidate)
                    
                    # Update byte tracking for S
                    # Note: We rely on the cache_snapshot to get the size of the existing object
                    if candidate in cache_snapshot.cache:
                        s_bytes -= cache_snapshot.cache[candidate].size
                    
                    # Reset frequency on promotion to validate utility in M later
                    freq[candidate] = 0
                else:
                    # Evict from S
                    victim_key = s_list.popleft()
                    if victim_key in cache_snapshot.cache:
                        s_bytes -= cache_snapshot.cache[victim_key].size
                    
                    # Add to Ghost (track that we evicted this probationary item)
                    g_map[victim_key] = True
                    
                    # Manage Ghost size (keep it roughly bounded by object count of cache)
                    if len(g_map) > len(cache_snapshot.cache):
                        g_map.popitem(last=False) # Remove oldest inserted key
        
        # Logic for M (if we didn't evict from S)
        if not evict_from_s:
            candidate = m_list[0] # Peek head of M
            candidate_freq = freq.get(candidate, 0)
            
            if candidate_freq > 0:
                # Second chance: Reinsert at tail of M, decrement frequency
                m_list.popleft()
                m_list.append(candidate)
                freq[candidate] = candidate_freq - 1
            else:
                # Evict from M
                victim_key = m_list.popleft()
                # Items evicted from M are usually not added to Ghost in S3-FIFO
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - Increment frequency, capped at 3.
    - Do NOT reorder queues. This ensures O(1) efficiency and scan resistance.
    '''
    global freq
    key = obj.key
    # 2 bits of frequency (0-3) is sufficient
    freq[key] = min(freq.get(key, 0) + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - If key is in Ghost: It was a mistake to evict it. Promote directly to M.
    - Else: Insert into S (probationary queue).
    '''
    global s_list, m_list, g_map, freq, s_bytes
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_map:
        # Ghost Hit: Restore to M
        m_list.append(key)
        del g_map[key]
    else:
        # Standard Insert: Add to S and update byte count
        s_list.append(key)
        s_bytes += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup:
    - Remove metadata for the evicted object.
    '''
    global freq
    
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8r_s46y9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkkuqzpxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw2jog22z.pickle

Iteration 81: New subsample score 0.382172 is not better than old score 0.500614, skipping
Iteration 82: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkgnlgycw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp04069qfr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpb1n6vce4.pickle

Iteration 82: Proposed new text for program: import math

# --- Global State ---

# L_value: The "Aging Factor" or "Inflation Value" from GDSF.
# It represents the priority of the most recently evicted object.
# This ensures that new objects entering the cache have a fair chance 
# against old, high-frequency objects.
L_value = 0.0

# Metadata Store: Maps key -> {'freq': int, 'h_val': float, 'last_access': int}
metadata_store = {}

# Ghost Cache: Maps key -> int (frequency)
# Used to remember the frequency of items that were evicted, allowing 
# us to restore their "heat" if they re-enter the cache quickly.
ghost_freq = {}

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Find the object with the minimum H-value (Priority).
    Tie-breaking: Least Recently Used (LRU).
    '''
    global L_value, metadata_store
    
    cache = cache_snapshot.cache
    
    victim_key = None
    min_h = float('inf')
    # Track min_access for LRU tie-breaking
    min_access = float('inf')
    
    # Iterate through valid cache keys to find the victim
    # While O(N), this is necessary for precise GDSF calculation without
    # maintaining a complex heap structure that might desync in this simulation harness.
    for key in cache:
        if key not in metadata_store:
            # Fallback for synchronization edge cases
            continue
            
        meta = metadata_store[key]
        h_val = meta['h_val']
        last_access = meta['last_access']
        
        # We look for the smallest H-value
        if h_val < min_h:
            min_h = h_val
            min_access = last_access
            victim_key = key
        elif h_val == min_h:
            # Tie-breaker: Evict the one accessed longest ago (LRU)
            if last_access < min_access:
                min_access = last_access
                victim_key = key
    
    # If we found a victim, update the global aging factor L
    # L becomes the priority of the item we just threw out.
    if victim_key is not None:
        L_value = min_h
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate H-value using the CURRENT L_value.
       This "resets" the aging of the object, bringing it up to the current
       cache temperature.
    '''
    global L_value, metadata_store
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # GDSF Update: H(p) = L + (Freq(p) / Size(p))
        # Note: We use the current global L_value, effectively "re-aging" the object
        meta['h_val'] = L_value + (meta['freq'] / obj.size)
    else:
        # Fallback if metadata is missing (rare sync issue)
        # Treat as a new insert effectively
        freq = 1
        if key in ghost_freq:
            freq = ghost_freq[key] + 1
        
        metadata_store[key] = {
            'freq': freq,
            'last_access': current_time,
            'h_val': L_value + (freq / obj.size)
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost cache for history.
    2. Calculate initial H-value.
    '''
    global L_value, metadata_store, ghost_freq
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Restore frequency from ghost cache if available
    # This helps significant items that were momentarily evicted to regain status
    initial_freq = 1
    if key in ghost_freq:
        initial_freq = ghost_freq[key] + 1
    
    # GDSF Insertion: H(p) = L + (Freq(p) / Size(p))
    h_val = L_value + (initial_freq / obj.size)
    
    metadata_store[key] = {
        'freq': initial_freq,
        'h_val': h_val,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move frequency data to Ghost cache.
    2. Clean up metadata.
    '''
    global metadata_store, ghost_freq
    
    if evicted_obj:
        evicted_key = evicted_obj.key
        
        if evicted_key in metadata_store:
            # Preserve frequency knowledge
            current_freq = metadata_store[evicted_key]['freq']
            ghost_freq[evicted_key] = current_freq
            
            # Remove from active metadata
            del metadata_store[evicted_key]
            
    # Maintenance: Limit ghost cache size to prevent memory leaks
    # Using a simple cap. If too large, remove arbitrary item (FIFO/Arbitrary due to dict)
    if len(ghost_freq) > 50000:
        # Remove the first key returned by iterator (pseudo-random/oldest depending on python ver)
        ghost_freq.pop(next(iter(ghost_freq)))
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps9ztxepz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpczoy2gpw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps91bih_l.pickle

Iteration 82: New subsample score 0.367155 is not better than old score 0.404869, skipping
Iteration 83: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqym7ozom.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1yzibrxa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7b0l_ypg.pickle

Iteration 83: Proposed new text for program: import math
import heapq

# Global metadata storage
# We map object keys to their metadata: [frequency, stored_H_value]
meta_map = {} 

# Priority Queue for efficient eviction finding: Stores (H_value, access_count, key)
# We include access_count as a tie-breaker (FIFO for same priority)
pq = []

# The "Aging Factor" L. 
# This represents the priority value of the last evicted object.
# It allows new items to compete with items that have high accumulated frequency.
L = 0.0

def calculate_priority(freq, size, L_val):
    '''
    GDSF Priority Calculation:
    H = L + (Frequency * Cost) / Size
    Cost is assumed 1.0 for hit-rate maximization.
    '''
    if size == 0: return float('inf') # Protect against zero division, prioritize keeping
    return L_val + (freq / size)

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction:
    Evicts the item with the lowest H-value (Priority).
    Updates L to the H-value of the evicted item.
    '''
    global pq, L, meta_map
    
    cache = cache_snapshot.cache
    victim_key = None
    
    # Clean priority queue of stale entries (Lazy Deletion)
    # The heap might contain old priorities for keys that have since been updated
    while pq:
        h_val, _, key = pq[0]
        
        # If key is no longer in cache, just pop and continue
        if key not in cache:
            heapq.heappop(pq)
            continue
            
        # Check if this is the current valid metadata for this key
        # meta_map stores [freq, current_H]
        if key in meta_map and meta_map[key][1] != h_val:
            # This heap entry is stale (the object was updated/hit recently), discard it
            heapq.heappop(pq)
            continue
            
        # Found the valid minimum
        victim_key = key
        # Update L to the priority of the evicted object
        L = h_val
        heapq.heappop(pq)
        break
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority (H-value) using current L.
    3. Update heap (lazy update via pushing new entry).
    '''
    global meta_map, pq, L
    
    k = obj.key
    if k not in meta_map:
        # Should technically be there if it's a hit, but safety first
        meta_map[k] = [1, 0.0]
    
    # Increment Frequency
    meta_map[k][0] += 1
    freq = meta_map[k][0]
    
    # Recalculate Priority
    # Note: We use the CURRENT global L, restoring the "Recency" aspect
    new_priority = calculate_priority(freq, obj.size, L)
    meta_map[k][1] = new_priority
    
    # Push new priority to heap. 
    # Old priority entry remains in heap but will be ignored by evict() due to check against meta_map
    # We use access_count (time) as tie breaker.
    heapq.heappush(pq, (new_priority, cache_snapshot.access_count, k))

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize Frequency to 1.
    2. Calculate Priority using current L.
    3. Push to heap.
    '''
    global meta_map, pq, L
    
    k = obj.key
    
    # Initialize metadata
    # If we have history for this key (from a previous eviction), we could potentially restore frequency,
    # but strictly following GDSF for variable sizing usually resets freq or uses a ghost cache.
    # To keep it robust without excessive memory usage, we reset frequency to 1 on re-insertion.
    freq = 1
    
    new_priority = calculate_priority(freq, obj.size, L)
    
    meta_map[k] = [freq, new_priority]
    heapq.heappush(pq, (new_priority, cache_snapshot.access_count, k))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the evicted object.
    '''
    global meta_map
    
    if evicted_obj and evicted_obj.key in meta_map:
        del meta_map[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdm9a888g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphtwsn11q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnyg8iwjl.pickle

Iteration 83: New subsample score 0.846059 is not better than old score 0.9012979999999999, skipping
Iteration 84: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpycyfbdhh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpccko6v_r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyoh6ogr2.pickle

Iteration 84: Proposed new text for program: from collections import deque

# Optimized S3-FIFO Implementation
# --------------------------------------------------------
# 1. S-Queue (Small/Probation) - 20% of capacity
# 2. M-Queue (Main/Protected) - 80% of capacity
# 3. Ghost Queue - Tracks evicted items to detect "recalls"
# 4. Frequency Awareness - Distinguish "one-hit" vs "hot" items

# Data Structures
s_queue = deque()      # Small Queue (FIFO) - Probationary items
m_queue = deque()      # Main Queue (FIFO) - Protected items
g_queue = deque()      # Ghost Queue (FIFO) - Keys of recently evicted items
g_set = set()          # Ghost Set - Fast lookup for g_queue
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 20% for the probationary queue provides a better buffer for bursty
# workloads while maintaining strong scan resistance.
SMALL_QUEUE_RATIO = 0.20
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    Prioritizes maintaining the Small Queue size constraint.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Loop until we find a valid victim
    while candidate_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafes to prevent popping from empty queues
        if evict_from_s and not s_queue:
            evict_from_s = False
        if not evict_from_s and not m_queue:
            evict_from_s = True
            if not s_queue: break # Should never happen if cache is full
            
        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            victim = s_queue[0] # Peek
            
            # Lazy cleanup: handle items deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Promotion: Item proved useful in probation -> Move to Main
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency. It must re-earn protection in M.
                freq[victim] = 0
            else:
                # Evict from S
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Record in Ghost Queue
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            victim = m_queue[0] # Peek
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Reinsert to tail of M, decay frequency
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # Keep ghost queue proportional to current item count (approx 2x is robust)
    current_item_count = len(cache_map)
    target_ghost_count = current_item_count * 2
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Cleanup stale head of ghost queue
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle insertion.
    If in Ghost -> Recall to Main (High priority).
    Else -> Insert to Small (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! Promote directly to Main.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9wfyg_4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyt7q2u_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3br_0cwd.pickle

Iteration 84: New subsample score 1.274397 is not better than old score 1.282847, skipping
Iteration 85: Selected program 20 score: 0.22391518749999997
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv6qv7xe5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_aicsyiw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp81v8br5l.pickle

Iteration 85: Proposed new text for program: import heapq
from collections import defaultdict

# Global Structures for GDSF
# Priority Queue: Stores tuples of (priority, tie_breaker, key)
pq = []

# Entry Finder: Maps key -> priority
# Used to handle lazy deletion in the heap. If the priority in the heap 
# doesn't match the priority in this map, the heap entry is stale.
entry_finder = {}

# Frequency Map: Maps key -> int
# Tracks frequency of objects. Persists even after eviction (Ghost memory)
# to make better decisions if the object is re-inserted.
frequencies = defaultdict(int)

# GDSF Aging Factor
L = 0.0

# Tie breaker for stable sorting in heap
tie_breaker = 0

def get_priority(freq, size):
    '''Calculate GDSF priority.'''
    global L
    # The core GDSF formula: L + (Frequency / Size)
    # We want to keep items with high frequency and low size.
    return L + (freq / float(size))

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest priority (Smallest H value).
    Updates L to the priority of the evicted victim.
    '''
    global L, pq, entry_finder
    
    victim_key = None
    
    while victim_key is None:
        if not pq:
            # Should not happen if logic is correct and cache is full
            # Fallback to any key if heap is empty (safety net)
            if cache_snapshot.cache:
                return next(iter(cache_snapshot.cache))
            return None

        # Pop the item with the lowest priority
        priority, _, key = heapq.heappop(pq)
        
        # Check if this heap entry is valid (Lazy Deletion check)
        if key in entry_finder and entry_finder[key] == priority:
            # Check if the key is actually in the cache (External deletion check)
            if key in cache_snapshot.cache:
                victim_key = key
                
                # GDSF Rule: Update L to the priority of the evicted object
                L = priority
                
                # Remove from active tracking
                del entry_finder[key]
            else:
                # Item was removed from cache externally, just clean up map
                del entry_finder[key]
        else:
            # This is a stale entry (updated later with higher priority), ignore
            pass
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency, recalculate priority, and push to heap.
    '''
    global L, pq, entry_finder, frequencies, tie_breaker
    
    key = obj.key
    size = obj.size
    
    # 1. Update Frequency
    frequencies[key] += 1
    
    # 2. Calculate new priority
    new_priority = get_priority(frequencies[key], size)
    
    # 3. Update active tracking
    entry_finder[key] = new_priority
    
    # 4. Push to heap (Lazy update: old entry remains but will be ignored on pop)
    tie_breaker += 1
    heapq.heappush(pq, (new_priority, tie_breaker, key))

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Initialize/Restore frequency, calculate priority, push to heap.
    '''
    global L, pq, entry_finder, frequencies, tie_breaker
    
    key = obj.key
    size = obj.size
    
    # 1. Update Frequency (Ghost memory: start from 1 or increment previous history)
    frequencies[key] += 1
    
    # 2. Calculate priority
    # Note: New items inherit the current L, giving them a fair start
    new_priority = get_priority(frequencies[key], size)
    
    # 3. Update active tracking
    entry_finder[key] = new_priority
    
    # 4. Push to heap
    tie_breaker += 1
    heapq.heappush(pq, (new_priority, tie_breaker, key))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Maintenance: Prune frequency history to prevent unbounded memory growth.
    '''
    global frequencies
    
    # If frequency map grows too large relative to cache capacity (e.g., 4x items),
    # we prune it to prevent memory leaks.
    if len(frequencies) > max(len(cache_snapshot.cache) * 4, 10000):
        # Simple cleanup strategy: Remove entries not currently in cache
        # This clears "Ghost" data, resetting learning for long-forgotten items.
        
        # Create a set of keys to keep (currently cached keys)
        keys_to_keep = set(cache_snapshot.cache.keys())
        
        # Rebuild frequencies dict
        # We use a list to avoid runtime error during iteration
        for k in list(frequencies.keys()):
            if k not in keys_to_keep:
                del frequencies[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3q26p4f9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv0h699y7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4_dyt34s.pickle

Iteration 85: New subsample score 0.153232 is not better than old score 0.15795099999999998, skipping
Iteration 86: Selected program 13 score: 0.2369623541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeat30edb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphrliia45.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa936guzd.pickle

Iteration 86: Proposed new text for program: import math

# GDSF (Greedy Dual-Size Frequency) Implementation
# Global state to maintain metadata and the inflation factor L
metadata_store = {}
L_value = 0.0

def get_priority(freq, size, current_L):
    '''
    Calculates the GDSF priority score.
    Priority = L + (Frequency / Size)
    
    We use size + 1 to avoid division by zero errors if size is reported as 0.
    We apply a slight dampener to size to prevent overly aggressive penalization 
    of large objects, though standard GDSF is linear.
    '''
    return current_L + (freq / float(size))

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict based on the lowest Priority (H-value).
    Updates the global aging factor L to the priority of the evicted victim.
    '''
    global metadata_store, L_value
    cache = cache_snapshot.cache
    
    victim_key = None
    min_priority = float('inf')
    victim_last_access = float('inf')

    # Iterate through the cache to find the object with the minimum priority.
    # While O(N), this ensures high accuracy.
    for key in cache:
        if key not in metadata_store:
            # Fallback if metadata is missing (should not happen in normal flow)
            # Treat as lowest priority to evict and self-correct
            return key
            
        meta = metadata_store[key]
        priority = meta['priority']
        last_access = meta['last_access']
        
        # We look for the smallest priority value
        if priority < min_priority:
            min_priority = priority
            victim_key = key
            victim_last_access = last_access
        elif priority == min_priority:
            # Tie-Breaker: LRU
            # If priorities are identical, evict the one accessed longest ago
            if last_access < victim_last_access:
                victim_key = key
                victim_last_access = last_access

    # Update the global aging factor L
    # This is the crucial step of GDSF: The system inflates value to the 
    # level of the evicted object, allowing new objects to compete.
    if victim_key is not None:
        L_value = min_priority
        return victim_key
        
    # Fallback if cache is empty (unlikely)
    return next(iter(cache)) if cache else None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority based on CURRENT L (re-aging).
    3. Update Last Access.
    '''
    global metadata_store, L_value
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in metadata_store:
        meta = metadata_store[key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        # Recalculate priority using current L. 
        # This "brings the object forward" in the aging process.
        meta['priority'] = get_priority(meta['freq'], obj.size, L_value)
    else:
        # Recovery for missing metadata
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate Priority based on CURRENT L.
    '''
    global metadata_store, L_value
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # New objects start with Frequency 1
    # They get boosted by the current system aging factor L_value
    priority = get_priority(1, obj.size, L_value)
    
    metadata_store[key] = {
        'freq': 1,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to prevent memory leaks.
    '''
    global metadata_store
    
    if evicted_obj and evicted_obj.key in metadata_store:
        del metadata_store[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpagdcpnp7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnar855rm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxvm0sm5g.pickle

Iteration 86: New subsample score 0.039122000000000004 is not better than old score 0.042795, skipping
Iteration 87: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_84h5tni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9mo067m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps3hnw6gj.pickle

Iteration 87: Proposed new text for program: import heapq

# GDSF (Greedy Dual Size Frequency) Implementation
# Designed to optimize hit rates for variable-sized objects.
# Prioritizes Small and Frequent objects, with an aging mechanism (L) to handle recency.

# Global State
# Priority Queue: Stores tuples of (priority, insertion_tick, key)
# Uses a min-heap, so items with lowest priority are evicted first.
pq = []

# Metadata: Maps key -> {'freq': int, 'priority': float, 'tick': int}
# Keeps track of the latest state for keys to handle lazy updates in the heap.
metadata = {}

# Aging Factor L:
# Monotonically increasing value that helps age out old frequent items
# so new items have a chance to stay.
L = 0.0

def get_priority(freq, size, current_L):
    '''
    Calculates priority based on GDSF formula.
    Priority = L + (Frequency / Size)
    
    Small objects with high frequency get high priority (stay in cache).
    Large objects or low frequency objects get low priority (evicted).
    The 'L' factor allows recent items to compete with old frequent items.
    '''
    # Safety check for size > 0 (though constraints say positive integer)
    safe_size = size if size > 0 else 1
    return current_L + (freq / safe_size)

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using GDSF policy.
    Removes the item with the lowest priority from the heap.
    '''
    global pq, metadata, L

    # We loop to handle "lazy deletion".
    # The heap may contain stale entries for keys that have been updated with new priorities.
    while pq:
        # Peek at the lowest priority item
        priority, tick, key = pq[0]
        
        # Validation 1: Is this key effectively in the cache?
        # We rely on our metadata map which tracks active cache items.
        if key not in metadata:
            heapq.heappop(pq)
            continue
            
        # Validation 2: Is this heap entry the latest version?
        # If the tick in the heap doesn't match the tick in metadata, it's an old entry.
        latest = metadata[key]
        if latest['tick'] != tick:
            heapq.heappop(pq)
            continue
        
        # If we passed checks, this is the valid victim (Lowest Priority)
        heapq.heappop(pq)
        
        # GDSF Main Logic: Update L to the priority of the evicted object.
        # This raises the bar for new items entering the cache.
        L = priority
        
        # Clean up metadata
        del metadata[key]
        
        return key
        
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit, we increase frequency and update priority.
    '''
    global pq, metadata, L
    
    k = obj.key
    current_tick = cache_snapshot.access_count
    
    # Update Frequency
    if k in metadata:
        freq = metadata[k]['freq'] + 1
    else:
        # Fallback if metadata desyncs (unlikely)
        freq = 1
        
    # Recalculate Priority
    # Note: We use the current L. We do not update L on hits.
    p = get_priority(freq, obj.size, L)
    
    # Update Metadata
    metadata[k] = {
        'freq': freq,
        'priority': p,
        'tick': current_tick
    }
    
    # Push new state to heap (Lazy Update)
    # The old state remains in the heap but will be ignored by evict() via the tick check
    heapq.heappush(pq, (p, current_tick, k))
    
    # Maintenance: Compact heap if it grows too large due to lazy updates
    # If heap is significantly larger than actual items, rebuild it to free memory.
    if len(pq) > max(1000, 3 * len(metadata)):
        new_pq = []
        for key, data in metadata.items():
            heapq.heappush(new_pq, (data['priority'], data['tick'], key))
        pq = new_pq

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, we initialize frequency and priority.
    '''
    global pq, metadata, L
    
    k = obj.key
    current_tick = cache_snapshot.access_count
    
    # New items start with frequency 1
    freq = 1
    
    # Calculate Priority
    # New items get L + (1/Size).
    # Since L is set to the priority of the last evicted victim,
    # New items start roughly at the level of the victim, plus their own utility (1/Size).
    p = get_priority(freq, obj.size, L)
    
    metadata[k] = {
        'freq': freq,
        'priority': p,
        'tick': current_tick
    }
    
    heapq.heappush(pq, (p, current_tick, k))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup function.
    '''
    global metadata
    # Ensure metadata is consistent with the cache state
    if evicted_obj and evicted_obj.key in metadata:
        del metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa9q2gl8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpetfhse_5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpw0rgaafh.pickle

Iteration 87: New subsample score 1.22774 is not better than old score 1.3380969999999999, skipping
Iteration 88: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphwqbvm0y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpswhp7i_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5htyq7lo.pickle

Iteration 88: Proposed new text for program: import heapq
import itertools

# --- GDSF Global State ---

# Priority Queue (Min-Heap)
# Stores entries: [priority, tie_breaker, key]
# We use a list format so we can mark entries as invalid (lazy deletion)
pq = []

# Entry Finder
# Maps key -> entry_list (reference to the list inside pq)
# Used to locate and invalidate heap entries O(1)
entry_finder = {}

# Metadata / Frequency Tracking
# Maps key -> frequency (int)
# Persists even after eviction (Ghost functionality) to allow re-entry boost
freq_map = {}

# GDSF Inflation Value (L)
# The priority of the last evicted object. Acts as the "water level" for aging.
L = 0.0

# Tie Breaker Counter
# Ensures FIFO/LRU behavior for items with identical priority scores
counter = itertools.count()

# Marker for removed heap entries (Lazy Deletion)
REMOVED = '<removed-task>'

def calculate_priority(freq, size):
    '''
    GDSF Score: L + (Frequency / Size)
    Maximizes Hit Rate (Hits / Request) by favoring small, popular items.
    '''
    # We add a tiny epsilon to size to prevent division by zero if size is somehow 0,
    # though context says positive integers.
    return L + (float(freq) / max(1, size))

def add_task(key, priority):
    '''
    Push a new entry to the heap.
    '''
    global pq, entry_finder, counter
    
    # Check if a valid entry already exists and remove it (logical update)
    if key in entry_finder:
        remove_task(key)
        
    count = next(counter)
    # Entry format: [priority, insertion_order, key]
    # Min-heap will pop lowest priority first. 
    # If priorities match, it pops lowest count (Oldest/LRU) first.
    entry = [priority, count, key]
    entry_finder[key] = entry
    heapq.heappush(pq, entry)

def remove_task(key):
    '''
    Mark an existing entry as REMOVED. 
    Actual removal from heap happens during evict (lazy).
    '''
    global entry_finder
    if key in entry_finder:
        entry = entry_finder.pop(key)
        entry[-1] = REMOVED

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using GDSF logic.
    Pop from Min-Heap (lowest H value).
    '''
    global pq, entry_finder, L
    
    victim_key = None
    
    while pq:
        # Pop the item with the lowest priority score
        priority, count, key = heapq.heappop(pq)
        
        if key is not REMOVED:
            # Valid victim found
            victim_key = key
            
            # GDSF Aging Logic:
            # Update L to the priority of the evicted item.
            # This ensures future insertions start with a baseline priority
            # relative to what was just evicted.
            L = priority
            
            # Remove from our entry tracker
            if key in entry_finder:
                del entry_finder[key]
            
            break
            
    # Fallback (should not happen in normal operation unless cache empty)
    if victim_key is None:
        # Just in case heap logic drifts, pick any key from cache
        if cache_snapshot.cache:
            victim_key = next(iter(cache_snapshot.cache))
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority (L stays same, F increases).
    3. Update Heap Position.
    '''
    global freq_map
    
    k = obj.key
    size = obj.size
    
    # 1. Update Frequency
    current_freq = freq_map.get(k, 0) + 1
    freq_map[k] = current_freq
    
    # 2. Recalculate Priority
    # Note: We use current global L. 
    new_priority = calculate_priority(current_freq, size)
    
    # 3. Update Heap (remove old, add new)
    add_task(k, new_priority)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize Frequency (Check ghosts).
    2. Calculate Priority.
    3. Push to Heap.
    '''
    global freq_map
    
    k = obj.key
    size = obj.size
    
    # 1. Determine Frequency
    # If k is in freq_map, it's a "Ghost" hit (re-entry). We keep the accumulated frequency.
    # If not, it's a cold miss, start at 1.
    if k not in freq_map:
        freq_map[k] = 1
    else:
        freq_map[k] += 1
        
    current_freq = freq_map[k]
    
    # 2. Calculate Priority
    # New items start at current L + their utility
    new_priority = calculate_priority(current_freq, size)
    
    # 3. Add to Heap
    add_task(k, new_priority)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup after eviction.
    The heap logic is largely handled in evict(), but we ensure
    consistency here.
    '''
    global entry_finder, freq_map
    
    # While we keep frequency data for ghosts, we might want to cap the size of 
    # freq_map to prevent memory leaks in extremely long traces.
    # A simple approach: if freq_map gets too huge relative to cache capacity, purge it.
    # (For this specific context, unbounded ghost map is usually acceptable for max hit rate, 
    # but strictly we could prune low-freq ghosts).
    
    # Clean up entry_finder if not already done (double safety)
    if evicted_obj.key in entry_finder:
        remove_task(evicted_obj.key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz1dx9tuw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwzqwdnok.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwdgch4pi.pickle

Iteration 88: New subsample score 0.12124900000000001 is not better than old score 0.23518999999999998, skipping
Iteration 89: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7qzp9pbb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8mnvj3pg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdfghz9hj.pickle

Iteration 89: Proposed new text for program: from collections import deque, OrderedDict

# S3-FIFO Global Metadata
# s_queue: Small FIFO queue for new items (probationary)
# m_queue: Main FIFO queue for popular items (protected)
# g_map: Ghost map (OrderedDict) to track history of recently evicted S items
# freq: Dictionary to track frequency bits (0-3)

s_queue = deque()
m_queue = deque()
g_map = OrderedDict()
freq = {}

# S3-FIFO Constants
# 10% of cache allocated to probationary queue is empirically optimal
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict using the S3-FIFO policy.
    Returns the key of the victim object.
    '''
    global s_queue, m_queue, g_map, freq
    
    # Calculate target size for the small queue based on current count
    cache_count = len(cache_snapshot.cache)
    s_target = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    while True:
        # --- Decision: Evict from S or M? ---
        # We evict from S if it exceeds its target size, 
        # OR if M is empty (we have no choice but to evict from S).
        evict_from_s = (len(s_queue) >= s_target) or (not m_queue)
        
        if evict_from_s:
            if not s_queue:
                # Fallback: S is empty, so we must evict from M (if M exists)
                if not m_queue:
                    return None # Should not happen if cache is full
                evict_from_s = False
            else:
                victim = s_queue[0] # Peek S head
                v_freq = freq.get(victim, 0)
                
                if v_freq > 0:
                    # Second Chance: Move from S -> M (Promotion)
                    s_queue.popleft()
                    m_queue.append(victim)
                    # Reset frequency to 0 so it has to earn its keep in M.
                    # This prevents one-hit wonders from clogging M for too long.
                    freq[victim] = 0
                else:
                    # Evict from S
                    key = s_queue.popleft()
                    
                    # Add to Ghost Map (Track history)
                    # Using OrderedDict as an LRU set for ghosts
                    g_map[key] = None
                    if len(g_map) > cache_count:
                        g_map.popitem(last=False) # Remove oldest ghost (FIFO)
                        
                    return key
        
        # --- Logic: Evict from M ---
        if not evict_from_s:
            if not m_queue:
                # Should not be reached due to checks above, but safety first
                return None
                
            victim = m_queue[0] # Peek M head
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Second Chance in M: Move to tail, decrement freq
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                key = m_queue.popleft()
                # Items evicted from M are NOT added to Ghost in standard S3-FIFO
                return key

def update_after_hit(cache_snapshot, obj):
    '''
    On cache hit, increment frequency to indicate utility.
    Capped at MAX_FREQ to simulate limited counter bits.
    '''
    global freq
    key = obj.key
    curr = freq.get(key, 0)
    if curr < MAX_FREQ:
        freq[key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, decide placement based on Ghost history.
    '''
    global s_queue, m_queue, g_map, freq
    key = obj.key
    
    # Initialize frequency
    freq[key] = 0
    
    if key in g_map:
        # Ghost Hit: This item was evicted recently but requested again.
        # It belongs in the Main queue (M).
        m_queue.append(key)
        del g_map[key]
    else:
        # Standard Insert: Place in Small queue (S) for probation.
        s_queue.append(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo7qz9vjf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpaldc52dv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz4t66sfa.pickle

Iteration 89: New subsample score 0.208419 is not better than old score 0.218713, skipping
Iteration 90: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv6igmb16.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7txdsx_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz75j48gr.pickle

Iteration 90: Proposed new text for program: import heapq
import itertools

# Global Data Structures for GDSF
# Priority Queue stores tuples: [priority, unique_tie_breaker, key]
pq = [] 
# Mapping from key to the entry list [priority, tie_breaker, key] inside the heap
# This allows us to mark entries as 'removed' for lazy deletion (O(1) invalidation)
entry_finder = {} 
# Track frequency of currently cached items
frequency = {}
# The aging factor (inflation value)
L = 0.0
# Counter for tie-breaking in the priority queue to maintain stability
counter = itertools.count()
# Placeholder for removed heap entries
REMOVED = '<removed-task>'

def add_task(key, priority):
    '''
    Helper to add a new entry to the heap and map.
    Removes the old entry if it exists (lazy deletion).
    '''
    global pq, entry_finder, counter, REMOVED
    
    if key in entry_finder:
        remove_task(key)
        
    count = next(counter)
    entry = [priority, count, key]
    entry_finder[key] = entry
    heapq.heappush(pq, entry)

def remove_task(key):
    '''
    Helper to mark an existing entry as removed.
    '''
    global entry_finder, REMOVED
    if key in entry_finder:
        entry = entry_finder.pop(key)
        entry[-1] = REMOVED

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the lowest H-value (Priority).
    GDSF Logic: Evict min(H). Set L = H_victim.
    '''
    global pq, entry_finder, REMOVED, L
    
    # Pop items from the heap until we find a valid (non-removed) one
    while pq:
        priority, count, key = heapq.heappop(pq)
        
        if key is not REMOVED:
            # Found the valid victim
            # Update the aging factor L to the priority of the evicted object
            L = priority
            
            # Clean up our frequency map and entry finder
            del entry_finder[key]
            if key in frequency:
                del frequency[key]
                
            return key
            
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority: H = L + (Freq / Size).
    3. Update Heap position.
    '''
    global frequency, L
    
    k = obj.key
    s = obj.size
    
    # Increment frequency
    freq = frequency.get(k, 0) + 1
    frequency[k] = freq
    
    # Recalculate priority
    # Note: We use the *current* L. The object gets a boost based on its new frequency.
    priority = L + (freq / float(s))
    
    # Update in heap (remove old, add new)
    add_task(k, priority)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency = 1.
    2. Calculate Priority: H = L + (1 / Size).
    3. Push to Heap.
    '''
    global frequency, L
    
    k = obj.key
    s = obj.size
    
    # Initialize frequency
    frequency[k] = 1
    
    # Calculate priority using current aging factor L
    priority = L + (1.0 / float(s))
    
    add_task(k, priority)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup is handled mostly in evict(), but this ensures consistency.
    '''
    global entry_finder, frequency
    
    # Just a safety check to ensure the evicted object is fully cleared from metadata
    if evicted_obj:
        k = evicted_obj.key
        if k in entry_finder:
            remove_task(k)
        if k in frequency:
            del frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpf3os7wja.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_0edyk4o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppg0bhmil.pickle

Iteration 90: New subsample score 1.384056 is not better than old score 1.454139, skipping
Iteration 91: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpldn_2att.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvfv79u1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwg5zz8oq.pickle

Iteration 91: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# We maintain state across function calls using global variables.

# Queue S (Small): FIFO queue for new insertions. 
# Acts as a filter for one-hit wonders.
small_queue = deque()
small_set = set()

# Queue M (Main): FIFO queue for items that have proven useful.
# Acts as the long-term storage for popular items.
main_queue = deque()
main_set = set()

# Ghost Queue: Tracks keys recently evicted from S to detect "early reentry".
ghost_set = set()
ghost_queue = deque() # Only stores keys, not full objects

# Frequency Sketch: Approximate frequency tracking using a simple dict.
# In a production system, this would be a Count-Min Sketch, but a dict works for simulation.
freq_map = {} 

# Constants for S3-FIFO tuning
SMALL_RATIO = 0.1  # Target size of Small queue relative to cache capacity

def get_freq(key):
    return freq_map.get(key, 0)

def inc_freq(key):
    freq_map[key] = min(freq_map.get(key, 0) + 1, 3) # Cap frequency at 3 (sufficient for S3-FIFO)

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using S3-FIFO logic.
    S3-FIFO evicts from Small (S) or Main (M) based on their sizes and item popularity.
    '''
    global small_queue, small_set, main_queue, main_set
    global ghost_queue, ghost_set, freq_map

    cache = cache_snapshot.cache
    
    # Identify the candidate queue to evict from.
    # Logic: If Small queue is larger than target allocation (10%), evict from Small.
    # Otherwise, evict from Main.
    
    # Calculate current usage of Small queue in bytes
    current_small_size = 0
    # Note: iterating to sum size is slow in real systems, but required here because
    # we don't track byte-size internally in our globals. 
    # Optimization: We check lengths first as a proxy.
    
    # Fast path: If S is empty, must evict from M.
    if not small_queue:
        return evict_from_main(cache)
        
    # Standard S3-FIFO logic checks count, but byte-size is the constraint here.
    # We estimate if S is "full" relative to the cache capacity.
    # To avoid O(N) size calc every time, we rely on the heuristic that if 
    # len(S) > 10% of total items, we scrutinize S.
    total_items = len(small_set) + len(main_set)
    if total_items > 0 and (len(small_set) / total_items) > SMALL_RATIO:
        victim = evict_from_small(cache)
        if victim: return victim
        # If S decided not to evict (e.g., items were promoted), fall through to Main
        
    return evict_from_main(cache)

def evict_from_small(cache):
    '''
    Tries to evict from Small queue.
    If tail of S has high frequency (>1) or is in Ghost, it gets promoted to M.
    Otherwise, it is evicted.
    '''
    global small_queue, small_set, main_queue, main_set
    global ghost_queue, ghost_set
    
    while small_queue:
        candidate_key = small_queue[0] # Peek head
        
        # Verify key exists (lazy removal handling)
        if candidate_key not in cache:
            small_queue.popleft()
            if candidate_key in small_set: small_set.remove(candidate_key)
            continue

        freq = get_freq(candidate_key)
        
        # Promotion Condition: Accessed more than once (freq > 1) 
        # Since insertion sets freq=0 or 1, a hit increments it.
        # Note: Some S3 implementations promote if freq > 0. We use freq > 1 for robustness.
        if freq > 1:
            # Promote to Main
            small_queue.popleft()
            small_set.remove(candidate_key)
            
            main_queue.append(candidate_key)
            main_set.add(candidate_key)
            # Loop continues to find next candidate
        else:
            # Evict this candidate
            small_queue.popleft()
            small_set.remove(candidate_key)
            
            # Add to Ghost
            ghost_set.add(candidate_key)
            ghost_queue.append(candidate_key)
            
            # Bound Ghost size
            if len(ghost_queue) > len(cache):
                rem = ghost_queue.popleft()
                if rem in ghost_set:
                    ghost_set.remove(rem)
                    
            return candidate_key
            
    # If S became empty during promotion
    return None

def evict_from_main(cache):
    '''
    Tries to evict from Main queue.
    Items in M are given a "second chance" based on frequency.
    '''
    global main_queue, main_set, small_queue
    
    while main_queue:
        candidate_key = main_queue[0] # Peek head
        
        # Verify key exists
        if candidate_key not in cache:
            main_queue.popleft()
            if candidate_key in main_set: main_set.remove(candidate_key)
            continue

        freq = get_freq(candidate_key)
        
        # Second Chance Logic:
        # If frequency is high (> 0), decrement and reinsert at tail (give chance).
        # We decrement freq to ensure it eventually gets evicted if not accessed again.
        if freq > 0:
            main_queue.popleft()
            # Decrement frequency (simulating aging)
            freq_map[candidate_key] = freq - 1 
            # Reinsert at tail
            main_queue.append(candidate_key)
        else:
            # Evict
            main_queue.popleft()
            main_set.remove(candidate_key)
            return candidate_key

    # Should not happen in a full cache, but fallback
    if small_queue:
        k = small_queue.popleft()
        small_set.remove(k)
        return k
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency.
    S3-FIFO is lazy; we don't move items between queues on read.
    Movements only happen during eviction (queue maintenance).
    '''
    inc_freq(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost, insert to Main (rescue).
    2. Else, insert to Small.
    '''
    global small_queue, small_set, main_queue, main_set
    global ghost_queue, ghost_set, freq_map
    
    key = obj.key
    
    # Initialize frequency logic
    # If it was in Ghost, it implies it was recently seen, so we bump frequency higher
    if key in ghost_set:
        # Rescue! It was recently evicted but needed again.
        # Insert directly into Main.
        main_queue.append(key)
        main_set.add(key)
        
        # Remove from Ghost
        ghost_set.remove(key)
        # Note: lazy removal from ghost_queue (we don't scan queue to remove)
        
        # Being in ghost implies at least 1 previous access + 1 now
        freq_map[key] = 2 
    else:
        # Standard Insert into Small
        small_queue.append(key)
        small_set.add(key)
        freq_map[key] = 0 # Initial frequency (0 means 'inserted but not reused yet')

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the specifically evicted object.
    Most logic is handled in evict(), but we ensure frequency map doesn't grow infinitely.
    '''
    global freq_map
    
    # We keep frequency data for a bit even after eviction (for Ghost hits),
    # but we should clean it up if it's not in Ghost to save memory.
    # However, strict S3-FIFO keeps minimal frequency bits.
    
    # If the evicted object is NOT in ghost (i.e., evicted from Main or failed S filter), 
    # we can remove its frequency tracking to prevent memory leaks in the map.
    # Exception: If it's in Ghost, we need the map entry? Actually, Ghost implies
    # "we remember we saw it". We can reset freq or keep it.
    
    # Optimization: To prevent the freq_map from growing indefinitely with keys
    # that are long gone, we check:
    if evicted_obj:
        k = evicted_obj.key
        if k not in ghost_set:
            if k in freq_map:
                del freq_map[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8_vxlq24.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzdppk1mk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmt2zs_kk.pickle

Iteration 91: New subsample score 1.234366 is not better than old score 1.23937, skipping
Iteration 92: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjrx3n4hy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmwj65t99.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0sygf594.pickle

Iteration 92: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# Maintained globally to persist state between function calls
s_queue = deque()       # Small/Probationary Queue (FIFO) - holds new items
m_queue = deque()       # Main/Protected Queue (FIFO) - holds popular items
g_queue = deque()       # Ghost Queue (FIFO of keys) - holds evicted history
g_set = set()           # Fast lookup for Ghost keys
freq = {}               # Frequency counters for resident objects

# Byte tracking for the resident queues
s_bytes = 0
m_bytes = 0

# Constants
# 10% capacity for Small queue is optimal for balancing recency vs frequency (SOSP'23)
SMALL_RATIO = 0.1  
# Increased max frequency to better distinguish hot items
MAX_FREQ = 7       
# Increased ghost ratio to catch longer loops/large working sets
GHOST_RATIO = 10   

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict using enhanced S3-FIFO logic.
    Prioritizes cleaning the Small queue to filter one-hit wonders.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    # Iterate until a valid victim is found
    while victim_key is None:
        
        # Decision: Evict from Small (S) or Main (M)?
        # We process S if it exceeds its budget OR if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Fallback: If S is empty, we must evict from M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue (Probation) ---
            if not s_queue: break 
            
            candidate = s_queue[0]
            
            # Defense against keys removed externally or stale state
            if candidate not in cache:
                s_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: Item had a hit in S -> Move to M
                s_queue.popleft()
                
                size = cache[candidate].size
                s_bytes -= size
                m_bytes += size
                
                m_queue.append(candidate)
                
                # Reset frequency upon entering M. It has earned protection, 
                # but must re-earn its keep inside M.
                freq[candidate] = 0
            else:
                # Eviction: Item failed probation in S -> Evict
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost Queue to track history of this eviction
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        
        else:
            # --- Eviction from Main Queue (Protected) ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Second Chance: Item has hits -> Reinsert at tail of M and decay freq
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Item exhausted frequency in M -> Evict
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # M-evictions are typically not added to Ghost in S3-FIFO
    
    # --- Ghost Queue Maintenance ---
    # Ensure ghost queue doesn't grow unbounded, but is large enough to be useful
    target_ghost_len = len(cache) * GHOST_RATIO
    
    # Remove oldest ghost entries if we exceed target size
    while len(g_set) > target_ghost_len and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Cleanup head of ghost queue if it contains stale keys (already removed from set)
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new insertions.
    - If in Ghost: Restore to Main (Recall).
    - Else: Insert into Small (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S. 
        # This implies it belongs to a larger loop/working set -> Promote directly to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert into Small Queue for probation.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppk_63mlo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpr32soqh5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5k2u0log.pickle

Iteration 92: New subsample score 1.354204 is not better than old score 1.358288, skipping
Iteration 93: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpx_0lmzfv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp20576gcx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp46brgp4n.pickle

Iteration 93: Proposed new text for program: # Improved S3-FIFO Algorithm
# S: Small FIFO queue (probationary) - Implemented as dict for O(1) ops
# M: Main FIFO queue (protected) - Implemented as dict for O(1) ops
# G: Ghost FIFO queue (history) - Implemented as dict for O(1) ops
# freq: Frequency tracker

# Initialize global structures
# Using dictionaries allows maintaining insertion order (Python 3.7+)
# while providing O(1) lookups and deletions.
s_queue = {}
m_queue = {}
g_queue = {}
freq = {}

# S3-FIFO parameter: 10% of cache allocated to probationary queue
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Improved S3-FIFO Eviction Logic:
    1. Uses dictionaries as Queues for O(1) performance and strict ordering.
    2. Evicts from S (Small/Probation) if S is larger than target size.
       - If S-item has freq > 0: Move to M (Main), reset freq to 0.
       - Else: Evict and add to Ghost.
    3. Else evicts from M (Main).
       - If M-item has freq > 0: Reinsert at M-tail, decrement freq.
       - Else: Evict.
    '''
    global s_queue, m_queue, g_queue, freq

    # Determine target size for the probationary queue
    cache_count = len(cache_snapshot.cache)
    s_target = max(1, int(cache_count * SMALL_QUEUE_RATIO))
    
    candidate_key = None
    
    # Loop until a victim is found and removed
    while candidate_key is None:
        
        # --- Logic: Select Queue to Evict From ---
        # We target S if it is overflowing, OR if M is empty (forced S eviction)
        if len(s_queue) >= s_target or not m_queue:
            if not s_queue:
                # Should typically not happen if cache is full, but strictly safe
                if m_queue:
                    victim_queue = m_queue
                    is_s_queue = False
                else:
                    break
            else:
                victim_queue = s_queue
                is_s_queue = True
        else:
            victim_queue = m_queue
            is_s_queue = False
            
        # --- Logic: Inspect Head of Queue (FIFO) ---
        # next(iter(d)) gives the first inserted key (Head) in O(1)
        victim_key = next(iter(victim_queue))
        victim_freq = freq.get(victim_key, 0)
        
        if is_s_queue:
            if victim_freq > 0:
                # Second Chance: Promote from S to M
                del s_queue[victim_key]
                m_queue[victim_key] = None # Insert at tail of M
                # Reset frequency to ensure it stays active in M only if actually accessed again
                freq[victim_key] = 0 
            else:
                # Evict from S
                candidate_key = victim_key
                del s_queue[victim_key]
                # Record in Ghost Queue (FIFO)
                g_queue[candidate_key] = None
        else:
            # M Queue Processing
            if victim_freq > 0:
                # Second Chance: Reinsert at tail of M, decrement utility
                del m_queue[victim_key]
                m_queue[victim_key] = None
                freq[victim_key] = victim_freq - 1
            else:
                # Evict from M
                candidate_key = victim_key
                del m_queue[victim_key]
                # Optional: Add M-evictions to Ghost to catch long-loop patterns
                g_queue[candidate_key] = None

    # --- Logic: Maintain Ghost Queue Size ---
    # Keep Ghost roughly the size of the cache to track history
    while len(g_queue) > cache_count:
        # Remove the oldest ghost (Head)
        oldest_ghost = next(iter(g_queue))
        del g_queue[oldest_ghost]

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - Increment frequency, capped at MAX_FREQ.
    - Do NOT move items between queues (O(1) efficiency).
    '''
    global freq
    
    key = obj.key
    curr_freq = freq.get(key, 0)
    
    if curr_freq < MAX_FREQ:
        freq[key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - Check Ghost Queue.
    - If in Ghost: It was evicted too early. Insert directly to Main (M).
    - Else: Insert into Small (S).
    '''
    global s_queue, m_queue, g_queue, freq
    
    key = obj.key
    freq[key] = 0 # Initialize frequency
    
    if key in g_queue:
        # Ghost Hit! Restore directly to Main Queue
        m_queue[key] = None
        del g_queue[key]
    else:
        # Standard Insert: Start in Probationary Queue
        s_queue[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup: Remove frequency metadata.
    '''
    global freq
    
    key = evicted_obj.key
    if key in freq:
        del freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk1ya_y39.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp04andni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn9n04q5b.pickle

Iteration 93: New subsample score 0.58983 is not better than old score 0.63685, skipping
Iteration 94: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbmce8qt6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfrseqmx4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgw0jit1c.pickle

Iteration 94: Proposed new text for program: from collections import deque

# Improved S3-FIFO Cache Eviction Algorithm
# --------------------------------------------------------
# This implementation focuses on segregating the cache into a Small/Probationary
# queue (S) and a Main/Protected queue (M). It uses a Ghost queue (G) to 
# capture items that are evicted early but return, signaling a need for promotion.

# Global State
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set for O(1) lookups
freq = {}              # Frequency counter

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of cache allocated to probation allows new items to be filtered
# without evicting valuable main items too early.
SMALL_QUEUE_RATIO = 0.1
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict when the cache is full.
    Uses S3-FIFO logic: 
    - Prefer evicting from S (Probation) if S is large.
    - Evict from M (Main) only if S is small or M items are unused.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    while candidate_key is None:
        
        # --- Strategy Selection ---
        # We process the Small Queue (S) if it exceeds its target size
        # OR if the Main Queue (M) is empty (forced S eviction).
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafe: If M is empty, we must use S. If S is empty, we must use M.
        if not m_queue and s_queue:
            evict_from_s = True
        elif not s_queue and m_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Processing Small Queue (Probation) ---
            if not s_queue: break
            
            victim_key = s_queue[0] # Peek
            
            # 1. Handle stale items (external deletions)
            if victim_key not in cache_map:
                s_queue.popleft()
                if victim_key in freq: del freq[victim_key]
                continue
                
            v_obj = cache_map[victim_key]
            v_size = v_obj.size
            v_freq = freq.get(victim_key, 0)
            
            if v_freq > 0:
                # HIT in Probation: Promote to Main
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim_key)
                m_bytes += v_size
                
                # Reset frequency on promotion. 
                # It enters M as a "new" resident and must earn hits in M to survive M's eviction.
                freq[victim_key] = 0
            else:
                # NO HIT: Evict
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Record in Ghost (only keys)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Processing Main Queue (Protected) ---
            if not m_queue: break
            
            victim_key = m_queue[0] # Peek
            
            # 1. Handle stale items
            if victim_key not in cache_map:
                m_queue.popleft()
                if victim_key in freq: del freq[victim_key]
                continue
                
            v_obj = cache_map[victim_key]
            v_size = v_obj.size
            v_freq = freq.get(victim_key, 0)
            
            if v_freq > 0:
                # HIT in Main: Reinsert (Second Chance)
                m_queue.popleft()
                m_queue.append(victim_key)
                
                # Decay frequency. 
                # This ensures items must be consistently popular to stay in M forever.
                freq[victim_key] = v_freq - 1
            else:
                # NO HIT: Evict from Main
                # Items evicted from M usually don't go to Ghost (they had their chance)
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Cleanup ---
    # We maintain a ghost history larger than the cache count to catch 
    # large looping patterns (e.g., Trace 17).
    current_count = len(cache_map)
    # A multiplier of 3 allows us to remember items that cycle just outside cache capacity.
    target_ghost_count = max(current_count, 100) * 3
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency, capped at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - If in Ghost: It's a "Recall" (Looping pattern) -> Insert to Main.
    - Else: New Object -> Insert to Small (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! Promote directly to Main.
        # This bypasses probation for items we know are recurring.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Probation
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for the specific evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0b4m_5l0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfcqm6mhv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqsik06cx.pickle

Iteration 94: New subsample score 0.971279 is not better than old score 0.975069, skipping
Iteration 95: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpc0hsq3im.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9_wtwelo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg57s3wg8.pickle

Iteration 95: Proposed new text for program: from collections import deque

# Optimized S3-FIFO Algorithm Implementation
# --------------------------------------------------------
# This implementation improves upon standard S3-FIFO by:
# 1. Extending Ghost Queue coverage to include items evicted from the Main queue (M),
#    capturing long-range cyclic patterns ("recalls").
# 2. Persisting frequency metadata for items in the Ghost Queue. This ensures that
#    recalled items retain their "heat" and are harder to evict subsequently.
# 3. Increasing MAX_FREQ to allow fine-grained distinction between "warm" and "very hot" items,
#    improving the effectiveness of the Main queue's approximate LRU behavior.
# 4. Strict byte-aware capacity management.

# Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO with Reinsertion)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Ghost Set (Fast lookup)
freq = {}              # Frequency counter

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% for probationary queue allows quick filtering of one-hit wonders/scans.
SMALL_QUEUE_RATIO = 0.1
# Higher cap allows hot items to survive more reinsertion cycles in M.
MAX_FREQ = 15

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict when cache is full.
    Implements S3-FIFO logic with enhanced Ghost and Frequency management.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Dynamic target size for Small Queue
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Iterate until a valid eviction candidate is found
    while candidate_key is None:
        
        # --- Selection Policy ---
        # Evict from S if it's oversized or if M is empty.
        # Otherwise, evict from M.
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
            
        # Failsafes for empty queues
        if evict_from_s and not s_queue: evict_from_s = False
        if not evict_from_s and not m_queue: evict_from_s = True
            
        if evict_from_s:
            # --- S-Queue Eviction (FIFO with Promotion) ---
            victim = s_queue[0]
            
            # Lazy Cleanup: Item deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
            
            v_size = cache_map[victim].size
            v_freq = freq.get(victim, 0)
            
            if v_freq > 0:
                # Promotion: Item was hit in S. Move to M.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency on promotion. 
                # It must prove its utility again within the M queue cycles.
                freq[victim] = 0
            else:
                # Eviction: Item was not hit in S.
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost: Track this eviction
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- M-Queue Eviction (Approximate LRU via Reinsertion) ---
            victim = m_queue[0]
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Reinsert to tail and decay frequency.
                # This approximates LRU: hot items cycle back to tail, cold items drift to head.
                m_queue.popleft()
                m_queue.append(victim)
                freq[victim] = v_freq - 1
            else:
                # Eviction: Item drifted to head of M with 0 frequency.
                candidate_key = m_queue.popleft()
                m_bytes -= v_size
                
                # Add M-evictions to Ghost too. 
                # This is crucial for workloads where hot items might be evicted 
                # but are likely to return (Recalls).
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)

    # --- Ghost Queue Maintenance ---
    # Ensure ghost queue doesn't grow unbounded. 
    # A size of 2x Cache Count is generally robust for trace loops.
    current_item_count = len(cache_map)
    target_ghost_count = current_item_count * 2 if current_item_count > 0 else 100
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        rem_key = g_queue.popleft()
        if rem_key in g_set:
            g_set.remove(rem_key)
            # IMPORTANT: Clean up frequency metadata only when forgetting the ghost.
            if rem_key in freq:
                del freq[rem_key]

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increments frequency count on cache hit.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    # Saturation arithmetic
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handles new object insertion.
    - If in Ghost -> Restore to M (Protected).
    - Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Ensure freq entry exists
    if k not in freq:
        freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: A "Recall".
        # Promote directly to Main Queue.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
        # We do NOT reset frequency here. 
        # The item retains the frequency it had (or accumulated) which helps it stick in M.
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size
        # New items start cold
        freq[k] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleans up metadata after the system performs the eviction.
    We only delete frequency if the item is NOT tracked in Ghost.
    '''
    global freq, g_set
    k = evicted_obj.key
    
    # If the eviction logic put it in Ghost, we keep the frequency.
    # Otherwise, it's truly gone.
    if k not in g_set:
        if k in freq:
            del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuz5c41ju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp52uw5yt3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5nst49k4.pickle

Iteration 95: New subsample score 0.535246 is not better than old score 0.539462, skipping
Iteration 96: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp38hhi4i5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3c_103kl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyuvjk37h.pickle

Iteration 96: Proposed new text for program: from collections import deque

# S3-FIFO Data Structures
# We use global variables to maintain state across function calls
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO of keys)
g_set = set()          # Fast lookup for Ghost keys
freq = {}              # Frequency/Access bits
s_bytes = 0            # Bytes in Small Queue
m_bytes = 0            # Bytes in Main Queue

# Constants
# 10% is the standard optimal split for S3-FIFO to balance scan resistance and hit rate
SMALL_RATIO = 0.1      

def evict(cache_snapshot, obj):
    '''
    Selects a victim using S3-FIFO logic with optimized One-Bit Clock frequency management.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    # Loop until a victim is selected
    while victim_key is None:
        
        # 1. Determine which queue to evict from
        # We evict from S if it exceeds the 10% threshold OR if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If S is empty (possible if target_s_size is small or items are huge), force M
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Eviction from Small Queue (S) ---
            candidate = s_queue[0]
            
            # Handling stale keys (deleted externally)
            if candidate not in cache:
                s_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
            
            # Check if accessed while in S
            # We treat freq > 0 as the "visited" bit
            if freq.get(candidate, 0) > 0:
                # Promotion: Hit in S -> Move to M (Protected)
                s_queue.popleft()
                
                size = cache[candidate].size
                s_bytes -= size
                m_queue.append(candidate)
                m_bytes += size
                
                # CRITICAL: Reset freq to 0. 
                # It has been rewarded with promotion. Now it must prove itself in M.
                freq[candidate] = 0
            else:
                # Eviction: Miss in S -> Evict
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost Queue to catch "slow loops"
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        
        else:
            # --- Eviction from Main Queue (M) ---
            candidate = m_queue[0]
            
            # Handling stale keys
            if candidate not in cache:
                m_queue.popleft()
                if candidate in freq: del freq[candidate]
                continue
            
            # Check if accessed while in M
            if freq.get(candidate, 0) > 0:
                # Second Chance: Hit in M -> Reinsert at Tail
                m_queue.popleft()
                m_queue.append(candidate)
                
                # CRITICAL: Reset freq to 0 (Clock Algorithm).
                # We don't just decrement; we require a NEW hit in the next pass to stay.
                freq[candidate] = 0
            else:
                # Eviction: Miss in M -> Evict
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # M-evicted items do not enter Ghost queue in standard S3-FIFO
    
    # --- Ghost Queue Maintenance ---
    # Constrain ghost queue size to the current number of items in cache.
    # This prevents very old, cold items from polluting M upon random re-access.
    current_item_count = len(cache)
    while len(g_set) > current_item_count and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Lazy cleanup of ghost head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency, acting as a "visited" bit.
    '''
    global freq
    k = obj.key
    # We cap at 3 to prevent integer overflow, though logic acts on > 0.
    curr = freq.get(k, 0)
    if curr < 3:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Handle new object insertion.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency to 0
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Item was recently evicted from S. 
        # This implies a loop pattern larger than S but within working set.
        # Direct promotion to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Insert into S (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk2w0kgeu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2vdaq4g8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfzul6tys.pickle

Iteration 96: New subsample score 0.8397109999999999 is better than old score 0.70196. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp28xianaz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptpzxgql4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1z1iu7ee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwjj74pfd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp12jjwr3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_2l1qa19.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqb0_dfxl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgsn28lm0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp53z1kf45.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpij6xffp8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp27h0acab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3r_dgufo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1kitmmkn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppl825h3n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2yim35r5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp14prezxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4n62coch.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgxdjs6dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptuobiztm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpukp68hee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpppiupage.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppjvzv7gg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp71bypbni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmw0n6yya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe48fis3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjaugsoog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvunmg8f0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmphaeqt9u2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpeqhrv4n7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1k41g9hx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4s1fzq4u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp5_wytkbw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpramchaft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpuk23mjpe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpv_mdm591.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkhz5qrsp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpl7ywxn84.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpymrn7daz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8upebk4j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpkbqaajrr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjnig9ehp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpe9jn42nu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp781pgsvh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpwyziqf52.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7mok9l28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7s7ga4eo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmhm2ah1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp7_nq94w1.pickle

Iteration 96: Full valset score for new program: 0.2525322708333333
Iteration 96: Full train_val score for new program: 0.2525322708333333
Iteration 96: Individual valset scores for new program: [0.498063, 0.472975, 0.481486, 0.436383, 0.494732, 0.48357, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.392096, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.389381, 0.026164, 0.058672, 0.058672, 0.269802, 0.355847, 0.804369, 0.892179, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.753404, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.144719, 0.126451, 0.164993, 0.052632, 0.35, 0.174888, 0.142777, 0.466258, 0.081699]
Iteration 96: New valset pareto front scores: [0.508986, 0.48173, 0.493122, 0.442518, 0.50361, 0.49043, 0.272727, 0.498624, 0.540937, 0.531017, 0.091667, 0.398757, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.39233, 0.026556, 0.058672, 0.058672, 0.288554, 0.391129, 0.849057, 0.894232, 0.176101, 0.084091, 0.056948, 0.050504, 0.050164, 0.763274, 0.083333, 0.09444, 0.197998, 0.641937, 0.125461, 0.15165, 0.138057, 0.167001, 0.055921, 0.366667, 0.182587, 0.1554, 0.466258, 0.094771]
Iteration 96: Full valset pareto front score: 0.26537429166666665
Iteration 96: Updated valset pareto front programs: [{8}, {8}, {8}, {8}, {8}, {8}, {2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 21, 22}, {0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 18, 19, 21, 22}, {8, 13, 5, 7}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {8, 13}, {5}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {13}, {13, 7}, {13}, {13}, {13}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22}, {7}, {8, 13, 5, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {13}, {8}, {8}, {13}, {5}, {20}, {20}, {7}, {7}, {5}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22}, {7}, {2}, {13, 7}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {2}, {5}, {2}, {20}, {2, 5, 6, 7, 13, 14}, {2}, {2}, {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {7}]
Iteration 96: Best valset aggregate score so far: 0.2580827708333333
Iteration 96: Best program as per aggregate score on train_val: 5
Iteration 96: Best program as per aggregate score on valset: 5
Iteration 96: Best score on valset: 0.2580827708333333
Iteration 96: Best score on train_val: 0.2580827708333333
Iteration 96: Linear pareto front program index: 5
Iteration 96: New program candidate index: 22
Iteration 97: Selected program 8 score: 0.24520908333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpnele8csp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_opq7opn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjh4jajc8.pickle

Iteration 97: Proposed new text for program: from collections import OrderedDict

# Global State Management
# We use OrderedDict to simulate LRU queues efficiently (O(1) for moves/deletes).
# Key -> Object mapping is implicit in cache_snapshot, here we store Key -> Metadata.

# S1: Probationary Segment (New items). FIFO/LRU behavior.
# Holds items accessed once recently.
s1_cache = OrderedDict()

# S2: Protected Segment (Frequent items). LRU behavior.
# Holds items accessed at least twice.
s2_cache = OrderedDict()

# Ghost Registry: Keeps track of keys recently evicted from S1 to detect "false positives" in eviction.
# Storing keys only.
ghost_registry = OrderedDict()

# Configuration
# S1_RATIO determines how much cache capacity is dedicated to the probationary segment.
# 20-30% is standard for SLRU to prevent scans from wiping out the protected segment.
S1_RATIO = 0.25 
MAX_GHOST_ENTRIES = 0  # Will be set dynamically based on cache capacity

def evict(cache_snapshot, obj):
    '''
    Eviction Logic:
    1. If S1 (Probationary) is over its budget OR has items, prefer evicting from S1.
       This acts as a filter for one-hit wonders.
    2. If S1 is empty, evict from S2 (Protected).
    '''
    global s1_cache, s2_cache, ghost_registry
    
    current_size = cache_snapshot.size
    capacity = cache_snapshot.capacity
    
    # Calculate target size for S1
    # We aim to keep S1 around 25% of capacity to protect the main working set in S2.
    target_s1_size = capacity * S1_RATIO
    
    # Calculate current usage (approximated based on tracking, or recalculated)
    # Since we can't trust external tracking perfectly, we calculate specifically for decision making.
    # Note: iterating to sum size is O(N), but we need accuracy. 
    # Optimization: In a real system, we would track this incrementally. 
    # Here, we assume the python loop is fast enough or cache_snapshot.size is the main constraint.
    
    # Heuristic:
    # If we have items in S1, we almost always want to evict from S1 first 
    # unless S1 is very small and S2 is huge.
    
    victim_key = None
    
    # Strategy: Evict from S1 (Probationary) first to protect S2 (Frequent).
    if len(s1_cache) > 0:
        # Pop the first item (FIFO/LRU behavior) from S1
        victim_key, _ = s1_cache.popitem(last=False)
        
        # Add to ghost registry
        ghost_registry[victim_key] = True
        if len(ghost_registry) > 20000: # Cap ghost size to prevent memory leaks
             ghost_registry.popitem(last=False)
             
    elif len(s2_cache) > 0:
        # If S1 is empty, we must evict from S2 (LRU)
        victim_key, _ = s2_cache.popitem(last=False)
        # We don't usually add S2 evictions to ghost, or we could.
        # Adding to ghost helps if the working set is slightly larger than cache.
        ghost_registry[victim_key] = True
        if len(ghost_registry) > 20000:
             ghost_registry.popitem(last=False)
             
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in S1: Promote to S2 (it has proven its worth).
    2. If in S2: Move to MRU position in S2.
    '''
    global s1_cache, s2_cache
    
    key = obj.key
    
    if key in s1_cache:
        # Promotion: S1 -> S2
        del s1_cache[key]
        s2_cache[key] = True # Value doesn't strictly matter, just key presence
        
    elif key in s2_cache:
        # Re-access in S2: Update LRU position (move to end)
        s2_cache.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. If key is in Ghost Registry: It was evicted recently but came back. 
       This implies it belongs in the Protected Segment (S2).
    2. Otherwise: Insert into S1 (Probationary).
    '''
    global s1_cache, s2_cache, ghost_registry
    
    key = obj.key
    
    if key in ghost_registry:
        # It was a "mistake" to evict it (or the cycle is short). Promote directly to S2.
        # Remove from ghost
        del ghost_registry[key]
        
        # Add to S2
        s2_cache[key] = True
    else:
        # Standard new insertion -> S1
        s1_cache[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup: Ensure the evicted object is removed from our tracking structures.
    '''
    global s1_cache, s2_cache
    
    if evicted_obj:
        key = evicted_obj.key
        # Ensure it is removed from our metadata if it wasn't removed in evict()
        # (e.g. if the system called evict, but then decided to evict something else externally,
        # though usually evict() returns the key that IS evicted).
        if key in s1_cache:
            del s1_cache[key]
        if key in s2_cache:
            del s2_cache[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmow7xf_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpk0dsjnli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpacp1zf0o.pickle

Iteration 97: New subsample score 0.5327620000000001 is not better than old score 0.553229, skipping
Iteration 98: Selected program 5 score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpu7y92dsl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxcz21nz2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpatt2u5m9.pickle

Iteration 98: Proposed new text for program: from collections import deque

# Optimized S3-FIFO Algorithm
# --------------------------------------------------------
# This implementation focuses on minimizing miss rates by strictly managing
# the ratio between probationary items (Small Queue) and protected items (Main Queue)
# based on byte size, while using a Ghost Queue to quickly rescue items that
# exhibit cyclic access patterns.

# Data Structures
# s_queue: Small/Probationary Queue (FIFO). New items enter here.
# m_queue: Main/Protected Queue (FIFO). Popular items stay here.
# g_queue: Ghost Queue (FIFO). Stores keys of recently evicted items.
# g_set:   Set for O(1) lookup of keys in g_queue.
# freq:    Frequency counter map {key: count}.

s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()
freq = {}

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% of cache capacity is reserved for the probationary queue.
# This allows One-Hit-Wonders to flow through quickly without polluting the Main cache.
SMALL_QUEUE_RATIO = 0.1

# Frequency cap. We only care if an item is accessed "a few times" vs "never".
# High exact counts are less useful than relative recency/frequency.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict to make space.
    Implements the S3-FIFO eviction policy.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # Loop until a victim is chosen
    while candidate_key is None:
        
        # --- Decision: Evict from S (Probation) or M (Protected)? ---
        # 1. If S is larger than its target ratio -> Evict from S.
        # 2. If M is empty -> Evict from S.
        # Otherwise -> Evict from M.
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafe: If logic says S but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
        
        # Failsafe: If logic says M but M is empty, switch to S
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            if not s_queue: break # Should not happen due to failsafes
            
            victim = s_queue[0] # Peek head
            
            # Lazy Cleanup: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Promote to Main (M)
                # It was accessed while in probation, so it's likely useful.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency. It enters M as a "new" resident.
                freq[victim] = 0
            else:
                # No hits in probation: Evict
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            if not m_queue: break
            
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsert in M (Second Chance)
                # It has been accessed while in M. Give it another round.
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency: Eventually unused items must drift to the tail.
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                # The item reached the tail of M with 0 frequency.
                candidate_key = m_queue.popleft()
                m_bytes -= v_size
                
                # Crucial for Trace 21/Looping patterns:
                # Even M-evictions go to Ghost. If it was popular once, evicted, 
                # and comes back soon, we want to restore it to M quickly.
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)

    # --- Ghost Queue Maintenance ---
    # We maintain the ghost queue size relative to the number of items currently in cache.
    # A generic rule of thumb is keeping it roughly the size of the cache count.
    current_item_count = len(cache_map)
    # If the cache is very small (filling up), we use a minimum floor.
    target_ghost_count = max(current_item_count, 10)
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    This works for both S-queue and M-queue items.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> "Recall" -> Insert to M (Protected).
    2. Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit!
        # This item was recently evicted. The fact it came back suggests 
        # it is part of a working set larger than the cache or a loop.
        # Promote straight to Main Queue.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue (Probation)
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprp4i9js9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdwv1b9yd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp46eiwoll.pickle

Iteration 98: New subsample score 1.501548 is not better than old score 1.504505, skipping
Iteration 99: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy799hwcw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpbl4d6_zr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpqrvpp2p6.pickle

Iteration 99: Proposed new text for program: from collections import deque

# Global State Management
# S3-FIFO structure: 
# - Small Queue (S): Buffers new items, filters scan/one-hit wonders.
# - Main Queue (M): Holds popular items.
# - Ghost Queue (G): Tracks recently evicted items from S to detect "early eviction" mistakes.
s_queue = deque()
m_queue = deque()
g_queue = deque()
g_set = set()      # Fast lookup for ghost keys
freq = {}          # Frequency bits (capped counters)

# Byte tracking
s_bytes = 0
m_bytes = 0

# Tunable Parameters
SMALL_RATIO = 0.1   # Target size ratio for Small Queue (10% is standard S3-FIFO recommendation)
MAX_FREQ = 3        # Cap on frequency counter (2 bits usually suffices)
GHOST_RATIO = 1.0   # Ghost queue size relative to cache object count (M)

def evict(cache_snapshot, obj):
    '''
    Selects a victim using S3-FIFO logic:
    1. If Small Queue > 10% size OR Main Queue is empty: Evict from Small.
    2. Else: Evict from Main.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for the probationary (Small) queue
    target_s_size = capacity * SMALL_RATIO
    
    victim_key = None
    
    while victim_key is None:
        # Determine which queue to operate on
        # Primary Rule: Clean up S if it's too big, or if M is empty.
        evict_from_s = False
        if s_bytes >= target_s_size or not m_queue:
            evict_from_s = True
        
        # Failsafe: If logic says S, but S is actually empty, force M.
        if evict_from_s and not s_queue:
            evict_from_s = False
            
        if evict_from_s:
            # --- Small Queue Processing ---
            if not s_queue: break # Should be impossible if cache is full
            
            candidate = s_queue[0]
            
            # Sanity check: if candidate was deleted externally
            if candidate not in cache:
                s_queue.popleft()
                continue
                
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Promotion: It was hit while in S. Move to M.
                s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                m_queue.append(candidate)
                m_bytes += size
                
                # Crucial S3-FIFO Logic: Reset frequency to 0 upon promotion.
                # It must prove itself again in M.
                freq[candidate] = 0
            else:
                # Eviction: No hits in S. Bye.
                victim_key = s_queue.popleft()
                size = cache[candidate].size
                s_bytes -= size
                
                # Add to Ghost Queue to catch "false negatives"
                # (Items evicted too early that return quickly)
                if victim_key not in g_set:
                    g_set.add(victim_key)
                    g_queue.append(victim_key)
        else:
            # --- Main Queue Processing ---
            if not m_queue: break
            
            candidate = m_queue[0]
            
            if candidate not in cache:
                m_queue.popleft()
                continue
            
            cand_freq = freq.get(candidate, 0)
            
            if cand_freq > 0:
                # Reinsertion: Give it a second chance, but decrement frequency
                m_queue.popleft()
                m_queue.append(candidate)
                freq[candidate] = cand_freq - 1
            else:
                # Eviction: Ran out of chances in M.
                victim_key = m_queue.popleft()
                size = cache[candidate].size
                m_bytes -= size
                # Note: We do NOT add M-evictions to Ghost queue in standard S3-FIFO.
    
    # --- Ghost Queue Cleanup ---
    # Keep ghost queue bounded (e.g., same number of items as current cache count)
    current_count = len(cache)
    max_ghost_items = max(current_count, 100) # Minimum buffer
    
    while len(g_set) > max_ghost_items and g_queue:
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)
            
    # Lazy cleanup of ghost head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency. Cap at MAX_FREQ to simulate limited bits.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Insert logic:
    - If in Ghost: It's a "Recall". Insert directly to Main (M).
    - Else: It's new. Insert to Small (S).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize frequency
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit: Used to be in S, got evicted, came back. 
        # This signals S was too small or item has a loop pattern.
        # Promote straight to M.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert: Goes to probationary S queue.
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata for evicted key.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprtdcpo3p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprlpurr_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgwoqxqmd.pickle

Iteration 99: New subsample score 0.48539999999999994 is not better than old score 0.515008, skipping
Iteration 100: Selected program 7 score: 0.2456767708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpvnlv_xvs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpo6k4dfkm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp3diye_oh.pickle

Iteration 100: Proposed new text for program: from collections import deque, OrderedDict

# Global Data Structures
# 1. Protected Segment (Main): Holds popular items. Managed as LRU.
#    Structure: OrderedDict (key -> object_key)
protected_lru = OrderedDict()

# 2. Probationary Segment (Window): Holds new items. Managed as LRU.
#    Structure: OrderedDict (key -> object_key)
probation_lru = OrderedDict()

# 3. Frequency Keeper: A mapping of key -> frequency count.
#    Maintains history even for evicted items to make better admission decisions.
freq_sketch = {}

# Constants
# PROTECTED_RATIO: Percentage of cache capacity reserved for the Protected segment.
# A ratio around 80% protected / 20% probation is generally effective for high hit rates.
PROTECTED_RATIO = 0.8
MAX_FREQ_COUNT = 15      # Cap frequency to prevent runaway values
RESET_INTERVAL = 10000   # Halve frequencies after this many accesses to handle shifts

# Global Metrics
current_protected_size = 0
current_probation_size = 0
access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts an object based on a Frequency-Aware Segmented LRU policy.
    
    Logic:
    1. We need to make space. The default victim is the LRU of the Probation segment.
    2. However, if Probation is empty, we must evict from Protected.
    3. If Probation is growing too large (byte limit), we pick the Probation LRU.
    4. Crucially, we allow items to migrate from Probation to Protected if they are hit.
       But if Protected is full, we must pick a victim there.
    '''
    global protected_lru, probation_lru, freq_sketch
    global current_protected_size, current_probation_size
    
    cache = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for the protected segment
    target_protected_size = capacity * PROTECTED_RATIO
    
    victim_key = None
    
    # Strategy:
    # If the Protected segment is over budget, we should ideally evict from there to 
    # make room for more promising items. However, usually, we evict from Probation (Window)
    # because that is where "one-hit wonders" (scans) live.
    
    # We enforce that evictions happen primarily from Probation to filter out scans.
    # But if Probation is empty, we act as a standard LRU (evict from Protected).
    
    if len(probation_lru) > 0:
        # Candidate for eviction is the LRU of the Probation queue (first item in OrderedDict)
        candidate_key, _ = next(iter(probation_lru.items()))
        
        # We perform a "Dynamic Admission Policy" check here implicitly by the queue structure:
        # Items in Probation that get hit are moved to Protected (in update_after_hit).
        # Items that don't get hit drift to the LRU position here.
        
        # However, we must ensure we don't just blindly keep Probation empty.
        # If Protected is overflowing, we might actually need to downgrade a Protected item
        # back to Probation, effectively evicting the Probation LRU eventually.
        
        # Simple Logic: Evict strictly from Probation LRU if it exists.
        victim_key = candidate_key
        probation_lru.popitem(last=False) # Remove from head (LRU)
        
        # Update internal byte tracking
        if victim_key in cache:
            current_probation_size -= cache[victim_key].size
            
    elif len(protected_lru) > 0:
        # Probation is empty, so we must evict from Protected LRU
        victim_key, _ = next(iter(protected_lru.items()))
        protected_lru.popitem(last=False)
        
        if victim_key in cache:
            current_protected_size -= cache[victim_key].size
            
    # Note: If both are empty (impossible if cache is full), victim_key remains None
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increase global frequency.
    2. Promotion logic:
       - If in Probation: Move to Protected.
       - If in Protected: Move to MRU (standard LRU update).
    '''
    global protected_lru, probation_lru, freq_sketch
    global current_protected_size, current_probation_size, access_counter
    
    k = obj.key
    size = obj.size
    access_counter += 1
    
    # 1. Update Frequency (with aging)
    freq_sketch[k] = min(freq_sketch.get(k, 0) + 1, MAX_FREQ_COUNT)
    
    # Periodic Aging to adapt to changing workloads
    if access_counter % RESET_INTERVAL == 0:
        for key in list(freq_sketch):
            freq_sketch[key] //= 2
            if freq_sketch[key] == 0:
                del freq_sketch[key]

    # 2. Update Position
    if k in protected_lru:
        # Already protected: Refresh recency (Move to Tail)
        protected_lru.move_to_end(k)
        
    elif k in probation_lru:
        # Hit in Probation -> Promote to Protected
        # Remove from Probation
        del probation_lru[k]
        current_probation_size -= size
        
        # Add to Protected MRU
        protected_lru[k] = None
        current_protected_size += size
        
        # Balancing: If Protected is now too big, demote its LRU back to Probation
        # This keeps the "Active" set fresh.
        target_protected_size = cache_snapshot.capacity * PROTECTED_RATIO
        while current_protected_size > target_protected_size and len(protected_lru) > 0:
            # Pop LRU from Protected
            demoted_key, _ = protected_lru.popitem(last=False)
            demoted_obj = cache_snapshot.cache[demoted_key]
            d_size = demoted_obj.size
            
            current_protected_size -= d_size
            
            # Move to MRU of Probation (give it a second chance in the window)
            probation_lru[demoted_key] = None
            current_probation_size += d_size

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Always insert into Probation (Probationary Segment).
    2. Update Frequency.
    '''
    global probation_lru, freq_sketch
    global current_probation_size, access_counter
    
    k = obj.key
    size = obj.size
    access_counter += 1
    
    # Initialize Frequency
    # If it was in sketch (history), increment. If not, start at 1.
    freq_sketch[k] = min(freq_sketch.get(k, 0) + 1, MAX_FREQ_COUNT)
    
    # Insert into Probation MRU
    probation_lru[k] = None
    current_probation_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up tracking for evicted object.
    '''
    global freq_sketch
    # We DO NOT delete from freq_sketch immediately.
    # Retaining history allows the algorithm to recognize "recurring" items 
    # that are larger than the cache (loops) or items that return shortly after eviction.
    
    # However, to prevent memory leaks in the simulator, we can lazily clean up 
    # very old items during the RESET_INTERVAL phase in update_after_hit.
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg_1cs4bn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptndhzb8h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplwy4vuv_.pickle

Iteration 100: New subsample score 0.16108699999999998 is not better than old score 0.177769, skipping
Iteration 101: Selected program 2 score: 0.2567872291666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0mkuiwfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2wm6tdfd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa4st_lng.pickle

Iteration 101: Proposed new text for program: from collections import OrderedDict

# Segmented LRU (SLRU) Implementation
# The cache is split into two segments:
# 1. Probationary Segment: For new items. Acts as a filter for scan traffic.
# 2. Protected Segment: For items with >1 hit.
#
# Logic: 
# - New items go to Probation.
# - Hit in Probation -> Promote to Protected.
# - Hit in Protected -> Update LRU position.
# - If Protected grows too large (bytes), demote its LRU item back to Probation.
# - Eviction always happens from Probation LRU first.

# Metadata: OrderedDict acts as an LRU queue (Key -> Size)
# Order: First item is LRU, Last item is MRU.
probation = OrderedDict()
protected = OrderedDict()

# Current size (in bytes) of the protected segment
protected_size = 0

# Configuration: Fraction of total capacity reserved for the Protected segment.
# 80% is a standard, robust value for SLRU to protect the working set.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict.
    Strategy: Always sacrifice the LRU item from the Probationary segment first.
    This ensures that scan traffic flushes out of Probation without affecting Protected items.
    '''
    global probation, protected
    
    # 1. Prefer evicting from Probation (filter segment)
    if probation:
        # peek at the first item (LRU)
        return next(iter(probation))
    
    # 2. Fallback: If Probation is empty, evict from Protected
    if protected:
        return next(iter(protected))
        
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    Updates metadata on cache hit.
    Handles promotion (Probation -> Protected) and demotion (Protected -> Probation).
    '''
    global probation, protected, protected_size
    
    key = obj.key
    
    if key in protected:
        # Easy case: Item is already protected, just refresh recency
        protected.move_to_end(key)
        
    elif key in probation:
        # Promotion: Item proved its worth. Move from Probation to Protected.
        size = probation.pop(key)
        protected[key] = size
        protected_size += size
        
        # Enforce Protected Segment Capacity (Byte-based)
        limit = cache_snapshot.capacity * PROTECTED_RATIO
        
        # If Protected is too big, demote the LRU item from Protected back to Probation
        # This keeps the Protected segment containing only the *most* recently frequent items
        while protected_size > limit and protected:
            # popitem(last=False) removes the first (LRU) item
            demoted_key, demoted_size = protected.popitem(last=False)
            
            # Demote to MRU of Probation (give it a second chance before death)
            probation[demoted_key] = demoted_size
            protected_size -= demoted_size

def update_after_insert(cache_snapshot, obj):
    '''
    Updates metadata on new insert.
    New items always start in the Probationary segment.
    '''
    global probation
    # Add to Probation (MRU position)
    probation[obj.key] = obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Updates metadata immediately after an object is evicted.
    Removes the object from our tracking structures.
    '''
    global probation, protected, protected_size
    
    key = evicted_obj.key
    
    if key in probation:
        del probation[key]
    elif key in protected:
        size = protected.pop(key)
        protected_size -= size
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjii8v0ub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppxnuvfyb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp9to7cvv_.pickle

Iteration 101: New subsample score 0.12584599999999999 is not better than old score 0.134486, skipping
Best program from optimization: from collections import deque

# Improved S3-FIFO Implementation
# --------------------------------------------------------
# Optimizations:
# 1. Enlarged Ghost Queue (2x Cache Count) to better capture "looping" scans (Trace 21).
# 2. Strict Byte-Awareness for S/M ratio management.
# 3. Robust "Frequency" reset/decay logic to balance Recency vs Frequency.

# Data Structures
s_queue = deque()      # Small/Probationary Queue (FIFO)
m_queue = deque()      # Main/Protected Queue (FIFO)
g_queue = deque()      # Ghost Queue (FIFO) - Keys only
g_set = set()          # Ghost Set - Fast lookup
freq = {}              # Frequency counter for resident objects

# Byte Tracking
s_bytes = 0
m_bytes = 0

# Tuning Parameters
# 10% for new items allows scans to pass through quickly without evicting valuable main items.
SMALL_QUEUE_RATIO = 0.1
# Frequency cap prevents integer overflow and focuses on "recent" utility.
MAX_FREQ = 3

def evict(cache_snapshot, obj):
    '''
    Optimized S3-FIFO Eviction.
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    cache_map = cache_snapshot.cache
    capacity = cache_snapshot.capacity
    
    # Target size for Small Queue in bytes
    s_target_bytes = capacity * SMALL_QUEUE_RATIO
    
    candidate_key = None
    
    # We loop until we find a valid victim to evict
    while candidate_key is None:
        
        # --- Decision: Evict from S or M? ---
        # 1. If S is larger than its target ratio -> Evict from S (prefer keeping M full of good items)
        # 2. If M is empty -> We must evict from S
        # Otherwise -> Evict from M
        
        evict_from_s = False
        if s_bytes >= s_target_bytes or not m_queue:
            evict_from_s = True
        
        # Failsafe: If decision is S, but S is empty, switch to M
        if evict_from_s and not s_queue:
            evict_from_s = False
        # Failsafe: If decision is M, but M is empty, switch to S
        if not evict_from_s and not m_queue:
            evict_from_s = True
            
        if evict_from_s:
            # --- Eviction Logic for Small Queue (S) ---
            if not s_queue: break # Should not be reached
            
            victim = s_queue[0] # Peek head
            
            # Lazy Cleanup: If object was deleted externally
            if victim not in cache_map:
                s_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Second Chance: Promote to Main (M)
                # It demonstrated utility while in probation.
                s_queue.popleft()
                s_bytes -= v_size
                
                m_queue.append(victim)
                m_bytes += v_size
                
                # Reset frequency. It enters M as a "new" resident.
                # It must earn its keep in M again (prevents old probationary hits from shielding it forever).
                freq[victim] = 0
            else:
                # No utility seen: Evict
                candidate_key = s_queue.popleft()
                s_bytes -= v_size
                
                # Add to Ghost (Record of eviction)
                if candidate_key not in g_set:
                    g_set.add(candidate_key)
                    g_queue.append(candidate_key)
        
        else:
            # --- Eviction Logic for Main Queue (M) ---
            if not m_queue: break
            
            victim = m_queue[0] # Peek head
            
            if victim not in cache_map:
                m_queue.popleft()
                continue
                
            v_freq = freq.get(victim, 0)
            v_size = cache_map[victim].size
            
            if v_freq > 0:
                # Reinsert in M (Second Chance)
                m_queue.popleft()
                m_queue.append(victim)
                # Decay frequency: this ensures unused items eventually drift to the tail
                freq[victim] = v_freq - 1
            else:
                # Evict from M
                # Items evicted from M usually don't go to Ghost in S3-FIFO (they had their chance)
                candidate_key = m_queue.popleft()
                m_bytes -= v_size

    # --- Ghost Queue Maintenance ---
    # We keep the ghost queue size proportional to the number of items in cache.
    # Increasing this ratio to 2.0 helps catch larger scanning loops/recurrences (Optimizing Trace 21).
    current_item_count = len(cache_map)
    target_ghost_count = current_item_count * 2
    
    while len(g_set) > target_ghost_count:
        if not g_queue: break
        oldest = g_queue.popleft()
        if oldest in g_set:
            g_set.remove(oldest)

    # Lazy cleanup of ghost queue head
    while g_queue and g_queue[0] not in g_set:
        g_queue.popleft()

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency cap at MAX_FREQ.
    '''
    global freq
    k = obj.key
    curr = freq.get(k, 0)
    if curr < MAX_FREQ:
        freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. If in Ghost -> It's a "Recall" -> Insert to M (Protected).
    2. Else -> Insert to S (Probation).
    '''
    global s_queue, m_queue, g_queue, g_set, freq, s_bytes, m_bytes
    
    k = obj.key
    size = obj.size
    
    # Initialize freq
    freq[k] = 0
    
    if k in g_set:
        # Ghost Hit! This item was evicted but returned.
        # This signals a long-term loop or cyclic pattern. Promote straight to Main Queue.
        m_queue.append(k)
        m_bytes += size
        g_set.remove(k)
    else:
        # Standard Insert -> Small Queue
        s_queue.append(k)
        s_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup frequency metadata.
    Queues are managed in evict(), but frequency is a global dict.
    '''
    global freq
    k = evicted_obj.key
    if k in freq:
        del freq[k]
Best program validation score: 0.2580827708333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd3mae01y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpp_nn4tcc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpspr5cr_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmptfb_4lp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp2b6fuuia.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6ylraubc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp1fafq0wt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplwt8g3t3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpn424fv2n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpjcqk_2w7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmplnbi7_9l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyjs9esqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_e2e8zjo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp56_ws84q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp93t6b_cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4jhd229r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp0afh_5sf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpz8z3t7ui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpxbbvgthf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd4kc5rds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp090b4bas.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6mdncbsb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_0cyrmgp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_ihlc68k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd5ybmzq6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfpm1zw5v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmprk_ql_us.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpfzicoir8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp6z4z0rqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpcsswabzi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp04opxhxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp_vl2uul1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpyahnyboo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp8j8pb4gq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp92yu_5g4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpmbknlv8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpd3roltje.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpy6wtl6g8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpgtgl04jr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpzrd9boji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp4c79mufx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpdkz1jp43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpa1wdxsqk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmppe107_2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmp29_itqke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmps_bwz9w2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpsxabcbp8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_0/tmp/tmpg8kxsy4o.pickle

Optimized program test score (avg over test): 0.2580827708333333
