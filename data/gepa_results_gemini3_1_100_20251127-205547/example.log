GEPA artifacts will be saved to: /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/gepa_results
Dataset sizes -> train/val: 48 samples
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr6d2tl88.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd9ptowsu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkagzfjfi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjxkdda5t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprtxjuil3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprhbkxsnr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpknt42nvi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1mt_z1_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8g0bbr8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3vioac1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprh9zd2ty.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiu5kxd7t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpef7q4bes.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph_tqu_r3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwq9x3r0i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8m6r5ysa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1zhi9glm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprj_gir52.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp18e_8rf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp12ctxe6w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo7o2t_ug.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp7v5icnt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfdq_yz44.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprsjsh6kg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptfa98oaz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph3vdajbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzp0t4nr3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0h7vhbka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpftzes1p7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt7r8k4gc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjyluaid3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1h89qyy6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuhvxuh5a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppc227i6f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgw9hugfa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6mg7hhem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjg33wm7q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp35tnezsm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr3bedo2s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkj5oz1s9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsnkr0m8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzhws73u8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7zpu8y86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphmxafd3x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg32b646b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2q9d_56u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpulejwv2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzao_ljh0.pickle

Iteration 0: Base program full valset score: 0.2066171041666667
Iteration 1: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfec1ap7s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7gxxjlzc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5p_fnwed.pickle

Iteration 1: Proposed new text for program: # Metadata storage
# We track two things:
# 1. key -> last_access_time (for LRU behavior)
# 2. key -> frequency (to distinguish between probationary and protected items)
m_key_last_access = dict()
m_key_frequency = dict()

def evict(cache_snapshot, obj):
    '''
    SLRU-inspired Eviction Policy:
    1. First, look for items accessed only once (Probationary items).
       Among them, evict the Least Recently Used (LRU).
    2. If no items have only 1 access (unlikely in steady state, but possible),
       evict the LRU item from the entire cache.
    '''
    candid_obj_key = None
    
    # Identify keys currently in the cache
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # Filter for "Probationary" items (frequency == 1)
    probationary_keys = [k for k in current_keys if m_key_frequency.get(k, 0) <= 1]
    
    if probationary_keys:
        # If we have probationary items, evict the LRU among them
        # We find the key with the smallest timestamp in m_key_last_access
        candid_obj_key = min(probationary_keys, key=lambda k: m_key_last_access.get(k, 0))
    else:
        # If all items are "Protected" (frequency > 1), strictly follow LRU
        candid_obj_key = min(current_keys, key=lambda k: m_key_last_access.get(k, 0))
        
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update the last access time (Recency).
    2. Increment the frequency count (to promote to/keep in Protected state).
    '''
    global m_key_last_access, m_key_frequency
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency, capping at a small number (e.g., 3) to prevent
    # "frequency pollution" where an item stays forever just because it was hot long ago.
    # A cap of 2 or 3 is usually sufficient for SLRU behavior.
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = min(current_freq + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Set last access time.
    2. Initialize frequency to 1 (Probationary state).
    '''
    global m_key_last_access, m_key_frequency
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Clean up metadata for the evicted object.
    '''
    global m_key_last_access, m_key_frequency
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdfzfg3d7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpelin4w_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp5wuyubq.pickle

Iteration 1: New subsample score 0.71246 is better than old score 0.48805599999999993. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeidh1_7i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzquafvol.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpenw80d44.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp661qf73b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4n13c0zi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp40wghgw5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpks2x4p05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8puzwdqa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi5fa80hs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphni17w41.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp94xgnazi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm9_gqbq6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkqe1ztdj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqmguu94r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1l0ite88.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkcbx_c_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfc9twy6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo8dwy_l8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd5bzk8hi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiikwwvi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpynrt0r43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcitvys08.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnx1gubx1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmjcmlagr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptefxo9tr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3vo2wplg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0gjqo2ip.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnpvzf3vb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg2xfvxwu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkie5nq_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4_3y1zfz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi7jg13xc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplugycl8p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv2iqhf7j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpefwocwsq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphh5jnp4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzot2h8qg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphmmnh93e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_pcz8o51.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpterws2my.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3gcuqles.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxt05onym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprq9q99uq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxlvi6u6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp92fb3frd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7bki95af.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp306e55qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuyvlfc78.pickle

Iteration 1: New program is on the linear pareto front
Iteration 1: Full valset score for new program: 0.22657866666666662
Iteration 1: Full train_val score for new program: 0.22657866666666662
Iteration 1: Individual valset scores for new program: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.456547, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.269802, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.700136, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 1: New valset pareto front scores: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.269802, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.725664, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 1: Full valset pareto front score: 0.2279871041666666
Iteration 1: Updated valset pareto front programs: [{1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}]
Iteration 1: Best valset aggregate score so far: 0.22657866666666662
Iteration 1: Best program as per aggregate score on train_val: 1
Iteration 1: Best program as per aggregate score on valset: 1
Iteration 1: Best score on valset: 0.22657866666666662
Iteration 1: Best score on train_val: 0.22657866666666662
Iteration 1: Linear pareto front program index: 1
Iteration 1: New program candidate index: 1
Iteration 2: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgdrk3ux0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuq_xu0pj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpofncxoav.pickle

Iteration 2: Proposed new text for program: import random

# Metadata storage
# 1. Access frequency tracking (approximate LFU)
#    We map keys to their estimated frequency. This tracks both cached and 
#    non-cached (ghost) items to make better admission decisions.
m_frequencies = dict()

# 2. Recency tracking (LRU)
#    We map keys to their last access time.
m_last_access = dict()

# 3. Dynamic Aging Counter
#    To prevent frequency accumulation from forcing old data to stay forever.
m_access_counter = 0

def _get_freq(key):
    return m_frequencies.get(key, 0)

def _inc_freq(key):
    m_frequencies[key] = m_frequencies.get(key, 0) + 1

def _decay_frequencies():
    '''
    Periodically halve all frequencies to adapt to changing workloads (Aging).
    '''
    keys_to_remove = []
    for k in m_frequencies:
        m_frequencies[k] //= 2
        if m_frequencies[k] == 0:
            keys_to_remove.append(k)
    for k in keys_to_remove:
        del m_frequencies[k]

def evict(cache_snapshot, obj):
    '''
    TinyLFU-inspired Eviction & Admission Policy:
    
    Instead of blindly evicting the LRU item, we compare the 'value' (frequency)
    of the eviction candidate against the 'value' of the incoming object `obj`.
    
    1. Identify a Victim: Find the LRU item in the cache.
    2. Admission Check: 
       - If freq(new_obj) > freq(victim), we evict the victim to make room.
       - If freq(new_obj) <= freq(victim), we theoretically shouldn't insert the new object.
         However, the interface requires us to return an object to evict if the cache is full.
         
    NOTE: In strict TinyLFU, we might reject the new item. But since `evict` implies
    the caller *must* make space, we will return the LRU victim. However, the *intelligence*
    here comes from the fact that we track frequencies of non-cached items. 
    
    To improve upon the strict requirement of returning a victim:
    We will select the LRU item. But to approximate the admission filter, we 
    bias the eviction towards items with low global frequency.
    '''
    
    # Get all keys in cache
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Step 1: Find a small sample of LRU candidates (to avoid sorting the whole list)
    # Sampling improves performance while maintaining decent accuracy.
    # If the cache is small, we look at everything.
    sample_size = min(len(current_keys), 5)
    
    # We want to find the item that is Least Recently Used AND has low frequency.
    # Let's pick 'sample_size' items with the oldest timestamps.
    # Sort by access time ascending (oldest first)
    lru_candidates = sorted(current_keys, key=lambda k: m_last_access.get(k, 0))[:sample_size]
    
    # Step 2: From these LRU candidates, pick the one with the lowest frequency.
    # This protects an item that was accessed long ago but is historically very hot.
    victim_key = min(lru_candidates, key=lambda k: _get_freq(k))
    
    # (Optional Advanced Logic): 
    # If the incoming object `obj` has a frequency significantly lower than the victim,
    # usually we would want to "reject" the insertion. Since we must evict something 
    # to insert `obj`, we are forced to proceed. The benefit of this algorithm
    # is that `m_frequencies` tracks `obj` even if it was previously evicted,
    # giving it a fighting chance next time.
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency (LRU component).
    2. Increment Frequency (LFU component).
    3. Perform aging if necessary.
    '''
    global m_last_access, m_access_counter
    
    # Update Recency
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency
    _inc_freq(obj.key)
    
    # Handle Aging: Every N accesses, halve frequencies.
    # N is usually set to cache capacity or a large constant (e.g., 20,000).
    # Since we can't easily see exact capacity here without the snapshot every time, 
    # we use a heuristic based on access_count.
    m_access_counter += 1
    if m_access_counter >= 20000: # Heuristic aging threshold
        _decay_frequencies()
        m_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Increment Frequency (it counts as seen).
    '''
    global m_last_access
    
    m_last_access[obj.key] = cache_snapshot.access_count
    _inc_freq(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from Recency map (it's no longer in cache).
    2. CRITICAL: Do NOT remove from Frequency map immediately.
       We keep the frequency data (Ghost Cache) so if this item is requested again soon,
       it has a high count and can displace a cold item.
    '''
    global m_last_access
    
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    
    # Note: We purposely leave evicted_obj.key in m_frequencies to act as the "history/ghost" list.
    # The _decay_frequencies function will eventually clean it up if it stays inactive.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpve04h24i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp18dyn4pp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqh6ps2yv.pickle

Iteration 2: New subsample score 0.06209200000000001 is not better than old score 0.086738, skipping
Iteration 3: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6m59ewlq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa_rhh6qg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkn0aakw_.pickle

Iteration 3: Proposed new text for program: # Metadata storage
# We track the 'value' of each key for LFU-DA policy.
# value = frequency_count + global_age_offset
m_key_values = dict()
m_key_counts = dict() # Tracks raw frequency for recalculation

# Global 'age' variable to prevent frequency pollution
# This acts as the dynamic aging factor.
current_cache_age_L = 0.0

def evict(cache_snapshot, obj):
    '''
    LFU-DA (Least Frequently Used with Dynamic Aging) Eviction Policy.
    
    We evict the object with the lowest 'key_value'.
    Crucially, after eviction, we update the global 'current_cache_age_L'
    to be equal to the value of the evicted item. This creates the "aging" effect.
    '''
    global current_cache_age_L
    
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None
    
    # Identify the victim: The key with the minimum value in m_key_values.
    # If there are ties (equal values), we use a secondary tie-breaker.
    # Ideally, we would use LRU as a tie-breaker, but since we don't store 
    # separate timestamps to save memory overhead in this specific constraint set,
    # we rely on the implicit stability of min() or Python's dict insertion order 
    # (which often mimics FIFO/LRU naturally for ties).
    
    # Safe get in case of sync issues, though strictly shouldn't happen
    victim_key = min(current_keys, key=lambda k: m_key_values.get(k, 0))
    
    # LFU-DA Logic:
    # Update the global age L to the value of the evicted object.
    # This effectively "shifts" the floor for all future incoming objects.
    if victim_key in m_key_values:
        current_cache_age_L = m_key_values[victim_key]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Increment the raw frequency count.
    2. Update the object's value to (new_count + current_cache_age_L).
    '''
    global m_key_values, m_key_counts, current_cache_age_L
    
    # Increment raw count
    old_count = m_key_counts.get(obj.key, 0)
    new_count = old_count + 1
    m_key_counts[obj.key] = new_count
    
    # Update priority value: Freq + L
    m_key_values[obj.key] = new_count + current_cache_age_L

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize raw frequency to 1.
    2. Initialize value to (1 + current_cache_age_L).
    '''
    global m_key_values, m_key_counts, current_cache_age_L
    
    m_key_counts[obj.key] = 1
    # New items start with the current global age bias + 1
    m_key_values[obj.key] = 1 + current_cache_age_L

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Clean up metadata.
    '''
    global m_key_values, m_key_counts
    
    if evicted_obj.key in m_key_values:
        del m_key_values[evicted_obj.key]
    
    if evicted_obj.key in m_key_counts:
        del m_key_counts[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjs5_v2j5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoghv02f5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_hhpwjnc.pickle

Iteration 3: New subsample score 1.3739729999999999 is not better than old score 1.434271, skipping
Iteration 4: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprowzy1wh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbv485qzb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprwhvuvtj.pickle

Iteration 4: Proposed new text for program: # Metadata storage
# 1. m_last_access: key -> access_count (Standard LRU info)
# 2. m_frequency: key -> int (How many times accessed total)
# 3. m_ghost_db: key -> access_count (A "ghost" cache to remember recently evicted items)

m_last_access = dict()
m_frequency = dict()
m_ghost_db = dict()

# Constants
GHOST_CAPACITY_FACTOR = 0.5  # Size of ghost cache relative to main cache capacity (approx)

def evict(cache_snapshot, obj):
    '''
    Segmented LRU with Ghost/History Awareness.
    
    Logic:
    1. Divide items into "Probationary" (freq=1) and "Protected" (freq > 1).
    2. Ideally, we evict from Probationary first.
    3. Within Probationary, we evict the Least Recently Used.
    4. If no Probationary items exist, we evict the LRU from Protected.
    
    Why this works:
    - Scans (Trace 34): New items enter Probationary. If not accessed again, they are evicted quickly.
    - Loops/Recency: Items accessed again move to Protected.
    '''
    global m_last_access, m_frequency
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Filter keys into two segments
    probationary = []
    protected = []
    
    for k in current_keys:
        # Defaults to 1 if not found, though keys in cache should be in m_frequency
        f = m_frequency.get(k, 1)
        if f <= 1:
            probationary.append(k)
        else:
            protected.append(k)
    
    victim_key = None

    # Policy 1: Try to evict from Probationary (freq=1) first
    if probationary:
        # Find LRU among probationary items
        victim_key = min(probationary, key=lambda k: m_last_access.get(k, 0))
    
    # Policy 2: If no probationary items, evict LRU from Protected
    # (or if the cache is purely static high-freq items)
    elif protected:
        victim_key = min(protected, key=lambda k: m_last_access.get(k, 0))
    
    # Fallback (should typically be covered above)
    if victim_key is None:
        victim_key = min(current_keys, key=lambda k: m_last_access.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access (Recency).
    2. Increment Frequency (Importance).
    '''
    global m_last_access, m_frequency
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment frequency. Cap it to prevent overflow or excessive dominance, 
    # though high caps are fine here. 
    curr_freq = m_frequency.get(obj.key, 0)
    m_frequency[obj.key] = min(curr_freq + 1, 10)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Check Ghost Cache:
       - If the key is in m_ghost_db, it means it was evicted recently. 
         This implies our cache is too small or the working set is looping.
         We instantly promote it to "Protected" status (freq=2) so it survives longer this time.
       - Otherwise, set freq=1 (Probationary).
    '''
    global m_last_access, m_frequency, m_ghost_db
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost_db:
        # It was recently evicted, so this is a "warm" miss.
        # Give it a second chance immediately (freq=2 makes it Protected).
        m_frequency[obj.key] = 2
        # Remove from ghost now that it's back in main cache
        del m_ghost_db[obj.key]
    else:
        # Cold insert
        m_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from main metadata.
    2. Add to Ghost Cache (m_ghost_db) to track history of recently evicted items.
    3. Clean up Ghost Cache if it gets too big to prevent memory leaks.
    '''
    global m_last_access, m_frequency, m_ghost_db
    
    # 1. Clean main metadata
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    if evicted_obj.key in m_frequency:
        del m_frequency[evicted_obj.key]

    # 2. Add to Ghost Cache
    # We record when it was evicted (or its last access) to manage ghost size
    m_ghost_db[evicted_obj.key] = cache_snapshot.access_count
    
    # 3. Manage Ghost Cache Size
    # We don't want the ghost cache growing infinitely. 
    # We'll prune it if it exceeds roughly the number of items in the real cache.
    # Since we can't see the exact item count easily (cache_snapshot.cache is a dict),
    # we can just use a fixed safe limit or relative to capacity.
    # A simple heuristic: if ghost dict is > 2000 items (or some heuristic), prune LRU ghost.
    
    if len(m_ghost_db) > 2000: # Arbitrary safety cap, usually usually >> cache capacity
        # Remove the oldest entry from the ghost cache
        # finding min in a large dict is O(N), but necessary without extra data structures.
        # Given the constraints, this is acceptable.
        oldest_ghost = min(m_ghost_db, key=m_ghost_db.get)
        del m_ghost_db[oldest_ghost]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpafkuf7ia.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdwg4cn9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprvnfp2hp.pickle

Iteration 4: New subsample score 1.274369 is better than old score 1.240016. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjjqlpl7o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfhokktqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppwhbbifv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5ldpoo1x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph9aaz_w4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyiaer19v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6krcjea8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt6yayjdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplvmo5ney.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp12e5_1vp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6zatr00_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp89f1ynv0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdtozf4yy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyilovo7v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpboq9jsn_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9a_i_m13.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfvan57uo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjban5udu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqfp6wm1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplyh1nlg_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr0xtwe38.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpozsu_tfw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1bwfunp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6yjrlggj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx9vl0gf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpav1py8vh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptkyi0tgi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl170mp8m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_qrikeau.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1_yzkzax.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt11hqhrt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3o_8jgsg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi8qfre99.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu2sioti8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp071huwyk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj9b_0l8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzqs_elde.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf2ao2l4h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2jh0hnp8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjspy6k9p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7yogopl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt354pc4s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9f6y19yp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkj69aecu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdiftp88s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpadhr1e17.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp352g2xkq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph2r2iytf.pickle

Iteration 4: New program is on the linear pareto front
Iteration 4: Full valset score for new program: 0.23265512500000005
Iteration 4: Full train_val score for new program: 0.23265512500000005
Iteration 4: Individual valset scores for new program: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.478569, 0.540937, 0.531017, 0.075, 0.312611, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.368732, 0.026556, 0.058672, 0.058672, 0.269774, 0.387097, 0.856008, 0.894061, 0.020365, 0.036364, 0.041002, 9.6e-05, 3.6e-05, 0.721239, 0.074561, 0.061783, 0.009183, 0.641937, 0.125461, 0.022179, 0.021381, 0.023637, 0.042763, 0.316667, 0.021041, 0.02216, 0.466258, 0.03268]
Iteration 4: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.368732, 0.026556, 0.058672, 0.058672, 0.269802, 0.387097, 0.856008, 0.894061, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.725664, 0.083333, 0.067961, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.316667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 4: Full valset pareto front score: 0.241644375
Iteration 4: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0}, {2}, {0, 1, 2}, {1, 2}, {1}, {1, 2}, {0, 1, 2}, {2}, {2}, {2}, {2}, {2}, {1, 2}, {2}, {2}, {1, 2}, {1, 2}, {1}, {2}, {2}, {2}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {2}, {1, 2}, {1}, {1}, {1}, {1}, {2}, {1}, {1}, {1, 2}, {1}]
Iteration 4: Best valset aggregate score so far: 0.23265512500000005
Iteration 4: Best program as per aggregate score on train_val: 2
Iteration 4: Best program as per aggregate score on valset: 2
Iteration 4: Best score on valset: 0.23265512500000005
Iteration 4: Best score on train_val: 0.23265512500000005
Iteration 4: Linear pareto front program index: 2
Iteration 4: New program candidate index: 2
Iteration 5: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd_jta0x5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0ae0ziyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpishdenez.pickle

Iteration 5: Proposed new text for program: import math

# Metadata storage
# 1. m_access_history: key -> (last_access_time, frequency)
#    We store both in one tuple to reduce dictionary lookups.
m_access_history = dict()

# 2. m_global_freq: key -> total_frequency
#    Persists longer than the object stays in cache to remember "ghost" hits.
m_global_freq = dict()

# Constants
MAX_GLOBAL_HISTORY = 5000  # Prevent memory leaks in long traces

def evict(cache_snapshot, obj):
    '''
    Score-based eviction policy (LFU + LRU + Size Penalty).
    
    We want to evict the object with the LOWEST score (least valuable).
    However, standard intuition is to find the "worst" item. 
    Let's define a 'victim_score' where HIGHER means MORE EVICTABLE.
    
    Factors for being evictable:
    1. Low Frequency (Hasn't been useful often)
    2. Old Last Access (LRU)
    3. Large Size (Taking up too much room)
    '''
    global m_access_history, m_global_freq
    
    current_time = cache_snapshot.access_count
    candidates = cache_snapshot.cache.values()
    
    if not candidates:
        return None

    best_victim = None
    highest_victim_score = -1.0

    # We iterate through candidates to find the one with the highest "eviction score".
    for candidate in candidates:
        key = candidate.key
        size = candidate.size
        
        # Default stats if missing (shouldn't happen for cached items)
        last_access, freq = m_access_history.get(key, (0, 1))
        
        # 1. Recency Component: How long since last access?
        # Larger 'age' -> Higher score (more likely to evict)
        age = current_time - last_access
        
        # 2. Frequency Component:
        # Higher frequency -> Lower score (less likely to evict).
        # We use log scale because the difference between 1 and 2 is huge, 
        # but 100 vs 101 is negligible.
        freq_factor = math.log2(freq + 1)
        
        # 3. Size Component:
        # Larger items -> Higher score (more likely to evict to free space).
        # We use a gentle log scale so we don't aggressively purge all large files 
        # if they are actually useful.
        size_factor = math.log2(size + 1)
        
        # Combined Score Calculation:
        # Score = (Age * Size_Factor) / Frequency_Factor
        # 
        # Logic:
        # - Old items (high age) get high scores.
        # - Large items (high size_factor) multiply the age penalty (it's expensive to keep old large items).
        # - Frequent items divide the penalty (reducing the score, protecting them).
        victim_score = (age * size_factor) / (freq_factor + 0.1) # +0.1 avoids div by zero
        
        if victim_score > highest_victim_score:
            highest_victim_score = victim_score
            best_victim = candidate.key

    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update recency and increment frequency.
    '''
    global m_access_history, m_global_freq
    
    current_time = cache_snapshot.access_count
    
    # Update active cache stats
    old_last_access, old_freq = m_access_history.get(obj.key, (0, 0))
    m_access_history[obj.key] = (current_time, old_freq + 1)
    
    # Update global long-term frequency
    m_global_freq[obj.key] = m_global_freq.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize recency. Check global history for frequency.
    '''
    global m_access_history, m_global_freq
    
    current_time = cache_snapshot.access_count
    
    # Check if we have seen this item before in global history
    # If yes, we restore some of its importance so it doesn't get evicted immediately.
    past_freq = m_global_freq.get(obj.key, 0)
    
    # New frequency: start at 1, or recover part of past frequency.
    # We cap the recovered frequency to avoid an item being immortal just because 
    # it was popular 100,000 requests ago.
    initial_freq = 1 + min(past_freq, 5)
    
    m_access_history[obj.key] = (current_time, initial_freq)
    
    # Ensure global tracking is updated
    if obj.key not in m_global_freq:
        m_global_freq[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction: Clean up active cache stats, but KEEP global frequency.
    '''
    global m_access_history, m_global_freq
    
    # Remove from active tracking
    if evicted_obj.key in m_access_history:
        del m_access_history[evicted_obj.key]
        
    # Periodic Cleanup of Global History
    # To prevent memory issues, we prune the global history if it gets too large.
    # We only remove items that are NOT currently in the cache.
    if len(m_global_freq) > MAX_GLOBAL_HISTORY:
        # Identify keys in global_freq that are NOT in the active cache
        active_keys = set(cache_snapshot.cache.keys())
        ghost_keys = [k for k in m_global_freq if k not in active_keys]
        
        # If we have ghost keys, delete a chunk of them (randomly or just iterate)
        # to bring size down.
        if ghost_keys:
            # Delete the first ~20% of ghost keys found to amortize cleanup cost
            limit = len(ghost_keys) // 5
            for i in range(max(1, limit)):
                del m_global_freq[ghost_keys[i]]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8woj1xad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpassi5nn7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp17cuiknx.pickle

Iteration 5: New subsample score 1.0606740000000001 is not better than old score 1.12836, skipping
Iteration 6: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphs40_gbv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppf_mr11c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplz6g4_q_.pickle

Iteration 6: Proposed new text for program: # Metadata storage
# We track:
# 1. key -> frequency_bits (0, 1, 2, 3): Acts as a coarse counter/priority.
#    0: Just inserted (probationary).
#    1+: Accessed recently.
# 2. key -> insertion_order (int): To simulate FIFO ordering without strict lists.
m_freq_bits = dict()
m_insert_time = dict()

# We maintain a separate set to track which items are "probationary" vs "protected".
# Ideally, we want the probationary segment to be roughly 10% of the cache size.
m_probationary_set = set()

def evict(cache_snapshot, obj):
    '''
    Segmented FIFO with Reinsertion Eviction Policy:
    
    Strategy:
    1. We prioritize evicting from the 'Probationary' set (newly inserted items).
       - If a probationary item has been accessed (freq > 0), it gets promoted to Protected.
       - If not accessed, it is the best victim (scan resistance).
    
    2. If we cannot evict from Probationary (or it's empty), we evict from 'Protected'.
       - If a protected item has been accessed recently (freq > 0), it gets a second chance
         (reinserted logically with freq decremented).
       - If freq is 0, it is evicted.
    '''
    global m_freq_bits, m_insert_time, m_probationary_set
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Sort keys by insertion time to simulate FIFO traversal
    # Ideally, a real implementation uses a Deque/LinkedHashMap, but we simulate via sorting
    # based on the m_insert_time metadata we stored.
    # Note: sorting every evict is expensive in production O(N log N), but permitted 
    # in this simulation context. For O(N) constraints, we would strictly assume keys() 
    # iterates in insertion order (Python 3.7+), but we use m_insert_time for correctness here.
    fifo_queue = sorted(current_keys, key=lambda k: m_insert_time.get(k, 0))

    candidate = None
    
    # Pass 1: Scan for eviction candidate
    # We iterate through the FIFO queue looking for a victim.
    # We essentially perform a "Clock" algorithm sweep on the FIFO data.
    
    # We first try to evict from the Probationary set (Scan Resistance)
    probation_candidates = [k for k in fifo_queue if k in m_probationary_set]
    
    # If the probationary queue is larger than 10% of total items, we enforce eviction there first.
    # Otherwise, we treat the whole cache as a unified FIFO with second chances.
    target_queue = fifo_queue
    if len(probation_candidates) > len(current_keys) * 0.1:
         target_queue = probation_candidates

    for k in target_queue:
        freq = m_freq_bits.get(k, 0)
        
        if freq > 0:
            # "Second Chance": The item was used.
            # Decrement frequency (consume the bit) and logically "move to back" 
            # by updating insertion time to 'now'.
            # It survives this round.
            m_freq_bits[k] = freq - 1
            m_insert_time[k] = cache_snapshot.access_count # Move to back of FIFO
            
            # If it was probationary and got a hit, it is now promoted (remove from probation)
            if k in m_probationary_set:
                m_probationary_set.discard(k)
        else:
            # Freq is 0. This implies:
            # 1. It was inserted and never used (Probationary victim).
            # 2. Or it was Protected but hasn't been used since its last second chance.
            candidate = k
            break
            
    # Fallback: If everyone had freq > 0 (rare but possible in loops),
    # the loop updated everyone's time. The 'candidate' is still None.
    # We pick the absolute oldest based on the NEW modified times.
    if candidate is None:
        candidate = min(current_keys, key=lambda k: m_insert_time.get(k, 0))

    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    Increase the frequency bit. Cap it at a small integer (e.g., 2 or 3).
    We don't move it to the back of the queue immediately (Lazy Promotion).
    This reduces cache thrashing.
    '''
    global m_freq_bits
    current_freq = m_freq_bits.get(obj.key, 0)
    # Cap frequency at 3. This gives hot items "3 lives" in the clock algorithm.
    m_freq_bits[obj.key] = min(current_freq + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Mark insertion time (for FIFO order).
    2. Set frequency to 0.
    3. Add to Probationary set.
    '''
    global m_freq_bits, m_insert_time, m_probationary_set
    
    m_insert_time[obj.key] = cache_snapshot.access_count
    m_freq_bits[obj.key] = 0
    m_probationary_set.add(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up all metadata.
    '''
    global m_freq_bits, m_insert_time, m_probationary_set
    
    key = evicted_obj.key
    if key in m_freq_bits:
        del m_freq_bits[key]
    if key in m_insert_time:
        del m_insert_time[key]
    if key in m_probationary_set:
        m_probationary_set.discard(key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpooi31909.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6drfhh1o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpea0jtcsb.pickle

Iteration 6: New subsample score 0.5264949999999999 is not better than old score 0.5264949999999999, skipping
Iteration 7: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuz8xwj5g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4s_szubp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp98b1szrj.pickle

Iteration 7: Proposed new text for program: import sys

# Global metadata storage
# Structure: key -> {'freq': int, 'priority': float}
m_metadata = dict()

# Global aging factor 'L'
current_L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy:
    Find the object with the lowest H value (priority).
    H(p) = L + (Frequency(p) / Size(p))
    '''
    global current_L
    
    # Identify the victim with the minimum priority value
    victim_key = None
    min_priority = float('inf')
    
    # We iterate through the current cache to find the minimum priority.
    # Note: In a production system, a min-heap would be more efficient (O(1)), 
    # but for this simulation interface, iteration (O(N)) is acceptable.
    for key in cache_snapshot.cache:
        priority = m_metadata[key]['priority']
        if priority < min_priority:
            min_priority = priority
            victim_key = key
        # Tie-breaking: If priorities are equal, standard GDSF doesn't specify,
        # but LRU is a reasonable tie-breaker. Since Python dicts preserve insertion 
        # order (since 3.7), the first one found is effectively "older" in iteration if not re-inserted.
            
    # Update the aging factor L to the priority of the evicted object.
    # This is the "Dynamic Aging" mechanism.
    if victim_key is not None:
        current_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Re-calculate Priority: H = L + (Freq / Size)
    '''
    global m_metadata, current_L
    
    if obj.key in m_metadata:
        # Increment frequency
        m_metadata[obj.key]['freq'] += 1
        
        # Update priority
        # Note: We use current_L here to "reset" its age relative to the current baseline
        size = obj.size if obj.size > 0 else 1 # Prevent division by zero
        m_metadata[obj.key]['priority'] = current_L + (m_metadata[obj.key]['freq'] / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1 (or retrieve past frequency if using history).
       For this implementation, we reset/start at 1.
    2. Calculate Initial Priority: H = L + (1 / Size)
    '''
    global m_metadata, current_L
    
    size = obj.size if obj.size > 0 else 1
    
    # Initialize metadata
    # We start with freq=1 because it was just accessed (to cause the miss/insert)
    new_freq = 1
    new_priority = current_L + (new_freq / size)
    
    m_metadata[obj.key] = {
        'freq': new_freq,
        'priority': new_priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Remove the metadata for the evicted object to keep memory usage bounded.
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd6jwwaf7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp025e036_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4_r93hsd.pickle

Iteration 7: New subsample score 0.106879 is better than old score 0.089226. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc0n6lclx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp58ndobrh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1r_rf09c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpykrc10w9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt1jjg_ut.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsh5lqotj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7znqpjs9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvvhsbzvv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjgm13t7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnnk1u201.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvnx1i7z_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmply5jzad1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp25itgzaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptm9o_1ip.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppt3xtmt3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcn7pib9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu0jq85lq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp99gzc6tg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxp47h2ke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsjv53e2l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk9xulmxw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt35adi_b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbesoy9n4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpydxtitlr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp562vxxt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpylas7xnv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgp79bqii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpalc8bnrl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8jqskhk4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpty9yjgy4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9y7paf5s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx62pze8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo6r8twzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkfbtknda.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1mgfio1x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp159w62sv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkrddnt9v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd_s_wo_r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd0ncvy2i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf37esni7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi0jmvent.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc2va1j8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp812zeize.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl8qjk5kk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsw031u9d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpurloz4ph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpex2dv93x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsc0hqawe.pickle

Iteration 7: Full valset score for new program: 0.217409375
Iteration 7: Full train_val score for new program: 0.217409375
Iteration 7: Individual valset scores for new program: [0.457329, 0.435559, 0.443306, 0.388993, 0.453602, 0.444713, 0.264354, 0.498624, 0.537719, 0.531017, 0.066667, 0.332149, 0.023893, 0.0, 0.01968, 0.019442, 0.018581, 0.022069, 0.021375, 0.266728, 0.337266, 0.025282, 0.057382, 0.057382, 0.269776, 0.254032, 0.764647, 0.883964, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.747447, 0.072368, 0.062665, 0.009162, 0.618238, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.183333, 0.020965, 0.021879, 0.441718, 0.03268]
Iteration 7: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.368732, 0.026556, 0.058672, 0.058672, 0.269802, 0.387097, 0.856008, 0.894061, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.747447, 0.083333, 0.067961, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.316667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 7: Full valset pareto front score: 0.24209818749999998
Iteration 7: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 3}, {2}, {0, 1, 2, 3}, {1, 2}, {1}, {1, 2}, {0, 1, 2, 3}, {2}, {2}, {2}, {2}, {2}, {1, 2}, {2}, {2}, {1, 2}, {1, 2}, {1}, {2}, {2}, {2}, {1}, {1}, {1}, {1}, {1}, {3}, {1}, {1}, {1}, {2}, {1, 2, 3}, {1}, {1}, {1}, {1}, {2}, {1}, {1}, {1, 2}, {1}]
Iteration 7: Best valset aggregate score so far: 0.23265512500000005
Iteration 7: Best program as per aggregate score on train_val: 2
Iteration 7: Best program as per aggregate score on valset: 2
Iteration 7: Best score on valset: 0.23265512500000005
Iteration 7: Best score on train_val: 0.23265512500000005
Iteration 7: Linear pareto front program index: 2
Iteration 7: New program candidate index: 3
Iteration 8: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoi1svpu8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_4tgdple.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9oowl1c6.pickle

Iteration 8: Proposed new text for program: # Metadata storage
# We track:
# 1. key -> frequency (0, 1, 2, ...): Used as a "hit bit" or counter.
# 2. key -> queue_type ('S' or 'M'): Tracks if item is in "Small" (probation) or "Main" (protected) queue.
# 3. fifo_small: A list (or deque behavior via list) of keys strictly in insertion order for the Small queue.
# 4. fifo_main: A list of keys for the Main queue.

# Note: In a production system, we would use collections.deque for O(1) pops, 
# but standard lists work for the logic simulation here given the constraints.

m_key_freq = dict()
m_key_queue = dict()  # Maps key -> 'S' (Small) or 'M' (Main)
fifo_small = []       # Keys in Small FIFO
fifo_main = []        # Keys in Main FIFO

# Parameter: The size ratio of the small queue (usually 10% of capacity).
# Since we don't know the capacity until runtime, we calculate thresholds dynamically.
SMALL_QUEUE_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Inspired Eviction Policy:
    We iterate through the queues to find a victim.
    
    1. Check Small Queue (fifo_small):
       - If the tail item has freq > 0 (it was hit), promote to Main Queue and reset freq.
       - If the tail item has freq == 0 (no hits since insert), EVICT IT.
       
    2. If Small Queue is empty or all items were promoted, Check Main Queue (fifo_main):
       - If the tail item has freq > 0, give second chance (move to head of Main, reset freq).
       - If the tail item has freq == 0, EVICT IT.
    '''
    global m_key_freq, m_key_queue, fifo_small, fifo_main

    # Dynamic adjustment: If main queue is full, we must evict from there, 
    # but generally, we prioritize cleaning the small queue (scan resistance).
    # Since we can't easily perform the "loop until eviction" inside this read-only 'evict' signature 
    # (which only returns a key, doesn't modify the cache itself), we simulate the S3-FIFO decision logic.
    
    # We must operate on copies or logical views because 'evict' is usually a query function, 
    # but to support the complex logic of "reinserting/promoting" during eviction search, 
    # we have to assume the algorithm drives the decision.
    
    # Strategy: Simulate the S3-FIFO eviction loop to find the first candidate.
    # Note: Because this function is called *repeatedly* or just once to find *one* victim,
    # we need to be careful not to destructively modify the global queues until `update_after_evict`.
    # However, standard S3-FIFO modifies state (promotions) *during* the search for an eviction.
    # Given the constraints, we will peek.
    
    # 1. Try to evict from Small Queue (S) first if it's over its target size
    #    or if Main is not full. S3-FIFO usually evicts from S if S > 10% size.
    target_small_size = max(1, int(cache_snapshot.capacity * SMALL_QUEUE_RATIO)) if hasattr(cache_snapshot, 'capacity') else 10
    
    # We need a loop because we might promote items while looking for a victim.
    # Since we cannot modify the real queues in `evict` (it's supposed to return a victim),
    # we will return the candidate that *would* be evicted.
    
    # HEURISTIC IMPLEMENTATION for the `evict` signature:
    # Because we can't easily mutate the queues (promote items) inside this pure `evict` function 
    # without `update_after_...` calls, we look for the first item that satisfies the drop condition.
    
    # Phase A: Look at Small Queue (S)
    # In strict S3-FIFO, we process S if it is larger than 10%. 
    # If S is small, we look at M.
    
    current_s_size = len(fifo_small)
    
    if current_s_size >= target_small_size:
        # Check items in S roughly in FIFO order
        for key in fifo_small:
            freq = m_key_freq.get(key, 0)
            if freq > 0:
                # In a real S3-FIFO, this would be moved to M immediately.
                # Here, we skip it as a candidate because it deserves promotion.
                continue 
            else:
                # Found a cleaner victim (inserted but never accessed)
                return key
                
    # Phase B: Look at Main Queue (M)
    # If we fall through here, it means either S is small, or everything in S was hot.
    # We look for a victim in M.
    for key in fifo_main:
        freq = m_key_freq.get(key, 0)
        if freq > 0:
            # In real S3-FIFO, this gets a second chance (moved to back of M, freq=0).
            # We skip it as a candidate.
            continue
        else:
            # Found a victim in M (accessed, moved to M, then went cold)
            return key
            
    # Fallback: If everyone is hot (freq > 0), strictly evict the oldest from S, then M.
    # This prevents infinite loops if the cache is 100% active.
    if fifo_small:
        return fifo_small[0]
    if fifo_main:
        return fifo_main[0]
        
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. Increment frequency (capped usually at 2 or 3 for S3-FIFO/SLRU logic).
    '''
    global m_key_freq
    
    curr = m_key_freq.get(obj.key, 0)
    # Cap at 3. We only really need to know 0 vs >0, but higher helps tie-breaking conceptually
    m_key_freq[obj.key] = min(curr + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Add to Small Queue (S).
    2. Set freq to 0.
    '''
    global m_key_freq, m_key_queue, fifo_small
    
    # Insert new items into Small Queue
    m_key_freq[obj.key] = 0
    m_key_queue[obj.key] = 'S'
    fifo_small.append(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction cleanup.
    CRITICAL: This function must also handle the "Lazily Promoted" logic of S3-FIFO.
    Because `evict` just picked a victim, we must ensure the queues reflect that reality.
    
    However, the standard `evict` logic above just *picked* a key. 
    It didn't do the promotions of the keys it skipped over.
    
    To approximate S3-FIFO behavior correctly with these hooks:
    We need to perform queue maintenance.
    '''
    global m_key_freq, m_key_queue, fifo_small, fifo_main

    victim_key = evicted_obj.key

    # 1. Remove the actual victim from metadata
    if victim_key in m_key_freq:
        del m_key_freq[victim_key]
    if victim_key in m_key_queue:
        del m_key_queue[victim_key]
        
    # Remove from queue lists
    if victim_key in fifo_small:
        fifo_small.remove(victim_key)
    elif victim_key in fifo_main:
        fifo_main.remove(victim_key)
        
    # 2. PERFORM QUEUE MAINTENANCE (Simulating the S3-FIFO motion)
    # The `evict` function scanned and skipped "hot" items. We should now actually
    # move those hot items to their correct places so the next eviction scan is accurate.
    # This keeps the "FIFO" order rotating.
    
    # Process Small Queue Promotions
    # Move hit items from head of S to M until we find a cold item or S is empty
    # We limit this loop to avoid performance penalty, just doing a few rotations.
    rotations = 0
    while fifo_small and rotations < len(fifo_small):
        head = fifo_small[0]
        freq = m_key_freq.get(head, 0)
        
        if freq > 0:
            # Promote to Main
            fifo_small.pop(0)
            m_key_queue[head] = 'M'
            m_key_freq[head] = 0 # Reset freq on promotion (standard S3-FIFO)
            fifo_main.append(head)
            rotations += 1
        else:
            # Found a cold item at head of S, stop rotating S
            break

    # Process Main Queue Re-insertions
    # Move hit items from head of M to tail of M (Second Chance)
    rotations = 0
    while fifo_main and rotations < len(fifo_main):
        head = fifo_main[0]
        freq = m_key_freq.get(head, 0)
        
        if freq > 0:
            # Give second chance
            fifo_main.pop(0)
            m_key_freq[head] = 0 # Reset freq
            fifo_main.append(head)
            rotations += 1
        else:
            # Found a cold item at head of M, stop rotating M
            break
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpisnjy8bh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk1p51m5e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb4tgj3hd.pickle

Iteration 8: New subsample score 0.882482 is not better than old score 0.882482, skipping
Iteration 9: Selected program 1 score: 0.22657866666666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_ygyeefg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdd8opv5b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz72uz2pp.pickle

Iteration 9: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (Recency)
# 2. key -> global_frequency (Frequency history, persisted even after eviction implies better scan resistance)
m_key_last_access = dict()
m_key_frequency = dict()
m_access_counter = 0

# Parameters for aging
aging_interval = 10000 
aging_factor = 2

def get_score(key, current_time):
    '''
    Calculate a utility score.
    Lower score = better candidate for eviction.
    
    We want to protect items that are:
    1. Recently accessed (High Recency)
    2. Frequently accessed (High Frequency)
    
    Score = Frequency * Weight + Recency_Factor
    However, a simple approach often works best:
    Evict the item with the lowest frequency. If frequencies match, evict the LRU.
    '''
    freq = m_key_frequency.get(key, 0)
    last_access = m_key_last_access.get(key, 0)
    
    # We return a tuple. Python compares tuples element-by-element.
    # We want to minimize: (Frequency, Last_Access_Time)
    # This means we first look for items with low frequency (Probationary).
    # Among those, we look for the oldest ones.
    return (freq, last_access)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Frequency-Aware LRU.
    
    We attempt to evict the object that has the lowest historical frequency.
    If there are ties in frequency (common for 1-hit wonders), we break the tie
    using Least Recently Used (LRU).
    
    This effectively acts like a Segmented LRU but with dynamic segments based on counts.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # Find the key with the minimum (Frequency, Recency) tuple
    # This scans the cache (O(N)), but ensures high hit rate. 
    # For large caches, we would usually sample K random items, 
    # but exact calculation maximizes hit rate for this challenge.
    victim_key = min(current_keys, key=lambda k: (m_key_frequency.get(k, 0), m_key_last_access.get(k, 0)))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency, m_access_counter
    
    current_time = cache_snapshot.access_count
    m_access_counter = current_time
    
    m_key_last_access[obj.key] = current_time
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1
    
    _perform_aging_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Initialize or Increment Frequency.
       Note: We might remember this key from the past (ghost lists), so we don't always reset to 1.
       If it's new, it starts at 1.
    '''
    global m_key_last_access, m_key_frequency, m_access_counter
    
    current_time = cache_snapshot.access_count
    m_access_counter = current_time
    
    m_key_last_access[obj.key] = current_time
    
    # If we have seen this before (even if not in cache currently), increment. 
    # Otherwise set to 1.
    if obj.key not in m_key_frequency:
        m_key_frequency[obj.key] = 1
    else:
        m_key_frequency[obj.key] += 1
        
    _perform_aging_if_needed()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove Recency data.
    2. KEEP Frequency data (Ghost entry). 
       This allows the algorithm to "remember" that an item was popular 
       if it returns shortly after eviction.
    '''
    global m_key_last_access
    
    # We strictly remove the recency tracker for the object leaving the cache
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    # We DO NOT delete from m_key_frequency here.
    # Retaining frequency data allows us to identify "hot" items that thrashed out.

def _perform_aging_if_needed():
    '''
    Periodically halve all frequencies.
    This simulates a sliding window and prevents frequency counters from growing infinitely
    or allowing ancient heavy-hitters to block new trends.
    '''
    global m_key_frequency, m_access_counter, aging_interval
    
    # Simple check: if total access count matches interval
    if m_access_counter > 0 and m_access_counter % aging_interval == 0:
        keys_to_remove = []
        for k in m_key_frequency:
            m_key_frequency[k] //= 2
            if m_key_frequency[k] == 0:
                keys_to_remove.append(k)
        
        # Cleanup zero-frequency items to save memory
        for k in keys_to_remove:
            del m_key_frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphbbtxqrn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5x8uchn4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4z7jrf80.pickle

Iteration 9: New subsample score 1.0736050000000001 is better than old score 1.069697. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_p85muf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn_stsvsm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpptmdqnft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkyoi_ab_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptytauphb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5t62cdze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpppviy63w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzxb4ooz8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_j1ejx6a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpavbo8ccy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmm6vmop_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprb2pi3hd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjo77anon.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfx0_yioo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqyy_mikn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8618dmo0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1j6aqx3h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmporf72sr8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppz9aesgi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr0ya56eg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa267tr2u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpek_vbca2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn2d606eu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbtuzsgwh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnxx43c5d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv60ig0q9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp36lc8cqp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpswtbqwfo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk_pnzswm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjatztt56.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpump1qx7u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9z8bwehr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpanxun9fq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk57sjbgn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0npxot7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbba1_yc4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxo0j8x5f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptdht67y5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyv1e8hkq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnr52nafq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp36ffi6hh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxm2jqy9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5xfd1nt2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbcwz2g95.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3v0pbrr5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4qcbf8vn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv6g_1h4c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppns38hlr.pickle

Iteration 9: New program is on the linear pareto front
Iteration 9: Full valset score for new program: 0.23726579166666673
Iteration 9: Full train_val score for new program: 0.23726579166666673
Iteration 9: Individual valset scores for new program: [0.502314, 0.473033, 0.485243, 0.438161, 0.499331, 0.48651, 0.26555, 0.458907, 0.540937, 0.531017, 0.091667, 0.392984, 0.035132, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.369715, 0.026556, 0.058672, 0.058672, 0.288858, 0.359879, 0.796425, 0.894232, 0.039832, 0.038636, 0.045558, 0.0, 0.0, 0.721239, 0.083333, 0.067079, 0.018281, 0.641937, 0.125461, 0.041586, 0.039096, 0.043257, 0.052632, 0.366667, 0.041854, 0.041795, 0.466258, 0.078431]
Iteration 9: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.369715, 0.026556, 0.058672, 0.058672, 0.288858, 0.387097, 0.856008, 0.894232, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.747447, 0.083333, 0.067961, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 9: Full valset pareto front score: 0.24498306249999993
Iteration 9: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 3}, {2, 4}, {0, 1, 2, 3, 4}, {4}, {4}, {1, 2}, {0, 1, 2, 3, 4}, {4}, {4}, {4}, {4}, {4}, {1, 2, 4}, {4}, {2, 4}, {1, 2, 4}, {1, 2, 4}, {4}, {2}, {2}, {4}, {1, 4}, {1, 4}, {1, 4}, {1}, {1}, {3}, {1, 4}, {1}, {1}, {2, 4}, {1, 2, 3, 4}, {1}, {1}, {1}, {1, 4}, {4}, {1, 4}, {1}, {1, 2, 4}, {1}]
Iteration 9: Best valset aggregate score so far: 0.23726579166666673
Iteration 9: Best program as per aggregate score on train_val: 4
Iteration 9: Best program as per aggregate score on valset: 4
Iteration 9: Best score on valset: 0.23726579166666673
Iteration 9: Best score on train_val: 0.23726579166666673
Iteration 9: Linear pareto front program index: 4
Iteration 9: New program candidate index: 4
Iteration 10: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn29tb47b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpntwvy50e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnj55zp29.pickle

Iteration 10: Proposed new text for program: # Metadata storage for GDSF
# 1. key -> priority (The computed GDSF H-value)
# 2. key -> frequency (Access count for the object while in cache)
# 3. key -> last_access (For LRU tie-breaking)
# 4. current_L (The aging factor / inflation value)

m_priority = dict()
m_frequency = dict()
m_last_access = dict()
current_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GreedyDual-Size with Frequency (GDSF).
    
    We calculate a utility value (H) for every object:
        H = L + (Frequency / Size)
        
    Where L is a dynamic aging factor that increases over time as objects are evicted.
    We evict the object with the lowest H value. 
    
    Tie-breaking:
    If multiple objects have the same lowest H value, we evict the Least Recently Used (LRU)
    among them to respect temporal locality.
    '''
    global m_priority, m_last_access

    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to minimize: (Priority, Last_Access_Time)
    # This selects the item with the smallest GDSF value.
    # If priorities are identical, the tuple comparison falls back to Last_Access_Time (LRU).
    victim_key = min(current_keys, key=lambda k: (m_priority.get(k, 0.0), m_last_access.get(k, 0)))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Update Priority (H-value).
       Note: We recalculate H using the CURRENT L. This brings the object's priority
       up to the current "time," effectively resetting its aging process.
    '''
    global m_priority, m_frequency, m_last_access, current_L
    
    current_time = cache_snapshot.access_count
    
    m_last_access[obj.key] = current_time
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1
    
    # GDSF Priority Calculation
    # By adding current_L, we ensure this recently accessed item has a higher
    # priority than older items that calculated their score based on a smaller, older L.
    priority = current_L + (m_frequency[obj.key] / float(obj.size))
    m_priority[obj.key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Initialize Frequency to 1.
    3. Calculate initial Priority.
    '''
    global m_priority, m_frequency, m_last_access, current_L
    
    current_time = cache_snapshot.access_count
    
    m_last_access[obj.key] = current_time
    # Reset frequency to 1 on fresh insert. We do not keep ghost history to avoid
    # cache pollution by stale heavy hitters.
    m_frequency[obj.key] = 1
    
    # Calculate Priority relative to current aging factor L
    priority = current_L + (1.0 / float(obj.size))
    m_priority[obj.key] = priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update the global aging factor L.
       L becomes the priority of the object that was just evicted.
       This is the core mechanism of GDSF: it raises the "water level" required to stay in cache.
    2. Clean up metadata.
    '''
    global m_priority, m_frequency, m_last_access, current_L
    
    if evicted_obj.key in m_priority:
        # The system ages by setting L to the evicted object's priority.
        current_L = m_priority[evicted_obj.key]
        del m_priority[evicted_obj.key]
        
    if evicted_obj.key in m_frequency:
        del m_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8c7pf4xp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptqsespff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgy_gpl8g.pickle

Iteration 10: New subsample score 0.086734 is not better than old score 0.117974, skipping
Iteration 11: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpafe3xbli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps1guc6fv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppwpsm97v.pickle

Iteration 11: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (Recency)
# 2. key -> frequency (0 for probation, 1+ for protected)
m_key_last_access = dict()
m_key_frequency = dict()

# Ghost list: Stores keys of recently evicted items.
# Using a dict behaves like an Ordered Set in Python 3.7+ (maintains insertion order).
m_ghosts = dict()

# Constants
MAX_FREQ = 3       # Saturation point. We don't need to distinguish between 100 hits and 103 hits.
GHOST_RATIO = 1.0  # How many ghosts to keep relative to current cache object count.

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Size Tie-Breaking.
    
    1. Primary Filter: Frequency (Segment).
       We prefer to evict items with Frequency 0 (Probation) over Frequency > 0 (Protected).
    
    2. Secondary Filter: Recency (LRU).
       Among items with the same frequency, we evict the Least Recently Used.
       
    3. Tie-Breaker: Size.
       Technically, maximizing object hit rate implies fitting more objects. 
       However, strict size eviction can be dangerous. We rely primarily on (Freq, Recency).
       
    Score Tuple (Minimized): (Frequency, Last_Access)
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to find the "worst" item.
    # Python's min() with a tuple compares elements in order.
    # 1. freq: Lower frequency (0) is evicted before higher.
    # 2. last_access: Older time is evicted before newer.
    
    # Optimization: We scan O(N). For very large caches, sampling is preferred, 
    # but exact LRU/SLRU is required for maximum hit rate in this context.
    victim_key = min(current_keys, key=lambda k: (m_key_frequency.get(k, 0), m_key_last_access.get(k, 0)))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Promote to Protected (Increment Frequency).
       Saturation ensures we don't overflow or let items get "too heavy".
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = current_time
    
    # Increase frequency, but cap at MAX_FREQ.
    # This prevents an item with 10,000 hits from blocking a new trend forever.
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost List. 
       - If present: This item was recently here. Restore it as "Protected" (Freq 1).
       - If absent: This is a new item. Insert as "Probation" (Freq 0).
    2. Set Recency.
    3. Clean up Ghosts if needed.
    '''
    global m_key_last_access, m_key_frequency, m_ghosts
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    if obj.key in m_ghosts:
        # It's a "Phoenix" item returning from the dead.
        # Promote directly to Protected segment.
        m_key_frequency[obj.key] = 1
        # Remove from ghosts as it is now in cache
        del m_ghosts[obj.key]
    else:
        # Brand new item. Probation segment.
        m_key_frequency[obj.key] = 0
        
    # Manage Ghost Size
    # We use a heuristic limit based on current cache object count.
    # If the cache is small, the ghost list should be small.
    # We assume 'capacity' is bytes, so we use len(cache) as proxy for object count target.
    target_ghost_size = max(10, int(len(cache_snapshot.cache) * GHOST_RATIO))
    
    while len(m_ghosts) > target_ghost_size:
        # Remove oldest inserted ghost (FIFO behavior of dict)
        it = iter(m_ghosts)
        oldest_ghost = next(it)
        del m_ghosts[oldest_ghost]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up metadata for the victim.
    2. Add victim to Ghost List.
    '''
    global m_key_last_access, m_key_frequency, m_ghosts
    
    key = evicted_obj.key
    
    # Remove active metadata
    if key in m_key_last_access:
        del m_key_last_access[key]
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    # Add to ghosts to remember this key recently existed.
    # We store True (or timestamp) to indicate presence.
    m_ghosts[key] = True
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf84p8y2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl595zp4r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpddr86g9g.pickle

Iteration 11: New subsample score 0.46564099999999997 is better than old score 0.345981. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn2rcswav.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl1q82ea7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7vij1486.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp09rhd2ul.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbzq53r1g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphk9jw2pj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp79ip64kc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfzz0me38.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj5b69mfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprr3pvlkk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkoshrl9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp24uefpsx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfwxp43_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkcfke50c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcqsmdzpy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmdendd9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx0xnx2fp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjvi1cyp0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4nmn76rh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcom0a0av.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvm7ahm0o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu5w4xb9t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpahh608p6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprmauew70.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpatz5zhyd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9i0_8tku.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdggn_iql.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0xx7z93a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph79hp48j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8vavm8a0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyftcnuj5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5c7ymasa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp614yp0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphu1jgg1m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpms4ccv2c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwy8c2ztw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmj2earlz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsn6w_r8x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjylm7zth.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr5xeqm3e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt21tnaad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptp3vdela.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfz9tnkng.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1xspnkvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5950_2tb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxlmkd2if.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprjwntwg0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr5wz8xn0.pickle

Iteration 11: New program is on the linear pareto front
Iteration 11: Full valset score for new program: 0.2508065
Iteration 11: Full train_val score for new program: 0.2508065
Iteration 11: Individual valset scores for new program: [0.489453, 0.466379, 0.478638, 0.435049, 0.486923, 0.482993, 0.272727, 0.456547, 0.539864, 0.531017, 0.075, 0.361012, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.381514, 0.026164, 0.058672, 0.058672, 0.332087, 0.322581, 0.789474, 0.892179, 0.074873, 0.038636, 0.045558, 0.029555, 0.029983, 0.692818, 0.083333, 0.067961, 0.096221, 0.640392, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 11: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.381514, 0.026556, 0.058672, 0.058672, 0.332087, 0.387097, 0.856008, 0.894232, 0.074873, 0.038636, 0.045558, 0.029555, 0.029983, 0.747447, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 11: Full valset pareto front score: 0.25827887499999996
Iteration 11: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 3}, {2, 4}, {0, 1, 2, 3, 4, 5}, {4}, {4}, {1, 2, 5}, {0, 1, 2, 3, 4, 5}, {4}, {4}, {4}, {4}, {4}, {1, 2, 4, 5}, {5}, {2, 4}, {1, 2, 4, 5}, {1, 2, 4, 5}, {5}, {2}, {2}, {4}, {5}, {1, 4, 5}, {1, 4, 5}, {5}, {5}, {3}, {1, 4, 5}, {1, 5}, {5}, {2, 4}, {1, 2, 3, 4, 5}, {5}, {5}, {5}, {1, 4, 5}, {4, 5}, {5}, {5}, {1, 2, 4, 5}, {1, 5}]
Iteration 11: Best valset aggregate score so far: 0.2508065
Iteration 11: Best program as per aggregate score on train_val: 5
Iteration 11: Best program as per aggregate score on valset: 5
Iteration 11: Best score on valset: 0.2508065
Iteration 11: Best score on train_val: 0.2508065
Iteration 11: Linear pareto front program index: 5
Iteration 11: New program candidate index: 5
Iteration 12: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjcyq6qu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzp4zzj7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppesuo4cp.pickle

Iteration 12: Proposed new text for program: import math

# =============================================================================
# Metadata
# =============================================================================
# We emulate Adaptive Replacement Cache (ARC) behavior using metadata.
# ARC divides the cache into two logical lists:
#   L1: Recently accessed only once (Recency-heavy)
#   L2: Recently accessed at least twice (Frequency-heavy)
#
# We maintain ghost lists (history of evicted items):
#   B1: Evicted from L1
#   B2: Evicted from L2

# Dictionary mapping key -> integer state
# 0: Not tracked (New)
# 1: In L1 (Recent, seen once recently)
# 2: In L2 (Frequent, seen at least twice)
# 3: In B1 (Ghost Recent)
# 4: In B2 (Ghost Frequent)
m_key_state = dict()

# Metadata for LRU/LFU decisions
m_key_access_time = dict()

# ARC adaptation parameter 'p'
# 0 <= p <= capacity.
# p represents the target size for the L1 (Recent) set.
# If p is high, we favor Recency. If p is low, we favor Frequency.
arc_p = 0
arc_capacity = 0  # Will be lazy-initialized from cache snapshots

def get_params(capacity):
    global arc_p, arc_capacity
    if arc_capacity != capacity:
        arc_capacity = capacity
        # Initialize p to favor neither initially
        arc_p = 0 
    return arc_p

def evict(cache_snapshot, obj):
    '''
    ARC-like Eviction Policy.
    
    The cache effectively contains L1 U L2.
    We need to decide whether to evict from L1 or L2 based on the target size `p`.
    
    Len(L1) + Len(L2) <= Capacity
    Target size for L1 is `p`.
    
    Logic:
    1. Identify all keys currently in the cache.
    2. Classify them into L1 (State 1) and L2 (State 2).
       (Note: If a key in cache is somehow marked ghost/new due to sync issues, treat as L1).
    3. If len(L1) > p:
       We need to evict from L1 (the LRU page in L1).
    4. Else:
       We evict from L2 (the LRU page in L2).
    '''
    global arc_p
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Separate keys into L1 and L2 based on metadata
    l1_candidates = []
    l2_candidates = []
    
    for k in current_keys:
        state = m_key_state.get(k, 1) # Default to L1 if unknown
        if state == 1:
            l1_candidates.append(k)
        else:
            # State 2, or technically anything else in cache counts as L2 logic here
            l2_candidates.append(k)

    # ARC Logic for choosing victim
    # If len(L1) > p, we evict the LRU of L1. 
    # Otherwise, we evict the LRU of L2.
    # Note: We must ensure we don't try to pop from empty lists.
    
    # However, strict ARC condition is slightly more complex when L1 hit B1.
    # Simplified approach for this interface:
    # If L1 has "too much" space allocated compared to P, trim L1.
    
    if l1_candidates and len(l1_candidates) > arc_p:
        # Evict LRU from L1
        victim = min(l1_candidates, key=lambda k: m_key_access_time.get(k, 0))
    elif l2_candidates:
        # Evict LRU from L2
        victim = min(l2_candidates, key=lambda k: m_key_access_time.get(k, 0))
    elif l1_candidates:
         # Fallback if L2 empty
        victim = min(l1_candidates, key=lambda k: m_key_access_time.get(k, 0))
    else:
        # Should not happen given check at top
        victim = current_keys[0]
        
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it was in L1, it moves to L2 (it is now frequent).
    If it was in L2, it stays in L2 and gets updated recency (MRU position).
    '''
    global m_key_state, m_key_access_time
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # Update recency
    m_key_access_time[key] = current_time
    
    # Promote to L2 (Frequent)
    # State 1 (L1) -> State 2 (L2)
    # State 2 (L2) -> State 2 (L2)
    m_key_state[key] = 2

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    We determine if this is a totally new item or a "Ghost" hit.
    
    Ghost hits adjust the parameter `p`.
    '''
    global m_key_state, m_key_access_time, arc_p, arc_capacity
    
    current_time = cache_snapshot.access_count
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # Ensure params initialized
    get_params(capacity)
    
    prev_state = m_key_state.get(key, 0)
    
    # Update Access Time
    m_key_access_time[key] = current_time
    
    if prev_state == 3: 
        # Hit in B1 (Ghost Recent). 
        # This implies L1 was too small. Increase p.
        # Delta = 1 if |B1| >= |B2|, else |B2| / |B1|
        # We approximate relative sizes by total count in states if needed, 
        # but standard delta=1 usually suffices for simpler implementations.
        # Let's use a simpler adaptive step size:
        # We need to estimate sizes of B1 and B2. Since we don't have explicit lists,
        # we assume delta = 1 is safe or calculate simply.
        
        delta = 1
        # To be more precise with ARC: delta = 1 if len(B1) >= len(B2) else len(B2)/len(B1)
        # But tracking len(B1) requires iterating the dict which is slow.
        # We will use a fixed step to be safe and performant.
        if arc_p < capacity:
            arc_p += delta
            
        # Move to L2 because it was accessed again? 
        # Standard ARC: New item goes to L2 if it was in B1? 
        # No, in standard ARC: x in B1 -> move to T2 (L2).
        m_key_state[key] = 2
        
    elif prev_state == 4:
        # Hit in B2 (Ghost Frequent).
        # This implies L2 was too small (L1 too big). Decrease p.
        delta = 1
        # Precise ARC: delta = 1 if len(B2) >= len(B1) else len(B1)/len(B2)
        if arc_p > 0:
            arc_p -= delta
            
        # Move to L2 (restoring a frequent item)
        m_key_state[key] = 2
        
    else:
        # Totally new item (State 0)
        # It enters L1.
        m_key_state[key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    The object leaves cache.cache, but we move it to Ghost lists.
    L1 -> B1 (State 3)
    L2 -> B2 (State 4)
    '''
    global m_key_state, m_key_access_time
    
    key = evicted_obj.key
    state = m_key_state.get(key, 1)
    
    # If it was L1 (1), it becomes B1 (3)
    if state == 1:
        m_key_state[key] = 3
    # If it was L2 (2), it becomes B2 (4)
    elif state == 2:
        m_key_state[key] = 4
        
    # We DO NOT remove from m_key_access_time immediately?
    # Actually, ghost hits don't strictly need the timestamp, just the state existence.
    # However, to prevent memory leaks in the metadata dictionary, we should cleanup ancient ghosts.
    # A simple way to clean up is: if map grows too large (> 2 * capacity), remove oldest.
    
    # Optional: cleanup metadata to prevent memory explosion on infinite traces
    if len(m_key_state) > 3 * cache_snapshot.capacity:
        # Remove a random old key or scan (Scan is expensive).
        # For this constraint, we rely on the fact that ghost entries are useful.
        # We will perform a lazy cleanup only if way too big.
        # Simple heuristic: remove oldest from access_time
        # Since this is O(N), do it rarely.
        pass

    # Note on Recency in Ghost lists:
    # Standard ARC keeps ghosts in LRU order too to evict from ghosts.
    # Here we simplify by just keeping the state.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7kpzod6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5fhxx5cf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt6k4j6qn.pickle

Iteration 12: New subsample score 0.625642 is not better than old score 0.630938, skipping
Iteration 13: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgkahvmy6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk8ll7drv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppixsplo8.pickle

Iteration 13: Proposed new text for program: import math

# Metadata storage
# 1. m_access_history: key -> list of last N access timestamps (to calc frequency/recency)
# 2. m_insert_time: key -> timestamp (to detect very old stagnant items)
m_access_history = dict()
m_insert_time = dict()

def evict(cache_snapshot, obj):
    '''
    Size-Aware Hyperbolic Caching Policy.
    
    The goal is to maximize the "value density" of the cache.
    Value of an item ~ (Frequency) / (Size * Age)
    
    We evict the item with the lowest value score.
    
    Score Calculation:
    - Frequency: Count of accesses.
    - Recency/Age: (Current Time - Last Access Time).
    - Size: Object size in bytes.
    
    Formula:
       Victim = min(keys, key=Score)
       Score = (Frequency^1.5) / (Size * (Current_Time - Last_Access_Time + 1))
       
    Interpretation:
    - High Frequency -> Keep.
    - Small Size -> Keep.
    - Recently Used (Small Age) -> Keep (Dividing by small number yields large score).
    - Large Size or Old Access -> Evict (Dividing by large number yields small score).
    '''
    global m_access_history
    
    current_time = cache_snapshot.access_count
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to find the object with the MINIMUM score to evict.
    # Score represents "Utility". Low score = Low Utility.
    
    best_victim = None
    min_score = float('inf')
    
    # Optimization: To avoid iterating seemingly infinite lists in Python for every evict,
    # we can sample if the cache is huge, but for standard constraints, a full pass is often okay.
    # If performance is a constraint, we could sample 50 random keys.
    # Assuming strict miss-rate optimization is priority over CPU cycles here.
    
    # Let's inspect a subset if cache is very large to keep CPU sane, 
    # but the prompt prioritizes miss-rate, so we scan all.
    
    for k in current_keys:
        cached_obj = cache_snapshot.cache[k]
        
        # 1. Get History
        history = m_access_history.get(k, [])
        if not history:
            freq = 1
            last_access = m_insert_time.get(k, 0)
        else:
            freq = len(history)
            last_access = history[-1]
            
        # 2. Calculate Age (Recency)
        # Add 1 to avoid division by zero
        age = (current_time - last_access) + 1
        
        # 3. Size
        # Logarithmic size dampening: extremely large objects are bad, 
        # but linear penalty might be too harsh for medium objects.
        # However, for Trace 13 (Hit rate 0.0), size penalty usually needs to be strict.
        # We will use raw size but ensure it's at least 1.
        size = max(1, cached_obj.size)
        
        # 4. Calculate Utility Score
        # We want to keep items with High Freq, Small Size, Low Age.
        # So Score should be proportionate to Freq, inverse to Size and Age.
        # Weighting Freq higher (power of 2) helps distinguish popular items.
        score = (freq ** 1.5) / (size * age)
        
        if score < min_score:
            min_score = score
            best_victim = k
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    Update access history for frequency and recency calculation.
    '''
    global m_access_history
    
    if obj.key not in m_access_history:
        m_access_history[obj.key] = []
        
    # Append current time
    m_access_history[obj.key].append(cache_snapshot.access_count)
    
    # Limit history length to prevent memory bloat, we only care about recent frequency density
    if len(m_access_history[obj.key]) > 10:
        m_access_history[obj.key].pop(0)

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata for new object.
    '''
    global m_access_history, m_insert_time
    
    # Initialize history
    m_access_history[obj.key] = [cache_snapshot.access_count]
    m_insert_time[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    '''
    global m_access_history, m_insert_time
    
    key = evicted_obj.key
    if key in m_access_history:
        del m_access_history[key]
    if key in m_insert_time:
        del m_insert_time[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpli1_tozh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmwquy7or.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsyus12ge.pickle

Iteration 13: New subsample score 0.653113 is not better than old score 0.662978, skipping
Iteration 14: Selected program 3 score: 0.217409375
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3ujubdc6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpntj8vthe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7dfhqifa.pickle

Iteration 14: Proposed new text for program: import sys

# Global metadata storage
# Structure: key -> {'entry_time': int, 'freq': int}
m_metadata = dict()

def evict(cache_snapshot, obj):
    '''
    Modified Hyperbolic Caching Policy:
    Evict the object with the lowest utility score.
    
    Score = (Frequency) / ( (CurrentTime - EntryTime) * Size )
    
    This prioritizes:
    1. Frequent items (high numerator)
    2. Recent items (small denominator time-delta)
    3. Small items (small denominator size)
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # Iterate through all objects currently in the cache
    for key in cache_snapshot.cache:
        item_data = m_metadata.get(key)
        
        # Safety check if metadata is missing (should not happen in normal flow)
        if not item_data:
            victim_key = key
            break
            
        cached_obj = cache_snapshot.cache[key]
        
        # Calculate Duration: Time since insertion or last major reset
        # We add a small epsilon (1) to prevent division by zero if accessed in the same tick
        duration = (current_time - item_data['entry_time']) + 1
        
        # Frequency
        freq = item_data['freq']
        
        # Size
        size = cached_obj.size if cached_obj.size > 0 else 1
        
        # Calculate Hyperbolic Score
        # We use a slight exponent on freq (1.0 is standard, higher weights frequency more)
        # We penalize size linearly.
        score = freq / (duration * size)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Crucially, we do NOT reset 'entry_time'. 
       In Hyperbolic caching, retaining the original entry time allows the 
       algorithm to naturally decay the score of items that have high frequency 
       but haven't been accessed in a long time (the denominator grows).
    '''
    global m_metadata
    
    if obj.key in m_metadata:
        m_metadata[obj.key]['freq'] += 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set entry_time to current access_count.
    2. Set frequency to 1.
    '''
    global m_metadata
    
    m_metadata[obj.key] = {
        'entry_time': cache_snapshot.access_count,
        'freq': 1
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to prevent memory leaks.
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpne9werpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0j_2r3_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1lr4gczm.pickle

Iteration 14: New subsample score 0.362834 is not better than old score 0.362834, skipping
Iteration 15: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxgy6hb8e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3ca5mfn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6ophl090.pickle

Iteration 15: Proposed new text for program: import math

# =============================================================================
# Greedy Dual Size Frequency (GDSF) with Ghost Lists
# =============================================================================

# Global State Management
# We use specific global dictionaries to maintain the state required for GDSF.
# 1. Frequency: How often an item is accessed.
# 2. Priority: The computed score (L + Freq/Size).
# 3. Last Access: For LRU tie-breaking.
# 4. Ghosts: To remember the frequency of recently evicted items.
# 5. Aging Factor (L): The priority baseline.

g_freq = dict()
g_priority = dict()
g_last_access = dict()
g_ghosts = dict()
g_aging_L = 0.0

def get_gdsf_score(freq, size, aging_factor):
    '''
    Calculates the GDSF score.
    Priority = L + (Frequency / Size)
    
    This formula balances:
    - Recency (via the rising L factor)
    - Frequency (numerator)
    - Size (denominator, penalizing large objects to maximize object hit rate)
    '''
    # Protect against division by zero, though obj.size is positive int per spec.
    if size <= 0: size = 1
    return aging_factor + (freq / size)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Minimum Priority (GDSF) with LRU Tie-Breaker.
    '''
    global g_priority, g_last_access
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # We want to find the "worst" item.
    # Tuple comparison in Python works element-by-element:
    # 1. Primary: Priority (Low priority is evicted first).
    # 2. Secondary: Last Access (Older timestamp is evicted first).
    # This O(N) scan ensures we always make the mathematically optimal local decision.
    victim_key = min(current_keys, key=lambda k: (g_priority.get(k, 0.0), g_last_access.get(k, 0)))
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Bring the item's priority up to the current Aging Factor (L).
    3. Update Recency.
    '''
    global g_freq, g_priority, g_last_access, g_aging_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Recency
    g_last_access[key] = current_time
    
    # Update Frequency
    # We allow frequency to grow naturally.
    new_freq = g_freq.get(key, 0) + 1
    g_freq[key] = new_freq
    
    # Recalculate Priority
    # By adding the current g_aging_L, we "refresh" this item, protecting it
    # from the immediate rising tide of eviction thresholds.
    g_priority[key] = get_gdsf_score(new_freq, obj.size, g_aging_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if item is a "Ghost" (recently evicted).
       - If yes: Restore old frequency (Phoenix).
       - If no: Frequency = 1.
    2. Calculate initial Priority based on current Aging Factor (L).
    3. Cleanup Ghost list size.
    '''
    global g_freq, g_priority, g_last_access, g_ghosts, g_aging_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    g_last_access[key] = current_time
    
    # 1. Frequency Logic (Ghost Restoration)
    if key in g_ghosts:
        # It was here recently. Restore frequency + 1.
        # This handles looping patterns where items are evicted just before reuse.
        freq = g_ghosts[key] + 1
        del g_ghosts[key]
    else:
        # Brand new item.
        freq = 1
        
    g_freq[key] = freq
    
    # 2. Calculate Priority
    # New items enter with the current baseline L + their inherent value.
    g_priority[key] = get_gdsf_score(freq, obj.size, g_aging_L)
    
    # 3. Manage Ghost Size
    # We limit the ghost list size proportional to the cache size 
    # to prevent unbounded memory growth.
    target_ghost_len = max(10, len(cache_snapshot.cache))
    
    while len(g_ghosts) > target_ghost_len:
        # Python 3.7+ dicts maintain insertion order. 
        # The first key is the oldest inserted ghost.
        oldest_ghost = next(iter(g_ghosts))
        del g_ghosts[oldest_ghost]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update Global Aging Factor (L) to the victim's priority.
       This raises the bar for all future items.
    2. Move victim metadata to Ghost list.
    3. Clean up active metadata.
    '''
    global g_freq, g_priority, g_last_access, g_ghosts, g_aging_L
    
    key = evicted_obj.key
    
    # 1. Update Aging Factor (L)
    # The cache "ages" by setting the new baseline L to the score of the 
    # item that was just forced out. We take max to ensure L is monotonic.
    victim_priority = g_priority.get(key, 0.0)
    if victim_priority > g_aging_L:
        g_aging_L = victim_priority
        
    # 2. Add to Ghosts
    # Store frequency so we know if this was a heavy hitter if it returns.
    if key in g_freq:
        g_ghosts[key] = g_freq[key]
        
    # 3. Clean active state
    if key in g_freq: del g_freq[key]
    if key in g_priority: del g_priority[key]
    if key in g_last_access: del g_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvtxlhgb3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7rkcx_f8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu5i0nx0x.pickle

Iteration 15: New subsample score 0.32775299999999996 is not better than old score 0.382119, skipping
Iteration 16: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxcttd3f6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_xef8weq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuba8umjg.pickle

Iteration 16: Proposed new text for program: import collections

# Parameters
# We dedicate a portion of the cache size (conceptually) to the "small" probationary queue.
# A common ratio is 10% for the small queue, 90% for the main queue.
# Since we don't control the storage layout directly (only the eviction logic), 
# we will simulate this by tracking which keys belong to "small" vs "main".
SMALL_QUEUE_RATIO = 0.1

# Metadata
# key -> frequency_bits (0, 1, 2, capped at a small number usually is enough, but we use int)
# 0 means inserted but not hit again. >0 means hit.
m_freq_bits = {}

# We maintain ordering using two Python lists/deques to simulate the queues.
# Deque is O(1) for popleft/append.
q_small = collections.deque() # Probationary items
q_main = collections.deque()  # Protected items

# Set for fast O(1) membership lookup
s_small = set()
s_main = set()

# To handle strict cache consistency with the provided snapshot:
# Because 'evict' is called when cache is full, and we might have drift between 
# our internal queues and the actual cache_snapshot.keys() (due to external deletions or 
# restarts in some simulation environments), we perform lazy cleanup or validation.

def evict(cache_snapshot, obj):
    '''
    S3-FIFO / 2Q Eviction Policy.
    
    1. If the Small Queue is larger than 10% of the total cache count, 
       we evict from the Small Queue (FIFO) to clear out one-hit wonders.
       However, if the item at the head of Small Queue has been hit (freq > 0), 
       it is PROMOTED to Main Queue instead of evicted.
       
    2. If Small Queue is small enough, we evict from Main Queue.
       We use a Second-Chance (Clock) mechanism here: 
       If the item at the head has been hit recently (freq > 0), we decrement 
       freq and move it to the back. We repeat until we find a victim with freq=0.
    '''
    global m_freq_bits, q_small, q_main, s_small, s_main
    
    # Sync Step: Ensure our queues only contain items actually in the cache.
    # In a real system, we'd handle deletes. Here, we assume standard flow.
    # However, if the cache is empty, return None.
    if not cache_snapshot.cache:
        return None

    # Threshold for small queue size
    target_small_size = max(1, int(len(cache_snapshot.cache) * SMALL_QUEUE_RATIO))
    
    victim = None

    # --- Phase 1: Try to evict from Small Queue if it's too big ---
    while len(s_small) > target_small_size or (len(s_main) == 0 and len(s_small) > 0):
        if not q_small:
            break
            
        candidate = q_small[0] # Peek
        
        # Validation: If candidate not in cache (drift), just remove and continue
        if candidate not in cache_snapshot.cache:
            q_small.popleft()
            if candidate in s_small: s_small.remove(candidate)
            continue
            
        freq = m_freq_bits.get(candidate, 0)
        
        if freq > 0:
            # PROMOTION: It was hit while in probationary queue.
            # Move to Main Queue.
            q_small.popleft()
            s_small.remove(candidate)
            
            q_main.append(candidate)
            s_main.add(candidate)
            # Reset frequency bits for the main queue logic (optional, but resetting to 0 makes it vulnerable 
            # only after one full cycle, effectively giving it one free pass). 
            # S3-FIFO usually resets to 0 here to treat it as a new main insertion.
            m_freq_bits[candidate] = 0 
        else:
            # EVICTION: It wasn't hit. It's a one-hit wonder. Bye.
            victim = candidate
            return victim

    # --- Phase 2: Evict from Main Queue (Clock/Second Chance) ---
    # If we fall through here, Small Queue is within limits or empty.
    # We must find a victim in Main Queue or force from Small if Main is empty.
    
    while True:
        # Fallback: if main is empty, pop from small (should happen rarely due to logic above)
        if not q_main:
            if q_small:
                c = q_small.popleft()
                if c in s_small: s_small.remove(c)
                return c
            else:
                # Should not happen if cache is not empty
                # Fallback to arbitrary key from snapshot
                return next(iter(cache_snapshot.cache))
        
        candidate = q_main[0] # Peek
        
        # Validation
        if candidate not in cache_snapshot.cache:
            q_main.popleft()
            if candidate in s_main: s_main.remove(candidate)
            continue
            
        freq = m_freq_bits.get(candidate, 0)
        
        if freq > 0:
            # SECOND CHANCE: It was accessed recently. 
            # Give it another life at the back of the queue, decrement frequency.
            q_main.popleft()
            q_main.append(candidate) # Reinsert at tail
            m_freq_bits[candidate] = freq - 1 # Decay usage
        else:
            # EVICTION: No recent accesses.
            victim = candidate
            return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is tracked, increment its frequency bit.
    Capping at 3 (2 bits) is usually sufficient to distinguish "hot" from "warm".
    '''
    global m_freq_bits
    
    # Simple increment, capped at 3
    curr = m_freq_bits.get(obj.key, 0)
    if curr < 3:
        m_freq_bits[obj.key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    New items enter the Small Queue (Probationary).
    Frequency is initialized to 0.
    '''
    global q_small, s_small, m_freq_bits
    
    # If somehow already exists (shouldn't on insert, but for safety)
    if obj.key in s_small or obj.key in s_main:
        update_after_hit(cache_snapshot, obj)
        return

    q_small.append(obj.key)
    s_small.add(obj.key)
    m_freq_bits[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up metadata.
    '''
    global m_freq_bits, s_small, s_main, q_small, q_main
    
    key = evicted_obj.key
    
    # Remove from frequency map
    if key in m_freq_bits:
        del m_freq_bits[key]
        
    # Remove from sets
    if key in s_small:
        s_small.remove(key)
        # Note: We don't remove from deque here because it's O(N).
        # We handle "dead" items in the deque lazily inside evict().
        
    if key in s_main:
        s_main.remove(key)
        # Same lazy deletion logic applies.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxki486o0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj4fb3bz6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp15jmvodc.pickle

Iteration 16: New subsample score 1.168299 is not better than old score 1.301999, skipping
Iteration 17: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgvbkqxgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaziicxe6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpglr3sqwh.pickle

Iteration 17: Proposed new text for program: import math
import random

# =============================================================================
# W-TinyLFU Inspired Cache Implementation
# =============================================================================
#
# Concepts:
# 1. Window Cache: A small LRU admission window (keep bursty/new items).
# 2. Main Cache (SLRU): Split into Probation and Protected segments.
# 3. Frequency Sketch: Keeps track of approx frequency of items (even outside cache).
#
# Flow:
# - New Insert -> Goes to Window Cache.
# - If Window Full -> Evict LRU from Window.
# - Window Victim vs Main Cache Victim? 
#   - We compare the Window Victim's frequency against the Probation Victim's frequency.
#   - If Window Victim is better, it enters Probation. Probation Victim is evicted.
#   - Else, Window Victim is dropped (rejected).
# =============================================================================

# -- Tunable Parameters --
WINDOW_CACHE_SIZE_RATIO = 0.01  # 1% of capacity for the admission window
PROBATION_SIZE_RATIO = 0.20     # 20% of the MAIN cache is probation
PROTECTED_SIZE_RATIO = 0.80     # 80% of the MAIN cache is protected

# -- State Variables --
# We simulate the Segmented LRU logical queues using sets for O(1) membership checks
# and a global LRU tracker for ordering.
# Note: The 'cache_snapshot' provides the physical storage. We just track logical positions.

# Sets to track which logical segment a key belongs to
s_window = set()      # Keys in the admission window
s_probation = set()   # Keys in the probation segment of main cache
s_protected = set()   # Keys in the protected segment of main cache

# Frequency Sketch (Simulated with dict for accuracy in this context)
# key -> frequency count
m_frequency = dict()

# Aging parameters for the frequency sketch
m_sketch_count = 0
RESET_SAMPLE_SIZE = 10000 # Period to halve counters to simulate sliding window

def _get_victim_from_segment(cache_snapshot, segment_set):
    '''
    Finds the LRU item within a specific logical segment (Window, Probation, or Protected).
    Since we don't have a linked list, we scan the snapshot to find the 
    oldest timestamp among keys in that set.
    '''
    # Current keys in the physical cache
    phys_keys = cache_snapshot.cache.keys()
    
    # Intersection of physical cache and logical segment 
    # (Handling potential sync issues if eviction/insert logic diverged slightly)
    candidates = [k for k in segment_set if k in phys_keys]
    
    if not candidates:
        return None

    # We need the access times. However, the provided context implies we don't 
    # have direct access to a "last_access_time" map in the snapshot object itself 
    # other than what we maintain.
    # We must rely on our own metadata if we tracked it, but we can assume 
    # standard LRU behavior requires tracking access time.
    # Let's use m_last_access_time which we will maintain.
    return min(candidates, key=lambda k: m_last_access_time.get(k, 0))

# Metadata for LRU tracking
m_last_access_time = dict()

def _admit(key):
    '''
    Logic to admit a key from Window eviction candidate into Main Cache (Probation).
    Returns True if admitted, False if rejected.
    '''
    # Candidate from Window
    candidate_freq = m_frequency.get(key, 0)
    
    # Victim from Probation (LRU of Probation)
    # We need to find the probation victim to compare against
    # This is a "hypothetical" victim selection for comparison
    # We can't access `cache_snapshot` here easily without passing it, 
    # but strictly speaking, admission logic is usually:
    # "Is candidate freq > victim freq?"
    
    # Since we can't easily scan probation here without snapshot, 
    # we defer the specific victim selection to the evict function logic.
    pass

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update global LRU time.
    2. Increment frequency sketch.
    3. Promote within SLRU (Probation -> Protected).
    '''
    global m_last_access_time, m_frequency, m_sketch_count
    global s_window, s_probation, s_protected
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_last_access_time[key] = current_time
    
    # 1. Update Frequency Sketch
    m_frequency[key] = m_frequency.get(key, 0) + 1
    m_sketch_count += 1
    
    # Aging the sketch to keep it fresh (Window-TinyLFU reset)
    if m_sketch_count >= RESET_SAMPLE_SIZE:
        for k in list(m_frequency.keys()):
            m_frequency[k] //= 2
            if m_frequency[k] == 0:
                del m_frequency[k]
        m_sketch_count = 0
        
    # 2. SLRU Promotion Logic
    # If hit in Window: It stays in Window (moves to MRU of window implicitly via time update).
    # If hit in Protected: Stays in Protected (MRU).
    # If hit in Probation: Promote to Protected.
    
    if key in s_probation:
        s_probation.remove(key)
        s_protected.add(key)
        
        # If Protected is full, move LRU of Protected to Probation (Demotion)
        # Capacity calculation:
        total_capacity = cache_snapshot.capacity # This is bytes, assuming count for logic simplification or 1 obj = 1 unit?
        # The prompt says capacity is size in bytes, but often algo challenges simplify to count. 
        # Assuming size based management is complex without maintaining sums, 
        # but let's stick to logical counts proportional to current cache size.
        
        # We can approximate capacity by count:
        current_count = len(cache_snapshot.cache)
        protected_capacity = int(current_count * PROTECTED_SIZE_RATIO) or 1
        
        if len(s_protected) > protected_capacity:
            # Demote LRU of Protected -> MRU of Probation
            # Find LRU of Protected
            prot_victim = _get_victim_from_segment(cache_snapshot, s_protected)
            if prot_victim:
                s_protected.remove(prot_victim)
                s_probation.add(prot_victim)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (New Item):
    1. Always insert into Window Cache first.
    2. Update LRU time and Frequency.
    '''
    global m_last_access_time, m_frequency, m_sketch_count
    global s_window
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_last_access_time[key] = current_time
    
    # Frequency update
    m_frequency[key] = m_frequency.get(key, 0) + 1
    m_sketch_count += 1
    
    # Always starts in Window
    s_window.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up logical sets and metadata.
    Frequency data is RETAINED (Ghost).
    '''
    global m_last_access_time
    global s_window, s_probation, s_protected
    
    e_key = evicted_obj.key
    
    # Remove from tracking sets
    if e_key in s_window: s_window.remove(e_key)
    if e_key in s_probation: s_probation.remove(e_key)
    if e_key in s_protected: s_protected.remove(e_key)
    
    # Remove Recency, Keep Frequency
    if e_key in m_last_access_time:
        del m_last_access_time[e_key]

def evict(cache_snapshot, obj):
    '''
    Eviction Logic (W-TinyLFU style):
    1. Determine if Window Cache is overflowing.
       Target Window Size = 1% of total count.
    
    Case A: Window is within capacity.
       - If total cache full, we usually evict from Main Cache (Probation LRU).
       
    Case B: Window is overflowing (New items pushing old window items out).
       - Identify LRU of Window (the candidate for eviction).
       - Identify LRU of Probation (the victim from Main Cache).
       - Compare Frequency(Window LRU) vs Frequency(Probation LRU).
       - If Window LRU has higher freq, it displaces Probation LRU. Evict Probation LRU.
         (Window LRU moves to Probation).
       - Else, Window LRU is not good enough. Evict Window LRU.
    '''
    global s_window, s_probation, s_protected, m_frequency
    
    current_keys = list(cache_snapshot.cache.keys())
    total_count = len(current_keys)
    if total_count == 0: return None

    # Calculate Target Sizes
    # We use counts to approximate proportions. 
    # For byte-sized caches, this is an approximation, but efficient.
    window_target = max(1, int(total_count * WINDOW_CACHE_SIZE_RATIO))
    
    # 1. Identify Candidates
    window_victim = _get_victim_from_segment(cache_snapshot, s_window)
    probation_victim = _get_victim_from_segment(cache_snapshot, s_probation)
    
    # Fallback if sets are desynchronized or empty
    if not window_victim and not probation_victim:
        # Fallback to pure LRU if internal accounting fails
        return min(current_keys, key=lambda k: m_last_access_time.get(k, 0))
    
    # 2. Decision Logic
    
    # If Window is growing too large (>1%), we MUST evict from somewhere to make space for flow.
    # We prefer to evict the Window LRU, UNLESS it is more popular than the Probation LRU.
    
    # If we have a window victim (Window not empty)
    if window_victim:
        # If Window is specifically overflowing relative to our design target
        if len(s_window) > window_target:
            
            # If we also have a probation victim, we compare them
            if probation_victim:
                freq_w = m_frequency.get(window_victim, 0)
                freq_p = m_frequency.get(probation_victim, 0)
                
                # Biased towards Main Cache: Window item must be strictly better
                if freq_w > freq_p:
                    # Admit Window Item to Main Cache (Probation)
                    # We evict the Probation Victim physically
                    # Logical move happens here to ensure state consistency before physical eviction
                    s_window.remove(window_victim)
                    s_probation.add(window_victim)
                    return probation_victim
                else:
                    # Reject Window Item
                    return window_victim
            else:
                # No probation victim (Main cache empty?), evict window
                return window_victim
        else:
            # Window is not overflowing, but Cache is Full (physically).
            # Usually implies Main Cache is full.
            # Standard policy: Evict from Probation LRU first.
            if probation_victim:
                return probation_victim
            # If Probation empty, try Protected LRU (should rarely happen if sizes tuned)
            protected_victim = _get_victim_from_segment(cache_snapshot, s_protected)
            if protected_victim:
                return protected_victim
            # If Main is empty but Cache Full, evict from Window
            return window_victim
            
    # Fallback
    if probation_victim: return probation_victim
    
    # Ultimate fallback
    return current_keys[0]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0bgkrlr_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprspl3jc3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_2s5gbm5.pickle

Iteration 17: New subsample score 0.46309100000000003 is not better than old score 0.518354, skipping
Iteration 18: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1x96ynva.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcsejvkvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphh53rbee.pickle

Iteration 18: Proposed new text for program: import random

# Metadata storage
# 1. key -> frequency (Approximate counting for long-term history)
# 2. key -> last_access_time (Recency for LRU tie-breaking and specific logic)
# 3. key -> insertion_time (To detect very new items)
m_frequency = dict()
m_last_access = dict()
m_insertion_time = dict()

# Counters and Configuration
m_total_accesses = 0
m_reset_counter = 0

# Configuration for frequency sketch simulation
# We want to track history slightly larger than cache capacity to identify "ghosts"
RESET_INTERVAL_FACTOR = 10  # Reset frequencies after N * Cache_Size accesses
MAX_FREQ_TRACKED_KEYS = 100000 # Cap the size of metadata to prevent OOM in simulation

def _update_frequency(key, amount=1):
    '''
    Update frequency with a "Freshness" mechanism.
    If the map gets too big, we prune it.
    '''
    global m_frequency, m_reset_counter
    
    current_freq = m_frequency.get(key, 0)
    m_frequency[key] = current_freq + amount
    m_reset_counter += 1
    
    # Dynamic Aging / Reset mechanism (Pseudo-CountMin Sketch reset)
    # This prevents historical heavy hitters from staying high forever if they stop being accessed.
    if m_reset_counter > MAX_FREQ_TRACKED_KEYS:
        # Halve all counters
        keys_to_delete = []
        for k in m_frequency:
            m_frequency[k] //= 2
            if m_frequency[k] == 0:
                keys_to_delete.append(k)
        
        for k in keys_to_delete:
            del m_frequency[k]
        
        m_reset_counter = 0

def evict(cache_snapshot, obj):
    '''
    Improved Eviction Policy:
    We approximate a TinyLFU admission policy using eviction.
    
    Candidates for eviction are chosen based on a score that weights Frequency heavily,
    but uses Recency to break ties or protect very new items.
    
    Algorithm:
    1. Sample a subset of keys (or all if cache is small) to reduce CPU overhead.
    2. Calculate a 'victim score' for each.
       Score = Frequency + (Recency_Bonus if recently used)
    3. Evict the item with the lowest Score.
       Note: If Frequencies are equal (e.g., all 1s), this naturally devolves to LRU.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
        
    # Optimization: If cache is huge, scanning all keys is slow. 
    # Sampling provides 99% accuracy of finding the global min with much less cost.
    # However, for maximum Hit Rate in this challenge, we scan all if < 5000 items,
    # otherwise we sample.
    candidates = current_keys
    if len(current_keys) > 5000:
        candidates = random.sample(current_keys, 100)

    current_time = cache_snapshot.access_count
    
    best_victim = None
    min_score = float('inf')
    min_recency = float('inf')

    for k in candidates:
        freq = m_frequency.get(k, 0)
        last_acc = m_last_access.get(k, 0)
        
        # SLRU-like protection:
        # If an item was inserted very recently (in the last 1% of accesses), 
        # give it a slight artificial boost to prevent immediate eviction 
        # before it has a chance to prove its worth.
        # But generally, we trust Frequency (LFU) primarily.
        
        # Primary Metric: Frequency.
        # Secondary Metric: Recency (Oldest timestamp is smallest).
        
        # We want to evict the item with Smallest Frequency.
        # If frequencies match, evict the one with Smallest Last Access (LRU).
        
        # Comparisons:
        if freq < min_score:
            min_score = freq
            min_recency = last_acc
            best_victim = k
        elif freq == min_score:
            # Tie-breaker: LRU
            if last_acc < min_recency:
                min_recency = last_acc
                best_victim = k
                
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Global Frequency (indicates popularity).
    2. Update Recency (indicates active usage).
    '''
    global m_last_access, m_total_accesses
    
    current_time = cache_snapshot.access_count
    m_total_accesses = current_time
    
    # Update Access Time
    m_last_access[obj.key] = current_time
    
    # Increment Frequency
    _update_frequency(obj.key, 1)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Update Frequency.
       Note: We retain frequency history even if the item wasn't in the cache previously
       (if it was in our metadata history), allowing items to "regain" status quickly.
    '''
    global m_last_access, m_insertion_time, m_total_accesses
    
    current_time = cache_snapshot.access_count
    m_total_accesses = current_time
    
    m_last_access[obj.key] = current_time
    m_insertion_time[obj.key] = current_time
    
    # If the key exists in our "Ghost" frequency map, it keeps its count.
    # Otherwise, it starts at 1.
    if obj.key not in m_frequency:
        _update_frequency(obj.key, 1)
    else:
        # It's a re-insertion (ghost hit).
        # We increment it to acknowledge it's back.
        _update_frequency(obj.key, 1)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from Recency map (it's no longer in cache).
    2. DO NOT remove from Frequency map yet.
       We keep the frequency count as "Ghost" data. If this item comes back soon,
       it will have a high frequency count and stick in the cache better.
    '''
    global m_last_access, m_insertion_time
    
    key = evicted_obj.key
    
    if key in m_last_access:
        del m_last_access[key]
    if key in m_insertion_time:
        del m_insertion_time[key]
        
    # We do NOT delete from m_frequency here. 
    # It will be cleaned up by the periodic aging/pruning in _update_frequency.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3fcvc780.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptikdds6_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgbik81kf.pickle

Iteration 18: New subsample score 0.9407220000000001 is not better than old score 0.944947, skipping
Iteration 19: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmkvttd86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuyhzycgt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_v0kpxxr.pickle

Iteration 19: Proposed new text for program: import math

# Metadata storage:
# key -> [frequency, priority, last_access_time]
m_metadata = dict()

# History storage (Ghosts):
# key -> frequency
# Used to restore frequency for items that were recently evicted but return.
m_history = dict()

# Global "Aging" Factor (L)
# This increases monotonically as items are evicted, invalidating old entries over time.
g_inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF Policy:
    Evict the object with the lowest Priority score (H).
    
    H = L + (Frequency / Size)
    
    Tie-breaking:
    If priorities are equal, evict the Least Recently Used (LRU) among them.
    '''
    global g_inflation_L, m_metadata
    
    if not m_metadata:
        return None

    # We want to find the item with the minimum Priority.
    # Metadata structure: value = [freq, priority, last_access]
    # We compare based on:
    # 1. Priority (index 1) - Primary GDSF metric
    # 2. Last Access (index 2) - LRU Tie-breaker
    
    victim_key = min(m_metadata, key=lambda k: (m_metadata[k][1], m_metadata[k][2]))
    
    # Update the global inflation factor L to the priority of the victim.
    # This sets the new "baseline" for object worth in the cache.
    g_inflation_L = m_metadata[victim_key][1]
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Priority using the current Inflation Factor (L).
    3. Update Recency.
    '''
    global m_metadata, g_inflation_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in m_metadata:
        # Increment frequency
        m_metadata[key][0] += 1
        freq = m_metadata[key][0]
        
        # Recalculate Priority: H = L + (Freq / Size)
        # Note: We use the CURRENT g_inflation_L. This effectively "resets" the 
        # object's aging process because it was just accessed.
        size = obj.size if obj.size > 0 else 1
        priority = g_inflation_L + (freq / size)
        
        # Update metadata
        m_metadata[key][1] = priority
        m_metadata[key][2] = current_time

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine Frequency (check History).
    2. Calculate Priority.
    3. Store Metadata.
    4. Clean up History if it grows too large.
    '''
    global m_metadata, m_history, g_inflation_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Frequency determination
    if key in m_history:
        # It's a returning item. Restore its previous popularity + 1.
        freq = m_history[key] + 1
        # Remove from history since it's now in the cache
        del m_history[key]
    else:
        # Brand new item
        freq = 1
        
    # Calculate Priority: H = L + (Freq / Size)
    size = obj.size if obj.size > 0 else 1
    priority = g_inflation_L + (freq / size)
    
    # Store: [Frequency, Priority, Last_Access]
    m_metadata[key] = [freq, priority, current_time]
    
    # Heuristic History Cleanup
    # Keep history size roughly proportional to cache size to prevent memory leaks in long traces.
    # Python 3.7+ dicts preserve insertion order, so we can pop the oldest ghost easily.
    target_history_size = max(100, len(cache_snapshot.cache) * 2)
    while len(m_history) > target_history_size:
        # Remove the first key in the iterator (oldest inserted)
        oldest_ghost_key = next(iter(m_history))
        del m_history[oldest_ghost_key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Persist the victim's frequency to History (for potential future restoration).
    2. Remove active metadata.
    '''
    global m_metadata, m_history
    
    key = evicted_obj.key
    
    if key in m_metadata:
        # Save frequency to history
        m_history[key] = m_metadata[key][0]
        # Remove from active metadata
        del m_metadata[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfgrnw1y7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwgipxret.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1fq_h0sr.pickle

Iteration 19: New subsample score 0.751074 is not better than old score 0.9266280000000001, skipping
Iteration 20: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppn6d7xlj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpab2z1i8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmo2h8m3w.pickle

Iteration 20: Proposed new text for program: import sys

# Metadata storage
# 1. key -> frequency (Number of hits)
m_key_frequency = dict()
# 2. Global cache age (The 'inflation' factor for LFU-DA)
m_cache_age = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU with Dynamic Aging (LFU-DA).
    
    We select the victim with the lowest "key value" (score).
    Score = Frequency of Key + (Age of Cache when inserted/updated)
    
    However, for simplicity in storage, we often store the absolute frequency count
    and manipulate the comparison logic. In this standard LFU-DA implementation:
    
    We simply look for the object with the smallest `m_key_frequency` value currently in the cache.
    Crucially, when we evict, we update the global `m_cache_age` to the frequency of the evicted item.
    '''
    global m_cache_age
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # Find the key with the minimum frequency score.
    # Tie-breaking: If frequencies are equal, Python's min uses the first one found,
    # effectively acting as a partial FIFO/Random tie-breaker which is usually sufficient.
    # A deeper tie-breaker (like recency) could be added, but LFU-DA is robust enough without it.
    victim_key = min(current_keys, key=lambda k: m_key_frequency.get(k, 0.0))
    
    # Update the global cache age. This is the core of Dynamic Aging.
    # The new "minimum bar" for entry becomes the score of the item we just kicked out.
    m_cache_age = m_key_frequency.get(victim_key, 0.0)
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment the frequency of the object.
    2. Reset its 'base' value relative to the current cache age is technically handled
       by just adding to the existing frequency in this specific variant of LFU-DA implementation.
       
    Standard LFU-DA Logic: New Score = Old Score + Weight
    '''
    global m_key_frequency
    
    # We add 1.0 to the frequency.
    # Note: In strict LFU-DA, hit updates usually just increment frequency.
    # The aging happens because new items start higher, making old items relatively smaller.
    current_freq = m_key_frequency.get(obj.key, 0.0)
    m_key_frequency[obj.key] = current_freq + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Assign the object a frequency equal to the global `m_cache_age` + 1.
    
    This is the "Aging" mechanism. New items don't start at 0 or 1; they start at the 
    current "eviction pressure" level (`m_cache_age`) plus an initial count.
    This prevents new items from being immediately evicted if the cache is hot,
    but also ensures they don't overtake established popular items instantly.
    '''
    global m_key_frequency, m_cache_age
    
    # If the object is entering the cache, it adopts the current age + 1
    m_key_frequency[obj.key] = m_cache_age + 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up metadata.
    
    In this implementation, we remove the frequency data for the evicted object.
    Unlike frequency-persistence algorithms (like LFU w/ Ghost Cache), LFU-DA 
    relies on the `m_cache_age` variable to carry historical load information, 
    so we can safely delete the key to save memory.
    '''
    global m_key_frequency
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprkgum521.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_zhxkto2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxn20cf8e.pickle

Iteration 20: New subsample score 1.231438 is not better than old score 1.262189, skipping
Iteration 21: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbc0i94sc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc7et4zjf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4kaxzjbx.pickle

Iteration 21: Proposed new text for program: # S3-FIFO Implementation
# ----------------------
# Uses three FIFO queues (simulated via Python dicts which preserve insertion order):
# 1. q_small: Entry point for new items.
# 2. q_main: Storage for frequently accessed items.
# 3. m_ghosts: History of evicted items to detect "returning" keys.

# Global State
m_freq = dict()     # Key -> Frequency (0, 1, 2...)
q_small = dict()    # Key -> True (Ordered Dict for Small Queue)
q_main = dict()     # Key -> True (Ordered Dict for Main Queue)
m_ghosts = dict()   # Key -> True (Ordered Dict for Ghosts)

# Constants
SMALL_Q_RATIO = 0.1 # 10% of cache dedicated to probation (Small Q)
MAX_FREQ_CAP = 3    # Cap frequency to avoid integer overflow, though 1-2 is usually enough

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Policy.
    Iterates through queues to find a victim with 0 frequency.
    Promotes utilized items from Small -> Main or Reinserts Main -> Main.
    '''
    global m_freq, q_small, q_main

    # Fallback if empty (should not happen in normal operation)
    if not q_small and not q_main:
        return next(iter(cache_snapshot.cache))

    # Calculate Target Size for Small Queue
    cache_count = len(cache_snapshot.cache)
    target_small_size = max(1, int(cache_count * SMALL_Q_RATIO))

    # Eviction Loop: Find the first item with freq=0
    # Guaranteed to terminate because we reset frequency to 0 upon reinsertion.
    while True:
        # 1. Decide which queue to check
        # We check Small queue if it's over its budget, or if Main is empty.
        if len(q_small) > target_small_size or not q_main:
            # Pick candidate from Small
            if not q_small:
                # Should typically not happen if logic is sound, but handle Main fallback
                candidate_key = next(iter(q_main))
                src = 'main'
            else:
                candidate_key = next(iter(q_small))
                src = 'small'
        else:
            # Pick candidate from Main
            candidate_key = next(iter(q_main))
            src = 'main'

        # 2. Check Frequency (Hit status)
        freq = m_freq.get(candidate_key, 0)

        if freq > 0:
            # -- SECOND CHANCE --
            # The item was used. Don't evict. Move/Reinsert it.
            
            if src == 'small':
                # Promotion: Small -> Main
                # Remove from current position (Head)
                del q_small[candidate_key]
                # Insert at Tail of Main
                q_main[candidate_key] = True
                # Reset frequency. It needs to prove itself again in Main.
                m_freq[candidate_key] = 0 
            else:
                # Reinsertion: Main -> Main
                # Remove from Head
                del q_main[candidate_key]
                # Insert at Tail
                q_main[candidate_key] = True
                # Reset frequency.
                m_freq[candidate_key] = 0
                
            # Loop continues to find next candidate...
        else:
            # -- VICTIM FOUND --
            # Item was not used since insertion/last check.
            return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Simply increment frequency.
    We do NOT move items between queues here. Moves happen lazily during eviction.
    This reduces overhead on hits.
    '''
    global m_freq
    k = obj.key
    # Increment frequency, capped at MAX_FREQ_CAP
    curr = m_freq.get(k, 0)
    if curr < MAX_FREQ_CAP:
        m_freq[k] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: 
    1. Initialize Frequency to 0.
    2. Place in Main Queue if it's a "Ghost" (returning item).
    3. Otherwise, place in Small Queue (Probation).
    '''
    global m_freq, q_small, q_main, m_ghosts
    
    k = obj.key
    m_freq[k] = 0
    
    if k in m_ghosts:
        # Phoenix: Rising from the dead. Promote to Main immediately.
        q_main[k] = True
        del m_ghosts[k]
    else:
        # Newbie: Insert into Small Queue.
        q_small[k] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction Cleanup:
    1. Remove victim from internal queues.
    2. Add victim to Ghost list.
    3. Prune Ghost list if it gets too big.
    '''
    global m_freq, q_small, q_main, m_ghosts
    
    k = evicted_obj.key
    
    # Clean up internal structures
    if k in q_small:
        del q_small[k]
    if k in q_main:
        del q_main[k]
    if k in m_freq:
        del m_freq[k]
        
    # Add to Ghosts
    m_ghosts[k] = True
    
    # Manage Ghost Size
    # We keep ghosts roughly proportional to the number of items in cache.
    target_ghost_size = len(cache_snapshot.cache)
    
    while len(m_ghosts) > target_ghost_size:
        # Remove oldest inserted ghost (FIFO behavior of dict)
        oldest_ghost = next(iter(m_ghosts))
        del m_ghosts[oldest_ghost]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp40hh5z4i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9ald0zs2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaw2qoc5m.pickle

Iteration 21: New subsample score 1.676329 is not better than old score 1.7051280000000002, skipping
Iteration 22: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpavqe310n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpus4p0nlz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptv8la8_2.pickle

Iteration 22: Proposed new text for program: import math

# Metadata storage
# GDSF requires tracking Frequency and a calculated Priority for every cached item.
# 1. key -> frequency (int)
m_freq = dict()
# 2. key -> priority (float) - The GDSF score
m_priority = dict()

# Ghost list: Stores frequency of recently evicted items to restore 'heat' if they return.
# key -> frequency (int)
m_ghosts = dict()

# Global state for GDSF
# 'L' acts as an aging factor (or inflation parameter) to handle Recency.
current_L = 0.0

# Constants
# Ghost ratio: How many ghosts to keep relative to the number of objects currently in cache.
GHOST_RATIO = 2.0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GDSF (Greedy Dual Size Frequency).
    
    We calculate a priority for every object:
        Priority = L + (Frequency / Size)
        
    We evict the object with the MINIMUM priority.
    
    Mechanism:
    - Large Size -> Lower Priority (Higher chance of eviction)
    - Low Frequency -> Lower Priority
    - Old items (inserted when L was small) -> Lower Priority compared to new L
    '''
    global current_L
    
    if not cache_snapshot.cache:
        return None
    
    # We need to find the key with the minimum priority.
    # We scan the current cache keys.
    candidate = None
    min_priority = float('inf')
    
    for key in cache_snapshot.cache:
        # Retrieve priority (default to 0.0 only if state is desynced, which shouldn't happen)
        p = m_priority.get(key, 0.0)
        
        if p < min_priority:
            min_priority = p
            candidate = key
            
    # GDSF Key Mechanism: Update L
    # The system "ages" by raising the floor (L) to the priority of the evicted item.
    if candidate is not None:
        current_L = min_priority
        
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority.
       New Priority = Current_L + (Frequency / Size).
       
    By adding Current_L, we bring this object to the "top" of the recency stack
    (resetting its age relative to the current floor).
    '''
    global current_L, m_freq, m_priority
    
    # Increment Frequency
    old_freq = m_freq.get(obj.key, 0)
    new_freq = old_freq + 1
    m_freq[obj.key] = new_freq
    
    # Update Priority
    # Cost is 1 for Object Hit Rate maximization.
    size = obj.size if obj.size > 0 else 1
    priority = current_L + (new_freq / size)
    m_priority[obj.key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghosts. If present, restore old frequency (Frequency Memory).
    2. Set Priority using Current_L.
    3. Trim Ghost list.
    '''
    global current_L, m_freq, m_priority, m_ghosts
    
    # 1. Frequency Determination
    if obj.key in m_ghosts:
        # "Phoenix" item: It was evicted recently.
        # Restore its importance + 1 for the current access.
        freq = m_ghosts[obj.key] + 1
        del m_ghosts[obj.key]
    else:
        # Brand new item
        freq = 1
        
    m_freq[obj.key] = freq
    
    # 2. Priority Calculation
    size = obj.size if obj.size > 0 else 1
    priority = current_L + (freq / size)
    m_priority[obj.key] = priority
    
    # 3. Ghost Management
    # Maintain a history proportional to cache object count
    # This ensures we don't store infinite history for cold items.
    target_ghost_size = max(50, int(len(cache_snapshot.cache) * GHOST_RATIO))
    
    while len(m_ghosts) > target_ghost_size:
        # Remove oldest inserted ghost (Python dict preserves insertion order)
        it = iter(m_ghosts)
        try:
            oldest_ghost = next(it)
            del m_ghosts[oldest_ghost]
        except StopIteration:
            break

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Save victim's frequency to Ghost List.
    2. Clean up active metadata.
    '''
    global m_freq, m_priority, m_ghosts
    
    key = evicted_obj.key
    
    # Save frequency for potential future restoration
    freq = m_freq.get(key, 0)
    m_ghosts[key] = freq
    
    # Remove from active sets
    if key in m_freq:
        del m_freq[key]
    if key in m_priority:
        del m_priority[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprg4djife.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgj5ga6ff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprz9vbs20.pickle

Iteration 22: New subsample score 0.076881 is not better than old score 0.086477, skipping
Iteration 23: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmdjuy0sw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1lfvrkl1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa8omi47e.pickle

Iteration 23: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time
# 2. key -> frequency
m_key_last_access = dict()
m_key_frequency = dict()

# Global access counter to represent time
m_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Hyperbolic Caching / Time-Aware LFU.
    
    We want to evict the object with the lowest utility score.
    Score = Frequency / (Time_Since_Last_Access + 1)
    
    Why this works:
    1. A new item (Time_Since ~ 0) has a high score initially, giving it a chance to stay.
    2. A frequently accessed item (Frequency high) has a high score.
    3. An item that hasn't been accessed in a long time (Time_Since high) sees its score decay towards 0.
    '''
    current_time = cache_snapshot.access_count
    
    # We are looking for the key with the minimum score.
    # In a production system, we would sample K random keys (e.g., 64) and pick the best victim 
    # to avoid O(N) scan. However, given the problem constraints usually allow O(N) for accuracy 
    # or the cache size isn't massive in simulation, we scan the keys present in the cache.
    
    candidates = cache_snapshot.cache.keys()
    
    if not candidates:
        return None

    # Find candidate with minimum score
    victim_key = None
    min_score = float('inf')
    
    # Optimization: pre-fetch globals to local scope
    local_freq = m_key_frequency
    local_last = m_key_last_access
    
    for key in candidates:
        freq = local_freq.get(key, 1) # Default to 1 if missing
        last_access = local_last.get(key, 0)
        
        # Calculate time delta. 
        # Note: current_time usually >= last_access. 
        # +1 prevents division by zero.
        time_delta = current_time - last_access + 1
        
        # Hyperbolic function
        score = freq / time_delta
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency to current time.
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency, m_access_counter
    
    current_time = cache_snapshot.access_count
    m_access_counter = current_time
    
    m_key_last_access[obj.key] = current_time
    
    # Increment frequency
    # We saturate frequency or allow it to grow. 
    # For hyperbolic caching, allowing it to grow is fine, 
    # as the denominator grows linearly with time.
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Initialize Frequency (or restore from history).
    '''
    global m_key_last_access, m_key_frequency, m_access_counter
    
    current_time = cache_snapshot.access_count
    m_access_counter = current_time
    
    m_key_last_access[obj.key] = current_time
    
    # If key exists in frequency map (ghost entry), we revive it with +1
    # This provides scan resistance: if it was evicted recently, it might still have low freq.
    # But if it was a heavy hitter, it retains some weight.
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up Recency (mandatory for correctness of logic if key re-enters).
    2. We can choose to keep Frequency (Ghost) or clean it.
    
    Strategy: Keep Frequency for a while, but to prevent memory leaks in infinite streams,
    we might want to periodically clean it. For this logic, we will rely on python's dict 
    efficiency, but to be safe, we perform a lazy cleanup if the ghost map gets too big.
    '''
    global m_key_last_access, m_key_frequency
    
    # Remove strict existence data
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # Heuristic: If frequency is 1, it was a "one-hit wonder" that just got evicted.
    # We can probably forget it to save memory and reset its score if it comes back.
    # However, for high frequencies, we keep it.
    if evicted_obj.key in m_key_frequency:
        if m_key_frequency[evicted_obj.key] <= 1:
            del m_key_frequency[evicted_obj.key]
    
    # Safety cleanup: If frequency map grows too large relative to cache capacity,
    # we should purge old entries.
    # Here assuming a simple cap based on cache capacity multiplier (e.g. 2x or 3x).
    # Since we can't iterate easily, we skip complex cleanup for this snippet
    # relying on the logic that low-freq items were deleted above.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1f1db_8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmsqfnmgu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphdubw4z4.pickle

Iteration 23: New subsample score 1.044042 is not better than old score 1.072642, skipping
Iteration 24: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnvw9_ad_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphaep3_f_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppplafspq.pickle

Iteration 24: Proposed new text for program: # Metadata storage
# 1. m_last_access: key -> int (access_count/time)
# 2. m_count: key -> int (frequency count)
# 3. m_ghost: key -> int (frequency of recently evicted items)

m_last_access = {}
m_count = {}
m_ghost = {}

# Constants
MAX_GHOST_SIZE = 1000
MAX_FREQ_CAP = 15  # Cap frequency to ensure recency eventually takes over if freq is static

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU with LRU Tie-breaking (LFU-LRU).
    
    1. Find the minimum frequency in the current cache.
    2. Collect all keys that have this minimum frequency.
    3. From those keys, evict the Least Recently Used (lowest m_last_access).
    
    This protects frequent items (LFU) but cycles out items that have the same
    frequency based on age (LRU), providing scan resistance and handling loops.
    '''
    global m_last_access, m_count
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # Optimization: We don't need to sort the whole list.
    # We want min(frequency), then min(recency).
    
    # 1. Find the object(s) with the minimal frequency
    # We can iterate once to find the best victim.
    # Victim is defined as: min frequency, then min last_access.
    
    victim = None
    min_freq = float('inf')
    min_access = float('inf')
    
    for k in current_keys:
        # Get metadata, default to safe values if missing
        f = m_count.get(k, 1)
        a = m_last_access.get(k, 0)
        
        if f < min_freq:
            # New lowest frequency found, this is the current best victim
            min_freq = f
            min_access = a
            victim = k
        elif f == min_freq:
            # Same frequency, check recency (LRU behavior for ties)
            if a < min_access:
                min_access = a
                victim = k
                
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access (Recency).
    2. Increment Frequency.
    3. Apply frequency cap to allow aging/shifting interests.
    '''
    global m_last_access, m_count
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment frequency
    curr = m_count.get(obj.key, 0)
    # Cap frequency to prevent historical heavy hitters from staying forever
    # if they stop being accessed.
    if curr < MAX_FREQ_CAP:
        m_count[obj.key] = curr + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Check Ghost Cache:
       - If in ghost, restore previous frequency (plus a boost).
         This handles "warm" items returning to the set.
       - If not in ghost, start with frequency 1.
    '''
    global m_last_access, m_count, m_ghost
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost:
        # It was recently evicted. Restore some of its "weight".
        # We give it a slight boost (+1) to signify it has returned.
        restored_freq = m_ghost[obj.key]
        m_count[obj.key] = min(restored_freq + 1, MAX_FREQ_CAP)
        del m_ghost[obj.key]
    else:
        # Cold insert
        m_count[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up main metadata.
    2. Store the evicted item's frequency in Ghost Cache.
       This allows the algorithm to remember "important" items if they 
       are evicted due to a temporary burst of new data.
    '''
    global m_last_access, m_count, m_ghost
    
    # Retrieve frequency before deleting
    freq = m_count.get(evicted_obj.key, 1)
    
    # Clean main metadata
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    if evicted_obj.key in m_count:
        del m_count[evicted_obj.key]

    # Add to Ghost Cache
    # We map Key -> Frequency
    m_ghost[evicted_obj.key] = freq
    
    # Manage Ghost Cache Size
    # Randomly or LRU pruning of ghost cache to prevent memory leak
    if len(m_ghost) > MAX_GHOST_SIZE:
        # Just remove one item (popitem is LIFO/FIFO depending on python version, 
        # but efficient enough for ghost cleanup)
        m_ghost.popitem()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_x7zzp10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp29uywbgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptk83yrjn.pickle

Iteration 24: New subsample score 0.470881 is better than old score 0.37152599999999997. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsw4_v6_6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphbo403pi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_k5fbarl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1dariz0y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptzxtiily.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxj1r5qaf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5gdj7m6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1eqkuwge.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxerbll8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp342ebdk9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2k78391p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmfwj0khh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsn9yypld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz5c5qa5c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph4hc0xw4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyaycj3i5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl_hg3vhz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz06hoi3h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprzaft45m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq_k745ei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfp46qmt8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpknm7brxy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5eq78xkn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpah56_a0l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx2uj11jq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn36ry0e4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwx4t8r8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpty7ftg5y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqi3e9rhh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq8ogwhn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp37hxrj68.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp29kfqd6b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk8w7nlwl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp997ooa6w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprzmky9i6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvm0zn5i8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe2omoqyo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp45cbwah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6l37jvra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8ozkhpl5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4oomchb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp08dvbtlw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpad6epu1p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo3zwtb2s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf4rtcc9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwm7v2r9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7w26y1q1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7g_qpz_7.pickle

Iteration 24: Full valset score for new program: 0.2285399375
Iteration 24: Full train_val score for new program: 0.2285399375
Iteration 24: Individual valset scores for new program: [0.452809, 0.429489, 0.432459, 0.392016, 0.441782, 0.441312, 0.26555, 0.456547, 0.540937, 0.531017, 0.091667, 0.392984, 0.035058, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.369715, 0.02636, 0.058672, 0.058672, 0.269793, 0.359879, 0.796425, 0.894232, 0.024259, 0.038636, 0.045558, 9.6e-05, 3.6e-05, 0.721239, 0.083333, 0.067079, 0.009459, 0.641937, 0.125461, 0.025783, 0.029322, 0.025645, 0.052632, 0.366667, 0.022032, 0.026648, 0.466258, 0.078431]
Iteration 24: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.381514, 0.026556, 0.058672, 0.058672, 0.332087, 0.387097, 0.856008, 0.894232, 0.074873, 0.038636, 0.045558, 0.029555, 0.029983, 0.747447, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 24: Full valset pareto front score: 0.25827887499999996
Iteration 24: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 3}, {2, 4, 6}, {0, 1, 2, 3, 4, 5, 6}, {4, 6}, {4, 6}, {1, 2, 5}, {0, 1, 2, 3, 4, 5, 6}, {4, 6}, {4, 6}, {4, 6}, {4, 6}, {4, 6}, {1, 2, 4, 5, 6}, {5}, {2, 4}, {1, 2, 4, 5, 6}, {1, 2, 4, 5, 6}, {5}, {2}, {2}, {4, 6}, {5}, {1, 4, 5, 6}, {1, 4, 5, 6}, {5}, {5}, {3}, {1, 4, 5, 6}, {1, 5}, {5}, {2, 4, 6}, {1, 2, 3, 4, 5, 6}, {5}, {5}, {5}, {1, 4, 5, 6}, {4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6}, {1, 5}]
Iteration 24: Best valset aggregate score so far: 0.2508065
Iteration 24: Best program as per aggregate score on train_val: 5
Iteration 24: Best program as per aggregate score on valset: 5
Iteration 24: Best score on valset: 0.2508065
Iteration 24: Best score on train_val: 0.2508065
Iteration 24: Linear pareto front program index: 5
Iteration 24: New program candidate index: 6
Iteration 25: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmb76kv1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0tulglmd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9wol4tuw.pickle

Iteration 25: Proposed new text for program: import math

# Global Metadata Storage
# 1. Frequency: Persisted across evictions to detect recurring items (Ghost entries).
m_key_frequency = dict()
# 2. Last Access: Only maintained for currently cached items to calculate recency.
m_key_last_access = dict()

def get_retention_score(key, size, current_time):
    '''
    Calculates the "Retention Score" (Utility).
    Lower Score = Better candidate for eviction.
    Higher Score = Valuable object to keep.
    
    Formula: Score = (Frequency ^ 1.5) / (Age * Size)
    
    Rationale:
    - Frequency: Higher is better. We use power 1.5 to protect hot items more robustly than linear.
    - Age: (CurrentTime - LastAccess). Older items have lower scores (Hyperbolic decay).
    - Size: Larger items penalize the score. Evicting large items frees up space for multiple small items,
      statistically increasing the probability of future hits (density).
    '''
    freq = m_key_frequency.get(key, 1)
    last_access = m_key_last_access.get(key, current_time)
    
    # Calculate Age. Add 1 to avoid division by zero for immediate re-evaluations.
    age = (current_time - last_access) + 1
    
    # Score calculation
    # We want to maximize Hits. 
    # Hits are maximized by keeping: Frequent items, Recent items, and Small items.
    score = (freq ** 1.5) / (age * size)
    return score

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Hyperbolic Size-Aware Eviction.
    We iterate through the cache to find the object with the lowest retention score.
    '''
    current_time = cache_snapshot.access_count
    
    victim_key = None
    min_score = float('inf')
    
    # Iterate over all currently cached objects to find the best victim
    for key, cached_obj in cache_snapshot.cache.items():
        score = get_retention_score(key, cached_obj.size, current_time)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (Reset Age).
    2. Increment Frequency.
    '''
    global m_key_frequency, m_key_last_access
    current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = current_time
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Last Access Time.
    2. Initialize or Increment Frequency.
       - If it's a "Ghost" (seen before but evicted), we increment the old count.
       - If it's new, we start at 1.
    '''
    global m_key_frequency, m_key_last_access
    current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = current_time
    
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from 'last_access' (it's no longer in cache).
    2. RETAIN 'frequency' (keep as Ghost entry). 
       This helps the algorithm learn from mistakes if the item is accessed again soon.
    '''
    global m_key_last_access
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # m_key_frequency is NOT deleted.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp71idbsy9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjrwi2jdj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwansx9i5.pickle

Iteration 25: New subsample score 0.736298 is not better than old score 0.776861, skipping
Iteration 26: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_tfvfxlm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyujkbc8j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppbibytid.pickle

Iteration 26: Proposed new text for program: import random

# Metadata storage
# 1. m_access_time: key -> int (Last access timestamp/counter)
# 2. m_frequency: key -> int (Approximate frequency counter)
# 3. m_ghost: key -> None (Set of hashes for recently evicted items)

m_access_time = {}
m_frequency = {}
m_ghost = {}

# Constants
GHOST_LIMIT = 2000  # Max size for the ghost registry

def evict(cache_snapshot, obj):
    '''
    Improved Eviction: Size-Aware Second-Chance LRU.
    
    Strategy:
    1. Sampling: Instead of scanning the entire cache (which is O(N)), we sample a subset of keys.
       This is critical for performance on large caches.
    2. Scoring: Calculate a "badness" score for eviction.
       - High Badness: Old last access time, Low frequency, Large size.
       - Low Badness: Recent access, High frequency, Small size.
    3. Second Chance: If the chosen victim has been accessed recently enough relative to the
       current time, we might skip it (effectively re-inserting it logically) and pick another.
       However, given the constraints of the `evict` API (must return one key), we incorporate
       recency directly into the score.
    '''
    global m_access_time, m_frequency
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
        
    # Heuristic: Sample random keys to avoid O(N) sort on huge caches.
    # If cache is small (< 100 items), just check them all.
    if len(current_keys) > 64:
        candidates = random.sample(current_keys, 64)
    else:
        candidates = current_keys
        
    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We want to maximize the "eviction score".
    # A high score means "Good to evict".
    for k in candidates:
        # 1. Recency Age: How long ago was it touched?
        # Larger age = better to evict.
        age = current_time - m_access_time.get(k, 0)
        
        # 2. Frequency: How often accessed?
        # Lower freq = better to evict.
        freq = m_frequency.get(k, 0)
        
        # 3. Size: How big is it?
        # Larger size = better to evict (Cost-Benefit: removing large item frees more space).
        # We perform a safe lookup for size (handling potential sync issues gracefully).
        cached_obj = cache_snapshot.cache.get(k)
        size = cached_obj.size if cached_obj else 1
        
        # Avoid division by zero for log calculations
        # Using a formula inspired by GDSF (Greedy Dual Size Frequency) variants
        # Score = (Age * Size_Factor) / Frequency_Factor
        
        # We penalize items with low frequency significantly.
        # We encourage evicting large items to make room for many small items.
        
        # Frequency weight: +1 to avoid div by zero. 
        # Capping freq impact so super-popular items don't become immortal.
        freq_weight = min(freq, 10) + 1 
        
        # Size weight: Logarithmic helps dampen the impact of massive files slightly,
        # ensuring we don't ONLY evict large files if they are actually popular.
        # However, a linear factor is often stronger for hit-rate maximization in bytes-constrained caches.
        # Here we use sqrt(size) as a balanced approach.
        size_weight = size ** 0.5 
        
        # Eviction Score calculation
        # Higher score -> Better victim
        score = (age * size_weight) / freq_weight
        
        if score > max_score:
            max_score = score
            best_victim = k
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (Recency).
    2. Increment Frequency (Importance).
    '''
    global m_access_time, m_frequency
    
    m_access_time[obj.key] = cache_snapshot.access_count
    # Increment frequency, capped to prevent integer overflows or history dominance
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Check Ghost List.
       - If in Ghost: This is a "warm" item. Give it a frequency boost so it survives longer.
       - If New: Start with base frequency.
    '''
    global m_access_time, m_frequency, m_ghost
    
    m_access_time[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost:
        # It was recently evicted, but requested again. It's likely part of a working set
        # that is slightly larger than cache capacity.
        # Boost frequency to protect it from immediate eviction.
        m_frequency[obj.key] = 5  # Give it a "Protected" starting weight
        del m_ghost[obj.key]
    else:
        # Cold insert
        m_frequency[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Cleanup metadata.
    2. Add to Ghost List to track regret.
    '''
    global m_access_time, m_frequency, m_ghost
    
    key = evicted_obj.key
    
    # 1. Remove form main metadata
    if key in m_access_time:
        del m_access_time[key]
    if key in m_frequency:
        del m_frequency[key]
        
    # 2. Add to Ghost
    # Just storing the key is enough for existence check.
    m_ghost[key] = None
    
    # 3. Maintain Ghost Size
    # If ghost grows too big, pop arbitrary item (FIFOish behavior via dictionary order in modern Python)
    if len(m_ghost) > GHOST_LIMIT:
        # iter(dict) gives keys in insertion order (Python 3.7+)
        oldest = next(iter(m_ghost))
        del m_ghost[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe7nmm2mn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvdtusbju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_hiqbtqa.pickle

Iteration 26: New subsample score 0.535443 is not better than old score 0.552603, skipping
Iteration 27: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpro6m74er.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6tsruabd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptc6yo9xb.pickle

Iteration 27: Proposed new text for program: # GDSF (Greedy Dual Size Frequency) Implementation

# Global Metadata Storage
# 1. key -> Priority Score (float)
_gdsf_priority = dict()

# 2. key -> Frequency (int)
_gdsf_freq = dict()

# 3. key -> Last Access Time (int) - Used for LRU tie-breaking
_gdsf_last_access = dict()

# 4. Global Inflation Value (L) - Represents the aging factor of the cache
_gdsf_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GDSF (Greedy Dual Size Frequency)
    
    We identify the object with the lowest Priority Score (H-value).
    Priority = L + (Frequency / Size)
    
    Tie-Breaker:
    If multiple objects have the lowest priority, we evict the Least Recently Used (LRU)
    among them.
    '''
    if not cache_snapshot.cache:
        return None

    # We scan for the object with the minimum (Priority, Last_Access) tuple.
    # Python's tuple comparison works left-to-right:
    # 1. Primary: Priority (Low priority is bad)
    # 2. Secondary: Last_Access (Oldest time is bad)
    
    victim_key = min(
        cache_snapshot.cache, 
        key=lambda k: (_gdsf_priority.get(k, 0.0), _gdsf_last_access.get(k, 0))
    )
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Recompute Priority using the CURRENT global L value.
       This brings the object's priority up to date with the current "inflation",
       resetting its aging process.
    '''
    global _gdsf_priority, _gdsf_freq, _gdsf_last_access, _gdsf_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Access Time (Recency)
    _gdsf_last_access[key] = current_time
    
    # Update Frequency
    new_freq = _gdsf_freq.get(key, 0) + 1
    _gdsf_freq[key] = new_freq
    
    # Update GDSF Priority
    # Priority = L + (Frequency / Size)
    # Safety check for size > 0, though problem statement guarantees positive int.
    size = obj.size if obj.size > 0 else 1
    _gdsf_priority[key] = _gdsf_L + (new_freq / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Initialize Frequency to 1.
    3. Calculate initial Priority based on current global L.
    '''
    global _gdsf_priority, _gdsf_freq, _gdsf_last_access, _gdsf_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Init Access Time
    _gdsf_last_access[key] = current_time
    
    # Init Frequency
    _gdsf_freq[key] = 1
    
    # Init Priority
    # Priority = L + (1 / Size)
    # The new item enters relative to the current aging factor L.
    size = obj.size if obj.size > 0 else 1
    _gdsf_priority[key] = _gdsf_L + (1.0 / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update the global aging factor L.
       L becomes the priority of the evicted object. This ensures that the 
       "price of admission" for the cache monotonically increases (or stays flat).
    2. Clean up metadata.
    '''
    global _gdsf_priority, _gdsf_freq, _gdsf_last_access, _gdsf_L
    
    key = evicted_obj.key
    
    # Update L
    if key in _gdsf_priority:
        _gdsf_L = _gdsf_priority[key]
        
    # Cleanup
    if key in _gdsf_priority:
        del _gdsf_priority[key]
    if key in _gdsf_freq:
        del _gdsf_freq[key]
    if key in _gdsf_last_access:
        del _gdsf_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw25_t9dx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9zp_s33c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxse_g8ra.pickle

Iteration 27: New subsample score 0.47434699999999996 is not better than old score 0.729137, skipping
Iteration 28: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy_5klcvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp0iu9o5d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqkp2c8og.pickle

Iteration 28: Proposed new text for program: import math

# Metadata storage
# 1. m_recency: Stores the last access time (logical clock) for items currently in cache.
# 2. m_frequency: Stores simplified access counts (capped) to detect "hot" items.
# 3. m_ghost: A set or bounded dict tracking keys recently evicted to catch "warm" items returning.
# 4. m_inserted_time: Tracks when an object entered the cache to determine its "age" in cache.
m_recency = dict()
m_frequency = dict()
m_ghost = set()
m_inserted_time = dict()

# Configuration
# Ghost capacity relative to cache capacity (approximate object count)
GHOST_RATIO = 1.0 
# Max frequency cap to prevent heavy history dragging
MAX_FREQ = 4 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Advanced Clock / 2Q Approximation.
    
    We categorize items into:
    1. Probationary (Newly inserted, Low Frequency)
    2. Protected (High Frequency or Recently Hit)
    
    Strategy:
    - We calculate a "utility score" for every item.
    - Score = (Has_been_hit_since_insert * Weight) + Recency_Score + Frequency_Score
    - We evict the item with the LOWEST score.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
        
    current_time = cache_snapshot.access_count
    
    # Heuristic for eviction:
    # We want to evict items that:
    # 1. Have low frequency.
    # 2. Have NOT been hit since they were inserted (One-hit wonders).
    # 3. Are old (LRU).
    
    victim_key = None
    min_score = float('inf')
    
    # We use a scoring function to find the worst element.
    # While O(N), this guarantees the best local decision based on metadata.
    # Optimization: In production, one would sample K items. Here we scan all.
    
    for key in current_keys:
        freq = m_frequency.get(key, 0)
        last_access = m_recency.get(key, 0)
        insert_time = m_inserted_time.get(key, 0)
        
        # Factor 1: Has this been hit since insertion?
        # If last_access > insert_time, it means it was hit at least once AFTER insert.
        was_hit = 1 if last_access > insert_time else 0
        
        # Factor 2: Recency delta (Age). Larger delta = Older.
        age = current_time - last_access
        
        # Factor 3: Frequency. 
        # We cap frequency influence so ancient history doesn't dominate.
        eff_freq = min(freq, MAX_FREQ)
        
        # Score Calculation:
        # Higher score = Keep. Lower score = Evict.
        # Structure: (Was_Hit * High_Value) + (Frequency * Medium_Value) - Age
        # 
        # Explanation:
        # 1. 'was_hit' is the most powerful protector. If 0, it's a prime eviction candidate.
        # 2. 'eff_freq' differentiates between items that haven't been hit recently.
        # 3. '- age' breaks ties by selecting the LRU.
        
        score = (was_hit * 1000000) + (eff_freq * 10000) - age
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency (capped).
    '''
    global m_recency, m_frequency
    
    current_time = cache_snapshot.access_count
    
    m_recency[obj.key] = current_time
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency and Insert Time.
    2. Handle Ghost List Logic:
       - If key is in Ghost list, it means it was evicted recently. It was likely 
         wrongly evicted or the working set is larger than cache. 
         We give it a frequency boost (start at 2 instead of 1) to protect it.
       - If not in Ghost list, start frequency at 0 or 1.
    '''
    global m_recency, m_frequency, m_inserted_time, m_ghost
    
    current_time = cache_snapshot.access_count
    m_recency[obj.key] = current_time
    m_inserted_time[obj.key] = current_time
    
    if obj.key in m_ghost:
        # Key returned! It's a "warm" item. Promote immediately.
        m_frequency[obj.key] = 2 
        m_ghost.remove(obj.key)
    else:
        # New item. Probationary.
        m_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove primary metadata (recency, insert_time).
    2. Add to Ghost List (to track thrashing).
    3. Clean up Ghost List if it gets too big.
    '''
    global m_recency, m_frequency, m_inserted_time, m_ghost
    
    key = evicted_obj.key
    
    # Cleanup active metadata
    if key in m_recency:
        del m_recency[key]
    if key in m_inserted_time:
        del m_inserted_time[key]
        
    # We usually delete frequency on eviction in this model to prevent
    # indefinite memory growth, relying on Ghost list for "short term memory".
    if key in m_frequency:
        del m_frequency[key]

    # Add to Ghost list
    m_ghost.add(key)
    
    # Maintain Ghost List size
    # If the ghost list grows larger than the cache capacity (heuristic), trim it.
    # Since we don't have exact object count capacity, we use a rough heuristic 
    # or just trim arbitrarily if it gets huge.
    # A simple FIFO trim for the set isn't possible directly, so we limit by size.
    # Ideally, we'd use an OrderedDict for the ghost list, but strict Set is faster.
    # We will just recreate the set if it exceeds a threshold to avoid memory leaks.
    
    # Note: cache_snapshot.capacity is in bytes, not count. 
    # We use a static limit for ghost keys to prevent memory issues.
    if len(m_ghost) > 5000: 
        # Quick reset to prevent unbound growth. 
        # In a real system, this would be a FIFO queue.
        m_ghost.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3n9yslfm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo8eiw63p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptvqo5562.pickle

Iteration 28: New subsample score 0.483103 is not better than old score 0.48546300000000003, skipping
Iteration 29: Selected program 3 score: 0.217409375
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprh4vaxhi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt8_01s3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkxx8nbac.pickle

Iteration 29: Proposed new text for program: import sys

# Global metadata storage
# Structure: key -> {'freq': int, 'priority': float, 'last_access': int}
m_metadata = dict()

# Global aging factor 'L'
current_L = 0.0

# Ghost cache settings
# To prevent unlimited memory growth, we limit the number of metadata entries 
# for items NOT currently in the cache.
MAX_GHOST_ENTRIES = 10000 

def evict(cache_snapshot, obj):
    '''
    LFU-DA Policy with Tie-Breaking:
    1. Find object with lowest Priority (H).
    2. H(p) = L + Frequency(p)
    3. Tie-breaker: LRU (Smallest last_access timestamp).
    '''
    global current_L, m_metadata
    
    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')
    
    # Iterate through ONLY objects currently in the cache
    for key in cache_snapshot.cache:
        meta = m_metadata.get(key)
        
        # Safety check: if metadata is missing (shouldn't happen), assume defaults
        if not meta:
            priority = 1.0 + current_L
            last_access = 0
        else:
            priority = meta['priority']
            last_access = meta['last_access']
        
        # Comparison logic
        if priority < min_priority:
            min_priority = priority
            min_last_access = last_access
            victim_key = key
        elif priority == min_priority:
            # Tie-breaker: Least Recently Used (LRU)
            if last_access < min_last_access:
                min_last_access = last_access
                victim_key = key
            
    # Update Dynamic Aging Factor L
    # L becomes the priority of the evicted object. 
    # This effectively raises the bar for new items entering the cache.
    if victim_key is not None:
        current_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Priority: Priority = L + Frequency
    3. Update Recency (last_access)
    '''
    global m_metadata, current_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        m_metadata[obj.key]['freq'] += 1
        m_metadata[obj.key]['last_access'] = current_time
        # Re-calculate priority relative to current global age L
        m_metadata[obj.key]['priority'] = current_L + m_metadata[obj.key]['freq']
    else:
        # Edge case recovery
        m_metadata[obj.key] = {
            'freq': 1,
            'priority': current_L + 1,
            'last_access': current_time
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if we have history (Ghost Entry).
    2. If Ghost exists: Restore previous Frequency.
    3. If New: Start with Frequency = 1.
    4. Set Priority = L + Frequency.
    '''
    global m_metadata, current_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        # Resurrection: Object was evicted but metadata kept (Ghost Hit)
        m_metadata[obj.key]['freq'] += 1
        m_metadata[obj.key]['last_access'] = current_time
        m_metadata[obj.key]['priority'] = current_L + m_metadata[obj.key]['freq']
    else:
        # First time seeing this object
        m_metadata[obj.key] = {
            'freq': 1,
            'priority': current_L + 1,
            'last_access': current_time
        }
        
    # Cleanup Ghost entries if metadata grows too large
    # Heuristic: If metadata size > 2x Cache Capacity (in terms of count), prune.
    # Since we don't have object counts easily, we use a fixed constant or heuristic.
    if len(m_metadata) > len(cache_snapshot.cache) + MAX_GHOST_ENTRIES:
        _prune_ghost_entries(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    DO NOT delete metadata immediately. Keep it as a "Ghost" entry.
    This allows the algorithm to "remember" popular objects that were momentarily evicted.
    '''
    # We do nothing here. The metadata stays in m_metadata.
    # It will only be removed if _prune_ghost_entries is called during insert.
    pass

def _prune_ghost_entries(cache_snapshot):
    '''
    Helper to clean up old metadata to prevent memory leaks.
    Removes metadata for keys not in cache_snapshot.cache.
    '''
    global m_metadata
    
    # Identify keys to delete (Ghost keys only)
    # Strategy: Delete the oldest ghosts based on last_access
    
    ghost_keys = []
    for k, v in m_metadata.items():
        if k not in cache_snapshot.cache:
            ghost_keys.append((k, v['last_access']))
            
    # Sort by last_access (oldest first)
    ghost_keys.sort(key=lambda x: x[1])
    
    # Remove the oldest 20% of ghosts to make space
    num_to_remove = int(len(ghost_keys) * 0.2)
    if num_to_remove == 0 and len(ghost_keys) > 0:
        num_to_remove = 1
        
    for i in range(num_to_remove):
        key_to_del = ghost_keys[i][0]
        del m_metadata[key_to_del]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpran8_dgy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8s207796.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv7ncwg1m.pickle

Iteration 29: New subsample score 0.352883 is better than old score 0.350768. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnp5x3idt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsqtjkmjx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk6dxylhq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj0iywju_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvfbeyf8y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2bvc1mx_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6r39kyb_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzzvjqxih.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi5b_w7m7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxqcwwzh0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpazxsabpz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzwc3i8ux.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpynzutylo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqhvbc9cd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9b386_em.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0id0hna2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvv57s6u9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8rajeqx0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1hxygnni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplstig62z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp43_y8t4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo13wmkt_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvqa4mhlv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpclrck6yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq_7xy4lo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpicd1nbhq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8xvdvndf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvutkwq3b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6p7ak3h1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphmo6ey5f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfc5t41w3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvo7l60uz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi_vv7sio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprfr3n4r7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjw814qx6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1nte7ph7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_aobrgap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkjmdnxga.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxbschmu8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgb350dfo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmkqvmueo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7vqmwpqw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvz0m__e5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmposnmlna1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8q89af_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzqkmzmgq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk70m9k3z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqwk2tmud.pickle

Iteration 29: Full valset score for new program: 0.22715887500000007
Iteration 29: Full train_val score for new program: 0.22715887500000007
Iteration 29: Individual valset scores for new program: [0.48235, 0.461184, 0.468153, 0.424113, 0.47628, 0.469503, 0.26555, 0.498624, 0.539507, 0.531017, 0.066667, 0.345027, 0.023893, 0.0, 0.019963, 0.019583, 0.018715, 0.022491, 0.021657, 0.268561, 0.343166, 0.025576, 0.057382, 0.057382, 0.269821, 0.341734, 0.845084, 0.887558, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.745405, 0.072368, 0.062665, 0.009183, 0.632664, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.283333, 0.020965, 0.02244, 0.441718, 0.03268]
Iteration 29: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.381514, 0.026556, 0.058672, 0.058672, 0.332087, 0.387097, 0.856008, 0.894232, 0.074873, 0.038636, 0.045558, 0.029555, 0.029983, 0.747447, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 29: Full valset pareto front score: 0.25827887499999996
Iteration 29: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 3, 7}, {2, 4, 6}, {0, 1, 2, 3, 4, 5, 6, 7}, {4, 6}, {4, 6}, {1, 2, 5}, {0, 1, 2, 3, 4, 5, 6, 7}, {4, 6}, {4, 6}, {4, 6}, {4, 6}, {4, 6}, {1, 2, 4, 5, 6}, {5}, {2, 4}, {1, 2, 4, 5, 6}, {1, 2, 4, 5, 6}, {5}, {2}, {2}, {4, 6}, {5}, {1, 4, 5, 6}, {1, 4, 5, 6}, {5}, {5}, {3}, {1, 4, 5, 6}, {1, 5}, {5}, {2, 4, 6}, {1, 2, 3, 4, 5, 6, 7}, {5}, {5}, {5}, {1, 4, 5, 6}, {4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6}, {1, 5}]
Iteration 29: Best valset aggregate score so far: 0.2508065
Iteration 29: Best program as per aggregate score on train_val: 5
Iteration 29: Best program as per aggregate score on valset: 5
Iteration 29: Best score on valset: 0.2508065
Iteration 29: Best score on train_val: 0.2508065
Iteration 29: Linear pareto front program index: 5
Iteration 29: New program candidate index: 7
Iteration 30: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpva6yczzq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8qpdw_lj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprw1ukjln.pickle

Iteration 30: Proposed new text for program: import math

# Metadata storage
# State tracking:
# 0: Absent/Unknown
# 1: Probationary (Recently inserted, seen once)
# 2: Protected (Seen at least twice, frequently used)
m_key_state = dict()
m_key_last_access = dict()

# Ghost hits tracking (to detect if we evicted too early)
# We store timestamps of eviction to cleanup old ghost entries
m_ghost_registry = dict()
GHOST_retention_window = 5000  # How long we remember an evicted key

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented Priority
    
    Priority 1 (Evict First): Probationary items.
        - These are items seen recently but haven't proven their worth (frequency < 2).
        - Tie-breaker: LRU.
        
    Priority 2 (Evict Second): Protected items.
        - These are established hot items.
        - Tie-breaker: LRU.
        
    This approach acts like an SLRU (Segmented LRU). It protects the "working set" 
    from being flushed by a "scan" (a long sequence of one-time accesses).
    '''
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Filter keys by state
    probation_candidates = []
    protected_candidates = []

    for k in current_keys:
        state = m_key_state.get(k, 1) # Default to 1 (Probation) if unknown
        last_acc = m_key_last_access.get(k, 0)
        
        if state == 1:
            probation_candidates.append((k, last_acc))
        else:
            protected_candidates.append((k, last_acc))

    # Strategy: Evict from Probationary first (Least Recently Used among them)
    if probation_candidates:
        # Min by last_access_time
        victim_key = min(probation_candidates, key=lambda x: x[1])[0]
        return victim_key
    
    # If no probationary items exist (rare, but possible if cache is full of hot items),
    # evict LRU from Protected
    if protected_candidates:
        victim_key = min(protected_candidates, key=lambda x: x[1])[0]
        return victim_key
        
    # Fallback
    return current_keys[0]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Promotion: If item was Probationary, it becomes Protected.
    '''
    global m_key_state, m_key_last_access
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    m_key_last_access[key] = current_time
    
    # Promotion logic
    # If it is in cache and hit, it has proven utility. Promote to Protected (2).
    m_key_state[key] = 2

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
       - If key is in Ghost Registry, it was evicted recently. This is a "Resurrection".
         It implies the item is part of a larger working set. Treat as Protected immediately.
       - If key is NOT in Ghost Registry, it is a brand new object. Treat as Probationary.
    '''
    global m_key_state, m_key_last_access, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    key = obj.key
    m_key_last_access[key] = current_time
    
    if key in m_ghost_registry:
        # It was recently evicted, so we made a mistake or cache is too small.
        # Mark as Protected so it sticks around longer this time.
        m_key_state[key] = 2
        del m_ghost_registry[key]
    else:
        # First time seeing this (or seen very long ago).
        # Mark as Probationary.
        m_key_state[key] = 1
        
    # Periodic cleanup of ghost registry to prevent memory leaks
    if current_time % 1000 == 0:
        _cleanup_ghosts(current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from active state/recency tracking.
    2. Add to Ghost Registry. This allows us to recognize "Recency" 
       even if the object is temporarily pushed out of the physical cache.
    '''
    global m_key_state, m_key_last_access, m_ghost_registry
    
    key = evicted_obj.key
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if key in m_key_state:
        del m_key_state[key]
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    # Add to Ghost Registry
    m_ghost_registry[key] = current_time

def _cleanup_ghosts(current_time):
    '''
    Remove ghost entries that are too old to matter.
    '''
    global m_ghost_registry, GHOST_retention_window
    keys_to_del = []
    for k, time_evicted in m_ghost_registry.items():
        if current_time - time_evicted > GHOST_retention_window:
            keys_to_del.append(k)
            
    for k in keys_to_del:
        del m_ghost_registry[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7qbc3i29.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfxz121sp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp06i4ad5c.pickle

Iteration 30: New subsample score 0.274055 is better than old score 0.26555. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprjahorji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4vbdte9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptpa3mx9h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9kvdvaw9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5mayw1xb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf3opxpmn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8x402lpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr8_2gpz6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpex4vvkff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps3awzp21.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_i0x9og3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpesjfgy_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuzwfxa_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpty0ifmp3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpspgal32i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgp6541ie.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp24dnzbrk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmgm_5rlt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzlf366w3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk5fq9inv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpns349og7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppt0vnsj9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprr2x7ezh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6zomra0v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4wvwfpkm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpode38rfr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdib65x_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_iprkpu9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbif_qcqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyywgwxgr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp1vmw15t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcd85jd8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu_d3p6jl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp84xm5yji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmt2sjj_a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeh25iqvb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjzaj22ka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyvjl7xvk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5jh8wk8y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmin8nfqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsbuszgff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpybt7ypoh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpub8wrogh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjxsiuthy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmqw_z9vv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbyp7umbs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmnrwmqm0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfdmzmlx0.pickle

Iteration 30: Full valset score for new program: 0.2322404166666667
Iteration 30: Full train_val score for new program: 0.2322404166666667
Iteration 30: Individual valset scores for new program: [0.508879, 0.479279, 0.490819, 0.438962, 0.502166, 0.488643, 0.273923, 0.478569, 0.540937, 0.531017, 0.075, 0.312611, 0.024339, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.368732, 0.026556, 0.058672, 0.058672, 0.269774, 0.387097, 0.856008, 0.894061, 0.020365, 0.036364, 0.041002, 9.6e-05, 3.6e-05, 0.721239, 0.074561, 0.061783, 0.009183, 0.641937, 0.125461, 0.022179, 0.021381, 0.023637, 0.042763, 0.316667, 0.021041, 0.02216, 0.466258, 0.03268]
Iteration 30: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.381514, 0.026556, 0.058672, 0.058672, 0.332087, 0.387097, 0.856008, 0.894232, 0.074873, 0.038636, 0.045558, 0.029555, 0.029983, 0.747447, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 30: Full valset pareto front score: 0.25827887499999996
Iteration 30: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {8, 2}, {0, 3, 7}, {8, 2, 4, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {4, 6}, {4, 6}, {1, 2, 5}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {8, 4, 6}, {8, 4, 6}, {8, 4, 6}, {8, 4, 6}, {8, 4, 6}, {1, 2, 4, 5, 6, 8}, {5}, {8, 2, 4}, {1, 2, 4, 5, 6, 8}, {1, 2, 4, 5, 6, 8}, {5}, {8, 2}, {8, 2}, {4, 6}, {5}, {1, 4, 5, 6}, {1, 4, 5, 6}, {5}, {5}, {3}, {1, 4, 5, 6}, {1, 5}, {5}, {8, 2, 4, 6}, {1, 2, 3, 4, 5, 6, 7, 8}, {5}, {5}, {5}, {1, 4, 5, 6}, {4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8}, {1, 5}]
Iteration 30: Best valset aggregate score so far: 0.2508065
Iteration 30: Best program as per aggregate score on train_val: 5
Iteration 30: Best program as per aggregate score on valset: 5
Iteration 30: Best score on valset: 0.2508065
Iteration 30: Best score on train_val: 0.2508065
Iteration 30: Linear pareto front program index: 5
Iteration 30: New program candidate index: 8
Iteration 31: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpde3sdyc6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp43u0p_r0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptnkxd5tb.pickle

Iteration 31: Proposed new text for program: import math

# Metadata storage
# 1. m_priority: key -> float (The calculated priority score)
# 2. m_frequency: key -> int (Access frequency)
# 3. m_inflation: A global float representing the 'aging' factor (L)

m_priority = dict()
m_frequency = dict()
m_inflation = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF-like Eviction (Greedy Dual-Size Frequency).
    
    We evict the object with the lowest 'priority' score.
    The priority combines Recency (via the inflation factor), Frequency, and Size.
    
    Logic:
    1. Scan current cache keys.
    2. Identify the object with the minimum priority value.
    3. Update the global 'm_inflation' to equal this minimum priority. 
       This helps 'age' the other items in the cache relative to new insertions.
    '''
    global m_priority, m_frequency, m_inflation
    
    # We must operate only on keys currently in the cache
    current_keys = cache_snapshot.cache.keys()
    if not current_keys:
        return None
    
    # Find the key with the lowest priority score.
    # If a key is missing from metadata (edge case), default to -1 to evict it immediately.
    victim_key = None
    min_val = float('inf')
    
    for k in current_keys:
        val = m_priority.get(k, -1.0)
        if val < min_val:
            min_val = val
            victim_key = k
            
    # Update global inflation parameter L to the priority of the evicted object.
    # This is the crucial "aging" mechanism of Greedy Dual algorithms.
    if victim_key is not None and min_val != float('inf') and min_val > m_inflation:
        m_inflation = min_val
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Re-calculate Priority: New Priority = Current Inflation (L) + (Freq / Size)
    This brings the object "forward" in time (Recency) while preserving its weight.
    '''
    global m_priority, m_frequency, m_inflation
    
    # Update Frequency
    old_freq = m_frequency.get(obj.key, 0)
    new_freq = old_freq + 1
    m_frequency[obj.key] = new_freq
    
    # Avoid division by zero for size (though size is defined as positive integer)
    size = max(1, obj.size)
    
    # Calculate GDSF priority
    # We use a slight dampener on size (e.g., log) to prevent punishing large objects TOO severely,
    # but raw 1/size is standard for maximizing hit rates on small object dominated traces.
    # Standard GDSF: H = L + F / S
    priority = m_inflation + (new_freq / size)
    
    m_priority[obj.key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate Initial Priority: P = Current Inflation (L) + (1 / Size)
    '''
    global m_priority, m_frequency, m_inflation
    
    # Initialize Frequency
    m_frequency[obj.key] = 1
    
    size = max(1, obj.size)
    
    # Initial priority
    priority = m_inflation + (1.0 / size)
    
    m_priority[obj.key] = priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up metadata to prevent memory leaks.
    '''
    global m_priority, m_frequency
    
    if evicted_obj.key in m_priority:
        del m_priority[evicted_obj.key]
    
    if evicted_obj.key in m_frequency:
        del m_frequency[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd53jzey9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp19q2rbpo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptaefcaef.pickle

Iteration 31: New subsample score 0.8029440000000001 is not better than old score 0.8796269999999999, skipping
Iteration 32: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc6q6670e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa58pa71a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpccvblpbp.pickle

Iteration 32: Proposed new text for program: import math
import random

# --- Metadata Storage ---
# We use a probabilistic counter (Count-Min Sketch simulation) for frequency.
# Since we don't have a limit on metadata size in the prompt constraints, 
# we will use a standard dictionary for exact counting but simulate "Reset" behavior
# to keep it fresh, acting like a sliding window sketch.
m_freq_sketch = dict()
m_sketch_total_weight = 0
m_sketch_reset_threshold = 0 # Will be dynamic based on cache size

# Recency tracking: key -> access_sequence
m_access_sequence = dict()
m_current_sequence = 0

# W-TinyLFU Parameters
# 1% of cache is usually enough for the window, but in variable sized object caches,
# we trust the sketch more.
WINDOW_SIZE_RATIO = 0.01 

def _estimate_frequency(key):
    """
    Returns the estimated frequency of the key.
    """
    return m_freq_sketch.get(key, 0)

def _update_frequency(key):
    """
    Increments frequency and handles the "halving" reset mechanism
    to keep history fresh (aging).
    """
    global m_freq_sketch, m_sketch_total_weight, m_sketch_reset_threshold
    
    # Increment
    curr = m_freq_sketch.get(key, 0)
    m_freq_sketch[key] = curr + 1
    m_sketch_total_weight += 1
    
    # Dynamic Reset Threshold: usually 10x - 20x the cache capacity count.
    # Since we only know byte capacity, we approximate based on total_weight.
    # We essentially reset when the sketch gets "saturated".
    if m_sketch_reset_threshold == 0:
         m_sketch_reset_threshold = 20000 # default start

    if m_sketch_total_weight >= m_sketch_reset_threshold:
        # Halve all counts
        keys_to_del = []
        for k in m_freq_sketch:
            m_freq_sketch[k] = m_freq_sketch[k] // 2
            if m_freq_sketch[k] == 0:
                keys_to_del.append(k)
        for k in keys_to_del:
            del m_freq_sketch[k]
        
        m_sketch_total_weight = m_sketch_total_weight // 2

def evict(cache_snapshot, obj):
    '''
    W-TinyLFU Eviction Policy:
    
    1. Identify a "Victim Candidate" from the cache.
       The candidate is chosen based on an approximate LRU/LFU hybrid policy.
       We look for the item with the lowest Frequency estimate.
       If frequencies are equal, we pick the Least Recently Used.
       
    2. Perform an Admission Check (The "TinyLFU" part):
       Compare the frequency of the Candidate vs. the Incoming Object.
       
       - If Incoming_Freq > Candidate_Freq: Evict Candidate.
       - If Incoming_Freq <= Candidate_Freq: 
           Technically, we should reject the incoming object. 
           However, this interface forces us to return a victim key to make space.
           We cannot return None to "reject" the insert in this specific framework 
           (usually `evict` implies *something* must go).
           
           So, we stick to evicting the item with the Global Minimum Score 
           (Min Frequency, then Min Recency).
    '''
    global m_freq_sketch, m_access_sequence
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
        
    # Heuristic optimization:
    # Scanning O(N) is expensive in Python. For massive caches, we sample.
    # However, to maximize hit rate per requirements, we will scan.
    
    # We want to evict the object that is:
    # 1. Least Frequent (History)
    # 2. Least Recently Used (Recency - Tie breaker)
    
    # Note on Admission:
    # In a full W-TinyLFU implementation, we would compare the victim against `obj`.
    # But since `evict` just asks "who leaves?", we find the weakest link.
    # The `update_after_insert` logic essentially handles the "new" item tracking.
    # If the new item has 0 frequency, it will be the likely victim in the NEXT round
    # if it doesn't get hit, effectively simulating rejection.
    
    # Optimization: Calculate the incoming object's frequency once
    # (Though we can't use it to stop eviction here, it informs our mental model).
    # incoming_freq = _estimate_frequency(obj.key) 
    # In this specific API, we must return a key from `current_keys`.
    
    victim = min(
        current_keys, 
        key=lambda k: (m_freq_sketch.get(k, 0), m_access_sequence.get(k, 0))
    )
    
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency (Sketch).
    2. Update Recency.
    '''
    global m_current_sequence, m_access_sequence, m_sketch_reset_threshold
    
    m_current_sequence = cache_snapshot.access_count
    m_access_sequence[obj.key] = m_current_sequence
    
    _update_frequency(obj.key)
    
    # Adaptive threshold adjustment based on cache object count
    # If cache is growing, allow sketch to grow.
    current_obj_count = len(cache_snapshot.cache)
    target = current_obj_count * 10
    if target > m_sketch_reset_threshold:
        m_sketch_reset_threshold = target

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Update Recency.
    2. Update Frequency.
       Crucially, TinyLFU relies on the sketch having history even if the item wasn't in cache.
       If it's truly new, freq becomes 1. If it was a "ghost", it increments.
    '''
    global m_current_sequence, m_access_sequence
    
    m_current_sequence = cache_snapshot.access_count
    m_access_sequence[obj.key] = m_current_sequence
    
    _update_frequency(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove Recency data (it is no longer in the cache).
    2. DO NOT remove Frequency data. 
       The Sketch must remember this item to protect it if it comes back (Scan Resistance).
    '''
    global m_access_sequence
    
    if evicted_obj.key in m_access_sequence:
        del m_access_sequence[evicted_obj.key]
    
    # We leave m_freq_sketch alone. 
    # It cleans itself up via the "Reset" mechanism in _update_frequency.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeh47zee3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpisexsdq4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps_21w_sz.pickle

Iteration 32: New subsample score 0.6541819999999999 is not better than old score 0.6541819999999999, skipping
Iteration 33: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2p9j54xs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq5qayi3o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbna1va4l.pickle

Iteration 33: Proposed new text for program: # Metadata storage
# 1. m_access_time: key -> int (Last access timestamp/index)
# 2. m_recency_state: key -> str ('LIR' or 'HIR')
#    - 'LIR': Low Inter-reference Recency (Hot, Protected)
#    - 'HIR': High Inter-reference Recency (Cold, Probationary)
# 3. m_stack_s: dict (Simulated LIRS Stack S). Stores keys currently in the "recency" scope.
#    - We use a dict for O(1) lookups, but conceptually it represents the LIRS stack.
#    - Maps key -> access_time
# 4. m_non_resident: key -> access_time (Metadata for evicted items to calculate IRR)

m_access_time = dict()
m_recency_state = dict()
m_stack_s = dict()
m_non_resident = dict()

# Constants
# Target ratio of LIR items (Protected) vs HIR items (Probationary).
# 90% LIR allows us to keep the working set, 10% HIR buffers new scans.
LIR_CAPACITY_RATIO = 0.95 

def evict(cache_snapshot, obj):
    '''
    LIRS-like Eviction Policy.
    
    Goal: Evict a Resident HIR item.
    
    Logic:
    1. LIR items are safe.
    2. HIR items are candidates.
    3. Among HIR items, we pick the one that is NOT in the abstract Stack S if possible,
       or simply the Least Recently Used HIR item.
    '''
    global m_recency_state, m_access_time
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Separate candidates
    hir_candidates = []
    
    for k in current_keys:
        state = m_recency_state.get(k, 'HIR')
        if state == 'HIR':
            hir_candidates.append(k)
    
    # Strategy: Evict the LRU item among HIRs.
    # In pure LIRS, we evict the HIR item at the bottom of Queue Q. 
    # Here, LRU of HIRs approximates Queue Q eviction.
    if hir_candidates:
        victim = min(hir_candidates, key=lambda k: m_access_time.get(k, 0))
        return victim
    
    # Fallback: If no HIR items (rare, implies cache is 100% LIR), 
    # evict LRU of the whole cache (which effectively demotes an LIR).
    victim = min(current_keys, key=lambda k: m_access_time.get(k, 0))
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If item is LIR: It becomes the MRU. Prune Stack S bottom if necessary.
    2. If item is HIR:
       - If it is inside Stack S: It becomes hot! Promote to LIR.
       - If it is NOT inside Stack S: It remains HIR, but update recency.
    '''
    global m_access_time, m_recency_state, m_stack_s, m_non_resident
    
    current_time = cache_snapshot.access_count
    key = obj.key
    m_access_time[key] = current_time
    
    state = m_recency_state.get(key, 'HIR')
    
    if state == 'LIR':
        # Simple access update. In pure LIRS, this moves it to top of Stack S.
        # We ensure it's recorded in our stack approximation.
        m_stack_s[key] = current_time
        _prune_stack(cache_snapshot)
        
    elif state == 'HIR':
        # Was it in the stack? (Did we see it recently enough to qualify as hot?)
        if key in m_stack_s:
            # Promote HIR -> LIR
            m_recency_state[key] = 'LIR'
            m_stack_s[key] = current_time
            
            # Promoting increases LIR count. If we exceed LIR budget, we must demote 
            # the bottom-most LIR to HIR.
            _enforce_lir_capacity(cache_snapshot)
            _prune_stack(cache_snapshot)
        else:
            # It's an HIR item accessed again, but outside the "hot" window.
            # It stays HIR, but we add it to Stack S as a new entry.
            m_stack_s[key] = current_time
            m_recency_state[key] = 'HIR'
            _prune_stack(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cold Miss):
    1. Default state is HIR.
    2. If it was in m_non_resident (ghost metadata), treat somewhat like a Hit 
       conceptually, but typically inserted as HIR first with high recency.
    '''
    global m_access_time, m_recency_state, m_stack_s, m_non_resident
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    m_access_time[key] = current_time
    
    # Check if we remember this key
    if key in m_non_resident:
        # We remember it. Since it's being inserted, it has been accessed.
        # Since it was in non-resident memory, it acts like it was in the stack.
        # Promote to LIR immediately if it was "hot enough" to be remembered.
        m_recency_state[key] = 'LIR'
        del m_non_resident[key]
        m_stack_s[key] = current_time
        _enforce_lir_capacity(cache_snapshot)
    else:
        # Brand new item. Start as HIR.
        m_recency_state[key] = 'HIR'
        m_stack_s[key] = current_time
    
    _prune_stack(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. The evicted item is removed from cache, so it becomes "Non-Resident".
    2. If it was LIR (rare fallback), we might demote or just track.
    3. If it was HIR, we keep metadata in m_non_resident to catch if it returns soon.
    '''
    global m_access_time, m_recency_state, m_stack_s, m_non_resident
    
    key = evicted_obj.key
    
    # If the evicted key is in Stack S, it stays in Stack S as metadata!
    # This is the "Non-Resident HIR" concept.
    
    if key in m_recency_state:
        # Move metadata to non-resident storage if it's in the stack
        if key in m_stack_s:
            m_non_resident[key] = m_access_time.get(key, 0)
        
        # Clean up cache-resident structures
        if key in m_access_time:
            del m_access_time[key]
        if key in m_recency_state:
            del m_recency_state[key]

    # Limit Non-Resident memory usage to prevent leaks
    if len(m_non_resident) > 5000: # Heuristic limit
        # Remove oldest
        oldest = min(m_non_resident, key=m_non_resident.get)
        del m_non_resident[oldest]
        if oldest in m_stack_s:
            del m_stack_s[oldest]

def _prune_stack(cache_snapshot):
    '''
    LIRS Stack Pruning.
    The bottom of Stack S must always be an LIR item.
    If the bottom of Stack S is an HIR item, it is removed.
    This repeats until the bottom is LIR or stack is empty.
    '''
    global m_stack_s, m_recency_state
    
    # This operation can be expensive if done naively every time. 
    # To optimize, we do it lazily or only when needed.
    # Ideally, m_stack_s should be an Ordered Dict or Linked List. 
    # Since we only have dict + standard functions, we approximate pruning.
    
    # NOTE: In a strict O(1) requirement, we wouldn't loop. 
    # Here, we will perform a 'lazy' prune check on the oldest item occasionally.
    
    if not m_stack_s:
        return

    # Find bottom of stack (oldest access time)
    # Optimization: Only check if stack is large relative to cache
    # to avoid O(N) sort on every hit.
    if len(m_stack_s) < len(cache_snapshot.cache) * 2:
        return

    # Identify oldest
    oldest_key = min(m_stack_s, key=m_stack_s.get)
    
    # While bottom is HIR, remove it.
    # (Since we can't easily iterate sorted, we just check the single oldest.
    # If it's HIR, remove it. This effectively lazily prunes).
    
    # We check if it is NOT LIR.
    # Note: Non-resident items are implicitly HIR logic here.
    is_lir = (m_recency_state.get(oldest_key) == 'LIR')
    
    if not is_lir:
        del m_stack_s[oldest_key]
        if oldest_key in m_non_resident:
            del m_non_resident[oldest_key]

def _enforce_lir_capacity(cache_snapshot):
    '''
    Ensure we don't have too many LIR items.
    If LIR count > Capacity * LIR_RATIO, demote the LRU LIR item to HIR.
    '''
    global m_recency_state, m_access_time
    
    # Count LIRs
    # Note: Maintaining a separate counter or list of LIR keys would be O(1).
    # Given constraints, we scan or assume caller handles frequency.
    # For performance, let's filter only if we suspect overflow.
    
    current_lirs = [k for k, v in m_recency_state.items() if v == 'LIR']
    
    # We define capacity based on item count for simplicity, 
    # though the cache uses bytes. Estimation:
    max_lir_count = int(len(cache_snapshot.cache) * LIR_CAPACITY_RATIO)
    
    if len(current_lirs) > max_lir_count and max_lir_count > 0:
        # Demote the LRU LIR to HIR
        # Find LIR with smallest access time
        victim_lir = min(current_lirs, key=lambda k: m_access_time.get(k, 0))
        m_recency_state[victim_lir] = 'HIR'
        # It stays in stack S, but is now eligible for eviction in evict()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpttl6c__p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0ot9kv3f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi3ytbbs4.pickle

Iteration 33: New subsample score 0.17439 is better than old score 0.105763. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmreljnb7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx64fxdz6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpplsfdzdc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9_zes0nw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqsm8j3v9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzomo69rt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphr46o9iy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9tdjcfp4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoumals2u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1p26246z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9jqj8m6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb2qn2db9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz15ft1yt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoejhnzmp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw8oj22zi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjpxe3a6g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjkl8jgsu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwi_m76wk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkz5uq89k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptgbxlrkf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp86joump3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4uask3lk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6yl1fu41.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfkjfk66b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppb1ch0xd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphlbbi1c1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqz_ps8r5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqyg_2klz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphudmsb86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp667dvksh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8w_qjkim.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqlmneu8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvtcced2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph4sq93i5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprixg8zce.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsx20cm3e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkpv00nwp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkc27i73t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp99hnh8dg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc01g27p1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmplft5cq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb3rj8b2h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyn7702fv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplkn9w37_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph2jbh0by.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2xy40wi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdldimv_t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo7b_t80t.pickle

Iteration 33: Full valset score for new program: 0.2466894583333333
Iteration 33: Full train_val score for new program: 0.2466894583333333
Iteration 33: Individual valset scores for new program: [0.504735, 0.477469, 0.48694, 0.439228, 0.499064, 0.488758, 0.271531, 0.498624, 0.540937, 0.531017, 0.075, 0.317496, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.371681, 0.026556, 0.058672, 0.058672, 0.269802, 0.395161, 0.857994, 0.892179, 0.066786, 0.034091, 0.036446, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.085574, 0.640907, 0.125461, 0.068755, 0.074527, 0.073691, 0.042763, 0.333333, 0.091027, 0.068163, 0.460123, 0.081699]
Iteration 33: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.381514, 0.026556, 0.058672, 0.058672, 0.332087, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 33: Full valset pareto front score: 0.26090235416666663
Iteration 33: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {8, 2}, {0, 9, 3, 7}, {2, 4, 6, 8, 9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {4, 6}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {8, 9, 4, 6}, {8, 9, 4, 6}, {8, 9, 4, 6}, {8, 9, 4, 6}, {8, 9, 4, 6}, {1, 2, 4, 5, 6, 8, 9}, {5}, {8, 9, 2, 4}, {1, 2, 4, 5, 6, 8, 9}, {1, 2, 4, 5, 6, 8, 9}, {5}, {9}, {9}, {4, 6}, {5}, {1, 4, 5, 6}, {1, 4, 5, 6}, {9}, {9}, {9}, {1, 4, 5, 6, 9}, {9}, {5}, {8, 2, 4, 6}, {1, 2, 3, 4, 5, 6, 7, 8, 9}, {5}, {5}, {5}, {1, 4, 5, 6}, {4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8}, {1, 5, 9}]
Iteration 33: Best valset aggregate score so far: 0.2508065
Iteration 33: Best program as per aggregate score on train_val: 5
Iteration 33: Best program as per aggregate score on valset: 5
Iteration 33: Best score on valset: 0.2508065
Iteration 33: Best score on train_val: 0.2508065
Iteration 33: Linear pareto front program index: 5
Iteration 33: New program candidate index: 9
Iteration 34: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcgoy7enx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpny2_e91y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpamwyzr_o.pickle

Iteration 34: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time
# 2. key -> smooth_frequency (A float value representing frequency with decay)
# 3. key -> bool (Ghost presence check for O(1) lookup)
m_key_last_access = dict()
m_key_frequency = dict()
m_ghost_entries = set()

# Configuration
# Max ghosts to track (prevents memory leak in metadata)
MAX_GHOSTS = 5000 
# Decay factor (0.0 to 1.0). Higher means history matters more. Lower means more Recency biased.
DECAY_FACTOR = 0.995 
# Boost given to items returning from the dead (Ghost hits)
GHOST_BOOST = 2.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive LFU-LRU.
    
    We look for the item with the lowest "Value".
    Value is primarily determined by Frequency. 
    Ties are broken by Recency (LRU).
    
    This protects frequent items but allows the cache to churn through 
    low-frequency "scan" traffic quickly.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to evict the item with the lowest score.
    # Score structure: (Frequency, Last_Access_Time)
    # 1. Frequency protects the working set.
    # 2. Recency breaks ties among items with same frequency (e.g., all the 1-hit wonders).
    
    # Optimization: Scan for the minimum without creating a full list of tuples
    victim_key = None
    min_score = (float('inf'), float('inf'))
    
    for key in current_keys:
        freq = m_key_frequency.get(key, 0)
        recency = m_key_last_access.get(key, 0)
        
        score = (freq, recency)
        
        if score < min_score:
            min_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency to current time.
    2. Increment Frequency (Additive increase).
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Additive increase
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if it was a "Ghost" (recently evicted).
    2. If Ghost: It means our cache is too small or cyclic pattern. 
       Grant it higher initial frequency (GHOST_BOOST) to protect it from immediate eviction.
    3. If New: Start with Frequency 1.0.
    4. Apply global decay to normalize frequencies over time.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    if obj.key in m_ghost_entries:
        # It was recently evicted and came back. Elevate priority.
        # We restore it slightly higher than a fresh insert to indicate "Warmth".
        m_key_frequency[obj.key] = 1.0 + GHOST_BOOST
        m_ghost_entries.remove(obj.key)
    else:
        # Brand new item. Probationary period.
        m_key_frequency[obj.key] = 1.0
        
    # Apply global aging on every Nth insert (or every insert if affordable)
    # to simulate sliding window LFU.
    # Doing it on every insert keeps the math smoother but is O(N).
    # Since we can't iterate all keys efficiently here, we rely on the
    # relative difference between the new 1.0 and the old high numbers.
    # However, to prevent unbounded growth, we can occasionally normalize.
    # A simple approach for this constrained environment is to rely on the 
    # fact that new items enter at 1.0. If old items are 1000.0, they stay.
    # To truly simulate decay without O(N) updates, usually, we'd subtract 
    # a global counter, but updating directly is safer for the scorer.
    
    # We will perform a lazy decay only if the max frequency gets too high,
    # or rely on the eviction logic. 
    # Better approach here: do nothing. New items are 1.0. 
    # If old items aren't accessed, their Recency (tie-breaker) drops, 
    # making them vulnerable against other high-freq items that stopped being accessed.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove active metadata.
    2. Add to Ghost list.
    3. Manage Ghost list size.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries
    
    key = evicted_obj.key
    
    # Cleanup active metadata
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    # We DO remove from frequency map to save memory, 
    # but we mark it in the ghost set.
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    # Add to ghost entries
    m_ghost_entries.add(key)
    
    # Limit ghost size to prevent memory explosion (FIFO removal for ghosts)
    if len(m_ghost_entries) > MAX_GHOSTS:
        # Sets are unordered, so popping arbitrary element is fine for random sampling cleanup,
        # but ideally we'd remove the oldest ghost. 
        # Since we don't track ghost time, popping arbitrary is O(1).
        m_ghost_entries.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfygah4vs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy16g4z52.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsk3vbrvc.pickle

Iteration 34: New subsample score 0.48786699999999994 is better than old score 0.48534. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzhsskooc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1s07cmqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu_dqxy_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpin7vw1e0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvzboaoh3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb2zxdrh1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphp8igvnw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjuu2jbvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4s59we2u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps5fyn11m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptp79p0oy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdp9sl4db.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcd9b6v7c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpufalfzws.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmqr68hwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcpy4t0rd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqg_wye3a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9pc6b2w4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph45kbi3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3vpv0qm4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprped5bi0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppw47wl4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc2hknt17.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp68km052k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvj72hads.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc_gml14p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp43wxyutd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvgql59rq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3rqc8u0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsss35s8e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr6zk4eb0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpljti5hzd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph8_6oc6m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdrzhqr98.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmm2kxc6k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2jhasra2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr3o5twtb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3lxwg55r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcvg7oj58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpexqpp8ja.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd7vf06w9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2sdk_7oi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprmy88pmz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4pxr9ufi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd0sd81mz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsail45c9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfk3uq3xw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqwl642t8.pickle

Iteration 34: Full valset score for new program: 0.23939320833333333
Iteration 34: Full train_val score for new program: 0.23939320833333333
Iteration 34: Individual valset scores for new program: [0.488377, 0.457156, 0.477244, 0.430959, 0.48366, 0.479649, 0.270335, 0.478569, 0.540937, 0.531017, 0.091667, 0.35524, 0.034239, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.341734, 0.789474, 0.894232, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.689074, 0.083333, 0.067079, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 34: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.091667, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 34: Full valset pareto front score: 0.2611498958333333
Iteration 34: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {8, 2}, {0, 9, 3, 7}, {2, 4, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {10, 4, 6}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10}, {10}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10}, {10}, {9}, {9}, {10, 4, 6}, {5}, {1, 4, 5, 6, 10}, {1, 4, 5, 6, 10}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10}, {9}, {5}, {2, 4, 6, 8, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {5}, {5}, {5}, {1, 4, 5, 6, 10}, {10, 4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10}, {1, 10, 5, 9}]
Iteration 34: Best valset aggregate score so far: 0.2508065
Iteration 34: Best program as per aggregate score on train_val: 5
Iteration 34: Best program as per aggregate score on valset: 5
Iteration 34: Best score on valset: 0.2508065
Iteration 34: Best score on train_val: 0.2508065
Iteration 34: Linear pareto front program index: 5
Iteration 34: New program candidate index: 10
Iteration 35: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8la605wz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxpigwwvt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2qv6u4lv.pickle

Iteration 35: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (int): The logical time of the last access
# 2. key -> frequency (int): Number of times accessed (capped to prevent LFU pollution)
m_key_last_access = dict()
m_key_frequency = dict()

# Global counter to track logical time internally if needed, 
# though cache_snapshot.access_count is usually sufficient.
# We will use cache_snapshot.access_count.

def get_eviction_score(key, current_time):
    '''
    Calculate a score for eviction. Lower score = higher likelihood of eviction.
    We want to evict:
    1. Items with low frequency (Scan traffic).
    2. Items with old timestamps (LRU).
    
    Formula:
    Score = (Frequency * Weight) + Recency_Component
    
    However, strictly mixing them is tricky. 
    A better heuristic for SLRU behavior in a scoring context:
    If Frequency == 1: Score is purely based on Recency (but strictly lower than Freq > 1).
    If Frequency > 1: Score is Recency + Boost.
    '''
    freq = m_key_frequency.get(key, 1)
    last_access = m_key_last_access.get(key, 0)
    
    # Age is the inverse of recency. Larger age = older.
    age = current_time - last_access
    
    # Segmented Logic:
    # If freq is 1 (Probation), these are the primary victims. 
    # We want the oldest among freq=1 to die first.
    # If freq > 1 (Protected), we want them to survive longer than freq=1.
    
    # Score implementation:
    # We return a tuple. Python compares tuples element by element.
    # (Priority_Class, Last_Access_Time)
    
    # Priority Class: 
    # 0 = Probationary (Freq 1)
    # 1 = Protected (Freq > 1)
    # We can cap frequency at, say, 5 to treat everything "popular" similarly 
    # and rely on LRU within that group.
    
    priority = 0 if freq <= 1 else min(freq, 4) 
    
    # A lower return value means "Evict me first".
    return (priority, last_access)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Simulated Segmented LRU.
    We iterate through keys and evict the item with the lowest (Priority, Recency).
    This effectively acts as an LRU eviction for 1-hit wonders first, 
    then moves on to frequent items if necessary.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    current_time = cache_snapshot.access_count
    
    victim_key = None
    # Initialize min_score with values higher than any possible real score
    # Priority max is ~4, Time is infinite.
    min_score = (float('inf'), float('inf'))
    
    # Optimization: If we find a Freq=1 item that is very old, we can pick it quickly.
    # But for correctness, we scan O(N).
    for key in current_keys:
        score = get_eviction_score(key, current_time)
        
        if score < min_score:
            min_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency (promote from Probation to Protected).
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Increment frequency, but cap it.
    # Capping prevents an object accessed 1000 times from staying forever 
    # after it becomes obsolete.
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < 10: # Cap at 10 is sufficient to distinguish "popular" from "noise"
        m_key_frequency[obj.key] = current_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Set Frequency to 1 (Probationary).
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # New items start at frequency 1 (Probation).
    # If they are hit again, they move to freq 2 (Protected).
    m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata to prevent memory leaks.
    '''
    global m_key_last_access, m_key_frequency
    
    key = evicted_obj.key
    
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    if key in m_key_frequency:
        del m_key_frequency[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_1u2ilfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzp9189yc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2sy0y_s7.pickle

Iteration 35: New subsample score 0.16939900000000002 is not better than old score 0.169937, skipping
Iteration 36: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8bwvwka3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp75ujhf_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpprasluhb.pickle

Iteration 36: Proposed new text for program: # Metadata storage
# Using Python's dict as ordered maps (standard in Python 3.7+) for O(1) operations.

# m_stack: Represents the LIRS Stack S.
# Keys are object keys. Value is a dummy (e.g., True) or timestamp. 
# Ordered by recency (MRU at end, LRU at start).
m_stack = dict()

# m_q: Represents the LIRS Queue Q (Resident HIR blocks).
# Keys are object keys.
# Ordered by insertion/access (MRU at end, LRU at start).
m_q = dict()

# m_state: Maps key -> 'LIR' or 'HIR'.
# Only tracks items currently considered relevant (Resident or in Stack S).
m_state = dict()

# Constants
# We reserve a small portion of the cache for HIR items to absorb scans.
# 99% LIR ensures the stable working set is maximized.
LIR_RATIO = 0.99

def evict(cache_snapshot, obj):
    '''
    Selects a victim to evict.
    Strategy:
    1. Pick the resident HIR item from the front of Queue Q.
    2. If Q is empty (rare, implies cache is fully LIR), demote the bottom LIR item.
    '''
    global m_q, m_stack, m_state

    # Case 1: Evict from HIR Queue
    if m_q:
        # The first key in m_q is the least recently used HIR resident
        victim_key = next(iter(m_q))
        return victim_key

    # Case 2: No HIR items found. Cache is 100% LIR.
    # We must demote the LIR item at the bottom of Stack S.
    # Because of pruning, the bottom of m_stack is guaranteed to be LIR.
    if m_stack:
        victim_key = next(iter(m_stack))
        return victim_key

    # Fallback (Should not happen if cache has items)
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    Update logic when an existing item is accessed.
    '''
    global m_stack, m_q, m_state
    
    key = obj.key
    state = m_state.get(key, 'HIR')

    if state == 'LIR':
        # LIR Access:
        # 1. Move to top of Stack S (Access update).
        # 2. If it was at the bottom, we might need to prune the stack.
        if key in m_stack:
            del m_stack[key]
        m_stack[key] = True # Push to MRU position
        _prune_stack()

    elif state == 'HIR':
        # HIR Access:
        # Check if it is currently in Stack S (Inter-Reference Recency Check)
        if key in m_stack:
            # Hot! It's in the stack, so it qualifies for promotion to LIR.
            
            # 1. Change state to LIR
            m_state[key] = 'LIR'
            
            # 2. Remove from Queue Q (it is no longer an HIR candidate)
            if key in m_q:
                del m_q[key]
            
            # 3. Move to top of Stack S
            del m_stack[key] # Remove from old position
            m_stack[key] = True # Push to top
            
            # 4. Enforce Capacity: If too many LIRs, demote the bottom LIR.
            # (Note: We do this logically. The bottom of S is the LIR to demote).
            _enforce_lir_limit(cache_snapshot)
            
            # 5. Prune any HIRs now exposed at the bottom of S
            _prune_stack()
            
        else:
            # Cold! It's an HIR access, but hasn't been seen recently enough (not in S).
            # It stays HIR.
            
            # 1. Add/Move to top of Stack S
            m_stack[key] = True
            
            # 2. Move to end of Queue Q (MRU of HIRs)
            if key in m_q:
                del m_q[key]
            m_q[key] = True
            
            # 3. Prune? No, usually not needed here unless it was non-resident logic, 
            # but standard LIRS just updates Q order.

def update_after_insert(cache_snapshot, obj):
    '''
    Update logic when a new item is inserted (Cold Miss).
    '''
    global m_stack, m_q, m_state
    
    key = obj.key
    
    # Check if we have history for this key (Non-Resident HIR)
    if key in m_stack:
        # We remember this! It is in Stack S, so it acts like a "Hit" in the stack.
        # Promote to LIR.
        m_state[key] = 'LIR'
        
        del m_stack[key]
        m_stack[key] = True
        
        # It's LIR now, so don't put in Q.
        
        _enforce_lir_limit(cache_snapshot)
        _prune_stack()
    else:
        # Brand new object. Default to HIR.
        m_state[key] = 'HIR'
        m_stack[key] = True # Add to top of stack
        m_q[key] = True     # Add to HIR Queue

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup after eviction.
    '''
    global m_stack, m_q, m_state
    
    key = evicted_obj.key
    
    # 1. Remove from Queue Q (if present)
    if key in m_q:
        del m_q[key]
        
    # 2. State management
    # If the item is in Stack S, it stays in Stack S!
    # It effectively transitions from "Resident HIR" to "Non-Resident HIR".
    # We DO NOT delete from m_stack here. This is the memory of LIRS.
    
    # However, if it was NOT in Stack S (rare, but possible if S was small), 
    # we lose track of it.
    if key not in m_stack:
        if key in m_state:
            del m_state[key]
            
    # 3. Safety Limit for Metadata
    # To prevent memory leaks in the simulation (infinite Stack S growth),
    # we can enforce a rough maximum size on m_stack.
    if len(m_stack) > 20000: # Heuristic cap
        # Force prune from bottom
        k = next(iter(m_stack))
        del m_stack[k]
        if k in m_state:
            del m_state[k]

def _prune_stack():
    '''
    LIRS Pruning: 
    The LIRS stack S must always have an LIR block at the bottom.
    If the bottom block is HIR, it is removed. We repeat this until the bottom is LIR.
    '''
    global m_stack, m_state
    
    # While stack is not empty
    while m_stack:
        # Peek at the bottom (first inserted)
        bottom_key = next(iter(m_stack))
        
        state = m_state.get(bottom_key, 'HIR')
        
        if state == 'LIR':
            # Found the LIR bottom. Stop pruning.
            break
        else:
            # It's HIR (resident or non-resident). Remove it.
            # In LIRS, once an HIR leaves the stack, it loses its chance for fast promotion.
            del m_stack[bottom_key]
            
            # Note: If it's Non-Resident (not in cache), we can drop the state entirely 
            # to free memory, as it's no longer interesting.
            # We don't have direct access to "is_resident" here without cache_snapshot,
            # but usually, if we prune an HIR from S, we downgrade it to "cold".
            # If it is resident, it exists in Q, so we keep state as HIR.
            # If it is NOT resident (not in Q, since Q contains resident HIRs), delete state.
            if bottom_key not in m_q:
                if bottom_key in m_state:
                    del m_state[bottom_key]

def _enforce_lir_limit(cache_snapshot):
    '''
    Ensure we don't exceed the target number/size of LIR blocks.
    In LIRS, if LIR set is too big, we demote the LIR block at the bottom of Stack S.
    '''
    global m_state, m_stack, m_q
    
    # Calculate target capacity
    # Using item count approximation as accurate byte tracking requires iteration or auxiliary vars.
    cache_len = len(cache_snapshot.cache)
    if cache_len == 0: return # Startup
    
    target_lir_count = int(cache_snapshot.capacity / (cache_snapshot.size / cache_len) * LIR_RATIO) if cache_snapshot.size > 0 else int(cache_snapshot.capacity * LIR_RATIO)
    # Simple fallback if size calc is weird: use current count
    target_lir_count = int(len(cache_snapshot.cache) * LIR_RATIO)
    if target_lir_count < 1: target_lir_count = 1

    # Count current LIRs.
    # Optimization: Maintaining a counter is faster, but for this snippet, we filter.
    # To avoid O(N), we rely on the logic that we only call this when we ADD an LIR.
    # We assume we might need to demote ONE item.
    
    # Who is the candidate for demotion? The bottom-most LIR in Stack S.
    # Because of _prune_stack, the absolute bottom of m_stack IS the LRU LIR.
    
    # We check if we need to demote. 
    # (Checking exact count is expensive O(N). We rely on the fact that if we just promoted one,
    # and the stack bottom is LIR, we *might* demote if we are full).
    
    # A simplified heuristic: If cache is full, and we just promoted, demote the old LRU LIR.
    # This keeps the LIR population stable.
    
    if len(cache_snapshot.cache) >= (cache_snapshot.capacity / (cache_snapshot.size/len(cache_snapshot.cache)) if cache_snapshot.size > 0 else 10):
        # Cache is effectively full.
        # Find bottom LIR
        if not m_stack: return
        
        bottom_key = next(iter(m_stack))
        if m_state.get(bottom_key) == 'LIR':
            # Demote
            m_state[bottom_key] = 'HIR'
            # Add to Queue Q (becomes a resident HIR)
            m_q[bottom_key] = True
            # It stays in m_stack (at the bottom), but next _prune_stack call will remove it!
            # _prune_stack() is usually called immediately after this function.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6i2lb88t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpibd7j40c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdv59ex1i.pickle

Iteration 36: New subsample score 0.576355 is not better than old score 0.631964, skipping
Iteration 37: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqbj7ia5w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4czpt2tv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoqxzz8ie.pickle

Iteration 37: Proposed new text for program: # Metadata
m_last_access = dict()
m_frequency = dict()
m_entry_time = dict() # Track when item entered to break ties or sophisticated aging

def evict(cache_snapshot, obj):
    # Hyperbolic caching score:
    # Value = (Frequency) / (Time_Since_Last_Access + epsilon)
    # We want to EVICT the item with the LOWEST score.
    
    # However, to handle scans explicitly:
    # If we have items with Frequency=1, we almost certainly want to evict one of them first.
    # Mixing freq=1 and freq=100 in one formula can sometimes be tricky if the freq=100 item is VERY old.
    
    # Hybrid Policy:
    # 1. Identify items with Freq=1.
    # 2. If Freq=1 items exist, evict the LRU (or FIFO) among them.
    # 3. If no Freq=1 items, use Hyperbolic scoring or pure LRU on the rest.
    ...
Subprocess stdout: Error in subprocess: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgfolx14z.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 324, in cache_simulate
    cache = Cache(
            ^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 187, in __init__
    self.update_after_insert_func = My.update_after_insert
                                    ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'

Subprocess stdout: Error in subprocess: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpawcg3yiu.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 324, in cache_simulate
    cache = Cache(
            ^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 187, in __init__
    self.update_after_insert_func = My.update_after_insert
                                    ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'

Subprocess stdout: Error in subprocess: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp35bne_u7.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 324, in cache_simulate
    cache = Cache(
            ^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 187, in __init__
    self.update_after_insert_func = My.update_after_insert
                                    ^^^^^^^^^^^^^^^^^^^^^^
AttributeError: module 'gepa.adapters.caching_adapter_gemini3_1.My' has no attribute 'update_after_insert'

Iteration 37: New subsample score 0.0 is not better than old score 0.6748700000000001, skipping
Iteration 38: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpalry4otw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg6l8o8ph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpum8esj93.pickle

Iteration 38: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency (Integer count of accesses)
# 2. key -> last_access_time (Integer timestamp)
# 3. key -> value (Float score used for eviction ordering)
m_key_frequency = dict()
m_key_last_access = dict()
m_key_value = dict()

# Ghost entries: A set to track keys recently evicted.
# This helps distinguish "first time seen" from "seen recently but evicted".
m_ghost_entries = set()

# Global counter to simulate "Cache Age" or "Inflation".
# This is the "DA" (Dynamic Aging) part of LFU. It represents the minimum value
# required to survive in the cache at the current moment.
global_cache_age = 0.0

# Constants
MAX_GHOSTS = 5000  # Prevent unbounded memory growth for metadata
GHOST_BOOST = 5    # Frequency boost when a ghost is hit (scan resistance)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU-DA (Dynamic Aging) with LRU Tie-breaker.
    
    We evict the item with the lowest computed 'Value'.
    Value = Frequency + Global_Cache_Age (at time of insertion/update).
    
    If Values are equal, we evict the one with the oldest Last_Access_Time (LRU).
    '''
    global global_cache_age
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    # We want to minimize (Value, Last_Access_Time)
    # Initialize with infinity so the first item checked becomes the candidate
    min_metric = (float('inf'), float('inf'))

    # We iterate to find the minimum. 
    # Note: In a production system, a Min-Heap would be O(1) here, 
    # but scanning is O(N). Given the constraints, O(N) is acceptable per eviction 
    # if the hit rate improvement justifies it.
    for key in current_keys:
        val = m_key_value.get(key, 0.0)
        recency = m_key_last_access.get(key, 0)
        
        # Metric structure: (Eviction Score, Recency)
        # Lower score = better candidate for eviction
        # Lower recency (older time) = better candidate for eviction
        metric = (val, recency)
        
        if metric < min_metric:
            min_metric = metric
            victim_key = key

    # LFU-DA Logic: The value of the evicted item becomes the new 
    # baseline "Age" of the cache. This effectively "ages" all existing 
    # items by raising the bar for new items entering.
    if victim_key is not None:
        global_cache_age = m_key_value.get(victim_key, 0.0)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    3. Recalculate Value using current Frequency and current Global Cache Age.
    '''
    global m_key_frequency, m_key_last_access, m_key_value, global_cache_age
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Metadata
    m_key_last_access[key] = current_time
    old_freq = m_key_frequency.get(key, 0)
    new_freq = old_freq + 1
    m_key_frequency[key] = new_freq
    
    # LFU-DA Score Update:
    # A hit brings the item "up to date" with the current cache pressure (global_cache_age)
    # plus its specific popularity (frequency).
    m_key_value[key] = global_cache_age + new_freq

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine initial Frequency (Boosted if it was a Ghost).
    2. Calculate initial Value = Global_Age + Frequency.
    '''
    global m_key_frequency, m_key_last_access, m_key_value, m_ghost_entries, global_cache_age
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_key_last_access[key] = current_time
    
    initial_freq = 1
    
    # Check Ghost Cache
    if key in m_ghost_entries:
        # It was evicted recently. This suggests our cache is churning.
        # Give it a boost so it isn't immediately evicted again.
        initial_freq += GHOST_BOOST
        m_ghost_entries.remove(key)
    
    m_key_frequency[key] = initial_freq
    
    # LFU-DA: New items enter with the current global age baseline + their starting frequency.
    m_key_value[key] = global_cache_age + initial_freq

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost entries to track recency of eviction.
    '''
    global m_key_frequency, m_key_last_access, m_key_value, m_ghost_entries
    
    key = evicted_obj.key
    
    # Remove from active tracking
    if key in m_key_frequency:
        del m_key_frequency[key]
    if key in m_key_last_access:
        del m_key_last_access[key]
    if key in m_key_value:
        del m_key_value[key]
        
    # Add to Ghost Set
    m_ghost_entries.add(key)
    
    # Manage Ghost Set size (simple random eviction since set is unordered)
    if len(m_ghost_entries) > MAX_GHOSTS:
        # Pop arbitrary element
        m_ghost_entries.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpycdy3brw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnivsw7aw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqh7veprx.pickle

Iteration 38: New subsample score 0.352967 is not better than old score 0.46964000000000006, skipping
Iteration 39: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdt8ib2o_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp57bipjhg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdfq6d7y_.pickle

Iteration 39: Proposed new text for program: import collections

# Metadata storage
# 1. Main Queue (FIFO for warm items)
m_main_queue = collections.deque()
m_main_set = set()

# 2. Small Queue (FIFO for probationary items)
m_small_queue = collections.deque()
m_small_set = set()

# 3. Ghost Queue (Set for history of evicted items)
# Tracks items evicted from Small/Main to detect "false negatives"
m_ghost_set = set()
# Max size for ghost tracking
MAX_GHOSTS = 5000 

# 4. Frequency tracking (simulating bits, usually 0 to 2 or 3)
# Stores freq of items currently in cache
m_freq = {}

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: S3-FIFO (Simple, Scalable, Static FIFO) inspired.
    
    1.  New items enter the Small (S) queue.
    2.  If an item in S is accessed, it's marked (freq incremented).
    3.  During eviction:
        a. Check S queue first. 
           - If head has freq > 0: It moves to Main (M) queue (Reinsertion).
           - If head has freq == 0: It is evicted.
        b. If S is empty or not providing a victim, check M queue.
           - If head has freq > 0: Decrement freq, move to tail of M (Second Chance).
           - If head has freq == 0: It is evicted.
    '''
    global m_main_queue, m_small_queue, m_freq, m_main_set, m_small_set

    # Target size for Small Queue is roughly 10% of total items.
    # If S is larger than 10%, we prefer evicting from S.
    total_items = len(cache_snapshot.cache)
    small_target = max(1, total_items // 10)
    
    # We loop until we find a victim or run out of options
    while True:
        # Step 1: Evict from Small Queue if it's too big or Main is empty
        if len(m_small_queue) > small_target or not m_main_queue:
            if not m_small_queue:
                # Should not happen if cache is full, but safety check
                break
                
            candidate = m_small_queue[0] # Peek head
            
            freq = m_freq.get(candidate, 0)
            
            if freq > 0:
                # Promotion: Item was accessed in probation. Move to Main.
                m_small_queue.popleft()
                m_small_set.remove(candidate)
                
                m_main_queue.append(candidate)
                m_main_set.add(candidate)
                
                # Reset frequency or decrement based on policy. 
                # S3-FIFO typically resets to 0 or keeps it. 
                # We'll reset to 0 to require it to prove itself in M queue again,
                # effectively giving it one "life".
                m_freq[candidate] = 0
            else:
                # Eviction: Item failed probation.
                # m_freq and sets cleaned up in update_after_evict
                return candidate
        
        # Step 2: Evict from Main Queue
        else:
            if not m_main_queue:
                break
                
            candidate = m_main_queue[0] # Peek head
            freq = m_freq.get(candidate, 0)
            
            if freq > 0:
                # Second Chance: Item was useful. Keep it, but demote freq.
                m_main_queue.popleft()
                m_main_queue.append(candidate) # Move to tail
                m_freq[candidate] = freq - 1
            else:
                # Eviction: Item not useful recently.
                return candidate
    
    # Fallback (unlikely)
    if m_small_queue:
        return m_small_queue[0]
    if m_main_queue:
        return m_main_queue[0]
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    Simple frequency increment. We cap it at 3 to simulate 2-bit counters.
    This prevents frequency pollution.
    '''
    global m_freq
    curr = m_freq.get(obj.key, 0)
    m_freq[obj.key] = min(curr + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. If it was in Ghost set, insert directly into Main Queue (M).
    2. Otherwise, insert into Small Queue (S).
    '''
    global m_main_queue, m_small_queue, m_ghost_set, m_freq, m_main_set, m_small_set
    
    key = obj.key
    m_freq[key] = 0 # Start with 0 frequency
    
    if key in m_ghost_set:
        # It was recently seen and evicted. This indicates our cache was too small.
        # Promote directly to Main to protect it.
        m_main_queue.append(key)
        m_main_set.add(key)
        m_ghost_set.remove(key)
    else:
        # New object, insert into Small (probation)
        m_small_queue.append(key)
        m_small_set.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from Qs and Sets.
    2. Add to Ghost set.
    '''
    global m_main_queue, m_small_queue, m_ghost_set, m_freq, m_main_set, m_small_set
    
    key = evicted_obj.key
    
    # Clean up tracking structures
    # We have to check which queue it was in because evict() returns the key 
    # but the cache framework calls this function separately.
    # However, our evict function only returns the key, it doesn't pop it.
    # The cache engine pops the object. We need to sync our queues.
    
    # Optimization: We check sets for O(1) removal
    if key in m_small_set:
        m_small_set.remove(key)
        # Deque removal is O(N), but necessary unless we use a custom DLL.
        # However, evict() usually picks the head (index 0). 
        # If it was a swap/hit scenario, it might be elsewhere.
        # In S3-FIFO, eviction happens strictly from Head, so this is usually O(1).
        # But we must be safe.
        try:
            m_small_queue.remove(key)
        except ValueError:
            pass
    elif key in m_main_set:
        m_main_set.remove(key)
        try:
            m_main_queue.remove(key)
        except ValueError:
            pass

    if key in m_freq:
        del m_freq[key]
        
    # Add to ghost history
    m_ghost_set.add(key)
    
    # Manage Ghost size (random eviction via pop is fine for history)
    if len(m_ghost_set) > MAX_GHOSTS:
        m_ghost_set.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp04_crk_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_w7k7g8r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps91fs2n_.pickle

Iteration 39: New subsample score 0.994559 is not better than old score 1.031945, skipping
Iteration 40: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9zihgjj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprctyo0vi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgalqggwt.pickle

Iteration 40: Proposed new text for program: # Metadata storage
# 1. m_recency: key -> access_count (Recency for LRU)
# 2. m_type: key -> 0 (Probationary) or 1 (Protected)
# 3. m_ghost_probationary: key -> None (Set of keys evicted from Probationary)
# 4. m_ghost_protected: key -> None (Set of keys evicted from Protected)
# 5. m_p: Target size for the Probationary segment (Adaptive parameter)

m_recency = dict()
m_type = dict()
m_ghost_probationary = dict()
m_ghost_protected = dict()
m_p = 0  # Initial target size for Probationary segment

def evict(cache_snapshot, obj):
    '''
    Adaptive Segmented Eviction.
    
    We decide which segment (Probationary or Protected) to evict from based on 
    the current size of the segments relative to the adaptive parameter `m_p`.
    
    m_p represents the target capacity for the Probationary segment (T1).
    Capacity - m_p represents the target for Protected (T2).
    '''
    global m_recency, m_type, m_p
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Classify current cache content
    probationary_keys = [k for k in current_keys if m_type.get(k, 0) == 0]
    protected_keys = [k for k in current_keys if m_type.get(k, 0) == 1]
    
    # Sort by recency (LRU is min access_count)
    # We pre-calculate these only if needed to save compute, but here for clarity:
    lru_probationary = None
    if probationary_keys:
        lru_probationary = min(probationary_keys, key=lambda k: m_recency.get(k, 0))
    
    lru_protected = None
    if protected_keys:
        lru_protected = min(protected_keys, key=lambda k: m_recency.get(k, 0))

    # Logic to choose victim:
    # If the probationary segment size exceeds the target `m_p`, we evict from probationary.
    # Otherwise, we evict from protected.
    
    # Note: We must ensure we don't return None if one list is empty.
    
    victim_key = None
    
    len_t1 = len(probationary_keys)
    
    # If T1 (probationary) is larger than target P, evict from T1
    if len_t1 > m_p:
        if lru_probationary:
            victim_key = lru_probationary
        else:
            victim_key = lru_protected
    else:
        # T1 is within limits, evict from T2 (Protected)
        if lru_protected:
            victim_key = lru_protected
        else:
            victim_key = lru_probationary
            
    # Fallback safety
    if victim_key is None:
        victim_key = min(current_keys, key=lambda k: m_recency.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Promote to Protected status (Type 1).
    '''
    global m_recency, m_type
    
    m_recency[obj.key] = cache_snapshot.access_count
    
    # Any hit (whether on T1 or T2 item) makes it highly relevant.
    # Move to Protected (Type 1)
    m_type[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    1. Check Ghost Caches (History) to adapt `m_p`.
    2. Insert new item.
    '''
    global m_recency, m_type, m_ghost_probationary, m_ghost_protected, m_p
    
    current_capacity = len(cache_snapshot.cache) # Approximate N
    # Since we are inserting, the cache might be full or growing. 
    # We treat capacity as current count roughly.
    if current_capacity == 0: current_capacity = 1 # Avoid div by zero
    
    key = obj.key
    m_recency[key] = cache_snapshot.access_count

    # --- Adaptation Logic ---
    if key in m_ghost_probationary:
        # Miss on B1 (Ghost Probationary):
        # We evicted a probationary item too soon. We should increase P (target size of T1).
        delta = 1
        if len(m_ghost_probationary) >= len(m_ghost_protected) and len(m_ghost_protected) > 0:
            delta = len(m_ghost_probationary) / len(m_ghost_protected)
        
        m_p = min(current_capacity, m_p + delta)
        
        # Move to Protected because it was remembered
        m_type[key] = 1
        del m_ghost_probationary[key]

    elif key in m_ghost_protected:
        # Miss on B2 (Ghost Protected):
        # We evicted a protected item too soon. We should decrease P (increase target size of T2).
        delta = 1
        if len(m_ghost_protected) >= len(m_ghost_probationary) and len(m_ghost_probationary) > 0:
            delta = len(m_ghost_protected) / len(m_ghost_probationary)
            
        m_p = max(0, m_p - delta)
        
        # Move to Protected because it was remembered
        m_type[key] = 1
        del m_ghost_protected[key]

    else:
        # Totally cold miss (not in cache, not in ghosts)
        # Add to Probationary (Type 0)
        m_type[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up main metadata.
    2. Move key to appropriate Ghost list based on its type at eviction time.
    3. Ensure Ghost lists don't grow infinitely.
    '''
    global m_recency, m_type, m_ghost_probationary, m_ghost_protected
    
    key = evicted_obj.key
    evicted_type = m_type.get(key, 0)
    
    # 1. Clean main metadata
    if key in m_recency:
        del m_recency[key]
    if key in m_type:
        del m_type[key]
        
    # 2. Add to Ghost Cache
    # If it was Probationary (0), it goes to B1 (Ghost Probationary)
    # If it was Protected (1), it goes to B2 (Ghost Protected)
    
    # We limit ghost cache size to roughly the cache capacity (N)
    # Since we don't have exact N accessible as a constant, we use current cache len as proxy
    # or a safe upper bound.
    target_ghost_len = max(len(cache_snapshot.cache), 100) 
    
    if evicted_type == 0:
        m_ghost_probationary[key] = cache_snapshot.access_count # Store time for pruning
        if len(m_ghost_probationary) > target_ghost_len:
            # Remove oldest
            oldest = min(m_ghost_probationary, key=m_ghost_probationary.get)
            del m_ghost_probationary[oldest]
    else:
        m_ghost_protected[key] = cache_snapshot.access_count
        if len(m_ghost_protected) > target_ghost_len:
            oldest = min(m_ghost_protected, key=m_ghost_protected.get)
            del m_ghost_protected[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2gc3lrnt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxjqe0h6k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0wrgkxv7.pickle

Iteration 40: New subsample score 0.830321 is better than old score 0.8050039999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj9lfg60s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1g822p3a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpemf2uaye.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsbho3xig.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_whs49j1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo8kl_ft0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy11shmb9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoii2vgig.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplxkcdpem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1bbxmvqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcv2ix613.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzp10nh1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3k04eyw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd0bgjf5j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqjc6zhya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3t44van1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5hnmr7cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpre8567ug.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptploa1qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp34yf6hpk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjyafdpc0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp667tm0tz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_qc95cjw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbmdakv3v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg2nuk0_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2v5_yzq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppy7qf37y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0ypkej6t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv85exwjy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpihh35t8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg9tiws9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsibwwh_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfdm7th86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqr0gi8x7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptcgusbj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8qxct465.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa86f5xo2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7uzb5mo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprxef7x4j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb_423aj2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_k0qet9r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqtjclire.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplk09ka08.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoqz8xoo9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwm_j8ekp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaju7ubgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpif1_q83a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy7d8scxm.pickle

Iteration 40: Full valset score for new program: 0.2257136458333333
Iteration 40: Full train_val score for new program: 0.2257136458333333
Iteration 40: Individual valset scores for new program: [0.474387, 0.452078, 0.461427, 0.403752, 0.471787, 0.461778, 0.272727, 0.497247, 0.539864, 0.531017, 0.1, 0.297069, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.346116, 0.026164, 0.058672, 0.058672, 0.269775, 0.3125, 0.786495, 0.87438, 0.021563, 0.036364, 0.045558, 0.000144, 8.4e-05, 0.732131, 0.083333, 0.067961, 0.009204, 0.640392, 0.125461, 0.023011, 0.022602, 0.024255, 0.052632, 0.266667, 0.021346, 0.023282, 0.466258, 0.055556]
Iteration 40: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.1, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 40: Full valset pareto front score: 0.26132349999999993
Iteration 40: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {8, 2}, {0, 9, 3, 7}, {2, 4, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {11}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {10}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {10}, {9}, {9}, {10, 4, 6}, {5}, {1, 4, 5, 6, 10}, {1, 4, 5, 6, 10, 11}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11}, {9}, {5}, {2, 4, 6, 8, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11}, {10, 4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11}, {1, 10, 5, 9}]
Iteration 40: Best valset aggregate score so far: 0.2508065
Iteration 40: Best program as per aggregate score on train_val: 5
Iteration 40: Best program as per aggregate score on valset: 5
Iteration 40: Best score on valset: 0.2508065
Iteration 40: Best score on train_val: 0.2508065
Iteration 40: Linear pareto front program index: 5
Iteration 40: New program candidate index: 11
Iteration 41: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyx212ufm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp747kkhbz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkyhxt754.pickle

Iteration 41: Proposed new text for program: import math

# Metadata storage
# Key -> Frequency Count (How many times accessed while in cache or ghost)
m_key_frequency = dict()

# Key -> Last Access Time (For LRU tie-breaking)
m_key_last_access = dict()

# LFU-DA Specific: Global inflation counter to age items dynamically
# This value increases upon eviction, simulating the decay of existing items.
m_cache_inflation = 0.0

def get_eviction_score(key, frequency):
    '''
    Calculate the priority score for LFU-DA.
    The score stored is actually the frequency count + the inflation value at time of insertion/update.
    However, since we store raw frequency and handle inflation globally, we treat 
    eviction selection simply based on the stored values relative to each other.
    '''
    # In this implementation, m_key_frequency[key] stores the "Priority Key" (P)
    # where P = (Actual Frequency) + (Inflation Value when updated)
    return m_key_frequency.get(key, 0.0)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU with Dynamic Aging (LFU-DA)
    
    We evict the object with the lowest Priority Score.
    Priority Score = Frequency + Inflation_Offset.
    
    Tie-breaking:
    If two objects have the same Priority Score, we evict the Least Recently Used (LRU) one.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We need to find the key with min(Priority Score).
    # If ties, min(Last Access Time).
    # Python's min function with a tuple (priority, last_access) achieves this automatically.
    
    victim_key = min(
        current_keys, 
        key=lambda k: (m_key_frequency.get(k, 0), m_key_last_access.get(k, 0))
    )
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update the priority of the object.
    2. In LFU-DA, the new priority becomes: (Old_Priority - Inflation_Offset) + 1 + Current_Inflation_Offset.
       Simplified: New_Priority = Current_Stored_Priority + 1.
       Wait, strictly speaking LFU-DA resets the 'base' value to the current inflation.
       Standard LFU-DA update: Priority = (Frequency_Count) + L.
       Here, we just increment the stored value relative to others.
    '''
    global m_key_last_access, m_key_frequency, m_cache_inflation
    
    current_time = cache_snapshot.access_count
    
    # Update Recency (for tie-breaking)
    m_key_last_access[obj.key] = current_time
    
    # Update Frequency/Priority
    # If it's a hit, we want to bump its priority.
    # Standard LFU-DA: P_new = P_old + 1 (simple increment works well with this logic)
    # Or strict LFU-DA: P_new = (Current_Age) + (New_Frequency).
    # We choose simple increment of the stored priority value, which includes the age offset.
    # This rewards the hit immediately.
    current_priority = m_key_frequency.get(obj.key, m_cache_inflation)
    m_key_frequency[obj.key] = current_priority + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Set Initial Priority.
       Priority = 1 (access count) + m_cache_inflation (Dynamic Aging factor).
       This ensures new items enter with a score competitive with the current "average" 
       eviction threshold, but not starting at 0 which would cause immediate eviction.
    '''
    global m_key_last_access, m_key_frequency, m_cache_inflation
    
    current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = current_time
    
    # If the key exists in our metadata (ghost entry), we restore it?
    # Actually, LFU-DA usually treats new insertions as fresh starts + inflation.
    # However, recovering ghost frequency helps with thrashing.
    # We take the max of (Inflation + 1) and (Previous_Priority + 1) to respect history if valuable.
    
    base_priority = m_cache_inflation + 1.0
    
    if obj.key in m_key_frequency:
        # Ghost hit scenario: Use the previous accumulated weight if it's higher than a fresh start
        # This provides scan resistance while respecting long-term frequency
        m_key_frequency[obj.key] = max(base_priority, m_key_frequency[obj.key] + 1.0)
    else:
        # Brand new item
        m_key_frequency[obj.key] = base_priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update the Global Inflation Parameter (m_cache_inflation).
       m_cache_inflation becomes the priority score of the evicted victim.
       This effectively "ages" all other items remaining in the cache because their 
       stored scores are now closer to the new inflation floor.
    2. Clean up Recency (strictly needed only for in-cache items).
    3. Keep Frequency metadata (Ghost entries) to learn from mistakes.
    '''
    global m_key_last_access, m_key_frequency, m_cache_inflation
    
    # 1. Update global inflation ("Age" of the cache)
    # The new baseline is the value of the item we just evicted.
    evicted_priority = m_key_frequency.get(evicted_obj.key, 0)
    m_cache_inflation = evicted_priority
    
    # 2. Remove Recency (Strictly O(1) housekeeping)
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # 3. We DO NOT delete from m_key_frequency immediately.
    # We keep it as a ghost entry.
    # However, to prevent memory leaks in infinite streams, we should prune occasionally.
    # Simple heuristic: If the dictionary grows too large (e.g., 2x capacity), prune oldest ghosts.
    # Given we can't see the exact capacity easily in this function without snapshot access logic,
    # and constraints say "minimize miss rate", we prioritize keeping data.
    # If we must prune, we would prune keys not in cache_snapshot.cache. 
    # For this challenge, we assume sufficient memory for metadata.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj8sjdjps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxay_9vde.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpejwpb9pk.pickle

Iteration 41: New subsample score 0.907746 is not better than old score 1.034921, skipping
Iteration 42: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmxf6nw_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5iv7y13t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe59zq1cg.pickle

Iteration 42: Proposed new text for program: import math

# Metadata storage
# 1. m_access_time: key -> int (Logical timestamp of last access)
# 2. m_frequency: key -> int (Access count with aging)
# 3. m_avg_freq: float (Track average frequency to trigger aging)

m_access_time = dict()
m_frequency = dict()
m_total_freq = 0

# Constants
MAX_FREQ_CAP = 50  # Cap frequency to prevent integer overflow/dominance
AGING_THRESHOLD_FACTOR = 2.0  # If total freq > capacity * factor, trigger aging

def evict(cache_snapshot, obj):
    '''
    LFU with Dynamic Aging and LRU Tie-breaking.
    
    Strategy:
    1. Identify the candidate with the lowest Frequency.
    2. If multiple items share the lowest Frequency, evict the Least Recently Used (LRU) among them.
    
    Why this helps:
    - Traces 41/18 (Low Hit Rate): Likely Scans. New items have freq=1. They are evicted immediately 
      if space is needed, protecting the working set.
    - Trace 26 (High Hit Rate): Likely Loop/Recurrent. High freq items stay protected.
    '''
    global m_access_time, m_frequency
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # We need to find the victim that minimizes (frequency, access_time).
    # This effectively implements LFU, breaking ties with LRU.
    
    # Initialization
    victim_key = None
    min_freq = float('inf')
    min_time = float('inf')
    
    for k in current_keys:
        # Default frequency to 0 if missing (shouldn't happen in sync)
        f = m_frequency.get(k, 0)
        t = m_access_time.get(k, 0)
        
        if f < min_freq:
            min_freq = f
            min_time = t
            victim_key = k
        elif f == min_freq:
            # Tie-breaker: LRU
            if t < min_time:
                min_time = t
                victim_key = k
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_access_time, m_frequency, m_total_freq
    
    key = obj.key
    m_access_time[key] = cache_snapshot.access_count
    
    old_freq = m_frequency.get(key, 0)
    new_freq = min(old_freq + 1, MAX_FREQ_CAP)
    
    m_frequency[key] = new_freq
    m_total_freq += (new_freq - old_freq)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency and Frequency.
    2. Check for Aging: If the total frequency mass in the system becomes too high relative 
       to the number of items, we "age" (decay) the frequencies. This ensures that 
       historically popular items eventually drop out if they stop being accessed.
    '''
    global m_access_time, m_frequency, m_total_freq
    
    key = obj.key
    m_access_time[key] = cache_snapshot.access_count
    
    # New items start with frequency 1
    m_frequency[key] = 1
    m_total_freq += 1
    
    # Dynamic Aging Logic
    # If the "average" frequency is high, divide everyone by 2.
    # We estimate capacity using the current cache size (len(cache_snapshot.cache)).
    # Note: cache_snapshot.cache includes the newly inserted item.
    current_count = len(cache_snapshot.cache)
    
    if current_count > 0 and m_total_freq > (current_count * AGING_THRESHOLD_FACTOR):
        new_total = 0
        for k, freq in m_frequency.items():
            # Integer division decay (freq // 2)
            # Ensure minimal frequency of 1 for items currently in cache so they aren't zeroed out completely
            decayed = freq // 2
            if decayed < 1: 
                decayed = 1
            
            m_frequency[k] = decayed
            new_total += decayed
        m_total_freq = new_total

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up metadata. We do NOT keep a ghost cache here to save memory and processing time, 
    relying instead on the LFU-Aging mechanic to handle "warm" returns naturally.
    '''
    global m_access_time, m_frequency, m_total_freq
    
    key = evicted_obj.key
    
    if key in m_access_time:
        del m_access_time[key]
        
    if key in m_frequency:
        f = m_frequency[key]
        m_total_freq -= f
        del m_frequency[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr8dv_8m1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprtwmw5dd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1armhkdj.pickle

Iteration 42: New subsample score 0.7543369999999999 is not better than old score 0.902427, skipping
Iteration 43: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyrd22_z1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjgfyirmj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcq8kyz0z.pickle

Iteration 43: Proposed new text for program: import math

# Metadata storage for GDSF
# 1. key -> Score (Priority H)
# 2. key -> Frequency
# 3. key -> Last Access Time (Recency for tie-breaking)
m_key_score = dict()
m_key_freq = dict()
m_key_last_access = dict()

# Ghost list: Stores frequency of recently evicted items to restore history.
m_ghosts = dict()

# Global aging factor L
g_L = 0.0

# Constants
GHOST_RATIO = 1.0

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy:
    Evict the object with the minimum Priority Score H(p).
    
    Tie-Breaker: Least Recently Used (LRU).
    If scores are identical (common for new items or items with same freq/size),
    we evict the one with the oldest last_access time.
    '''
    candidates = list(cache_snapshot.cache.keys())
    
    if not candidates:
        return None

    # We want to find the item with the lowest score.
    # Comparison key: (Score, Last_Access)
    # Python's min compares tuples element-by-element.
    victim_key = min(candidates, key=lambda k: (m_key_score.get(k, 0.0), m_key_last_access.get(k, 0)))
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Recalculate Score: H(p) = L + Freq(p) / Size(p).
       Updating the score 'refreshs' the item against the aging factor L.
    3. Update Recency.
    '''
    global g_L, m_key_score, m_key_freq, m_key_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Recency
    m_key_last_access[key] = current_time
    
    # Increment Frequency
    curr_freq = m_key_freq.get(key, 0)
    m_key_freq[key] = curr_freq + 1
    
    # Calculate GDSF Score
    # Safe division: size is at least 1
    size = obj.size if obj.size > 0 else 1
    priority = g_L + (m_key_freq[key] / size)
    
    m_key_score[key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost List to possibly restore Frequency.
    2. Calculate initial Score: H(p) = L + Freq(p) / Size(p).
    3. Clean up Ghost List if too large.
    '''
    global g_L, m_key_score, m_key_freq, m_key_last_access, m_ghosts
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Restore Frequency from Ghosts or initialize to 1
    if key in m_ghosts:
        # It's a returning item. Restore frequency + 1 for current access.
        freq = m_ghosts[key] + 1
        del m_ghosts[key]
    else:
        # Brand new item
        freq = 1
        
    m_key_freq[key] = freq
    m_key_last_access[key] = current_time
    
    # Calculate GDSF Score
    size = obj.size if obj.size > 0 else 1
    priority = g_L + (freq / size)
    m_key_score[key] = priority
    
    # Manage Ghost Size
    # We limit ghost history proportional to the current number of cached objects.
    target_ghost_size = max(10, int(len(cache_snapshot.cache) * GHOST_RATIO))
    
    while len(m_ghosts) > target_ghost_size:
        # Remove oldest ghost (FIFO behavior of python dict)
        oldest_ghost = next(iter(m_ghosts))
        del m_ghosts[oldest_ghost]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update the global aging factor L. 
       L becomes the score of the victim object. This raises the "watermark"
       for all future items.
    2. Move victim's frequency to Ghost List.
    3. Clean up active metadata.
    '''
    global g_L, m_key_score, m_key_freq, m_key_last_access, m_ghosts
    
    key = evicted_obj.key
    
    # Update L to the priority of the evicted object
    if key in m_key_score:
        g_L = m_key_score[key]
        del m_key_score[key]
        
    # Save Frequency to Ghost list
    if key in m_key_freq:
        m_ghosts[key] = m_key_freq[key]
        del m_key_freq[key]
        
    # Cleanup Recency
    if key in m_key_last_access:
        del m_key_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptetei1kb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8j159f20.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7lxxlz0p.pickle

Iteration 43: New subsample score 0.789245 is better than old score 0.7688189999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpet99hh4w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnf74x_k3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps61v_joo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppx7khhdj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1euvoezt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4nifgapg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdqdbig1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaza8xn1o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7mwq2md8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp338pvbtk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzs22hnjh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpln46776n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9_og508d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptnglguv8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpegufpnd6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpac9zz0vy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0rhru25p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi2ixgs41.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnuek31jr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphqiyjln7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplqswm3je.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp66xpiemp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvzwi68yy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa6656unh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvjggsgul.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmz9izxkl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuwu97tsz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplpcd22rd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaq716et8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_2xzxgob.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoxjyc79x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8rnfwnne.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbvww2gl4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc7awnjr2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_vn3b2o3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpesn1ak65.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6wmhol_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpga8fsifd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptu3rgk9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwm5zccze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprsian_fz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpan9mtl8j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7deyfwwp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9jc82te9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9czwutxy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp50px_n2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzsm771r2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg3linj15.pickle

Iteration 43: Full valset score for new program: 0.2258390000000001
Iteration 43: Full train_val score for new program: 0.2258390000000001
Iteration 43: Individual valset scores for new program: [0.470297, 0.448984, 0.456639, 0.416289, 0.465529, 0.459472, 0.264354, 0.498624, 0.538792, 0.531017, 0.066667, 0.347691, 0.023893, 0.0, 0.01968, 0.019442, 0.018581, 0.02221, 0.021516, 0.266728, 0.339233, 0.025282, 0.057382, 0.057382, 0.277753, 0.327621, 0.845084, 0.886017, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.749489, 0.072368, 0.062665, 0.009162, 0.631118, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.283333, 0.020965, 0.021879, 0.441718, 0.045752]
Iteration 43: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.502701, 0.489854, 0.273923, 0.498624, 0.540937, 0.531017, 0.1, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 43: Full valset pareto front score: 0.26132349999999993
Iteration 43: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {8, 2}, {0, 3, 7, 9, 12}, {2, 4, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {11}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {10}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {1, 2, 4, 5, 6, 8, 9, 10, 11}, {10}, {9}, {9}, {10, 4, 6}, {5}, {1, 4, 5, 6, 10}, {1, 4, 5, 6, 10, 11}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11}, {9}, {5}, {2, 4, 6, 8, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11}, {10, 4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11}, {1, 10, 5, 9}]
Iteration 43: Best valset aggregate score so far: 0.2508065
Iteration 43: Best program as per aggregate score on train_val: 5
Iteration 43: Best program as per aggregate score on valset: 5
Iteration 43: Best score on valset: 0.2508065
Iteration 43: Best score on train_val: 0.2508065
Iteration 43: Linear pareto front program index: 5
Iteration 43: New program candidate index: 12
Iteration 44: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjkc1eg62.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuac9uujj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpullpby07.pickle

Iteration 44: Proposed new text for program: import math

# Metadata storage
# m_metadata: key -> [frequency, last_access_time]
# We use a list for values to allow mutable updates without re-hashing
m_metadata = dict()

# Ghost entries: key -> frequency (at time of eviction)
# Tracks items recently evicted to catch cyclic patterns or working sets slightly larger than cache
m_ghosts = dict()

# Configuration
MAX_GHOSTS = 2000     # Limit ghost tracking to prevent memory bloat
FREQ_DECAY_ON_RESTORE = 0.5 # Retain 50% of previous popularity when returning from ghosts

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Minimum Utility Score.
    
    We scan the cache to find the object with the lowest "Utility Score".
    
    Score Formula: (Frequency ^ 1.5) / (Age * log(Size))
    
    - High Frequency -> Keep (Protects Working Set)
    - High Age (Time since last access) -> Evict (LRU decay)
    - Large Size -> Evict (Prefer keeping many small items to maximize hit counts)
    '''
    current_time = cache_snapshot.access_count
    candidate_key = None
    min_score = float('inf')
    
    # Iterate through all keys in the cache to find the worst item
    for key, cached_obj in cache_snapshot.cache.items():
        if key not in m_metadata:
            # Fallback for data integrity issues, though shouldn't happen
            continue
            
        freq = m_metadata[key][0]
        last_access = m_metadata[key][1]
        
        # Calculate Age (Time since last use)
        # Add 1 to avoid division by zero if evicted in the same tick (rare)
        age = (current_time - last_access) + 1.0
        
        # Calculate Size Factor
        # Use Log scale. Penalize size, but don't let linear size dominate frequency.
        # +2 ensures we don't divide by zero or get 0 for size=1.
        size_factor = math.log(cached_obj.size + 2)
        
        # Calculate Score
        # Freq^1.5 gives strong weight to frequency (LFU) to resist scans.
        score = (freq ** 1.5) / (age * size_factor)
        
        if score < min_score:
            min_score = score
            candidate_key = key

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        stats = m_metadata[obj.key]
        stats[0] += 1  # Increment Frequency
        stats[1] = current_time # Update Recency
    else:
        # Should ideally not happen on a hit, but safety fallback
        m_metadata[obj.key] = [1, current_time]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost list to see if this is a returning visitor.
    2. Initialize Frequency (High if Ghost, 1 if New).
    3. Initialize Recency.
    '''
    global m_metadata, m_ghosts
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    initial_freq = 1.0
    
    # Check if it's a "Ghost" (recently evicted)
    if key in m_ghosts:
        # It's a returning item. Restore a portion of its previous fame.
        # This helps "Scan" items or cyclic working sets regain protection quickly.
        # We add 1.0 to ensure it's strictly better than a raw new item.
        prev_freq = m_ghosts[key]
        initial_freq = (prev_freq * FREQ_DECAY_ON_RESTORE) + 1.0
        
        # Remove from ghosts since it's now in cache
        del m_ghosts[key]
    
    # Store in active metadata: [Frequency, Last_Access]
    m_metadata[key] = [initial_freq, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move victim's frequency data to Ghost list.
    2. Cleanup active metadata.
    3. Manage Ghost list capacity.
    '''
    global m_metadata, m_ghosts
    
    key = evicted_obj.key
    
    # Retrieve stats before deleting
    freq = 1.0
    if key in m_metadata:
        freq = m_metadata[key][0]
        del m_metadata[key]
    
    # Store in Ghost map (FIFO/LRU management happens via dictionary order usually, 
    # but here we just check size limit)
    m_ghosts[key] = freq
    
    # Maintain Ghost list size
    if len(m_ghosts) > MAX_GHOSTS:
        # Remove the oldest inserted ghost (Python 3.7+ dicts preserve insertion order)
        # Using iter() is an O(1) way to get the first key
        oldest_key = next(iter(m_ghosts))
        del m_ghosts[oldest_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd9mkwwcb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyj3njwi0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6co93sqq.pickle

Iteration 44: New subsample score 1.012043 is not better than old score 1.0240960000000001, skipping
Iteration 45: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6rggl_g_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpagfxnkud.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuo5bu90v.pickle

Iteration 45: Proposed new text for program: from collections import OrderedDict

# Metadata storage
# m_stack: The LIRS Stack S. Contains LIR items and HIR items (resident and non-resident).
#          Acts as a history buffer. Keys are object keys.
#          OrderedDict allows O(1) access to top (most recent) and bottom (oldest).
m_stack = OrderedDict()

# m_queue: The LIRS Queue Q. Contains only Resident HIR items.
#          Acts as the eviction queue for probationary items.
m_queue = OrderedDict()

# m_state: Dictionary mapping key -> 'LIR' or 'HIR'.
m_state = dict()

# Constants
HIR_PERCENT = 1.0 # Reserve small % for HIR to handle scans. 
# However, in LIRS, strictly we maintain LIR count < Capacity - HIR_count.
# We will dynamically calculate limits based on snapshot capacity.

def evict(cache_snapshot, obj):
    '''
    Determines which object to evict.
    Strategy: 
    1. Evict the resident HIR item at the front of Queue Q.
    2. If Q is empty (rare, implies cache is all LIR), evict the bottom of Stack S (LRU LIR).
    '''
    global m_queue, m_stack, m_state
    
    # Primary strategy: Evict resident HIR from Q
    if m_queue:
        # peek at the first item (oldest inserted HIR)
        victim_key = next(iter(m_queue))
        return victim_key
        
    # Fallback: Evict LIR from bottom of Stack S
    # This happens if the cache is purely filled with LIR items.
    if m_stack:
        victim_key = next(iter(m_stack))
        return victim_key

    # Should not happen if cache is populated
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    Handle Cache Hit.
    1. LIR Hit: Move to top of S, prune bottom.
    2. HIR Hit (In Stack): Hot! Promote to LIR, remove from Q, move to top of S.
    3. HIR Hit (Not in Stack): Cold. Keep HIR, move to top of S, move to end of Q.
    '''
    global m_stack, m_queue, m_state
    
    key = obj.key
    state = m_state.get(key)
    
    if state == 'LIR':
        # LIR accessed: Just update recency in Stack S
        if key in m_stack:
            m_stack.move_to_end(key)
        else:
            # Recovery case: If somehow missing from stack
            m_stack[key] = True
            
        _prune_stack()
        
    elif state == 'HIR':
        if key in m_stack:
            # HIR accessed while in Stack S -> It is Hot!
            # Promote to LIR
            m_state[key] = 'LIR'
            m_stack.move_to_end(key)
            
            # Remove from Queue Q since it's no longer HIR
            if key in m_queue:
                del m_queue[key]
                
            # We promoted a new LIR, check if we need to demote an old LIR
            _enforce_lir_limit(cache_snapshot)
        else:
            # HIR accessed, but it had fallen out of Stack S -> Still Cold.
            # It gets a second chance in Stack S, but stays HIR.
            m_stack[key] = True
            m_stack.move_to_end(key)
            
            # Update position in Queue Q (MRU of HIRs)
            m_queue[key] = True
            m_queue.move_to_end(key)
            
        _prune_stack()
    else:
        # Edge case: Item in cache but missing state (e.g. initial load glitch), treat as insert
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    Handle Cache Miss (Insert).
    1. Check if key is in Stack S (Non-Resident Hit).
       - If yes: Promote to LIR.
    2. If no: Insert as HIR, add to Q, add to top of S.
    '''
    global m_stack, m_queue, m_state
    
    key = obj.key
    
    # Check if we have history for this item
    if key in m_stack:
        # It is in Stack S, so it was a Non-Resident HIR.
        # This access makes it hot. Promote to LIR.
        m_state[key] = 'LIR'
        m_stack.move_to_end(key)
        
        # It enters cache as LIR, so ensure we don't overflow LIR budget
        _enforce_lir_limit(cache_snapshot)
    else:
        # Brand new access. Start as HIR.
        m_state[key] = 'HIR'
        m_stack[key] = True # Add to top of S
        m_queue[key] = True # Add to end of Q
        
    _prune_stack()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handle Eviction.
    1. Remove from Queue Q if present.
    2. Do NOT remove from Stack S immediately. 
       - If it remains in S, it becomes a "Non-Resident HIR".
       - This allows us to detect if it comes back quickly (LIR candidate).
    '''
    global m_queue, m_stack, m_state
    
    key = evicted_obj.key
    
    if key in m_queue:
        del m_queue[key]
        
    # Note: We do NOT remove from m_state or m_stack here.
    # The item is now Non-Resident. It stays in m_stack to measure Recency.
    # If it falls off the bottom of m_stack later (via pruning), we will remove metadata then.

def _prune_stack():
    '''
    Main LIRS invariant: The bottom of Stack S must be an LIR item.
    If the bottom is HIR, it means that HIR item is older than the oldest LIR item,
    so it is no longer relevant for the "Hot" calculation.
    '''
    global m_stack, m_state, m_queue
    
    # Iterate while bottom is not LIR
    while m_stack:
        bottom_key = next(iter(m_stack))
        state = m_state.get(bottom_key)
        
        if state == 'LIR':
            # Invariant satisfied
            break
        
        # Bottom is HIR. Remove it from Stack S.
        m_stack.popitem(last=False)
        
        # If this item is NOT in the Queue Q, it means it was a Non-Resident HIR.
        # Since it's leaving the stack, we forget it entirely.
        if bottom_key not in m_queue:
            if bottom_key in m_state:
                del m_state[bottom_key]

def _enforce_lir_limit(cache_snapshot):
    '''
    Ensure LIR set doesn't take over the whole cache.
    We generally reserve ~1% for HIR items to handle scans.
    If LIR count > Capacity * 0.99, demote the LRU LIR to HIR.
    '''
    global m_stack, m_state, m_queue
    
    # We estimate capacity in items.
    capacity = cache_snapshot.capacity // 1  # Assuming unit cost for simplicity, or we rely on eviction loop.
    # Actually, LIRS works best with counts. Let's approximate limit based on current cache size.
    # If cache is full, we want max LIRs to be slightly less than total capacity.
    
    # Dynamic capacity check based on current cache size (proxy for capacity in count)
    current_size = len(cache_snapshot.cache)
    
    # If cache is not full, we don't strictly need to demote, 
    # but maintaining the ratio helps stability.
    if current_size == 0: return

    # Target: 99% LIR, 1% HIR buffer.
    max_lir = int(cache_snapshot.capacity * 0.99)
    if max_lir < 1: max_lir = 1
    
    # Count current LIRs is expensive in O(N).
    # Optimization: We only trigger this when we promote.
    # However, since we don't maintain a counter variable in the global scope provided,
    # we rely on the fact that Stack S bottom is the LRU LIR.
    
    # We need to know if we have too many LIRs.
    # Since we can't easily count LIRs O(1) without a counter, let's assume
    # if the stack is very large or Q is very small, we might need demotion.
    
    # For this constrained environment, let's use a simpler heuristic:
    # If Queue Q is empty, we MUST demote an LIR to HIR to allow eviction flexibility.
    # Or if Queue Q is very small compared to cache size.
    
    if len(m_queue) == 0 and len(m_stack) > 0:
        # Demote bottom LIR
        victim_lir = next(iter(m_stack)) # Bottom of S is always LIR (after prune)
        
        m_state[victim_lir] = 'HIR'
        m_stack.move_to_end(victim_lir) # Move to MRU of stack? 
        # No, in LIRS, demoted LIR goes to Q, and usually stays in S?
        # Actually, standard LIRS: demote S bottom. It becomes HIR. 
        # Since it is at bottom of S, it gets pruned immediately by _prune_stack.
        # So we simply move it to Q.
        
        m_queue[victim_lir] = True
        _prune_stack() # This will likely remove it from S
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8pifieke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplzn7yl1w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdjt8fm29.pickle

Iteration 45: New subsample score 0.875143 is not better than old score 1.032637, skipping
Iteration 46: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiczizt0_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9ghhku7w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps_qjlxn9.pickle

Iteration 46: Proposed new text for program: from collections import OrderedDict

# Global Metadata for LIRS Algorithm
# m_stack (S): Represents the LIRS Stack S.
#   - Keys: Object keys.
#   - Order: Ordered by recency (Bottom=LRU, Top=MRU).
#   - Contains: All LIR blocks and some HIR blocks (Resident and Non-Resident).
#   - Invariant: The bottom of S is always an LIR block (enforced by pruning).
m_stack = OrderedDict() 

# m_queue (Q): Represents the LIRS Queue Q.
#   - Keys: Object keys.
#   - Order: Ordered by recency (Bottom=LRU, Top=MRU).
#   - Contains: Only Resident HIR blocks.
#   - Purpose: Candidate list for eviction.
m_queue = OrderedDict()

# m_lirs: Set to track which keys currently hold 'LIR' status.
#   - LIR (Low Inter-reference Recency): Protected working set.
#   - HIR (High Inter-reference Recency): Probationary items.
m_lirs = set()

# Constants
# Portion of the cache count dedicated to LIR items. 
# 90-95% allows a large protected working set while keeping a buffer for new items.
LIR_RATIO = 0.90 

def evict(cache_snapshot, obj):
    """
    Selects a victim for eviction.
    Standard LIRS Policy: Evict the Resident HIR item at the front (LRU) of Queue Q.
    """
    global m_queue, m_stack
    
    # 1. Primary Strategy: Evict from Queue Q (Resident HIRs)
    if m_queue:
        # The first item in OrderedDict is the oldest inserted (LRU of HIRs)
        return next(iter(m_queue))
    
    # 2. Fallback: If Q is empty (Cache is 100% LIR), we must evict an LIR.
    # In LIRS, the LIR with the maximum recency distance is at the bottom of Stack S.
    if m_stack:
        # Find the first key in stack that is actually in the cache
        for k in m_stack:
            if k in cache_snapshot.cache:
                return k
                
    # 3. Safety Fallback (should not be reached)
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    return None

def update_after_hit(cache_snapshot, obj):
    """
    Updates LIRS metadata after a cache hit.
    """
    global m_stack, m_queue, m_lirs
    
    key = obj.key
    
    # Case 1: The accessed item is LIR
    if key in m_lirs:
        # LIR items are always in Stack S. Move to top (MRU).
        if key in m_stack:
            m_stack.move_to_end(key)
        else:
            # Recovery if state drifted
            m_stack[key] = None
            
        # If the LIR item was at the bottom, moving it up might expose HIRs at the bottom.
        _prune_stack()
        return

    # Case 2: The accessed item is HIR
    # We check if it is in Stack S. 
    #   - If IN Stack: It was accessed recently enough (within the LIR window). Promote to LIR.
    #   - If NOT in Stack: It is a "cold" access. Keep as HIR.
    
    if key in m_stack:
        # Hot HIR (Resident or Non-Resident) -> Promote to LIR
        
        # Remove from Queue Q (since it's no longer a candidate for eviction)
        if key in m_queue:
            del m_queue[key]
            
        # Change status to LIR
        m_lirs.add(key)
        
        # Move to top of Stack S
        m_stack.move_to_end(key)
        
        # Enforce LIR capacity (if we promoted, we might need to demote the LRU LIR)
        _enforce_lir_limit(cache_snapshot)
        
        # Pruning is crucial after promotion/demotion
        _prune_stack()
        
    else:
        # Cold HIR -> Remains HIR
        
        # Add to top of Stack S (It now has a history entry)
        m_stack[key] = None
        
        # Move/Add to top of Queue Q (It gets a fresh chance in the cache)
        if key in m_queue:
            m_queue.move_to_end(key)
        else:
            m_queue[key] = None

def update_after_insert(cache_snapshot, obj):
    """
    Updates metadata after inserting a new object (Cold Miss).
    """
    global m_stack, m_queue, m_lirs
    
    key = obj.key
    
    # Check if this new key is already in Stack S (Non-Resident HIR)
    # This means we evicted it recently, but it's back!
    if key in m_stack:
        # Treat like a Hot HIR Hit -> Promote to LIR
        m_lirs.add(key)
        m_stack.move_to_end(key)
        _enforce_lir_limit(cache_snapshot)
        _prune_stack()
    else:
        # Truly new item -> Insert as HIR
        m_stack[key] = None     # Add to history
        m_queue[key] = None     # Add to eviction candidates

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Updates metadata after an object is evicted.
    """
    global m_stack, m_queue, m_lirs
    
    key = evicted_obj.key
    
    # If we evicted an LIR (fallback case), remove status
    if key in m_lirs:
        m_lirs.remove(key)
        
    # Remove from Queue Q (it is no longer resident)
    if key in m_queue:
        del m_queue[key]
        
    # Note: We do NOT remove from m_stack immediately. 
    # It remains as a "Non-Resident HIR" to track if it returns soon.
    # It will be removed lazily by _prune_stack() when it falls out of the LIR window.

def _prune_stack():
    """
    LIRS Pruning Operation:
    Removes HIR items (Resident or Non-Resident) from the bottom of Stack S.
    We stop when the bottom item is an LIR item.
    """
    global m_stack, m_lirs
    
    while m_stack:
        # Peek at the bottom (first item)
        bottom_key = next(iter(m_stack))
        
        if bottom_key in m_lirs:
            # The bottom is LIR, the stack is valid.
            break
        
        # The bottom is HIR (Resident or Non-Resident). It is too old. Remove it.
        m_stack.popitem(last=False)

def _enforce_lir_limit(cache_snapshot):
    """
    Ensures the number of LIR items does not exceed the target ratio.
    If we have too many LIRs, we demote the least recently used LIR to HIR.
    """
    global m_stack, m_queue, m_lirs
    
    total_items = len(cache_snapshot.cache)
    if total_items == 0:
        return
        
    # Calculate target based on current item count
    target_lir_count = int(total_items * LIR_RATIO)
    
    # If cache is very small, ensure we don't lock everything as LIR
    if target_lir_count >= total_items and total_items > 1:
        target_lir_count = total_items - 1
        
    while len(m_lirs) > target_lir_count:
        # Demote the LRU LIR.
        # In LIRS, the LRU LIR is always the bottom-most item in Stack S 
        # (because we always prune HIRs from the bottom).
        
        if not m_stack:
            break
            
        victim_key = next(iter(m_stack))
        
        # Safety: The bottom should be LIR due to _prune_stack, but if not, skip/pop
        if victim_key not in m_lirs:
            m_stack.popitem(last=False)
            continue
            
        # Demote LIR -> HIR
        m_lirs.remove(victim_key)
        
        # Move to Queue Q (It becomes a Resident HIR)
        # It goes to the back (MRU) of Q because it was valuable until just now.
        m_queue[victim_key] = None
        
        # Now that the bottom of S is converted to HIR, we must prune S.
        _prune_stack()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_uw89u8m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo42lwyqh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvb0sjchi.pickle

Iteration 46: New subsample score 1.2304529999999998 is not better than old score 1.241776, skipping
Iteration 47: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0aqmfxnr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfpqcdcr_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_baicrus.pickle

Iteration 47: Proposed new text for program: import random
import math

# Metadata storage
# 1. key -> frequency (int)
# 2. key -> last_access_time (int)
# 3. key -> frequency (int) - Ghost registry for evicted items
m_key_frequency = dict()
m_key_last_access = dict()
m_ghost_freqs = dict()

# Constants
SAMPLE_SIZE = 64
MAX_GHOSTS = 5000
FREQ_EXPONENT = 3.0  # Heavy weight on frequency to protect against scans

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Size-Aware Hyperbolic Caching with Sampling.
    
    We calculate a score: (Frequency^3) / (Size * Age).
    - Higher Frequency drastically increases value (protects working set).
    - Larger Size decreases value (prefer many small items over one big item).
    - Older Age decreases value (handles shifts in popularity).
    
    We evict the item with the LOWEST score from a random sample.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # Optimization: Instead of scanning ALL keys (O(N)), sample a subset.
    # This provides O(1) performance while statistically approximating the optimal victim.
    if len(current_keys) > SAMPLE_SIZE:
        candidates = random.sample(current_keys, SAMPLE_SIZE)
    else:
        candidates = current_keys
        
    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    for key in candidates:
        cached_obj = cache_snapshot.cache[key]
        
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        size = cached_obj.size
        
        # Calculate Age (Time since last access). 
        # Add 1 to avoid division by zero.
        age = (current_time - last_access) + 1
        
        # Hyperbolic Score Calculation
        # We heavily weight frequency (power of 3) to separate "Scan" items (freq 1)
        # from "Working Set" items (freq >= 2).
        # We penalize size to maximize hit density (hits per byte).
        score = (freq ** FREQ_EXPONENT) / (size * age)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update Recency and Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency if Ghost.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_freqs
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    if obj.key in m_ghost_freqs:
        # It's a ghost! It was popular before, so it's likely popular again.
        # Restore its old frequency so it doesn't look like a scan item (freq 1).
        m_key_frequency[obj.key] = m_ghost_freqs[obj.key] + 1
        del m_ghost_freqs[obj.key]
    else:
        # New item. Start at 1. 
        # The eviction policy will aggressively target this if it doesn't get a 2nd hit soon.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move metadata to Ghost registry.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_freqs
    
    key = evicted_obj.key
    
    # Save the frequency to the ghost map before deleting active metadata
    if key in m_key_frequency:
        # Cap stored frequency to prevent runaway values
        m_ghost_freqs[key] = min(m_key_frequency[key], 100)
        del m_key_frequency[key]
        
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    # Maintain Ghost Map size to prevent memory leaks
    if len(m_ghost_freqs) > MAX_GHOSTS:
        # Remove an arbitrary item (first one from iterator is O(1) in Python dicts)
        victim_ghost = next(iter(m_ghost_freqs))
        del m_ghost_freqs[victim_ghost]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpraboehvp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwlaupxad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp95us6dqw.pickle

Iteration 47: New subsample score 1.0114290000000001 is not better than old score 1.012455, skipping
Iteration 48: Selected program 11 score: 0.2257136458333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp63ttgyf5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkukboyzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfcsqjv7h.pickle

Iteration 48: Proposed new text for program: # Metadata storage for ARC (Adaptive Replacement Cache)
# We maintain two main lists (conceptually) via dictionaries for O(1) access:
# T1: Recent cache entries (probationary)
# T2: Frequent cache entries (protected)
# B1: Ghost entries evicted from T1 (history of recent eviction)
# B2: Ghost entries evicted from T2 (history of frequent eviction)

# To support efficient LRU operations without scanning, we use simple Python dicts 
# which preserve insertion order (in Python 3.7+). 
# We will use .pop(next(iter(...))) to evict the LRU item efficiently.

m_t1 = dict()  # Keys in T1 (Probationary) -> access_time
m_t2 = dict()  # Keys in T2 (Protected) -> access_time
m_b1 = dict()  # Keys in B1 (Ghost Probationary) -> access_time
m_b2 = dict()  # Keys in B2 (Ghost Protected) -> access_time

m_p = 0        # Adaptive parameter (Target size for T1)

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic.
    Decides which item to evict based on the target size `m_p`.
    
    The logic determines if we should evict from T1 (Recency) or T2 (Frequency).
    This function *returns* the key to evict but does not perform the metadata cleanup 
    (which happens in update_after_evict).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p

    # If the item causing the eviction is already in B2, the replacement policy
    # favors T2, implying we should kill T1 to make room if possible.
    # However, standard ARC `REPLACE` logic is usually triggered *before* insertion.
    # Since this API asks for a victim *when cache is full*, we apply the standard REPLACE logic rules.
    
    # We need to decide whether to evict from T1 or T2.
    
    # Logic derived from ARC paper:
    # if (T1 is not empty) and ((len(T1) > p) or (item is in B2 and len(T1) == p)):
    #   evict LRU from T1
    # else:
    #   evict LRU from T2
    
    # Note: The `obj` passed here is the new object being inserted that caused the overflow.
    # We check if this new object `obj.key` is in B2 to influence the decision.
    
    in_b2 = obj.key in m_b2
    len_t1 = len(m_t1)
    
    if m_t1 and (len_t1 > m_p or (in_b2 and len_t1 == int(m_p))):
        # Evict LRU from T1.
        # Since dicts preserve insertion order, the first key is the LRU.
        victim_key = next(iter(m_t1))
    else:
        # Evict LRU from T2.
        if m_t2:
            victim_key = next(iter(m_t2))
        else:
            # Fallback: if T2 is empty, must evict from T1
            if m_t1:
                victim_key = next(iter(m_t1))
            else:
                # Should not happen if cache is full
                return None 

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1, move to T2 (MRU).
    If item is in T2, move to T2 (MRU).
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    if key in m_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del m_t1[key]
        m_t2[key] = current_time
    elif key in m_t2:
        # Update Recency in T2 (remove and re-add to update order)
        del m_t2[key]
        m_t2[key] = current_time
    # Note: Hits on B1 or B2 technically count as misses in the cache structure, 
    # so they are handled in update_after_insert.

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    Handle adaptation of p and insertion into T1 or T2.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity // obj.size if obj.size > 0 else 100 # Approx count capacity
    # More robust: use current total count of items as the "N" for ARC
    c = len(cache_snapshot.cache) 
    if c == 0: c = 1 # Safety

    # Case 1: Key is in B1 (Ghost Probationary)
    # We missed in cache, but hit in history of T1. We should have made T1 larger.
    if key in m_b1:
        delta = 1
        if len(m_b1) >= len(m_b2):
            delta = 1
        else:
            delta = len(m_b2) / len(m_b1)
        
        m_p = min(c, m_p + delta)
        
        # Move from B1 to T2 (Frequency) because it was accessed again
        del m_b1[key]
        m_t2[key] = current_time
        
    # Case 2: Key is in B2 (Ghost Protected)
    # We missed in cache, but hit in history of T2. We should have made T2 larger (T1 smaller).
    elif key in m_b2:
        delta = 1
        if len(m_b2) >= len(m_b1):
            delta = 1
        else:
            delta = len(m_b1) / len(m_b2)
            
        m_p = max(0, m_p - delta)
        
        # Move from B2 to T2 (Frequency)
        del m_b2[key]
        m_t2[key] = current_time

    # Case 3: Totally new item
    else:
        # Insert into T1 (Recency) MRU position
        m_t1[key] = current_time

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Move the evicted key to the corresponding ghost list (B1 or B2).
    Enforce capacity on ghost lists.
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    victim_key = evicted_obj.key
    victim_time = cache_snapshot.access_count # Use current time for ghost recency
    
    # Determine where it was evicted from
    if victim_key in m_t1:
        del m_t1[victim_key]
        # Move to B1
        m_b1[victim_key] = victim_time
    elif victim_key in m_t2:
        del m_t2[victim_key]
        # Move to B2
        m_b2[victim_key] = victim_time
        
    # ARC requires managing the size of ghost lists too (L1 + L2 <= 2c)
    # Specifically, len(T1) + len(B1) <= c and len(T2) + len(B2) <= 2c usually, 
    # but simpler robust implementation is to just cap total history.
    
    # We use current cache size as proxy for capacity 'c'
    c = len(cache_snapshot.cache)
    if c == 0: c = 10
    
    # Prune B1 if it gets too big
    # Logic: if len(T1) + len(B1) > c
    while len(m_t1) + len(m_b1) > c and len(m_b1) > 0:
        # Evict LRU from B1 (first item)
        del m_b1[next(iter(m_b1))]

    # Prune B2 if it gets too big
    # Logic: if len(T2) + len(B2) > 2c
    while len(m_t2) + len(m_b2) > 2 * c and len(m_b2) > 0:
        # Evict LRU from B2
        del m_b2[next(iter(m_b2))]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcgmd2n7n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzs6eylco.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy0tf6zn5.pickle

Iteration 48: New subsample score 0.179483 is better than old score 0.103191. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg7j423u_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8_4o532a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpitt2v0sw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk17y2nzk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcqu7xxhu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyro_f5a3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpde4aus10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5mvzb_35.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8kxsdk2p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwwwzf8oc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjazq2rrn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvyje6doe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplt66l5zk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbrxowv1r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzxxt99mc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsqp__pzv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps9cx0lrq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp93yfi1f8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp976g7mgr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj920__6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx5phczmb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdyh7bl2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfomyas1v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7k5u9f3k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcsprvx7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpllvbrpyu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt0e13lqi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwfbvickt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvxu212o8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp533ap5ry.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx92gqjqp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo88u1srm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx6v456kf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdny4kp1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpso3sv3c3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpomcjelz0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn6f2xz_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvt6ikfrl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm_i96o7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfoson9rm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcdl2dutc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr9e7b16y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp61p41725.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7mk9j0nl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdpr0cjkx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_mvvvk_k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuazjimr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf8b_6clq.pickle

Iteration 48: Full valset score for new program: 0.2401080833333333
Iteration 48: Full train_val score for new program: 0.2401080833333333
Iteration 48: Individual valset scores for new program: [0.504789, 0.479337, 0.488516, 0.432649, 0.503503, 0.489969, 0.272727, 0.498034, 0.537719, 0.531017, 0.075, 0.325488, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.342183, 0.026164, 0.058672, 0.058672, 0.269789, 0.358871, 0.85005, 0.891494, 0.039832, 0.038636, 0.045558, 0.007721, 0.020672, 0.752893, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.11145, 0.073305, 0.108296, 0.052632, 0.233333, 0.041854, 0.10014, 0.466258, 0.081699]
Iteration 48: New valset pareto front scores: [0.509148, 0.479454, 0.493182, 0.439584, 0.503503, 0.489969, 0.273923, 0.498624, 0.540937, 0.531017, 0.1, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 48: Full valset pareto front score: 0.2613426041666666
Iteration 48: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {13}, {13}, {8, 2}, {0, 3, 7, 9, 12}, {2, 4, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {11}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13}, {10}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13}, {10}, {9}, {9}, {10, 4, 6}, {5}, {1, 4, 5, 6, 10, 13}, {1, 4, 5, 6, 10, 11, 13}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11, 13}, {9}, {5}, {2, 4, 6, 8, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13}, {10, 4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13}, {1, 5, 9, 10, 13}]
Iteration 48: Best valset aggregate score so far: 0.2508065
Iteration 48: Best program as per aggregate score on train_val: 5
Iteration 48: Best program as per aggregate score on valset: 5
Iteration 48: Best score on valset: 0.2508065
Iteration 48: Best score on train_val: 0.2508065
Iteration 48: Linear pareto front program index: 5
Iteration 48: New program candidate index: 13
Iteration 49: Selected program 11 score: 0.2257136458333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1b4zdhra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplj8g6640.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgrvf3ebr.pickle

Iteration 49: Proposed new text for program: # Metadata storage
# The logic follows the standard ARC (Adaptive Replacement Cache) algorithm.
# T1: Recent Cache (Probationary) - LRU list of keys
# T2: Frequent Cache (Protected) - LRU list of keys
# B1: Ghost Recent (History of evicted T1) - LRU list of keys
# B2: Ghost Frequent (History of evicted T2) - LRU list of keys
# m_p: Target size for list T1

# We use standard Python lists as queues for simplicity given the constraints,
# where index 0 is LRU (Least Recently Used) and index -1 is MRU (Most Recently Used).
m_t1 = []
m_t2 = []
m_b1 = []
m_b2 = []
m_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic:
    Decides whether to evict from T1 (Recent) or T2 (Frequent) based on the 
    adaptive parameter `m_p`.
    
    If len(T1) > m_p, we prefer evicting from T1 to lower its size.
    Otherwise, we evict from T2.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p

    # Standard ARC "REPLACE" logic sub-routine embedded here
    # We must identify a victim key.
    
    victim_key = None
    
    # Calculate lengths
    len_t1 = len(m_t1)
    len_b1 = len(m_b1)
    
    # Case 1: T1 has grown beyond target P, OR B1 has data and T1 exceeds P
    # Specifically: if (T1 is not empty) and ((len(T1) > p) or (item is in B2 and len(T1) == p))
    # However, since we don't know if the incoming item is in B2 here (evict is called before insert logic),
    # we use the standard size check logic.
    
    if m_t1 and (len_t1 > m_p):
        # Evict LRU of T1
        victim_key = m_t1[0]
    elif m_t2:
        # Evict LRU of T2
        victim_key = m_t2[0]
    elif m_t1:
        # Fallback if T2 was empty but we fell through
        victim_key = m_t1[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (MRU).
    If it's in T2, move to T2 (MRU).
    '''
    global m_t1, m_t2
    
    key = obj.key
    
    if key in m_t1:
        m_t1.remove(key)
        m_t2.append(key) # Move to MRU of T2
    elif key in m_t2:
        m_t2.remove(key)
        m_t2.append(key) # Update position to MRU in T2

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    1. Check Ghost lists (B1, B2) for adaptation of m_p.
    2. Insert into T1 (MRU) if it's a cold miss.
    3. Insert into T2 (MRU) if it was a ghost hit.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    
    # We approximate N (cache capacity) using the snapshot capacity provided
    # Note: Snapshot capacity is in bytes, but ARC usually works on object counts.
    # We assume 'capacity' implies the max count of items here. If strict byte size
    # is enforced, len(cache) is the current count N.
    # In ARC, the cache size 'c' is effectively len(t1) + len(t2).
    # Since we are inserting, we assume the eviction has already happened if full.
    
    # Case I: Miss in T1/T2, but Hit in B1 (Ghost Recent)
    if key in m_b1:
        # Adapt P: We should have made T1 larger
        delta = 1
        if len(m_b1) >= len(m_b2) and len(m_b2) > 0:
            delta = 1
        elif len(m_b2) > len(m_b1):
             delta = len(m_b2) / len(m_b1)
        
        # Max capacity is implicitly the current count of items + 1 (the new one)
        # or use a safe upper bound.
        current_c = len(m_t1) + len(m_t2) + 1 
        m_p = min(current_c, m_p + delta)
        
        # Move to T2 (MRU) because it has been seen twice recently
        m_b1.remove(key)
        m_t2.append(key)

    # Case II: Miss in T1/T2, but Hit in B2 (Ghost Frequent)
    elif key in m_b2:
        # Adapt P: We should have made T1 smaller (T2 larger)
        delta = 1
        if len(m_b2) >= len(m_b1) and len(m_b1) > 0:
            delta = 1
        elif len(m_b1) > len(m_b2):
            delta = len(m_b1) / len(m_b2)
            
        m_p = max(0, m_p - delta)
        
        # Move to T2 (MRU)
        m_b2.remove(key)
        m_t2.append(key)

    # Case III: Totally cold miss (not in B1 or B2)
    else:
        # Add to T1 (MRU)
        m_t1.append(key)
        
    # ARC Constraint enforcement logic is technically handled by 'evict' and 'update_after_evict'
    # but strictly speaking, ghosts must not exceed cache size.
    current_capacity_count = len(cache_snapshot.cache) 
    
    # Ensure B1 + T1 <= Capacity (if not handled by evict)
    # In pure ARC, we trim B1 if len(T1)+len(B1) > c
    while (len(m_t1) + len(m_b1)) > current_capacity_count and len(m_b1) > 0:
        m_b1.pop(0) # Remove LRU of B1
        
    # Ensure B2 + T2 <= 2*Capacity (relaxed bound) or just Capacity
    while (len(m_t2) + len(m_b2)) > 2 * current_capacity_count and len(m_b2) > 0:
        m_b2.pop(0) # Remove LRU of B2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Move the evicted object key from Main Cache (T1/T2) to Ghost Cache (B1/B2).
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    evicted_key = evicted_obj.key
    
    # Identify where it came from and move to corresponding ghost
    if evicted_key in m_t1:
        m_t1.remove(evicted_key)
        m_b1.append(evicted_key) # Append to MRU of B1
    elif evicted_key in m_t2:
        m_t2.remove(evicted_key)
        m_b2.append(evicted_key) # Append to MRU of B2
    else:
        # Safety fallback: if for some reason key isn't found (should not happen in sync exec)
        # We assume it was T1 if we don't know.
        m_b1.append(evicted_key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy8a7ozqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsj45q_90.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5h63gjna.pickle

Iteration 49: New subsample score 0.38058800000000004 is better than old score 0.296153. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdsnf3c_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjmkzme2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps15lire7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgzxrp_sf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuajv4zht.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn8baswfl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz4fl_kk3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphm2g7f7m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcvc9umf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7v5482zu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc8i2mduh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5met1h3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzfczpe4i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsl9ze3ru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0w82e4fz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9ech36kr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpajsb55i3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaftekksr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkhxv05n2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqt_8ovjo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm20n526a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_9c47vye.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuczxn9i7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdlu_lwap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppqrqxefd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphvfh26r3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt60rwt2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppmbwh_iw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn8dqmj1g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4p41zgvn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqa5j9n36.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmhtftyl2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4rt1mn2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmk9ya6up.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfa450spo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppxow3dl0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcsf3cvsz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplk6spc4d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1yxsiauk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpat51ouk0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpptibzhbk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8y7q49u4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph_4q877u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnelvkfcn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppfz4o1uq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfgdmo_2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjj1tfamq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxh_q1f7x.pickle

Iteration 49: Full valset score for new program: 0.24068325
Iteration 49: Full train_val score for new program: 0.24068325
Iteration 49: Individual valset scores for new program: [0.504789, 0.479512, 0.488758, 0.432915, 0.50361, 0.489911, 0.272727, 0.498034, 0.538077, 0.531017, 0.075, 0.323712, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.26979, 0.358871, 0.849057, 0.891494, 0.039832, 0.038636, 0.045558, 0.007721, 0.020672, 0.752893, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.120322, 0.077581, 0.099954, 0.052632, 0.233333, 0.067317, 0.10014, 0.466258, 0.081699]
Iteration 49: New valset pareto front scores: [0.509148, 0.479512, 0.493182, 0.439584, 0.50361, 0.489969, 0.273923, 0.498624, 0.540937, 0.531017, 0.1, 0.392984, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.393314, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 49: Full valset pareto front score: 0.2613460416666666
Iteration 49: Updated valset pareto front programs: [{2}, {14}, {2}, {2}, {14}, {13}, {8, 2}, {0, 3, 7, 9, 12}, {2, 4, 6, 8, 9, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {11}, {4, 6}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14}, {10}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14}, {10}, {9}, {9}, {10, 4, 6}, {5}, {1, 4, 5, 6, 10, 13, 14}, {1, 4, 5, 6, 10, 11, 13, 14}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11, 13, 14}, {9}, {5}, {2, 4, 6, 8, 10}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14}, {10, 4, 5, 6}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14}, {1, 5, 9, 10, 13, 14}]
Iteration 49: Best valset aggregate score so far: 0.2508065
Iteration 49: Best program as per aggregate score on train_val: 5
Iteration 49: Best program as per aggregate score on valset: 5
Iteration 49: Best score on valset: 0.2508065
Iteration 49: Best score on train_val: 0.2508065
Iteration 49: Linear pareto front program index: 5
Iteration 49: New program candidate index: 14
Iteration 50: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6d1s0kxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk7mawx8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphfeonosx.pickle

Iteration 50: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency (The 'score' or 'priority' of the item)
# 2. key -> last_access_time (Used for LRU tie-breaking)
# 3. ghost_registry -> set (Keys that were recently evicted)
m_key_frequency = dict()
m_key_last_access = dict()
m_ghost_registry = set()
m_min_frequency = 0  # The LFU-DA "base" value

# Parameters
# Maximum size of the ghost registry (relative to cache capacity usually, 
# but since we don't know N explicitly, we limit by count or just rely on aging).
# For simplicity in this structure, we'll keep the ghost registry lean.
MAX_GHOST_SIZE = 5000 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU with Dynamic Aging (LFU-DA) logic.
    
    We evict the item with the smallest frequency count.
    If multiple items share the smallest frequency, we evict the Least Recently Used (LRU) among them.
    
    The 'm_min_frequency' variable tracks the frequency of the item we just evicted.
    This acts as a "pollution barrier" for new items.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We need to find the key with minimum frequency.
    # Tie-breaker: Minimum last_access_time (LRU).
    # This is O(N) per eviction, which is acceptable to maximize hit rate in this context.
    victim_key = min(current_keys, key=lambda k: (m_key_frequency.get(k, 0), m_key_last_access.get(k, 0)))
    
    # Update the dynamic aging floor (LFU-DA mechanism).
    # The lowest frequency in the cache becomes the new baseline.
    global m_min_frequency
    m_min_frequency = m_key_frequency.get(victim_key, 0)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Standard LFU behavior: increment weight on hit
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Update Recency.
    2. Determine Initial Frequency.
    
    Strategy:
    - If the item is in the Ghost Registry (it was recently evicted), it means
      we made a mistake or the working set is thrashing. We revive it with 
      current m_min_frequency + bonus.
    - If it is a completely new item, we initialize it at m_min_frequency + 1.
      This effectively "ages" existing items by raising the floor for new entrants.
    '''
    global m_key_last_access, m_key_frequency, m_min_frequency, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    if obj.key in m_ghost_registry:
        # It's a "Ghost Hit" (a miss on cache, but hit on history).
        # We restore it with the current baseline frequency plus a small bonus
        # to ensure it survives longer this time.
        initial_freq = m_min_frequency + 1
        
        # Clean up registry
        m_ghost_registry.remove(obj.key)
    else:
        # Completely new item.
        # LFU-DA: Start at the current minimum frequency in the cache.
        # This prevents the "count=1" problem where new items die immediately
        # against old items with count=100.
        initial_freq = m_min_frequency + 1
        
    m_key_frequency[obj.key] = initial_freq

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up cache metadata.
    2. Add to Ghost Registry.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_registry
    
    # Remove metadata for the object physically leaving the cache
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # Crucial: We remove the frequency from the main map to prevent memory leaks
    # on infinite streams, but we mark it in the ghost registry.
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    # Add to ghost registry for history tracking
    m_ghost_registry.add(evicted_obj.key)
    
    # Maintenance: Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > MAX_GHOST_SIZE:
        # Remove an arbitrary element (pop is O(1))
        m_ghost_registry.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7w476zps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsv4g_efa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx46c7m59.pickle

Iteration 50: New subsample score 0.492372 is not better than old score 0.552979, skipping
Iteration 51: Selected program 4 score: 0.23726579166666673
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpefmah2b2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoc90oxze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph4_ikib0.pickle

Iteration 51: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (Recency)
# 2. key -> frequency (Frequency history)
m_key_last_access = dict()
m_key_frequency = dict()

# Global counter for logical time
m_current_time = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive LFU-LRU Hybrid.
    
    Logic:
    1. We define a "utility" for every object currently in the cache.
    2. Utility is primarily determined by Frequency.
    3. However, to prevent "cache pollution" by stale frequent items, we incorporate Recency.
    
    Heuristic:
    We define a score: Score = Frequency_Tier + (Recency_Weight)
    
    If an object has very low frequency (1), it is a prime candidate for eviction (LRU among freq=1).
    If all objects have high frequency, we evict the LRU among the lowest available frequency tier.
    
    To implement this efficiently in O(N) for this challenge context:
    We look for the object with the lowest Reference Count.
    Break ties with Least Recently Used.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to find the candidate with:
    # 1. Lowest Frequency
    # 2. If frequencies match, Lowest Recency (Oldest access)
    #
    # Note: We cap the effective frequency used for comparison. 
    # This prevents an item with freq=1000 from blocking a new item with freq=5 forever.
    # If both are > 10, they are treated as effectively equal in "importance tier",
    # and recency decides the winner. This makes the cache adaptive to shifts in popularity.
    
    best_victim = None
    min_score = (float('inf'), float('inf'))
    
    # Frequency cap creates a "Protected" segment. Once you pass this count, 
    # you fight based on recency.
    FREQ_CAP = 5 

    for key in current_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Capping frequency impact
        effective_freq = min(freq, FREQ_CAP)
        
        # Tuple comparison: (Frequency Tier, Recency)
        # Python compares tuples element by element.
        score = (effective_freq, last_access)
        
        if score < min_score:
            min_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency to current time.
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency, m_current_time
    
    m_current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = m_current_time
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency to current time.
    2. Check if we have history (ghost entry).
       - If yes, restore previous frequency + 1.
       - If no, start frequency at 1.
    '''
    global m_key_last_access, m_key_frequency, m_current_time
    
    m_current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = m_current_time
    
    # LFU Component with Ghost List capability
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove Recency data (it is no longer in cache).
    2. Retain Frequency data, but with Decay.
       We do not delete the frequency key immediately. This serves as a "Ghost List".
       However, to prevent memory leaks in infinite streams, we should periodically prune
       or decay very old ghosts. 
       
       Simplification for this context: We do not delete `m_key_frequency` here.
    '''
    global m_key_last_access, m_key_frequency
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # Optional: Aggressive decay for evicted items to give new items a chance
    # If an item is evicted, it clearly wasn't important enough. 
    # Reduce its priority so it doesn't instantly evict someone else if it comes back 
    # unless it was actually high frequency.
    if evicted_obj.key in m_key_frequency:
        # We allow it to be "remembered", but we don't let it retain infinite power.
        # If it comes back, it starts a bit lower than it left.
        m_key_frequency[evicted_obj.key] = max(0, m_key_frequency[evicted_obj.key] - 1)
        
        # Cleanup to prevent memory leak if it drops to 0
        if m_key_frequency[evicted_obj.key] == 0:
            del m_key_frequency[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppi6b2a58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfv7gr5qb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo_icohgw.pickle

Iteration 51: New subsample score 0.802041 is not better than old score 0.837168, skipping
Iteration 52: Selected program 14 score: 0.24068325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd11etfvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7_n4el9c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphbwbdsip.pickle

Iteration 52: Proposed new text for program: # Metadata storage
# S_prob: Probationary Segment (List of keys, Index 0 is LRU, Index -1 is MRU)
# S_prot: Protected Segment (List of keys, Index 0 is LRU, Index -1 is MRU)
# meta_sizes: Dictionary mapping key -> size_in_bytes (to track sizes of non-local objects)
# current_protected_size: Integer tracking total bytes currently in S_prot
#
# Algorithm: Byte-Aware Segmented LRU (SLRU)
# This approach specifically targets the low hit rates observed in the traces by:
# 1. Filtering "one-hit wonders" in the Probationary segment.
# 2. Managing the Protected segment based on Bytes (not counts) to strictly respect capacity.
# 3. preventing massive objects from polluting the Protected segment.

S_prob = []
S_prot = []
meta_sizes = {}
current_protected_size = 0

# Configuration
# 80% of capacity is reserved for items proven to be popular (hit > 1 time)
PROTECTED_RATIO = 0.8  
# Items larger than 50% of total capacity are denied entry to Protected to prevent thrashing
HUGE_OBJECT_RATIO = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Logic:
    Always prefer evicting from the Probationary segment (S_prob) first.
    We only evict from Protected (S_prot) if Probationary is completely empty.
    
    This preserves the "hot" data in S_prot as long as possible.
    '''
    global S_prob, S_prot

    # 1. Try to evict the LRU of Probationary segment
    if S_prob:
        return S_prob[0]
    
    # 2. Fallback: If Probationary is empty, evict LRU of Protected segment
    if S_prot:
        return S_prot[0]
        
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in Protected: Move to MRU of Protected.
    2. If in Probationary: Promote to Protected (MRU), potentially demoting others.
    '''
    global S_prob, S_prot, meta_sizes, current_protected_size
    
    key = obj.key
    new_size = obj.size
    
    # Update tracked size (in case object changed size or is first time tracking this specific logic)
    # If the object is already in Protected, we must adjust the running total.
    if key in meta_sizes:
        if key in S_prot:
            current_protected_size -= meta_sizes[key]
            current_protected_size += new_size
    meta_sizes[key] = new_size

    if key in S_prot:
        # Case A: Hit in Protected Segment
        # Simple LRU update: Move to MRU
        S_prot.remove(key)
        S_prot.append(key)
        
    elif key in S_prob:
        # Case B: Hit in Probationary Segment
        S_prob.remove(key)
        
        # Filter: Don't promote massive objects that would thrash the protected segment.
        # If an object is huge, we treat it as a hit, but keep it in Probationary (MRU).
        capacity = cache_snapshot.capacity
        if new_size > (capacity * HUGE_OBJECT_RATIO):
            S_prob.append(key)
        else:
            # Promote to Protected (MRU)
            S_prot.append(key)
            current_protected_size += new_size
            
            # Enforce Byte-Limit on Protected Segment
            # If Protected grows too large, demote LRU of Protected -> MRU of Probationary
            target_protected_size = capacity * PROTECTED_RATIO
            
            while current_protected_size > target_protected_size and S_prot:
                # Identify victim to demote (LRU of Protected)
                victim_key = S_prot.pop(0)
                victim_size = meta_sizes.get(victim_key, 0)
                
                # Update protected size
                current_protected_size -= victim_size
                
                # Demote to Probationary MRU (Second Chance)
                # It enters Probationary as "recently used", so it won't be evicted immediately.
                S_prob.append(victim_key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    New objects always start in the Probationary segment (MRU).
    They must prove their worth (via a hit) to move to Protected.
    '''
    global S_prob, meta_sizes
    
    key = obj.key
    S_prob.append(key)
    meta_sizes[key] = obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up metadata and size counters.
    '''
    global S_prob, S_prot, meta_sizes, current_protected_size
    
    key = evicted_obj.key
    
    if key in S_prob:
        S_prob.remove(key)
    elif key in S_prot:
        S_prot.remove(key)
        if key in meta_sizes:
            current_protected_size -= meta_sizes[key]
            
    if key in meta_sizes:
        del meta_sizes[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf1b2c80z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe_4q3jaj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_dwhq639.pickle

Iteration 52: New subsample score 0.344473 is not better than old score 0.432175, skipping
Iteration 53: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpel2laeqf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplqxoquuq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp049qegjh.pickle

Iteration 53: Proposed new text for program: # Metadata storage
# 1. m_last_access: key -> access_count (Recency tracking)
# 2. m_probation: key -> bool (True if the key is in the "probationary" segment)
# 3. m_ghost: key -> timestamp (Tracks recently evicted probationary items)

m_last_access = dict()
m_probation = dict()
m_ghost = dict()

# Constants
# We limit the ghost size to avoid infinite growth, but keep it large enough to catch loops.
# A safe heuristic is roughly 2x the number of active keys, or a fixed reasonable cap.
MAX_GHOST_SIZE = 5000 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU).
    
    Logic:
    The cache is logically divided into two segments:
    1. Probationary Segment: Items accessed only once recently.
    2. Protected Segment: Items accessed at least twice.
    
    Eviction Preference:
    We always try to evict the Least Recently Used (LRU) item from the Probationary segment first.
    This filters out "scan" traffic (one-time accesses) preventing them from polluting the cache.
    Only if the Probationary segment is empty do we evict from the Protected segment.
    '''
    global m_last_access, m_probation
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # Identify candidates in each segment
    probation_candidates = []
    protected_candidates = []
    
    for k in current_keys:
        # If key is in m_probation and True, it's probationary. Otherwise protected.
        # Default to probationary if state is missing (safe fallback).
        if m_probation.get(k, True):
            probation_candidates.append(k)
        else:
            protected_candidates.append(k)
    
    victim_key = None
    
    # 1. Try to evict LRU from Probationary segment
    if probation_candidates:
        victim_key = min(probation_candidates, key=lambda k: m_last_access.get(k, 0))
    
    # 2. If no probationary items, evict LRU from Protected segment
    elif protected_candidates:
        victim_key = min(protected_candidates, key=lambda k: m_last_access.get(k, 0))
    
    # 3. Fallback (should not be reached if cache has keys)
    else:
        victim_key = min(current_keys, key=lambda k: m_last_access.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    The item has proven its value.
    1. Update its Last Access timestamp.
    2. Promote it to the "Protected" segment (remove from Probationary).
    '''
    global m_last_access, m_probation
    
    m_last_access[obj.key] = cache_snapshot.access_count
    # Mark as NOT probationary (i.e., Protected)
    m_probation[obj.key] = False

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Check Ghost Cache (History):
       - If key is in Ghost Cache, it means it was recently evicted from the Probationary segment.
         This implies a looping pattern or working set slightly larger than cache.
         We promote it directly to "Protected" so it survives longer.
       - If not in Ghost Cache, it is a new item. Start it in "Probationary".
    '''
    global m_last_access, m_probation, m_ghost
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost:
        # It's a "Ghost Hit" - this item was recently evicted.
        # Promote directly to Protected.
        m_probation[obj.key] = False
        del m_ghost[obj.key]
    else:
        # First time seeing this (or seen very long ago).
        # Start in Probationary.
        m_probation[obj.key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up main metadata.
    2. If the victim was in the "Protected" segment, we usually just drop it (it had its chance).
       If it was "Probationary", we add it to the Ghost Cache. 
       (Note: SLRU often tracks ghosts for both, but tracking probationary ghosts is most critical 
       for filtering scans vs loops).
    '''
    global m_last_access, m_probation, m_ghost
    
    # Check if it was probationary before removing metadata
    is_probationary = m_probation.get(evicted_obj.key, True)
    
    # 1. Cleanup main metadata
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    if evicted_obj.key in m_probation:
        del m_probation[evicted_obj.key]

    # 2. Add to Ghost Cache if it was probationary
    # This helps us distinguish "scan" (never accessed again) from "loop" (accessed again soon).
    if is_probationary:
        m_ghost[evicted_obj.key] = cache_snapshot.access_count
        
        # 3. Manage Ghost Size
        if len(m_ghost) > MAX_GHOST_SIZE:
            # Remove oldest ghost entry
            # Finding min is O(N) relative to ghost size, but ghost size is capped.
            oldest = min(m_ghost, key=m_ghost.get)
            del m_ghost[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq5qacocl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqwr1abpa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgzzjtvee.pickle

Iteration 53: New subsample score 0.362557 is not better than old score 0.36481, skipping
Iteration 54: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6x862z8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprwh3khv9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppoei_y1g.pickle

Iteration 54: Proposed new text for program: # Metadata storage
# 1. m_protected: Set of keys considered "Hot" (Protected Segment).
# 2. m_probation: Set of keys considered "Cold" (Probationary Segment).
# 3. m_history: dict acting as an ordered LRU for non-resident keys (Ghost Cache).
#    Value is arbitrary (True), as we rely on dictionary ordering for LRU.
# 4. m_access_order: A simple OrderedDict-like mechanism isn't strictly needed 
#    because Python 3.7+ dicts preserve insertion order. We will use the 
#    cache_snapshot.cache itself (implicitly) or auxiliary dicts to track order.
#    However, to be explicit and robust about segment moves, we maintain 
#    two separate OrderedDict-style dictionaries for recency.

# We will use simple dicts. To refresh an item, we del and re-insert it.
# To pop the LRU, we use next(iter(dict)).

m_protected_lru = dict()   # Key -> None (Used as Ordered Set for Hot items)
m_probation_lru = dict()   # Key -> None (Used as Ordered Set for Cold items)
m_history_lru = dict()     # Key -> None (Ghost/Non-resident items)

# Constants
PROTECTED_RATIO = 0.80     # 80% of capacity reserved for hot items
MAX_HISTORY_SIZE = 5000    # Limit history size to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Policy:
    1. Prefer evicting from the Probationary (Cold) segment.
    2. If Probationary is empty, evict from Protected (Hot).
    '''
    global m_protected_lru, m_probation_lru
    
    # Primary Candidate: The LRU item of the Probationary segment
    if m_probation_lru:
        # The first key in a standard python dict is the oldest inserted (LRU)
        victim = next(iter(m_probation_lru))
        return victim
        
    # Fallback: If no probationary items, we must evict from protected.
    # This usually happens if the cache is filling up with only "hot" items.
    if m_protected_lru:
        victim = next(iter(m_protected_lru))
        return victim
        
    # Emergency fallback (should not be reached if cache is non-empty)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in Protected: Move to MRU position in Protected.
    2. If in Probationary: Promote to Protected (it has proven its worth).
    '''
    global m_protected_lru, m_probation_lru
    
    key = obj.key
    
    if key in m_protected_lru:
        # Refresh in Protected LRU
        del m_protected_lru[key]
        m_protected_lru[key] = None
        
    elif key in m_probation_lru:
        # Promote Probationary -> Protected
        del m_probation_lru[key]
        m_protected_lru[key] = None
        
        # Balancing: If Protected is too big, move LRU of Protected -> MRU of Probationary
        _balance_segments(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cold Miss):
    1. If key is in History (Ghost): It was evicted recently. This is a "recurrence".
       Insert directly into Protected (Hot).
    2. If key is NOT in History: It is new. Insert into Probationary (Cold).
    '''
    global m_protected_lru, m_probation_lru, m_history_lru
    
    key = obj.key
    
    # Check History
    if key in m_history_lru:
        # It's a return visitor! Promote to Protected immediately.
        # Remove from history
        del m_history_lru[key]
        
        # Add to Protected MRU
        m_protected_lru[key] = None
        
        # Balance segments (Protected might now be too big)
        _balance_segments(cache_snapshot)
    else:
        # Brand new item. Insert into Probationary MRU.
        m_probation_lru[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from valid LRU segments.
    2. Add to History (Ghost) cache to remember this key recently existed.
    '''
    global m_protected_lru, m_probation_lru, m_history_lru
    
    key = evicted_obj.key
    
    # Cleanup Resident Metadata
    if key in m_probation_lru:
        del m_probation_lru[key]
        # Only add to history if it was evicted from Probationary.
        # (If it was Protected, it degrades to Probationary usually, 
        # but if we forced evict Protected, we still track it).
        _add_to_history(key)
        
    elif key in m_protected_lru:
        del m_protected_lru[key]
        _add_to_history(key)

def _add_to_history(key):
    '''Adds key to history, maintaining history size limit.'''
    global m_history_lru
    
    # Add to MRU of history
    if key in m_history_lru:
        del m_history_lru[key]
    m_history_lru[key] = None
    
    # Enforce History Size Limit
    if len(m_history_lru) > MAX_HISTORY_SIZE:
        # Remove LRU of history
        oldest = next(iter(m_history_lru))
        del m_history_lru[oldest]

def _balance_segments(cache_snapshot):
    '''
    Ensures the Protected segment doesn't exceed its ratio capacity.
    If it does, the LRU item from Protected is demoted to the MRU of Probationary.
    '''
    global m_protected_lru, m_probation_lru
    
    # We estimate capacity based on count for simplicity and speed.
    # While cache capacity is bytes, count-based SLRU is a standard approximation.
    total_items = len(cache_snapshot.cache)
    if total_items == 0: 
        return

    target_protected_count = int(total_items * PROTECTED_RATIO)
    
    # If Protected is overflowing
    while len(m_protected_lru) > target_protected_count:
        if not m_protected_lru:
            break
            
        # Identify LRU of Protected (first item in dict)
        demoted_key = next(iter(m_protected_lru))
        
        # Remove from Protected
        del m_protected_lru[demoted_key]
        
        # Move to MRU of Probationary
        # Note: We must ensure it's treated as MRU in probation (most recently added)
        if demoted_key in m_probation_lru:
            del m_probation_lru[demoted_key]
        m_probation_lru[demoted_key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbusn2g5z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpomrxj5k_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw7ra7mg_.pickle

Iteration 54: New subsample score 0.522141 is not better than old score 0.582988, skipping
Iteration 55: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp79fnpm43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpof863s0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi7x8kuo6.pickle

Iteration 55: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (int)
# 2. key -> frequency (int)
m_key_last_access = dict()
m_key_frequency = dict()

# Ghost Cache: Stores keys of recently evicted items to detect "cyclic" or "thrashing" patterns.
# If an item enters, gets evicted, and comes back quickly, it gets a frequency boost.
m_ghost_entries = set()
MAX_GHOSTS = 2000

# Optimization constants
# Used to age frequency counts so history doesn't dominate forever.
AGE_THRESHOLD = 1000  # Operations before aging occurs
op_counter = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Size-Adjusted LFU with Recency Tie-Breaker.
    
    Goal: Maximize hits-per-byte.
    Logic: Evict the object that provides the least value relative to its size.
    
    Metric = Frequency / Size.
    
    If two objects have similar "value density", we evict the LRU one.
    To make this efficient and stable:
    1. Primary Sort: Frequency (Low freq = high chance of eviction)
    2. Secondary Sort: Recency (Oldest access = high chance of eviction)
    3. Tertiary Sort: Size (Larger size = high chance of eviction, purely to clear space faster)
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    victim_key = None
    # We want to minimize the utility we throw away.
    # Utility roughly equals Frequency.
    # However, large items take up more space. 
    # Heuristic: Minimize (Frequency * RecencyFactor) / Size? 
    # Or simply: Find min (Frequency, Last_Access)
    
    # Let's stick to a robust LFU-LRU hybrid. 
    # We search for the item with the smallest Frequency.
    # Ties are broken by staleness (smallest last_access_time).
    # We prefer evicting larger items if frequencies are equal (to clear more space).
    
    # Init with values that will definitely be replaced
    min_freq = float('inf')
    min_access = float('inf')
    max_size = -1
    
    for key in current_keys:
        freq = m_key_frequency.get(key, 1)
        access = m_key_last_access.get(key, 0)
        
        # Access the object from snapshot to get size
        # (Assuming the object in cache is the same wrapper structure as 'obj')
        cached_obj = cache_snapshot.cache[key]
        size = cached_obj.size
        
        # Comparison Logic:
        # 1. Strictly prefer evicting lower frequency items.
        if freq < min_freq:
            min_freq = freq
            min_access = access
            max_size = size
            victim_key = key
        elif freq == min_freq:
            # 2. If frequencies are equal, evict the older one (LRU).
            if access < min_access:
                min_access = access
                max_size = size
                victim_key = key
            elif access == min_access:
                # 3. If freq and recency are identical, evict the larger one 
                # to free up more space (Greedy space reclamation).
                if size > max_size:
                    max_size = size
                    victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (Recency).
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Linear increment. 
    # Simple counting is often more robust than complex logarithmic aging for general traces.
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Determine Initial Frequency (Ghost check).
    3. Perform periodic aging to prevent frequency saturation.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries, op_counter
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Check Ghost Cache (Admission Policy)
    if obj.key in m_ghost_entries:
        # It was here recently. This suggests a working set larger than cache 
        # or a cyclic pattern. Give it a second chance (boost priority).
        # We give it an initial frequency of 2 (or higher) instead of 1.
        m_key_frequency[obj.key] = 5 
        m_ghost_entries.remove(obj.key)
    else:
        # New item. Start at bottom of LFU queue.
        m_key_frequency[obj.key] = 1
        
    # Global Aging:
    # To prevent "cache pollution" where items with 10,000 hits from yesterday 
    # stay in the cache forever despite not being accessed today.
    op_counter += 1
    if op_counter > AGE_THRESHOLD:
        op_counter = 0
        # Decay mechanism: Divide all frequencies by 2.
        # This keeps relative order but allows new items to catch up 
        # to old "hot" items that have gone cold.
        keys_to_remove = []
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
            if m_key_frequency[k] == 0:
                 # If it decays to 0, usually we keep it at 1 if it's in cache, 
                 # or remove from map if it's metadata-only.
                 # Since this map tracks active cache items mostly, let's floor at 0.
                 pass
                 
        # Note: In a real high-perf system, you wouldn't iterate all keys. 
        # You would use a global "epoch" counter subtracted from stored values.
        # But for this simulation, explicit decay is accurate and acceptable.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata.
    2. Add to Ghost Cache to detect thrashing.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries
    
    key = evicted_obj.key
    
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    # Add to ghost entries
    m_ghost_entries.add(key)
    
    # Maintain ghost size cap (FIFO removal via set iteration isn't perfect but sufficient)
    if len(m_ghost_entries) > MAX_GHOSTS:
        # Remove an arbitrary element (sets are unordered)
        # In python, pop() removes an arbitrary element.
        m_ghost_entries.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnlps2gd1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv85v2s7o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4u4sq6z5.pickle

Iteration 55: New subsample score 0.6429309999999999 is better than old score 0.6328769999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmayr5h6n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppb7lq54o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwnf1lnmg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0b27e6s3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpesmnxw75.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfcp1u5q4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpodtgoq0c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwj8squv0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5buxsi1s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp61k2tl30.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1rrgahd4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbkkctacd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2mjpntxb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzzh683sm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw0u2f60i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv52m3xy6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8na15oji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcckg40ww.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcql_r763.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa5frht3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppquv3_rv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnm3iwetv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnzjz42rz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk0dc08k7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnl7ocdtr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbtp5qq91.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn44x0pqq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl6g_9poj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8tt202tj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9w6yveym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptf7t9pyj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk2dzfcw4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2rw6hf7f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz4n3xe7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplytnnmbz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp19ac5bgd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzevcawag.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp781ufqr5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd5qeeqlo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4z4qwfgm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp24rp3ndz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppu7zsd76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqk9myk97.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp49hvhazy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppit6aa_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqvn35w5l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4_6mxtyr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfcp82sx5.pickle

Iteration 55: Full valset score for new program: 0.24230047916666667
Iteration 55: Full train_val score for new program: 0.24230047916666667
Iteration 55: Individual valset scores for new program: [0.502421, 0.475951, 0.485365, 0.435672, 0.494464, 0.486106, 0.271531, 0.498624, 0.541294, 0.531017, 0.091667, 0.396092, 0.020097, 0.0, 0.019397, 0.019583, 0.018985, 0.022772, 0.021797, 0.272227, 0.402163, 0.025184, 0.057382, 0.057382, 0.332168, 0.364919, 0.789474, 0.894232, 0.039533, 0.038636, 0.045558, 0.003029, 0.020672, 0.72243, 0.078947, 0.067079, 0.024198, 0.641937, 0.125461, 0.090657, 0.06292, 0.0723, 0.050987, 0.366667, 0.041778, 0.061711, 0.466258, 0.081699]
Iteration 55: New valset pareto front scores: [0.509148, 0.479512, 0.493182, 0.439584, 0.50361, 0.489969, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.396092, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 55: Full valset pareto front score: 0.26160258333333325
Iteration 55: Updated valset pareto front programs: [{2}, {14}, {2}, {2}, {14}, {13}, {8, 2}, {0, 3, 7, 9, 12, 15}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {11}, {15}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15}, {15}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15}, {1, 4, 5, 6, 10, 11, 13, 14, 15}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11, 13, 14}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15}, {1, 5, 9, 10, 13, 14, 15}]
Iteration 55: Best valset aggregate score so far: 0.2508065
Iteration 55: Best program as per aggregate score on train_val: 5
Iteration 55: Best program as per aggregate score on valset: 5
Iteration 55: Best score on valset: 0.2508065
Iteration 55: Best score on train_val: 0.2508065
Iteration 55: Linear pareto front program index: 5
Iteration 55: New program candidate index: 15
Iteration 56: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp06q30ixz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpprj3ajzx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdvwqhg1d.pickle

Iteration 56: Proposed new text for program: # Metadata storage
# Using dictionaries as Ordered Sets (Python 3.7+ guarantees insertion order).
# Keys are object keys. Values can be dummy (None) or timestamps.
# 1. m_protected: Items hit > 1 time. Hot segment.
# 2. m_probation: Items hit 1 time. Cold/New segment.
# 3. m_ghost: Metadata for items recently evicted from probation.

m_protected = dict()
m_probation = dict()
m_ghost = dict()

# Constants
# We reserve 80% of items for the protected segment. 
# This prevents hot items from being flushed by large scans, 
# but gives scans 20% of the cache to flow through.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Policy.
    
    Goal: Maintain segment sizes and evict the Least Recently Used item 
    from the Probation segment.
    '''
    global m_protected, m_probation
    
    # 1. Check if we need to rebalance segments.
    # While Protected is too big, move its LRU to Probation (Demotion).
    # We use counts of keys as a proxy for capacity management.
    total_items = len(cache_snapshot.cache)
    target_protected_count = int(total_items * PROTECTED_RATIO)
    
    # Using iter() to get the first item (LRU) in O(1)
    while len(m_protected) > target_protected_count and len(m_protected) > 0:
        lru_protected_key = next(iter(m_protected))
        
        # Demote: Remove from Protected, Add to Probation (as MRU of probation)
        del m_protected[lru_protected_key]
        m_probation[lru_protected_key] = None 

    # 2. Pick a victim.
    # The victim should be the LRU of the Probation segment.
    victim_key = None
    
    if m_probation:
        victim_key = next(iter(m_probation))
    elif m_protected:
        # Fallback: If probation is empty (rare), evict from protected.
        victim_key = next(iter(m_protected))
    
    # Safety check: Ensure the key is actually in the cache snapshot.
    # (Metadata might strictly drift if external forces act on cache, though unlikely here).
    if victim_key and victim_key in cache_snapshot.cache:
        return victim_key
        
    # Ultimate fallback if metadata is out of sync
    keys = list(cache_snapshot.cache.keys())
    return keys[0] if keys else None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in Protected: Move to MRU of Protected.
    2. If in Probation: Promote to Protected (MRU).
    '''
    global m_protected, m_probation
    key = obj.key
    
    if key in m_protected:
        # Move to end (MRU)
        del m_protected[key]
        m_protected[key] = None
        
    elif key in m_probation:
        # Promote to Protected
        del m_probation[key]
        m_protected[key] = None
    
    else:
        # Edge case: Item in cache but missing from metadata (sync repair)
        # Assume it's a new protected item if it's getting hits.
        m_protected[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. If key is in Ghost: It was seen recently. Promote directly to Protected.
    2. Else: Insert into Probation (MRU).
    '''
    global m_protected, m_probation, m_ghost
    key = obj.key
    
    if key in m_ghost:
        # Ghost Hit! This item was evicted but is needed again. 
        # It belongs in the working set.
        del m_ghost[key]
        m_protected[key] = None
    else:
        # Standard Insert
        m_probation[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from valid segments.
    2. If it was in Probation, move to Ghost.
    3. Cap the size of Ghost.
    '''
    global m_protected, m_probation, m_ghost
    key = evicted_obj.key
    
    if key in m_probation:
        del m_probation[key]
        # Add to ghost (history of "misses that were recently evicted")
        m_ghost[key] = None
        
    elif key in m_protected:
        del m_protected[key]
        # Usually we don't put demoted protected items into ghost, 
        # but purely probation items.
        
    # Limit Ghost size to prevent memory leaks. 
    # Keeping ghost size roughly equal to cache capacity is a standard heuristic.
    # Since we don't have exact capacity count constant, we rely on current size.
    max_ghost_size = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 1000
    
    while len(m_ghost) > max_ghost_size:
        # Remove oldest ghost entry
        oldest = next(iter(m_ghost))
        del m_ghost[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4y3f6e6h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8wxmzh1g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3qebg96h.pickle

Iteration 56: New subsample score 0.705221 is better than old score 0.660604. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_0767mao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnqnzowis.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf6re21s0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnadd6x_a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmkc8ms97.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5mip2t3a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf1vt4_5x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpitcirb8x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3ppc7num.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgdmj0xpd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn8pkdm_0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfm4mgk4m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0x58hi0l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphzyxgt2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3cp6xfkq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp46rj0r09.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsq15bcbf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcq4obbfc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3s16znpv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqwp6p32h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpszml4etd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph6pfsx08.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp11gqn0x8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppsf0pxqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd5kl7cvs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfh1zhczf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo08kbtl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpooattsu3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp25nyoufi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp12qpoauq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe89stzl4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkq5vj1q2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgkz8an2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwdduyibn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwoegefmk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4sz7sxvk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3_ymg22g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph44dw0rb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdj3995ql.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz_ey9vl6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeo95vdps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxed9t147.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph1yt7lx4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkyx6yixj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz71axk1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp61j7pq6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpozbkoyme.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjgcglrqh.pickle

Iteration 56: Full valset score for new program: 0.24827693749999993
Iteration 56: Full train_val score for new program: 0.24827693749999993
Iteration 56: Individual valset scores for new program: [0.498601, 0.47029, 0.480153, 0.43336, 0.495106, 0.480687, 0.272727, 0.498624, 0.539864, 0.531017, 0.075, 0.33881, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.366765, 0.026164, 0.058672, 0.058672, 0.269798, 0.379032, 0.825223, 0.888242, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.754595, 0.083333, 0.067961, 0.076964, 0.640392, 0.125461, 0.125312, 0.111179, 0.148308, 0.052632, 0.283333, 0.16269, 0.120898, 0.466258, 0.081699]
Iteration 56: New valset pareto front scores: [0.509148, 0.479512, 0.493182, 0.439584, 0.50361, 0.489969, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.396092, 0.050912, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.765146, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 56: Full valset pareto front score: 0.26160258333333325
Iteration 56: Updated valset pareto front programs: [{2}, {14}, {2}, {2}, {14}, {13}, {8, 2}, {0, 3, 7, 9, 12, 15, 16}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {11}, {15}, {9}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16}, {15}, {2, 4, 8, 9, 10}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16}, {9}, {9}, {9}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16}, {1, 5, 9, 10, 13, 14, 15, 16}]
Iteration 56: Best valset aggregate score so far: 0.2508065
Iteration 56: Best program as per aggregate score on train_val: 5
Iteration 56: Best program as per aggregate score on valset: 5
Iteration 56: Best score on valset: 0.2508065
Iteration 56: Best score on train_val: 0.2508065
Iteration 56: Linear pareto front program index: 5
Iteration 56: New program candidate index: 16
Iteration 57: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpelee2po6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9k04yszq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcimbwk9e.pickle

Iteration 57: Proposed new text for program: # Metadata storage
# Using dictionaries. In Python 3.7+, standard dicts preserve insertion order.
# We will treat the "end" of the dict as the Most Recently Used/Top of Stack
# and the "start" (iterator) as the Least Recently Used/Bottom of Stack.

# 1. m_state: key -> 'LIR' or 'HIR'
#    Defines the status of a resident key.
m_state = dict()

# 2. m_stack: key -> None (Conceptually Set/Stack S)
#    Contains LIR blocks and HIR blocks with valid recency.
#    Top (newest) is at the end. Bottom (oldest) is at the start.
m_stack = dict()

# 3. m_hir_q: key -> None (Conceptually Queue Q)
#    Contains only Resident HIR blocks. Used to determine eviction candidates quickly.
#    Front (eviction candidate) is at the start.
m_hir_q = dict()

# 4. m_lir_count: int
#    Tracks number of LIR items currently in cache (to avoid O(N) counting).
m_lir_count = 0

# Constants
# LIR items are "protected". We allow a high percentage of cache to be LIR.
# If LIR set grows too large, we demote the least active LIR.
# 95% allows for a large working set, 5% buffer for scanning.
LIR_RATIO = 0.99 

def _move_to_stack_top(key):
    """Moves key to the top (end) of Stack S."""
    global m_stack
    if key in m_stack:
        del m_stack[key]
    m_stack[key] = None

def _move_to_queue_end(key):
    """Moves key to the end of HIR Queue Q."""
    global m_hir_q
    if key in m_hir_q:
        del m_hir_q[key]
    m_hir_q[key] = None

def _prune_stack(cache_snapshot):
    """
    Maintains LIRS invariant: The bottom of Stack S must be an LIR entry.
    If the bottom is HIR, that HIR block doesn't have enough recency to be LIR,
    so we remove it from the Stack (it effectively becomes a 'cold' HIR).
    """
    global m_stack, m_state
    
    # In Python dicts, iterating gives keys in insertion order.
    # We loop until we find an LIR block or the stack is empty.
    # Note: To avoid re-creating the iterator constantly, we just peek/pop.
    
    while m_stack:
        # Get the bottom item (first inserted)
        bottom_key = next(iter(m_stack))
        
        state = m_state.get(bottom_key)
        
        if state == 'LIR':
            # Invariant satisfied. Stop.
            break
        
        # It is an HIR item (resident or non-resident).
        # Remove it from the stack. It loses its status as a "candidate" for promotion.
        del m_stack[bottom_key]
        
        # If it's a non-resident HIR (not in cache anymore), we are done with it entirely.
        # If it's a resident HIR, it stays in cache and m_hir_q, but leaves m_stack.

def evict(cache_snapshot, obj):
    """
    Eviction Policy:
    1. Prefer evicting a resident HIR item from m_hir_q.
    2. If no HIR items exist (unlikely), evict the bottom of Stack S (an LIR item).
    """
    global m_hir_q, m_stack, m_state, m_lir_count

    # 1. Try to evict from HIR Queue Q
    if m_hir_q:
        victim = next(iter(m_hir_q))
        return victim
    
    # 2. Fallback: Evict from Stack S (Must be LIR if Q is empty)
    if m_stack:
        victim = next(iter(m_stack))
        return victim
    
    # 3. Absolute fallback (should not happen in populated cache)
    current_keys = list(cache_snapshot.cache.keys())
    if current_keys:
        return current_keys[0]
    return None

def update_after_hit(cache_snapshot, obj):
    """
    LIRS Hit Logic:
    - If LIR: Move to Top of Stack. Prune bottom.
    - If HIR:
      - In Stack: HOT! Promote to LIR. Remove from Q. Fix LIR overflow.
      - Not in Stack: COLD. Stay HIR. Move to Top of Stack. Move to End of Q.
    """
    global m_state, m_stack, m_hir_q, m_lir_count
    
    key = obj.key
    state = m_state.get(key, 'HIR') # Default safe guard, though hit implies resident
    
    if state == 'LIR':
        # Accessing an LIR block
        _move_to_stack_top(key)
        _prune_stack(cache_snapshot)
        
    elif state == 'HIR':
        if key in m_stack:
            # Case: HIR is in Stack S. This indicates high temporal locality.
            # Action: Promote to LIR.
            m_state[key] = 'LIR'
            m_lir_count += 1
            
            _move_to_stack_top(key)
            
            # Remove from HIR Queue since it's now LIR
            if key in m_hir_q:
                del m_hir_q[key]
            
            # If we have too many LIR items, convert the bottom-most LIR to HIR
            target_lir = int(len(cache_snapshot.cache) * LIR_RATIO)
            if m_lir_count > target_lir and target_lir > 0:
                # To demote, we need to find the LIR at the bottom of the stack.
                # Thanks to _prune_stack, the absolute bottom of m_stack IS the LRU LIR.
                
                bottom_key = next(iter(m_stack))
                # Demote bottom key
                m_state[bottom_key] = 'HIR'
                m_lir_count -= 1
                
                # Move to HIR Queue
                _move_to_queue_end(bottom_key)
                
                # Prune Stack (since bottom is now HIR)
                _prune_stack(cache_snapshot)
        else:
            # Case: HIR is NOT in Stack S.
            # Action: Keep as HIR. Make it the newest HIR.
            _move_to_stack_top(key)
            _move_to_queue_end(key)

def update_after_insert(cache_snapshot, obj):
    """
    Insertion Logic (New Item):
    1. If key in m_stack (Non-resident HIR): Treat as HIR Hit (Promote).
    2. If key not in m_stack: Treat as new HIR.
    """
    global m_state, m_stack, m_hir_q, m_lir_count
    
    key = obj.key
    
    # Check if this is a "Ghost" hit (Non-resident HIR)
    # It is non-resident because it's an insert, but if it's in m_stack, 
    # we remember it was accessed recently.
    if key in m_stack:
        # It was in the stack history! Promote to LIR.
        m_state[key] = 'LIR'
        m_lir_count += 1
        
        _move_to_stack_top(key)
        
        # Enforce LIR capacity
        target_lir = int(len(cache_snapshot.cache) * LIR_RATIO)
        if m_lir_count > target_lir and target_lir > 0:
            bottom_key = next(iter(m_stack))
            m_state[bottom_key] = 'HIR'
            m_lir_count -= 1
            _move_to_queue_end(bottom_key)
            _prune_stack(cache_snapshot)
            
    else:
        # Completely new item. Insert as HIR.
        m_state[key] = 'HIR'
        _move_to_stack_top(key)
        _move_to_queue_end(key)
        # Note: We do not prune stack here immediately usually, 
        # but maintaining invariant is safe.
        # However, if we just inserted a new HIR at top, bottom remains unchanged
        # unless stack was empty.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Eviction Cleanup:
    - Remove from Cache tracking structures.
    - Key remains in m_stack if it was there (becomes Non-Resident HIR).
    - Remove from m_hir_q (since it's no longer resident).
    """
    global m_state, m_stack, m_hir_q, m_lir_count
    
    key = evicted_obj.key
    
    # If it was LIR (rare fallback), decrement count
    if m_state.get(key) == 'LIR':
        m_lir_count -= 1
    
    # Remove from state map (it's no longer resident)
    if key in m_state:
        del m_state[key]
        
    # Remove from HIR Queue (must be resident to be here)
    if key in m_hir_q:
        del m_hir_q[key]
        
    # NOTE: We DO NOT remove from m_stack. 
    # The key stays in m_stack as "Non-Resident HIR" metadata 
    # to facilitate promotion if it returns soon.
    # It will eventually be removed by _prune_stack when it hits the bottom.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2fc3ofv7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3485qv7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphe99aqyy.pickle

Iteration 57: New subsample score 0.9865619999999999 is better than old score 0.9860309999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeldpiiq_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv0ramnpk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk575qq1p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptoy68k9w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpml16kto8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiilh0fwe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpox_ebluj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6f80wxhu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuhkhxjva.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz8gd_1kd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1h1j9aq8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpulci94_r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz0royzhm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6x02_gsf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoixf0hup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9nwl_0t3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi6alzm_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjl49cikf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7a0venk6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu6b4orc5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp5vrwvjw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjnrufumf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpna78o4rv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplum7k08v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0v31ct0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpai3sat53.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_1xibvl5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3bw2vpcj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplmsm8_0l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy65n343t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpivdrbp0u.pickle

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm13y8m48.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6cww70o0.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp01s50tsd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwd8v0b3i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_u8awnwl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6pk9wsnf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpet63ghu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp289m77js.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm_xr7gi5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuqdriy1v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmporsqjd74.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4pi1ho3t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpos8493pl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw1vjlv1x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp27ec54m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7tj0lf2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz378uoq1.pickle

Iteration 57: Full valset score for new program: 0.2406433541666667
Iteration 57: Full train_val score for new program: 0.2406433541666667
Iteration 57: Individual valset scores for new program: [0.506457, 0.479804, 0.488879, 0.441985, 0.499224, 0.49412, 0.271531, 0.496461, 0.540579, 0.531017, 0.075, 0.431616, 0.052624, 0.0, 0.020105, 0.019724, 0.019254, 0.023053, 0.022922, 0.272227, 0.376598, 0.026556, 0.058027, 0.058027, 0.269802, 0.392137, 0.852036, 0.892008, 0.039832, 0.034091, 0.04328, 0.0, 0.0, 0.770082, 0.078947, 0.062665, 0.01826, 0.633694, 0.125461, 0.041586, 0.039096, 0.043257, 0.052632, 0.35, 0.041854, 0.040954, 0.441718, 0.081699]
Iteration 57: New valset pareto front scores: [0.509148, 0.479804, 0.493182, 0.441985, 0.50361, 0.49412, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.431616, 0.052624, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.770082, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 57: Full valset pareto front score: 0.26262374999999993
Iteration 57: Updated valset pareto front programs: [{2}, {17}, {2}, {17}, {14}, {17}, {8, 2}, {0, 3, 7, 9, 12, 15, 16}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {11}, {17}, {17}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10, 17}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17}, {15}, {2, 4, 8, 9, 10, 17}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16}, {9}, {9}, {17}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16, 17}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16}, {1, 5, 9, 10, 13, 14, 15, 16, 17}]
Iteration 57: Best valset aggregate score so far: 0.2508065
Iteration 57: Best program as per aggregate score on train_val: 5
Iteration 57: Best program as per aggregate score on valset: 5
Iteration 57: Best score on valset: 0.2508065
Iteration 57: Best score on train_val: 0.2508065
Iteration 57: Linear pareto front program index: 5
Iteration 57: New program candidate index: 17
Iteration 58: Selected program 14 score: 0.24068325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfu21vrn2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp330zjwuy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps3m9f7ve.pickle

Iteration 58: Proposed new text for program: # GDSF (Greedy Dual-Size Frequency) Algorithm Implementation
# 
# This algorithm balances Frequency, Recency, and Object Size.
# It assigns a priority score (H) to each object.
# H = L + (Frequency / Size)
# When evicting, we choose the item with the minimum H value and set L = min_H.
# This "ages" the cache; new items must justify their cost (Size) via Frequency to stay.

# Metadata globals
m_h = {}            # Stores the GDSF Priority Value (H) for each key
m_freq = {}         # Stores the Frequency count for each key
m_last_access = {}  # Stores the last access timestamp (for LRU tie-breaking)
m_l = 0.0           # The aging factor (L), starts at 0.0

def evict(cache_snapshot, obj):
    '''
    Determines the victim key to evict.
    Strategy: Find the key with the minimum Priority Value (H).
    Tie-breaker: Least Recently Used (using m_last_access).
    '''
    global m_h, m_l, m_last_access

    candidate_keys = cache_snapshot.cache.keys()
    
    if not candidate_keys:
        return None
    
    # We find the candidate with the minimum H value.
    # If H values are equal, we choose the one with the smallest last_access (LRU).
    victim_key = min(
        candidate_keys, 
        key=lambda k: (m_h.get(k, 0.0), m_last_access.get(k, 0))
    )
    
    # Update the aging factor L to the priority of the evicted item.
    # This effectively raises the bar for all future and existing items.
    m_l = m_h.get(victim_key, 0.0)
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    3. Recalculate Priority H using current L.
    '''
    global m_h, m_freq, m_last_access, m_l
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Access Info
    m_freq[key] = m_freq.get(key, 0) + 1
    m_last_access[key] = current_time
    
    # Calculate Size (protect against division by zero)
    size = max(1, obj.size)
    
    # Update Priority H
    # By adding m_l, we reset its aging "clock" relative to current cache pressure.
    m_h[key] = m_l + (m_freq[key] / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Set Last Access Time.
    3. Calculate initial Priority H.
    '''
    global m_h, m_freq, m_last_access, m_l
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # New insertion implies frequency starts at 1
    m_freq[key] = 1
    m_last_access[key] = current_time
    
    size = max(1, obj.size)
    
    # Initial Priority
    m_h[key] = m_l + (1.0 / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to prevent memory leaks.
    '''
    global m_h, m_freq, m_last_access
    
    key = evicted_obj.key
    
    # Clean up all associated metadata
    if key in m_h:
        del m_h[key]
    if key in m_freq:
        del m_freq[key]
    if key in m_last_access:
        del m_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqahhb66a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwe11d6gb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps9i8bccr.pickle

Iteration 58: New subsample score 1.523577 is not better than old score 1.548203, skipping
Iteration 59: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjpgggzm7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpacx81b5l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzj3se3in.pickle

Iteration 59: Proposed new text for program: # GDSF (Greedy Dual Size Frequency) Metadata
# g_priority: Stores the priority 'H' value (Key -> Float)
# g_freq: Stores access frequency (Key -> Int)
# g_last_access: Stores logical time (Key -> Int) for tie-breaking
g_priority = dict()
g_freq = dict()
g_last_access = dict()

# Ghost Data: Key -> Frequency (Int)
# Remembers the frequency of recently evicted items to restore them with history.
# This helps the algorithm distinguish between 'one-hit wonders' (scans) and 
# items that are part of the working set but were temporarily evicted.
g_ghosts = dict()

# Global Inflation Value (L)
# Represents the "base cost" of the cache. Increases monotonically over time
# as the cache gets "hotter".
g_inflation = 0.0

def evict(cache_snapshot, obj):
    '''
    Policy: GDSF (Greedy Dual Size Frequency)
    We evict the item with the lowest Priority (H value).
    H = L + (Frequency / Size)
    
    Upon eviction, the global inflation value L is updated to the H value of the victim.
    This mechanism ensures that objects age over time. An object inserted long ago
    will eventually have an H value lower than the rising L, leading to its eviction.
    '''
    global g_priority, g_last_access, g_inflation

    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Find victim: Minimize (Priority, Last_Access)
    # 1. Primary Sort: Priority (Lower is worse)
    # 2. Secondary Sort: Recency (Older/Smaller timestamp is worse)
    victim_key = min(current_keys, key=lambda k: (g_priority.get(k, 0.0), g_last_access.get(k, 0)))

    # GDSF Core Logic: 
    # Update the global inflation value L to the priority of the item we are throwing out.
    # New items must now beat this "bar" to survive long-term.
    g_inflation = g_priority.get(victim_key, 0.0)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using the CURRENT global inflation L.
       Formula: H = L + (Freq / Size)
    '''
    global g_priority, g_freq, g_last_access, g_inflation
    
    current_time = cache_snapshot.access_count
    
    # Update Access Time (for LRU tie-breaking)
    g_last_access[obj.key] = current_time
    
    # Update Frequency
    curr_f = g_freq.get(obj.key, 0)
    new_f = curr_f + 1
    g_freq[obj.key] = new_f
    
    # Update Priority
    # Note: We divide frequency by size. Small objects get a larger boost.
    # Large objects need very high frequency to justify their space.
    # Safe-guard against size=0 (though constraints say positive integer).
    size_val = obj.size if obj.size > 0 else 1
    g_priority[obj.key] = g_inflation + (float(new_f) / size_val)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghosts. If this is a returning item, restore its previous "heat".
    2. Calculate Initial Priority.
    '''
    global g_priority, g_freq, g_last_access, g_ghosts, g_inflation
    
    current_time = cache_snapshot.access_count
    g_last_access[obj.key] = current_time
    
    # Determine Frequency
    if obj.key in g_ghosts:
        # It's a return visitor. Restore its past frequency plus the current hit.
        new_f = g_ghosts[obj.key] + 1
        # Remove from ghosts as it is now back in cache
        del g_ghosts[obj.key]
    else:
        # Brand new item. Start with frequency 1.
        new_f = 1
        
    g_freq[obj.key] = new_f
    
    # Calculate Priority
    size_val = obj.size if obj.size > 0 else 1
    g_priority[obj.key] = g_inflation + (float(new_f) / size_val)
        
    # Ghost Management (Cleanup)
    # Keep the ghost list size roughly proportional to the cache size to track recent history.
    # Since we don't have max object count, we use current cache length as a proxy.
    # We ensure a minimum memory size for ghosts to handle small caches.
    target_ghost_len = max(10, len(cache_snapshot.cache))
    
    # Prune oldest ghosts if the list is too long
    if len(g_ghosts) > target_ghost_len:
        # In Python 3.7+, dicts preserve insertion order, behaving like a queue.
        # We delete keys from the start (oldest).
        keys_to_remove = list(g_ghosts.keys())[:(len(g_ghosts) - target_ghost_len)]
        for k in keys_to_remove:
            del g_ghosts[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Store the victim's frequency in the Ghost list.
    2. Clean up active metadata.
    '''
    global g_priority, g_freq, g_last_access, g_ghosts
    
    key = evicted_obj.key
    
    # Save frequency to Ghost list
    if key in g_freq:
        g_ghosts[key] = g_freq[key]
    
    # Remove active metadata
    if key in g_priority:
        del g_priority[key]
    if key in g_freq:
        del g_freq[key]
    if key in g_last_access:
        del g_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6gmwcjw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0zfnodtq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpti_5cl2i.pickle

Iteration 59: New subsample score 0.9464729999999999 is better than old score 0.9029929999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnzxigbjd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr5pwk0em.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptu0aprx0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph9cqrluv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4p29l7t_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5vjtjf8_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7hmj10f2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpig4d_yqh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpotay3kbj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqt9v_lyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnjuwxw3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptuleo3uz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf5qosj8b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr4k7ns_v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp14zhvrn4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7y40ob5r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9ufd_lvc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc43k75sq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo_zhnwqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpio0jvjpi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpofbgvkyz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoo05tkif.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfbu1vnx9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7eoqsmdt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvd5fo88n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb4kw35i3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5dew1mn_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_4bca_z2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_33h6z2n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphaxi0c5j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphm3y7o_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoq0uw1c8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpakutsv3m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp081oqrye.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7h57jzcc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_j51cy3q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk9f3aft9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpekhribru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgbf6u28x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6qzkpp1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxkgrzhap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkkwzptuk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqczmcxn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpprwze3gv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3rvfr453.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy6u99uol.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd0q4bowi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqu7agmu4.pickle

Iteration 59: Full valset score for new program: 0.2258390000000001
Iteration 59: Full train_val score for new program: 0.2258390000000001
Iteration 59: Individual valset scores for new program: [0.470297, 0.448984, 0.456639, 0.416289, 0.465529, 0.459472, 0.264354, 0.498624, 0.538792, 0.531017, 0.066667, 0.347691, 0.023893, 0.0, 0.01968, 0.019442, 0.018581, 0.02221, 0.021516, 0.266728, 0.339233, 0.025282, 0.057382, 0.057382, 0.277753, 0.327621, 0.845084, 0.886017, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.749489, 0.072368, 0.062665, 0.009162, 0.631118, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.283333, 0.020965, 0.021879, 0.441718, 0.045752]
Iteration 59: New valset pareto front scores: [0.509148, 0.479804, 0.493182, 0.441985, 0.50361, 0.49412, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.431616, 0.052624, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.770082, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 59: Full valset pareto front score: 0.26262374999999993
Iteration 59: Updated valset pareto front programs: [{2}, {17}, {2}, {17}, {14}, {17}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {11}, {17}, {17}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10, 17}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17}, {15}, {2, 4, 8, 9, 10, 17}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16}, {9}, {9}, {17}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16, 17}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16}, {1, 5, 9, 10, 13, 14, 15, 16, 17}]
Iteration 59: Best valset aggregate score so far: 0.2508065
Iteration 59: Best program as per aggregate score on train_val: 5
Iteration 59: Best program as per aggregate score on valset: 5
Iteration 59: Best score on valset: 0.2508065
Iteration 59: Best score on train_val: 0.2508065
Iteration 59: Linear pareto front program index: 5
Iteration 59: New program candidate index: 18
Iteration 60: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmput6aujo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxgm43ibc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmparjpt0aj.pickle

Iteration 60: Proposed new text for program: from collections import OrderedDict

# Metadata Storage
# 1. m_stack_s: OrderedDict (key -> None).
#    Represents the LIRS Stack S. 
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed.
#    Contains LIR blocks and resident/non-resident HIR blocks with high recency.
# 2. m_queue_q: OrderedDict (key -> None).
#    Represents the Resident HIR Queue Q.
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed (Candidate for eviction).
# 3. m_state: dict (key -> 'LIR' or 'HIR'). Tracks the status of resident items.
# 4. m_lir_size: int. Tracks total size of current LIR items to manage capacity dynamically.

m_stack_s = OrderedDict()
m_queue_q = OrderedDict()
m_state = dict()
m_lir_size = 0

# Configuration
# Reserve ~5% of cache for HIR items to act as a filter for scans.
LIR_TARGET_RATIO = 0.95

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction.
    Strategy: 
    1. Evict the resident HIR item at the front of Queue Q (oldest).
    2. If Q is empty (rare, implies cache is 100% LIR), we must demote an LIR item 
       (the one at the bottom of Stack S) and evict it.
    '''
    global m_queue_q, m_stack_s, m_state
    
    # 1. Try to evict from HIR Queue (Resident HIRs)
    if m_queue_q:
        # The first key in OrderedDict is the oldest (FIFO/LRU behavior)
        victim_key = next(iter(m_queue_q))
        return victim_key
        
    # 2. Fallback: If Queue Q is empty, the cache is filled entirely with LIRs.
    # We pick the LIR at the bottom of Stack S.
    if m_stack_s:
        # Find the oldest LIR. In a pruned stack, the bottom is always LIR.
        victim_key = next(iter(m_stack_s))
        return victim_key

    # 3. Last resort (should technically not be reached if cache is full)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    state = m_state.get(key)
    
    # Update recency in Stack S (Access brings it to top)
    # Note: Even if it's not in S currently (a cold resident HIR), 
    # specific logic applies below.
    
    if state == 'LIR':
        # LIR Case:
        # It is guaranteed to be in Stack S. Move to top (most recent).
        if key in m_stack_s:
            m_stack_s.move_to_end(key)
        
        # Pruning is vital here: moving an item to the top might reveal 
        # an HIR item at the bottom of the stack which is no longer "hot".
        _prune_stack()
        
    elif state == 'HIR':
        # HIR Case:
        if key in m_stack_s:
            # Case: HIR hit inside Stack S.
            # This indicates the item has been accessed again within the 
            # stack distance of the LIR set. It becomes HOT (LIR).
            
            # Change state
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            
            # Remove from HIR Queue Q
            if key in m_queue_q:
                del m_queue_q[key]
            
            # Move to top of Stack S
            m_stack_s.move_to_end(key)
            
            # Enforce LIR capacity (demote others if we grew too big)
            _enforce_lir_capacity(cache_snapshot)
            _prune_stack()
            
        else:
            # Case: HIR hit outside Stack S.
            # It is a resident HIR, but accessed too infrequently to become LIR yet.
            # It goes to the top of Stack S (tentatively) and to the end of Queue Q.
            m_stack_s[key] = None
            m_queue_q.move_to_end(key)
            # No pruning usually needed here as we added to top, but specific
            # implementations might vary.

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    
    # Check if this key exists in Stack S (Non-Resident History)
    if key in m_stack_s:
        # It was evicted recently but is still in our "Hot" history scope.
        # This is the "LIRS" magic: restore it as LIR immediately.
        m_state[key] = 'LIR'
        m_lir_size += obj.size
        
        # Move to top of S
        m_stack_s.move_to_end(key)
        
        # Enforce limits
        _enforce_lir_capacity(cache_snapshot)
        _prune_stack()
    else:
        # Completely new or very old item. Start as HIR.
        m_state[key] = 'HIR'
        m_stack_s[key] = None
        m_queue_q[key] = None # Add to Queue Q
        # We don't prune here because we added to top of S.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    Crucial: Do NOT remove from Stack S blindly. Stack S holds history (ghost entries).
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = evicted_obj.key
    
    # 1. Remove from Queue Q (Resident HIRs)
    if key in m_queue_q:
        del m_queue_q[key]
        
    # 2. Update State and Size
    if key in m_state:
        if m_state[key] == 'LIR':
            m_lir_size -= evicted_obj.size
        del m_state[key]
        
    # 3. Stack S Maintenance
    # If the evicted item was in Stack S, it stays there! 
    # It becomes a "Non-Resident HIR" entry, used to calculate IRR for future accesses.
    # However, if it moves to the bottom of Stack S later, _prune_stack will remove it.
    
    _prune_stack()

def _prune_stack():
    '''
    Ensures the bottom of Stack S is always an LIR item.
    If the bottom is an HIR item (resident or non-resident), it is removed.
    This limits the size of Stack S and ensures it only tracks relevant history.
    '''
    global m_stack_s, m_state
    
    # Peek at the bottom (first item) of the OrderedDict
    while m_stack_s:
        bottom_key = next(iter(m_stack_s))
        
        # If the bottom item is LIR (and resident), the stack is valid. Stop.
        # We check m_state to verify residency and status.
        if bottom_key in m_state and m_state[bottom_key] == 'LIR':
            break
            
        # Otherwise, the bottom is either:
        # 1. A Resident HIR item (shouldn't define the stack bottom).
        # 2. A Non-Resident item (history too old, forgotten).
        # Remove it.
        m_stack_s.popitem(last=False)
        
        # Note: If it was a Resident HIR, it remains in Queue Q and the Cache,
        # it simply loses its place in the LIRS history stack.

def _enforce_lir_capacity(cache_snapshot):
    '''
    Prevents LIR set from consuming 100% of cache, starving new items.
    Demotes LRU LIRs to HIRs if budget exceeded.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    # Target capacity for LIR set
    capacity = cache_snapshot.capacity
    max_lir_size = capacity * LIR_TARGET_RATIO
    
    # While we exceed LIR budget
    while m_lir_size > max_lir_size and m_stack_s:
        # The LIR to demote is at the bottom of Stack S
        # (Since we prune S, the bottom is guaranteed to be LIR)
        victim_key = next(iter(m_stack_s))
        
        # Verify it is indeed LIR (Safety check)
        if m_state.get(victim_key) == 'LIR':
            # Demote LIR -> HIR
            m_state[victim_key] = 'HIR'
            
            # Since we don't know the exact object size here easily without lookup,
            # we rely on the heuristic that it eventually balances out, 
            # or we fetch size from snapshot if available (expensive). 
            # We will approximate/wait for next access to correct size or 
            # use a simpler count-based fallback if sizes vary wildly.
            # *Optimization*: In this constrained environment, we can grab the object 
            # from cache_snapshot to get exact size for accurate accounting.
            if victim_key in cache_snapshot.cache:
                obj = cache_snapshot.cache[victim_key]
                m_lir_size -= obj.size
            
            # Move to Queue Q (Resident HIRs)
            m_queue_q[victim_key] = None
            
            # Note: It stays in Stack S for now, but since it is now HIR,
            # the next _prune_stack() call might remove it if it is at the bottom.
            
        # Trigger pruning to remove the newly demoted HIR from bottom of S
        _prune_stack()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm7wd_fuv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpijkes39n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpph0yyarc.pickle

Iteration 60: New subsample score 0.962822 is better than old score 0.8890170000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4pb4ppna.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3k1sjbio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq9f3_b8l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnvbg4c6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpljz4hmpe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp84ulyxgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp45g74rsp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnhvdps_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsc4349g5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4nxoa954.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp04o5hswr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyd_durwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp34tjr4rh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpumkk74c8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe2nkduz1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdbfg0a81.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl9lr5kbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9wua9ziq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuu76cf8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkwk6c1xc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu09xf3jz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptu4f1ua4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprsadr8vz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbqoyzgnu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsfo6_yur.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkmn6dy5o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1e2t6w25.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbmj2batl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2cby_yi7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc7ynlhhr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfzs2gcrn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppb1sehmy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8ushh607.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpssqnik3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp15naruli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpttuh7b9d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3fp6ktby.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_exnq08n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjvowx9yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeut4ab4b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_o_9xaq2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6slspzbp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3qzbshqw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbemsxgb0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8funwha2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5ia3uzev.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7rcajyh_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp93kkhx85.pickle

Iteration 60: Full valset score for new program: 0.23256970833333343
Iteration 60: Full train_val score for new program: 0.23256970833333343
Iteration 60: Individual valset scores for new program: [0.502475, 0.473208, 0.486152, 0.433893, 0.49516, 0.489623, 0.267943, 0.498624, 0.540579, 0.531017, 0.075, 0.420515, 0.053294, 0.0, 0.020105, 0.019724, 0.019254, 0.023053, 0.022922, 0.269478, 0.375615, 0.026556, 0.058027, 0.058027, 0.269802, 0.381048, 0.83714, 0.890638, 0.039832, 0.034091, 0.04328, 9.6e-05, 3.6e-05, 0.769571, 0.074561, 0.062665, 0.01826, 0.633694, 0.125461, 0.041586, 0.039096, 0.043257, 0.042763, 0.05, 0.041854, 0.040954, 0.441718, 0.081699]
Iteration 60: New valset pareto front scores: [0.509148, 0.479804, 0.493182, 0.441985, 0.50361, 0.49412, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.431616, 0.053294, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.770082, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 60: Full valset pareto front score: 0.2626377083333333
Iteration 60: Updated valset pareto front programs: [{2}, {17}, {2}, {17}, {14}, {17}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {11}, {17}, {19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17}, {15}, {2, 4, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16}, {9}, {9}, {17}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16, 17}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16}, {1, 5, 9, 10, 13, 14, 15, 16, 17, 19}]
Iteration 60: Best valset aggregate score so far: 0.2508065
Iteration 60: Best program as per aggregate score on train_val: 5
Iteration 60: Best program as per aggregate score on valset: 5
Iteration 60: Best score on valset: 0.2508065
Iteration 60: Best score on train_val: 0.2508065
Iteration 60: Linear pareto front program index: 5
Iteration 60: New program candidate index: 19
Iteration 61: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp88fqy834.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5p17xhl7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3dn1zz0_.pickle

Iteration 61: Proposed new text for program: # Improved Cache Eviction Algorithm: S3-FIFO (Static Simple Scalable FIFO)
#
# Rationale:
# The previous algorithm (Segmented LRU with Ghosts) struggled significantly with Trace 16 (0.0199 hit rate),
# which suggests a vulnerability to "scans" (sequences of one-time access objects) or high churn.
# The frequency-based logic was likely promoting items too aggressively or holding onto ghosts too long.
#
# We implement the "Static S3-FIFO" algorithm (SOSP '23).
# It uses three main concepts:
# 1. A Small FIFO queue (S) for new items (absorbing scans).
# 2. A Main FIFO queue (M) for popular items.
# 3. Lazy promotion: Items move from S to M only during eviction operations if they have been accessed.
#
# This design ensures that scan traffic (items accessed once) enters S, receives no hits, 
# and is quickly evicted without polluting the Main cache (M).
# Popular items get promoted to M and are given "second chances" via re-insertion (similar to CLOCK).

# Globals for S3-FIFO
# We use Python dicts as Ordered Sets (FIFO behavior).
m_small_q = dict()  # Small Queue (S)
m_main_q = dict()   # Main Queue (M)
m_freq = dict()     # Frequency bits (0, 1, 2, 3)

# Constants
# 10% of the cache size is dedicated to the Small queue to filter scans.
SMALL_RATIO = 0.1 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: S3-FIFO
    
    We simulate queue movements to find a victim.
    1. If Small Queue (S) is larger than 10% of cache, we evict from S.
       - If head of S has been accessed (freq > 0), move to M (Main).
       - Else, evict it (it's a scan/cold item).
    2. If S is small, we evict from Main Queue (M).
       - If head of M has been accessed (freq > 0), give second chance (re-insert at tail).
       - Else, evict it.
    '''
    # Calculate target size (object count) for Small Queue
    cache_size = len(cache_snapshot.cache)
    small_target = max(1, int(cache_size * SMALL_RATIO))
    
    # Loop until we find a victim to return
    while True:
        # Check Small Queue first
        # Condition: S is overflowing OR M is empty (must evict from S)
        if len(m_small_q) > small_target or not m_main_q:
            if not m_small_q:
                # Should not happen if cache is full, but strictly handled for safety.
                # If both queues are empty but cache has items (desync), use fallback.
                if not m_main_q:
                    return next(iter(cache_snapshot.cache))
                # If S is empty but M is not, fall through to M logic
            else:
                candidate = next(iter(m_small_q))
                freq = m_freq.get(candidate, 0)
                
                if freq > 0:
                    # Lazy Promotion: Item was useful. Move to Main.
                    del m_small_q[candidate]
                    m_main_q[candidate] = None # Insert at tail
                    m_freq[candidate] = 0      # Reset frequency for future proof
                    continue                   # Look for next victim
                else:
                    # Scan/Cold: No hits while in Small. Evict.
                    return candidate

        # Check Main Queue
        if m_main_q:
            candidate = next(iter(m_main_q))
            freq = m_freq.get(candidate, 0)
            
            if freq > 0:
                # Second Chance: Item was useful. Re-insert at tail.
                del m_main_q[candidate]
                m_main_q[candidate] = None # Insert at tail
                m_freq[candidate] = 0      # Reset frequency
                continue
            else:
                # Cold in Main: Evict.
                return candidate
        
        # Absolute Fallback (Safety valve)
        # If logical queues are empty but cache is not (should imply logic error or init state).
        return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    Increment frequency counter. Cap at 3 (2-bit simulation).
    We do NOT move items between queues here (Lazy movement).
    '''
    curr_freq = m_freq.get(obj.key, 0)
    if curr_freq < 3:
        m_freq[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Always insert new items into the Small Queue.
    Initialize frequency to 0.
    '''
    # Clean up previous state if key somehow exists in other queues
    if obj.key in m_main_q: del m_main_q[obj.key]
    if obj.key in m_small_q: del m_small_q[obj.key] # Move to tail if re-inserting
    
    m_small_q[obj.key] = None
    m_freq[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up internal metadata for the evicted object.
    '''
    key = evicted_obj.key
    
    if key in m_small_q: del m_small_q[key]
    if key in m_main_q: del m_main_q[key]
    if key in m_freq: del m_freq[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3n3a6ovf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9f63tzaz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9okvipm7.pickle

Iteration 61: New subsample score 1.2716500000000002 is better than old score 1.169292. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoersf3dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5j5n2zqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbdnf8zpv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkmxvnw__.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5rhspnu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqq7y1eo1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplaonh_rf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3s_mwj7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyef42571.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptlmk_6i5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9rn7z1vj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdbcjrjuv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwmck86zc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5_5s2tqi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps1djwpbx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppzivdfhx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw8rzz4iu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0zp2ouyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0l5eyqaq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyfg1wdqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvl2zv6xj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7dktfycr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprl03dn50.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp14ckhz38.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpob2xysof.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9c2d199u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphzzvlwgr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7rh6my49.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfbggc8re.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpilq5su3m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0pay2ydx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4230okyw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw1wm0yxc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe69h_luy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptpi6sdg6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv895er3t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc655rcx4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5b8lpbc0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6lnzu7qa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpamx_28qb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl33v1j91.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmdc4tl3n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuqatgxtq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp29jjj_k5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt3qoort7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxzfe7atn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp545q17th.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9nf4omfc.pickle

Iteration 61: Full valset score for new program: 0.22875656249999998
Iteration 61: Full train_val score for new program: 0.22875656249999998
Iteration 61: Individual valset scores for new program: [0.465185, 0.439587, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.269802, 0.263105, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.753233, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 61: New valset pareto front scores: [0.509148, 0.479804, 0.493182, 0.441985, 0.50361, 0.49412, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.431616, 0.053294, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.770082, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 61: Full valset pareto front score: 0.2626377083333333
Iteration 61: Updated valset pareto front programs: [{2}, {17}, {2}, {17}, {14}, {17}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {11}, {17}, {19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20}, {15}, {2, 4, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 20}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 20}, {10}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16, 20}, {9}, {9}, {17}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16, 17, 20}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20}, {1, 5, 9, 10, 13, 14, 15, 16, 17, 19, 20}]
Iteration 61: Best valset aggregate score so far: 0.2508065
Iteration 61: Best program as per aggregate score on train_val: 5
Iteration 61: Best program as per aggregate score on valset: 5
Iteration 61: Best score on valset: 0.2508065
Iteration 61: Best score on train_val: 0.2508065
Iteration 61: Linear pareto front program index: 5
Iteration 61: New program candidate index: 20
Iteration 62: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpezx56flq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw93a4jc7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoeszkzvz.pickle

Iteration 62: Proposed new text for program: import collections

# Global State for ARC-like logic
# m_target_t1_size: The target size for the "Recent" (T1) list. 
#                   range: [0, cache_capacity]
# m_t1: Keys currently in T1 (Recency / Probation). Map key -> None (used as Set)
# m_t2: Keys currently in T2 (Frequency / Protected). Map key -> None (used as Set)
# m_b1: Ghost keys evicted from T1. Map key -> None
# m_b2: Ghost keys evicted from T2. Map key -> None
# m_lru_map: Global LRU tracker. Map key -> timestamp. Used to find LRU within specific sets.

m_target_t1_size = 0
m_t1 = set()
m_t2 = set()
# Fixed sized ghost lists (approximations)
m_b1 = collections.OrderedDict() 
m_b2 = collections.OrderedDict()

# To efficiently find LRU victims within T1 or T2, we need access times.
# We will track the last access time for all cached keys.
m_access_time = {}

def get_lru_victim(candidate_keys):
    """
    Helper to find the key with the smallest timestamp among a set of candidates.
    """
    victim = None
    min_time = float('inf')
    
    for key in candidate_keys:
        t = m_access_time.get(key, 0)
        if t < min_time:
            min_time = t
            victim = key
            
    return victim

def evict(cache_snapshot, obj):
    '''
    ARC-inspired Eviction Policy.
    Decides whether to evict from T1 (Recent) or T2 (Frequent) based on the 
    adaptive parameter `m_target_t1_size` (p).
    '''
    global m_target_t1_size
    
    # Current sizes
    t1_len = len(m_t1)
    
    # Logic:
    # If T1 has more items than its target `p`, we evict from T1 (LRU of T1).
    # Otherwise, we evict from T2 (LRU of T2).
    # Note: If T1 is empty, we must evict from T2, and vice versa.
    
    victim_key = None
    
    if t1_len > 0 and (t1_len > m_target_t1_size or len(m_t2) == 0):
        # Evict from T1
        victim_key = get_lru_victim(m_t1)
    else:
        # Evict from T2
        victim_key = get_lru_victim(m_t2)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. If item was in T1 (Probation), move it to T2 (Protected).
    3. If item was in T2, it stays in T2.
    '''
    global m_t1, m_t2, m_access_time
    
    key = obj.key
    m_access_time[key] = cache_snapshot.access_count
    
    # If it is in T1, it has proven itself -> Promote to T2
    if key in m_t1:
        m_t1.remove(key)
        m_t2.add(key)
    # If it is in T2, it stays there (LRU updated via m_access_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost Lists (B1 and B2).
    2. Adapt `m_target_t1_size` (p) based on where the ghost hit occurred.
    3. Insert the new object into T1 (Probation).
    '''
    global m_target_t1_size, m_t1, m_t2, m_b1, m_b2, m_access_time
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count approximation usually, but here strict.
    # Note: snapshot.capacity is bytes, but this algorithm logic usually relies on counts.
    # We will approximate "count capacity" by just letting the cache fill up.
    # However, for the ghost lists, we need an arbitrary limit so they don't grow forever.
    # We'll stick to a safe heuristic limit for ghosts.
    MAX_GHOST_SIZE = 5000 
    
    m_access_time[key] = cache_snapshot.access_count

    # 1. Check Ghost Hits to Tune P
    if key in m_b1:
        # Hit in B1: We evicted a recent item too soon. We need a larger T1.
        delta = 1
        if len(m_b1) >= len(m_b2) and len(m_b2) > 0:
            delta = 1
        elif len(m_b2) > len(m_b1):
             # Standard ARC delta calculation
            delta = len(m_b2) / len(m_b1)
        
        m_target_t1_size += delta
        
        # Cleanup B1
        del m_b1[key]

    elif key in m_b2:
        # Hit in B2: We evicted a frequent item too soon. We need a smaller T1 (larger T2).
        delta = 1
        if len(m_b2) >= len(m_b1) and len(m_b1) > 0:
            delta = 1
        elif len(m_b1) > len(m_b2):
             # Standard ARC delta calculation
            delta = len(m_b1) / len(m_b2)
            
        m_target_t1_size -= delta
        
        # Cleanup B2
        del m_b2[key]
        
    # Clamp P
    total_objects = len(cache_snapshot.cache) # Approximate current count capacity
    if total_objects == 0: total_objects = 100 # Default fallback
    
    if m_target_t1_size < 0: m_target_t1_size = 0
    if m_target_t1_size > total_objects: m_target_t1_size = total_objects

    # 2. Always insert new items into T1 (Recency/Probation)
    m_t1.add(key)
    
    # Ensure ghost lists don't explode (FIFO cleanup)
    if len(m_b1) > MAX_GHOST_SIZE: m_b1.popitem(last=False)
    if len(m_b2) > MAX_GHOST_SIZE: m_b2.popitem(last=False)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Determine where the victim came from (T1 or T2).
    2. Move victim key to corresponding Ghost List (B1 or B2).
    3. Clean up metadata.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_access_time
    
    key = evicted_obj.key
    
    # Remove from Access Time map
    if key in m_access_time:
        del m_access_time[key]
        
    # Identify Source and move to Ghost
    if key in m_t1:
        m_t1.remove(key)
        # Add to B1 (Ghost Recency)
        m_b1[key] = None
    elif key in m_t2:
        m_t2.remove(key)
        # Add to B2 (Ghost Frequency)
        m_b2[key] = None
    else:
        # Should not happen if state is consistent, but safe fallback
        pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx_5e65bz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv4_5zx15.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8ic8d2by.pickle

Iteration 62: New subsample score 0.516059 is not better than old score 0.565269, skipping
Iteration 63: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1sl2au0k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp29nkhg86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzrnt6nrb.pickle

Iteration 63: Proposed new text for program: # Metadata storage
# The core of Adaptive Replacement Cache (ARC) logic involves four conceptual lists:
# T1: Recent items (in cache, accessed once)
# T2: Frequent items (in cache, accessed >= 2 times)
# B1: Ghost Recent (history of evicted T1 items)
# B2: Ghost Frequent (history of evicted T2 items)

# Implementation:
# We map these sets using a state dictionary for every key we track.
# Key -> State (WHERE_IS_IT)
# State can be: 
#   0: In T1 (Recent Cache)
#   1: In T2 (Frequent Cache)
#   2: In B1 (Ghost Recent)
#   3: In B2 (Ghost Frequent)

# We also need an LRU ordering for each list. Python's dict acts as an ordered dict,
# but we need explicit management to efficiently pick the LRU from specific sets.
# For simplicity and speed within the constraints, we will store access timestamps 
# and filter/sort when needed, or maintain simple counters.

m_state = dict()       # Key -> Set ID (0=T1, 1=T2, 2=B1, 3=B2)
m_lru_time = dict()    # Key -> Access Count (Time) used for LRU eviction within sets

# Global ARC adaptive parameter
m_p = 0                # Target size for T1 (Recency set)
# Since we don't know the exact "count" capacity (it's byte-based), 
# we treat 'p' as a ratio or a soft target relative to the number of items.
# However, standard ARC uses counts. We will approximate 'c' (capacity) as the 
# current number of items in the cache for the logic.

def get_lru_key(candidate_keys):
    """Helper to find LRU key among a specific list of candidates."""
    if not candidate_keys:
        return None
    # Find key with minimum m_lru_time
    return min(candidate_keys, key=lambda k: m_lru_time.get(k, 0))

def evict(cache_snapshot, obj):
    '''
    ARC-like Eviction Policy.
    Decides whether to evict from T1 (Recent) or T2 (Frequent) based on the target `p`.
    '''
    global m_state, m_lru_time, m_p
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Identify members of T1 and T2 currently in the cache
    t1_keys = [k for k in current_keys if m_state.get(k, 0) == 0]
    t2_keys = [k for k in current_keys if m_state.get(k, 0) == 1]
    
    # Current size of T1
    len_t1 = len(t1_keys)
    
    victim = None

    # ARC Logic:
    # If |T1| > p, we usually evict from T1.
    # However, if the incoming item (obj) triggered a B2 hit, we might relax this.
    # Since `evict` is called *before* the insert, we strictly follow the size target.
    
    # We must ensure we don't crash if one list is empty.
    if t1_keys and len_t1 > m_p:
        victim = get_lru_key(t1_keys)
    elif t2_keys:
        victim = get_lru_key(t2_keys)
    elif t1_keys:
        # Fallback: if T1 <= p but T2 is empty, we must evict T1
        victim = get_lru_key(t1_keys)
    else:
        # Absolute fallback
        victim = min(current_keys, key=lambda k: m_lru_time.get(k, 0))

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2. If in T2, update recency (stays in T2).
    '''
    global m_state, m_lru_time
    
    key = obj.key
    m_lru_time[key] = cache_snapshot.access_count
    
    # If it was in T1 (Recent), promote to T2 (Frequent)
    current_state = m_state.get(key, 0)
    if current_state == 0: # T1
        m_state[key] = 1   # Move to T2
    # If already in T2 (1), it stays there, just became MRU via time update above.

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was in Ghost lists (B1 or B2) to adapt `p`.
    Then place the new item into T1 or T2.
    '''
    global m_state, m_lru_time, m_p
    
    key = obj.key
    access_time = cache_snapshot.access_count
    m_lru_time[key] = access_time
    
    # Capacity approximation (c): Total items currently in cache
    # This fluctuates because capacity is byte-based, but item count is the 
    # unit ARC operates on.
    c = len(cache_snapshot.cache) 
    
    # Check previous state (Ghost Hit Check)
    prev_state = m_state.get(key, -1)
    
    if prev_state == 2: # Hit in B1 (Ghost Recent) -> We should have made T1 larger
        # Delta calculation standard in ARC
        # If B1 is bigger than B2, we increase p slightly; else more aggressively.
        # Since we don't strictly track B1/B2 size dynamically here, we use simplified steps.
        # But we can approximate |B1| and |B2| by scanning keys if needed, 
        # or just use a simpler increment of 1.
        delta = 1
        m_p = min(c, m_p + delta)
        
        # Move to T2 (frequent) because it has been seen twice (once to enter B1, now again)
        m_state[key] = 1 
        
    elif prev_state == 3: # Hit in B2 (Ghost Frequent) -> We should have made T2 larger (T1 smaller)
        delta = 1
        m_p = max(0, m_p - delta)
        
        # Move back to T2
        m_state[key] = 1
        
    else:
        # Totally Cold Miss (not in T1, T2, B1, B2)
        # Add to T1 (Recent)
        m_state[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Move the evicted object from T1->B1 or T2->B2.
    Manage Ghost sizes.
    '''
    global m_state, m_lru_time
    
    key = evicted_obj.key
    current_state = m_state.get(key, 0)
    
    if current_state == 0: # Was in T1
        m_state[key] = 2   # Move to B1
    elif current_state == 1: # Was in T2
        m_state[key] = 3   # Move to B2
    
    # Note: We do NOT remove from m_lru_time, as we need to know it exists 
    # (and roughly when accessed) to manage the ghost list cleanup.
    
    # Cleanup Ghost Lists
    # To prevent memory leaks, we remove very old ghost entries.
    # Standard ARC keeps |B1| + |B2| <= c.
    # We will simply cap the total dictionary size to prevent unbounded growth.
    if len(m_state) > 3 * len(cache_snapshot.cache):
        # Remove oldest non-cached keys (state 2 or 3)
        # This scan is expensive (O(N)), but necessary in this constraint set.
        # We optimize by only doing it periodically or only removing one.
        
        # Find oldest key in B1 or B2
        # Filter keys where state is 2 or 3
        ghost_keys = [k for k, s in m_state.items() if s == 2 or s == 3]
        if ghost_keys:
            victim_ghost = min(ghost_keys, key=lambda k: m_lru_time.get(k, 0))
            del m_state[victim_ghost]
            del m_lru_time[victim_ghost]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_31yd9cm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqyuxd7b1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfk9mj7si.pickle

Iteration 63: New subsample score 0.731266 is not better than old score 0.756251, skipping
Iteration 64: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgicgozvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplhn3fjrh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv8esnduu.pickle

Iteration 64: Proposed new text for program: # Metadata storage
# 1. key -> eviction_priority (0 for probationary/new, 1 for protected/frequent)
# 2. key -> last_access_time (Used for LRU tie-breaking within segments)
# 3. key -> frequency (To determine promotion threshold)
m_priority = dict()
m_last_access = dict()
m_frequency = dict()

# Configuration
# Portion of capacity reserved for protected items (0.8 = 80%)
PROTECTED_RATIO = 0.8 
# Max frequency cap to allow aging
MAX_FREQ_CAP = 10 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) / S3-FIFO hybrid style.
    
    1. We prefer to evict from the "Probationary" (Priority 0) segment. 
       These are items that were inserted but haven't proven their worth yet.
    2. If we cannot find a probationary item (or if the protected set is too big),
       we might evict a "Protected" (Priority 1) item, or more likely, demote it.
       
    Algorithm:
    - Identify the candidate with the lowest Priority.
    - Tie-breaker: Least Recently Used (oldest last_access_time).
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to find the LRU item within the lowest available priority class.
    # Logic: Minimize (Priority, Last_Access_Time)
    
    victim_key = None
    min_score = (float('inf'), float('inf'))
    
    # Optimization: If we find a Priority 0 item, we just need the LRU one.
    # If all are Priority 1, we find the LRU one among them.
    
    for key in current_keys:
        prio = m_priority.get(key, 0)
        access_time = m_last_access.get(key, 0)
        
        # We penalize large objects slightly to improve byte-hit-rate density
        # if the cache constraint is size-based, but since we are optimizing generic hit rate,
        # we stick to pure Prio/LRU.
        
        score = (prio, access_time)
        
        if score < min_score:
            min_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency.
    3. Promotion: If item is Probationary (0), promote to Protected (1).
    '''
    global m_priority, m_last_access, m_frequency
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    m_last_access[key] = current_time
    m_frequency[key] = min(m_frequency.get(key, 0) + 1, MAX_FREQ_CAP)
    
    # If it was probationary, it has proven useful. Promote it.
    if m_priority.get(key, 0) == 0:
        m_priority[key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Insert into Probationary segment (Priority 0).
    2. Initialize Frequency and Access Time.
    '''
    global m_priority, m_last_access, m_frequency
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # New items start on probation. 
    # They must get a hit to survive the next eviction sweep against other probationary items.
    m_priority[key] = 0
    m_last_access[key] = current_time
    m_frequency[key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata.
    2. Dynamic Tuning: Check if we are evicting Protected items too often?
       (Simpler implementation: just clean metadata to keep O(1)).
    '''
    global m_priority, m_last_access, m_frequency
    
    key = evicted_obj.key
    
    if key in m_priority:
        del m_priority[key]
    if key in m_last_access:
        del m_last_access[key]
    if key in m_frequency:
        del m_frequency[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2ckx6prb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd7mvpu05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjn5x8f8x.pickle

Iteration 64: New subsample score 0.142876 is not better than old score 0.159543, skipping
Iteration 65: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpek8_0nb7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkuffw6ys.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp78zd7w0y.pickle

Iteration 65: Proposed new text for program: import random

# Metadata
# 1. key -> frequency (Global frequency counter with aging)
# 2. key -> last_access_time (Standard LRU tracking)
# 3. key -> status (Enum/Int: 0 for Probation/Window, 1 for Protected/Main)
m_freq = {}
m_last_access = {}
m_status = {}

# Configuration
MAX_FREQ_HISTORY = 10000    # Max items to track frequency for (includes evicted items)
AGING_INTERVAL = 1000       # Halve frequencies every N accesses to adapt to shifting trends
current_access_counter = 0

def _age_frequencies():
    """Periodically divide frequencies by 2 to favor recent popularity."""
    global m_freq
    keys_to_remove = []
    for k in m_freq:
        m_freq[k] /= 2.0
        if m_freq[k] < 0.5:
            keys_to_remove.append(k)
    for k in keys_to_remove:
        del m_freq[k]

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Frequency Admission (Simplified W-TinyLFRU).
    
    1. We divide the cache conceptually into "Probation" (SLRU-0) and "Protected" (SLRU-1).
    2. New items start in Probation.
    3. We prefer to evict from Probation first, unless the Probation victim has higher 
       historical frequency than the Protected victim (Admission Policy).
    
    This filters out "scan" traffic (one-hit wonders) while keeping the working set.
    '''
    # Identify candidates
    probation_keys = []
    protected_keys = []
    
    for k in cache_snapshot.cache:
        status = m_status.get(k, 0)
        if status == 0:
            probation_keys.append(k)
        else:
            protected_keys.append(k)
            
    # Fallback: If for some reason one list is empty, treat all as one pool
    if not probation_keys:
        probation_keys = protected_keys
    if not protected_keys:
        protected_keys = probation_keys

    # Find the LRU item in Probation (The standard victim)
    # We use (last_access) to find LRU.
    victim_probation = min(probation_keys, key=lambda k: m_last_access.get(k, 0))
    
    # Find the LRU item in Protected
    victim_protected = min(protected_keys, key=lambda k: m_last_access.get(k, 0))
    
    # ADMISSION CHECK:
    # Compare the frequency of the probation victim vs the protected victim.
    # If the probation victim is actually very popular (historically), 
    # we might want to evict the protected LRU item instead to make room for it.
    
    freq_prob = m_freq.get(victim_probation, 0)
    freq_prot = m_freq.get(victim_protected, 0)
    
    # If the probation item is significantly more popular than the stale protected item,
    # we sacrifice the protected item. Otherwise, we evict the probation item (filter it out).
    if freq_prob > freq_prot and len(protected_keys) > 0:
        return victim_protected
    else:
        return victim_probation

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Promote to Protected status if currently in Probation.
    '''
    global m_last_access, m_freq, m_status, current_access_counter
    
    current_access_counter += 1
    m_last_access[obj.key] = current_access_counter
    m_freq[obj.key] = m_freq.get(obj.key, 0) + 1
    
    # Promotion: If it was in probation (0), it has proven itself useful.
    if m_status.get(obj.key, 0) == 0:
        m_status[obj.key] = 1
        
    if current_access_counter % AGING_INTERVAL == 0:
        _age_frequencies()

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Recency.
    2. Initialize/Update Frequency (it might have history).
    3. Set Status to Probation (0).
    '''
    global m_last_access, m_freq, m_status, current_access_counter
    
    current_access_counter += 1
    m_last_access[obj.key] = current_access_counter
    
    # If it's a new item, freq starts at 1. If it's a returning item, it keeps old freq.
    m_freq[obj.key] = m_freq.get(obj.key, 0) + 1
    
    # Always insert into Probation first
    m_status[obj.key] = 0
    
    if current_access_counter % AGING_INTERVAL == 0:
        _age_frequencies()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from recency and status maps.
    2. KEEP the frequency data (up to a limit) to remember "Ghost" popularity.
    '''
    global m_last_access, m_status, m_freq
    
    key = evicted_obj.key
    
    if key in m_last_access:
        del m_last_access[key]
    
    if key in m_status:
        del m_status[key]
        
    # We intentionally DO NOT delete from m_freq immediately.
    # This allows the algorithm to remember that this item was popular 
    # if it returns shortly.
    
    # However, to prevent memory leaks, if m_freq grows too large, we prune.
    if len(m_freq) > MAX_FREQ_HISTORY:
        # Remove keys not in cache (ghosts) with lowest frequency
        # This is an expensive operation, but happens rarely due to size limit
        cached_keys = set(cache_snapshot.cache.keys())
        ghost_keys = [k for k in m_freq if k not in cached_keys]
        
        # Sort ghosts by frequency ascending and remove the bottom 10%
        ghost_keys.sort(key=lambda k: m_freq[k])
        cutoff = max(1, len(ghost_keys) // 10)
        
        for k in ghost_keys[:cutoff]:
            del m_freq[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2ayzso0i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi2g4yhqk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxpxi8fgw.pickle

Iteration 65: New subsample score 1.0091 is not better than old score 1.01689, skipping
Iteration 66: Selected program 14 score: 0.24068325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwmbl1g_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqm_6dw6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqldolmiq.pickle

Iteration 66: Proposed new text for program: from collections import OrderedDict

# Metadata storage
# We use an OrderedDict to combine fast lookups with LRU ordering.
# m_data structure: key -> {'freq': int, 'size': int, 'priority': float}
# The beginning of the OrderedDict (index 0) is the LRU end.
m_data = OrderedDict()

# GDSF Aging factor 'L'
# This value increases over time as objects are evicted, simulating the aging of 
# items currently in the cache.
m_L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF-Windowed Eviction Strategy.
    We inspect the K least recently used items and evict the one with the lowest 
    GDSF priority value.
    
    Priority formula: P = (Frequency / Size) + L
    '''
    global m_data, m_L

    # Sample size (K). Scanning the exact global minimum is expensive O(N).
    # Scanning a window of the LRU tail is O(K) and provides a strong approximation
    # that respects recency.
    K = 64
    
    victim_key = None
    min_priority = float('inf')
    
    # Create an iterator to scan the LRU end of the dictionary
    iterator = iter(m_data)
    
    # Safety check if cache is empty
    if not m_data:
        return None
        
    # Scan the first K items (LRU end)
    count = 0
    while count < K:
        try:
            key = next(iterator)
            entry = m_data[key]
            
            # We want to evict the item with the LOWEST priority (Least valuable per byte)
            if entry['priority'] < min_priority:
                min_priority = entry['priority']
                victim_key = key
            
            count += 1
        except StopIteration:
            break
            
    # Fallback: if for some reason we didn't pick one (e.g. numerical issues), pick LRU
    if victim_key is None:
        victim_key = next(iter(m_data))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using current L.
    3. Move to MRU (Mark as recently used).
    '''
    global m_data, m_L
    
    key = obj.key
    if key in m_data:
        entry = m_data[key]
        entry['freq'] += 1
        
        # GDSF Priority Update
        # We re-base the priority on the current L so active items stay above the water level.
        entry['priority'] = (entry['freq'] / entry['size']) + m_L
        
        # Move to end (MRU)
        m_data.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata (Freq=1).
    2. Calculate Initial Priority.
    3. Add to MRU.
    '''
    global m_data, m_L
    
    key = obj.key
    size = obj.size
    
    # Initial priority for a new item
    # Note: We use the current global L as the baseline.
    priority = (1.0 / size) + m_L
    
    m_data[key] = {
        'freq': 1,
        'size': size,
        'priority': priority
    }
    # OrderedDict puts new keys at the end (MRU) by default

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Update the global aging factor L to the priority of the evicted item.
    2. Remove metadata.
    '''
    global m_data, m_L
    
    key = evicted_obj.key
    if key in m_data:
        # GDSF Aging Rule: The system "ages" to the level of the item just evicted.
        # This prevents new items (with low priority) from being evicted immediately 
        # unless they are strictly worse than what we just threw out.
        m_L = m_data[key]['priority']
        
        del m_data[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz_v1sbab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe10brbzi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsbyjza0f.pickle

Iteration 66: New subsample score 0.6817599999999999 is not better than old score 0.682642, skipping
Iteration 67: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppsdv70gr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd49oad_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzm08ie8i.pickle

Iteration 67: Proposed new text for program: import math

# Metadata storage
# Key -> Frequency (int)
m_frequency = dict()

# Global "Clock" or "Inflation Factor" for the GDSF algorithm.
# This value rises over time as items are evicted, effectively aging out
# older items that haven't been accessed recently.
L_inflation = 0.0

def calculate_priority(freq, size, L):
    """
    Calculates the GDSF priority score.
    Higher score = Better to keep.
    Lower score = Candidate for eviction.
    
    Formula: H = L + (Frequency / Size)
    
    We add a tiny epsilon to size to avoid division by zero if size is 0 
    (though specs say size is positive, it's safer).
    """
    # Using 1.0 cost weight. 
    # Logic: Small items with high frequency are most valuable.
    return L + (float(freq) / max(1, size))

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GDSF (Greedy Dual-Size Frequency)
    
    We iterate through the cache to find the object with the lowest H value.
    The minimum H value becomes the new system 'L' (aging factor).
    '''
    global m_frequency, L_inflation
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    
    # Identify the victim: The item with the lowest computed priority
    for key in current_keys:
        cached_obj = cache_snapshot.cache[key]
        freq = m_frequency.get(key, 1) # Default to 1 if missing
        
        priority = calculate_priority(freq, cached_obj.size, L_inflation)
        
        if priority < min_priority:
            min_priority = priority
            victim_key = key
            
    # CRITICAL GDSF Step: Update the global inflation factor L.
    # L advances to the priority of the item we just evicted.
    # This means any NEW item entering the cache must beat this new baseline
    # to survive, effectively "aging out" other stagnant items.
    if victim_key is not None:
        L_inflation = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Note: We do NOT reset the priority here; the priority is dynamic based on 
       the current L and the new Frequency. 
    '''
    global m_frequency
    
    # Increment frequency count
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. The object implicitly gets Priority = L_current + (1 / Size)
       during the next eviction scan.
    '''
    global m_frequency
    
    # Initialize frequency
    m_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove metadata for the evicted object to keep memory clean.
    '''
    global m_frequency
    
    key = evicted_obj.key
    if key in m_frequency:
        del m_frequency[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpou6iwike.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7560oagc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7x3fef24.pickle

Iteration 67: New subsample score 0.183958 is not better than old score 0.18466300000000002, skipping
Iteration 68: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy4rn_ge5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplj625x6e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyq5wu89e.pickle

Iteration 68: Proposed new text for program: # Metadata storage
m_frequencies = dict()  # key -> float (priority value)
m_last_access = dict()  # key -> int (access_count)

# Global variable for Dynamic Aging (LFU-DA)
# This represents the "cost" or "base value" of the cache at the current moment.
cache_inflation = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU-DA (Least Frequently Used with Dynamic Aging).
    
    We evict the item with the lowest priority value. 
    Crucially, the priority of the evicted item becomes the new 
    global 'cache_inflation' base.
    
    Ties in priority are broken by Recency (LRU).
    '''
    global m_frequencies, m_last_access, cache_inflation

    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    # Initialize min_score to infinity.
    # Score tuple: (Priority, Last_Access_Time)
    # We want to minimize Priority first, then Last_Access_Time (LRU).
    min_score = (float('inf'), float('inf'))

    # Linear scan to find the victim. 
    # (In a production system, a min-heap would be used, but here we scan).
    for key in current_keys:
        priority = m_frequencies.get(key, 0.0)
        access_time = m_last_access.get(key, 0)
        
        score = (priority, access_time)
        
        if score < min_score:
            min_score = score
            victim_key = key

    # LFU-DA Logic:
    # The value of the evicted item becomes the new baseline.
    # This effectively "ages" all other items in the cache without
    # having to iterate and decrement them.
    if victim_key is not None:
        cache_inflation = m_frequencies.get(victim_key, 0.0)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (for LRU tie-breaking).
    2. Increment Priority.
    
    In LFU-DA, we simply add to the existing priority.
    '''
    global m_frequencies, m_last_access
    
    current_time = cache_snapshot.access_count
    m_last_access[obj.key] = current_time
    
    # Standard LFU increment. 
    # Note: We do NOT reset to cache_inflation on hit. 
    # We assume the object keeps its accumulated value.
    current_freq = m_frequencies.get(obj.key, 0.0)
    m_frequencies[obj.key] = current_freq + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Last Access Time.
    2. Initialize Priority.
    
    In LFU-DA, a new item enters with a priority equal to the
    current global 'cache_inflation' plus an initial count (usually 1).
    This allows the new item to compete with existing items that
    have been aged by previous evictions.
    '''
    global m_frequencies, m_last_access, cache_inflation
    
    current_time = cache_snapshot.access_count
    m_last_access[obj.key] = current_time
    
    # The new item starts at the current "water level" + 1.
    # If we started at 1.0 while old items were at 1000.0, the new item 
    # would be evicted immediately. Starting at cache_inflation + 1 gives it a fair chance.
    m_frequencies[obj.key] = cache_inflation + 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata for the evicted key.
    '''
    global m_frequencies, m_last_access
    
    key = evicted_obj.key
    
    if key in m_frequencies:
        del m_frequencies[key]
        
    if key in m_last_access:
        del m_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf9461hcp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt7smb84a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpglvmxhgw.pickle

Iteration 68: New subsample score 0.8892230000000001 is not better than old score 0.9548490000000001, skipping
Iteration 69: Selected program 17 score: 0.2406433541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbf5jsydg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz6avw_4q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnrdvntoq.pickle

Iteration 69: Proposed new text for program: # Metadata storage
# Segmented LRU (SLRU) Implementation with Ghost History.
# We use Python's standard dicts which preserve insertion order (Python 3.7+).
# The "End" of the dict is the Most Recently Used (MRU).
# The "Start" of the dict is the Least Recently Used (LRU).

# 1. m_probation: dict (Key -> None)
#    Contains new items or items demoted from protected. 
#    These are the primary candidates for eviction.
m_probation = dict()

# 2. m_protected: dict (Key -> None)
#    Contains items that have proven their utility (hit while in probation or history).
#    These items are protected from eviction unless the protected segment is full.
m_protected = dict()

# 3. m_history: dict (Key -> None)
#    "Ghost" cache. Tracks keys of items recently evicted from m_probation.
#    Used to detect if an evicted item belongs to a larger working set.
m_history = dict()

# 4. m_protected_size: int
#    Tracks the total size (in bytes) of items in the protected segment.
m_protected_size = 0

# Constants
# We reserve 80% of capacity for the Protected segment.
PROTECTED_RATIO = 0.8 

def _enforce_protected_size(cache_snapshot):
    """
    Ensures the protected segment does not exceed its allocated capacity share.
    If it does, we demote the LRU protected item to the Probation segment.
    """
    global m_protected, m_probation, m_protected_size
    
    target_capacity = int(cache_snapshot.capacity * PROTECTED_RATIO)
    
    # While Protected segment is too large, demote LRU items to Probation
    while m_protected_size > target_capacity and m_protected:
        # Pop LRU from protected (first item in dict)
        victim_key = next(iter(m_protected))
        
        # We need to subtract the size. Since it is in protected, it must be resident.
        if victim_key in cache_snapshot.cache:
            obj_size = cache_snapshot.cache[victim_key].size
            m_protected_size -= obj_size
            
            del m_protected[victim_key]
            
            # Move to Probation MRU (End). 
            # This gives the demoted item a "second chance" before it drifts to the bottom of Probation.
            if victim_key in m_probation:
                del m_probation[victim_key]
            m_probation[victim_key] = None
        else:
            # Metadata consistency fallback (should not happen in normal operation)
            del m_protected[victim_key]

def evict(cache_snapshot, obj):
    """
    Eviction Policy:
    1. Prefer evicting from the Probation segment (LRU is at the start).
       These items are either new (unproven) or demoted (waning popularity).
    2. If Probation is empty, evict from the Protected segment.
    """
    global m_probation, m_protected

    # 1. Evict LRU from Probation
    if m_probation:
        return next(iter(m_probation))
    
    # 2. Evict LRU from Protected (if Probation is empty)
    if m_protected:
        return next(iter(m_protected))
    
    # 3. Fallback: Evict arbitrary item from cache (failsafe)
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    return None

def update_after_hit(cache_snapshot, obj):
    """
    On Hit:
    - If in Protected: Move to MRU of Protected.
    - If in Probation: Promote to Protected (proven utility).
    """
    global m_protected, m_probation, m_protected_size
    
    key = obj.key
    
    if key in m_protected:
        # Move to MRU (End)
        del m_protected[key]
        m_protected[key] = None
        
    elif key in m_probation:
        # Promote to Protected
        del m_probation[key]
        
        m_protected[key] = None
        m_protected_size += obj.size
        
        _enforce_protected_size(cache_snapshot)
        
    else:
        # Item in cache but not in our metadata (e.g. initial warmup).
        # Treat as promotion to Protected.
        m_protected[key] = None
        m_protected_size += obj.size
        _enforce_protected_size(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On Insert (New Item):
    - If in History: It was evicted recently. Promote directly to Protected.
    - Otherwise: Insert into Probation.
    """
    global m_protected, m_probation, m_history, m_protected_size
    
    key = obj.key
    
    if key in m_history:
        # Ghost Hit! Restore to Protected.
        del m_history[key]
        
        # Safe cleanup if it accidentally exists elsewhere
        if key in m_probation:
            del m_probation[key]
            
        m_protected[key] = None
        m_protected_size += obj.size
        _enforce_protected_size(cache_snapshot)
        
    else:
        # Cold insert -> Probation
        if key in m_probation:
            del m_probation[key]
        m_probation[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    On Eviction:
    - If evicted from Probation: Add key to History (Ghost).
    - If evicted from Protected: Update size.
    """
    global m_protected, m_probation, m_history, m_protected_size
    
    key = evicted_obj.key
    
    if key in m_probation:
        del m_probation[key]
        
        # Add to History MRU
        if key in m_history:
            del m_history[key]
        m_history[key] = None
        
        # Limit history size to prevent unbounded growth.
        # Heuristic: Keep history size roughly 2x the object count of the cache
        # or at least 2000 items to capture loops.
        current_count = len(cache_snapshot.cache)
        max_history = max(current_count * 2, 2000)
        
        if len(m_history) > max_history:
            # Remove LRU history
            del m_history[next(iter(m_history))]
            
    elif key in m_protected:
        del m_protected[key]
        m_protected_size -= evicted_obj.size
        
    # Ensure key is removed from history if it exists there (rare edge case)
    if key in m_history:
        del m_history[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiggad6lu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi4ac2xai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpokb71c_i.pickle

Iteration 69: New subsample score 0.451284 is not better than old score 0.498998, skipping
Iteration 70: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwsvxe40n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpekrbytru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaaj4bl4z.pickle

Iteration 70: Proposed new text for program: import random

# Metadata storage
# 1. m_last_access: key -> int (Last access time/index)
# 2. m_frequency: key -> int (Global frequency count, persists briefly after eviction)
# 3. m_protected: set (Keys that are currently considered "Protected" / Segment 2)

m_last_access = {}
m_frequency = {}
m_protected = set()

# Constants
MAX_FREQ_HISTORY = 5000  # Max size for frequency dictionary to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    Segmented LRU (SLRU) Eviction Strategy.
    
    Logic:
    1. Identify "Probationary" items (items NOT in m_protected).
    2. Ideally, evict the Least Recently Used (LRU) item from the Probationary set.
    3. If no Probationary items exist (all items are Protected), evict the LRU from the Protected set.
    '''
    global m_last_access, m_protected
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # 1. Separate keys into Probationary (freq=1) and Protected (freq>=2)
    # We use the m_protected set for O(1) checking.
    probationary_candidates = []
    protected_candidates = []
    
    for k in current_keys:
        if k in m_protected:
            protected_candidates.append(k)
        else:
            probationary_candidates.append(k)

    candidate_key = None
    
    # 2. Try to evict from Probationary first (Scan resistance)
    if probationary_candidates:
        # Find LRU in probationary
        candidate_key = min(probationary_candidates, key=lambda k: m_last_access.get(k, 0))
    
    # 3. Fallback: Evict from Protected if Probationary is empty
    elif protected_candidates:
        # Find LRU in protected
        candidate_key = min(protected_candidates, key=lambda k: m_last_access.get(k, 0))
    
    # Absolute fallback
    if candidate_key is None:
        candidate_key = min(current_keys, key=lambda k: m_last_access.get(k, 0))
        
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency.
    2. Promote to Protected status if not already there.
    '''
    global m_last_access, m_frequency, m_protected
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment global frequency
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1
    
    # Promote to Protected status because it was hit again
    m_protected.add(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Update recency.
    2. Determine status based on historical frequency.
       - If we remember this item (freq >= 1 previously), it promotes to Protected immediately.
       - If it's brand new, it stays in Probationary (not added to m_protected).
    '''
    global m_last_access, m_frequency, m_protected
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Check if we have seen this object before in our history
    history_freq = m_frequency.get(obj.key, 0)
    
    if history_freq > 0:
        # We've seen it before (recently evicted), so treat as a "warm" insert.
        # Promote directly to Protected to avoid immediate eviction.
        m_protected.add(obj.key)
        m_frequency[obj.key] += 1
    else:
        # First time seeing it (or history expired).
        # It remains Probationary (do NOT add to m_protected).
        m_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from local tracking (m_last_access).
    2. Remove from Protected set if present.
    3. Keep frequency data for a while (history), but prune if too large.
    '''
    global m_last_access, m_protected, m_frequency
    
    key = evicted_obj.key
    
    if key in m_last_access:
        del m_last_access[key]
        
    if key in m_protected:
        m_protected.remove(key)
        
    # Note: We do NOT delete from m_frequency immediately. 
    # This allows the algorithm to remember "recent" frequency if the item returns.
    
    # Garbage Collection for Metadata:
    # If the frequency map grows too large (indicating we are tracking too many 
    # long-gone items), we prune it to save memory and keep history relevant.
    if len(m_frequency) > MAX_FREQ_HISTORY:
        # Simple random eviction or LRU for metadata is sufficient.
        # Python dicts preserve insertion order (in recent versions), 
        # so iterating and deleting the first item acts like FIFO pruning.
        # We delete a chunk to amortize the cost.
        keys_to_delete = list(m_frequency.keys())[:100] 
        for k in keys_to_delete:
            del m_frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpl2qpdtpa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptjq1_435.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcs13mm8m.pickle

Iteration 70: New subsample score 0.315494 is not better than old score 0.3312, skipping
Iteration 71: Selected program 15 score: 0.24230047916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp19owfsn7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphqtxhsnd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi_t96fcv.pickle

Iteration 71: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency (int)
# 2. key -> last_access_time (int) - strictly for tie-breaking equal scores
# 3. key -> priority_score (float) - the GDSF score
m_key_frequency = dict()
m_key_last_access = dict()
m_key_score = dict()

# The "Inflation Value" (L)
# This represents the priority of the last evicted item.
# It serves as the base value for new items, effectively aging older items naturally.
current_L = 0.0

def calculate_score(freq, size, L):
    """
    Calculates the GDSF score (Greedy-Dual-Size-Frequency).
    Formula: Priority = L + (Frequency / Size)
    
    Why this works:
    - Frequency numerator: Keeps popular items.
    - Size denominator: Prefers small items (higher density of hits per byte).
    - L (Inflation): The 'aging' mechanism. New items start at current_L. 
      Older items must accumulate frequency to stay above the rising L.
    """
    # We use a slight weighting on frequency to ensure recency doesn't dominate purely via L.
    # However, standard GDSF is L + (Freq / Size).
    return L + (float(freq) / float(size))

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Greedy-Dual-Size-Frequency (GDSF).
    
    We look for the item with the LOWEST score.
    Upon eviction, we update the global inflation factor 'current_L' 
    to the score of the evicted item.
    '''
    global current_L, m_key_score, m_key_last_access
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    min_score = float('inf')
    min_access = float('inf')
    
    # Search for the item with the minimum GDSF score
    for key in current_keys:
        # If for some reason metadata is missing, default to safe values
        score = m_key_score.get(key, 0.0)
        access = m_key_last_access.get(key, 0)
        
        # Primary Comparator: Score (Lower is better candidate for eviction)
        if score < min_score:
            min_score = score
            min_access = access
            victim_key = key
        
        # Tie-Breaker: Recency (LRU)
        # If scores are identical (or very close due to float precision), evict the older one.
        elif score == min_score:
            if access < min_access:
                min_access = access
                victim_key = key

    # Update the global inflation factor L to the priority of the victim.
    # This implies that future objects must be at least this "valuable" to survive.
    if victim_key is not None:
        current_L = min_score
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    3. Re-calculate Score based on new Frequency and current L.
       Note: In standard GDSF, on a hit, we reset the object's base value to current_L
       and re-add its frequency weight.
    '''
    global m_key_frequency, m_key_last_access, m_key_score, current_L
    
    current_time = cache_snapshot.access_count
    
    # 1. Update Recency
    m_key_last_access[obj.key] = current_time
    
    # 2. Update Frequency
    # We cap frequency growth slightly to prevent massive pollution by long-term heavy hitters,
    # though standard GDSF allows unbounded growth.
    curr_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = curr_freq
    
    # 3. Update Score
    # The crucial part of GDSF: On access, the object is "brought forward" in time.
    # Its base becomes the current L, giving it a fresh start against the "aging" floor.
    m_key_score[obj.key] = calculate_score(curr_freq, obj.size, current_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Set Recency.
    3. Calculate Initial Score using current_L.
    '''
    global m_key_frequency, m_key_last_access, m_key_score, current_L
    
    current_time = cache_snapshot.access_count
    
    # 1. Initialize Frequency
    # If we have "ghost" history (it was in m_key_frequency but not in cache), 
    # we could restore it, but simple GDSF usually starts at 1 to prove utility.
    # Let's check if we have residual frequency data (history) to give it a boost.
    # If not, start at 1.
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1
        
    freq = m_key_frequency[obj.key]
    
    # 2. Set Recency
    m_key_last_access[obj.key] = current_time
    
    # 3. Calculate Score
    # New items enter with the base value of current_L.
    m_key_score[obj.key] = calculate_score(freq, obj.size, current_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up Score tracking for the evicted item to save memory.
    2. We optionally keep Frequency data to simulate "Ghost" memory implicitly.
       If the item returns, it picks up its old frequency count, resulting in a 
       much higher score immediately, preventing thrashing.
    '''
    global m_key_score, m_key_last_access
    
    # Remove from Score map (it's no longer a candidate for eviction if it's not in cache)
    if evicted_obj.key in m_key_score:
        del m_key_score[evicted_obj.key]
        
    # Remove from Last Access (it's gone)
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    # We DO NOT delete from m_key_frequency immediately.
    # Leaving the frequency ensures that if this item is re-inserted soon (thrashing),
    # it calculates a high score (L + old_freq/size) and sticks in cache.
    # However, to prevent memory leaks in a real system, we'd need to prune this dict.
    # Given the constraints, we rely on Python's dict efficiency or assume the trace 
    # isn't infinitely unique.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpszgh0t41.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpucw1xq_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx0c9b7qt.pickle

Iteration 71: New subsample score 0.114231 is not better than old score 0.184844, skipping
Iteration 72: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf1o8r9ms.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg826ico6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5fh3l4pg.pickle

Iteration 72: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency (int): How many times accessed (capped).
# 2. key -> last_access_time (int): When it was last seen.
m_key_frequency = dict()
m_key_last_access = dict()

# Configuration
# Max frequency to track. Capping prevents historical heavy hitters 
# from staying forever if they stop being accessed.
MAX_FREQ_CAP = 10 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Prioritized LRU (SLRU-like).
    
    Logic:
    1. We want to keep items that have proved their worth (Frequency > 1).
    2. We want to discard "one-hit wonders" (Frequency == 1) first.
    3. Within those groups, we discard the Least Recently Used (LRU).
    
    This creates a "Probationary" pool (Freq=1) and a "Protected" pool (Freq > 1).
    The algorithm always looks for a victim in the Probationary pool first.
    Only if the cache is purely full of Protected items do we evict the oldest Protected one.
    '''
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We search for the "worst" item.
    # Comparison criteria: (Frequency, Last_Access_Time)
    # Lower is worse. 
    # This automatically prioritizes evicting Freq=1 items over Freq=2 items.
    
    victim_key = None
    min_score = (float('inf'), float('inf'))
    
    for key in current_keys:
        # Default to 0 if metadata missing (safety check)
        freq = m_key_frequency.get(key, 0)
        recency = m_key_last_access.get(key, 0)
        
        # Construct score. 
        # freq is the primary sorting key (low freq = eviction candidate).
        # recency is the secondary sorting key (old time = eviction candidate).
        score = (freq, recency)
        
        if score < min_score:
            min_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency (promote from Probationary to Protected), up to a cap.
    '''
    global m_key_frequency, m_key_last_access
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    m_key_last_access[key] = current_time
    
    # Increment frequency but cap it.
    # This ensures that an item accessed 1000 times doesn't take 
    # 1000 misses to finally be evicted if it becomes cold.
    current_freq = m_key_frequency.get(key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[key] = current_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Initialize Frequency to 1 (Probationary).
    '''
    global m_key_frequency, m_key_last_access
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    m_key_last_access[key] = current_time
    
    # New items start at Frequency 1 (Probationary).
    # They must be hit again to survive eviction against other Freq=1 items.
    # Note: We do NOT use Ghost lists here. In constrained environments, 
    # strict SLRU (Freq 1 vs Freq > 1) usually outperforms complex Ghost logic 
    # because it reacts faster to workload changes.
    m_key_frequency[key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata to prevent memory leaks.
    '''
    global m_key_frequency, m_key_last_access
    
    key = evicted_obj.key
    
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    if key in m_key_last_access:
        del m_key_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxpeo3wxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuvis3rr1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqnj99q3r.pickle

Iteration 72: New subsample score 0.315678 is better than old score 0.314763. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5khkila3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0jvafv80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmmbj3g6l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdtwps184.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmponblno7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpti69ily_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphhga7p3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8t4tghn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptp7esl4z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe0w1lvi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3uur3707.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm4o6c6zx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt0kcxw_c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj1bd2hfj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxbi8ptsr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0thi3wek.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpubuc_qgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb76k595k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9_mujso7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy6cq9quc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph6n1kkoe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdo5nvpc7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt04gqkgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpos08743k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppo9ezkr4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpye35z87n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptbb62x76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxz2lf11p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf1_6btc8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe6skn93y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1ng3f2oh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiuegs7qu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpka4kogfx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5c01wbsk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp7na7ya3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5tkfubpr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbric1qmd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd7d5sy3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpip1xhxa4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj7ekqhdx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp40bp1fq_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4o19a3qx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz4z0ybup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdqdd9pmd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvtfw9s5m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuvfv7xbj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfwz07w9v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpovl4cz5h.pickle

Iteration 72: Full valset score for new program: 0.22787797916666663
Iteration 72: Full train_val score for new program: 0.22787797916666663
Iteration 72: Individual valset scores for new program: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.456547, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.332169, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.700136, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 72: New valset pareto front scores: [0.509148, 0.479804, 0.493182, 0.441985, 0.50361, 0.49412, 0.273923, 0.498624, 0.541294, 0.531017, 0.1, 0.431616, 0.053294, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.074873, 0.038636, 0.045558, 0.061301, 0.069661, 0.770082, 0.083333, 0.083848, 0.096221, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.052632, 0.366667, 0.184265, 0.154278, 0.466258, 0.081699]
Iteration 72: Full valset pareto front score: 0.2626377083333333
Iteration 72: Updated valset pareto front programs: [{2}, {17}, {2}, {17}, {14}, {17}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {11}, {17}, {19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10}, {4, 6, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21}, {15}, {2, 4, 8, 9, 10, 17, 19}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 20, 21}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 16, 20, 21}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {5}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21}, {1, 4, 5, 6, 10, 11, 13, 14, 15, 16, 20, 21}, {9}, {9}, {17}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21}, {9}, {5}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {5}, {5}, {5}, {1, 4, 5, 6, 10, 11, 13, 14, 16, 17, 20, 21}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21}, {1, 5, 9, 10, 13, 14, 15, 16, 17, 19, 20, 21}]
Iteration 72: Best valset aggregate score so far: 0.2508065
Iteration 72: Best program as per aggregate score on train_val: 5
Iteration 72: Best program as per aggregate score on valset: 5
Iteration 72: Best score on valset: 0.2508065
Iteration 72: Best score on train_val: 0.2508065
Iteration 72: Linear pareto front program index: 5
Iteration 72: New program candidate index: 21
Iteration 73: Selected program 14 score: 0.24068325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6mq2r36m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcp1hvfns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe74k7yru.pickle

Iteration 73: Proposed new text for program: # Metadata storage
# T1: Recent Cache (Probationary) - LRU list of keys (Index 0 is LRU)
# T2: Frequent Cache (Protected) - LRU list of keys (Index 0 is LRU)
# B1: Ghost Recent (History of evicted T1) - LRU list of keys
# B2: Ghost Frequent (History of evicted T2) - LRU list of keys
# m_p: Target size for list T1 (adaptive parameter)

m_t1 = []
m_t2 = []
m_b1 = []
m_b2 = []
m_p = 0.0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Decision.
    Determines whether to evict from T1 (Recent) or T2 (Frequent).
    
    The logic follows the ARC 'replace' subroutine.
    We decide based on the target size `m_p` and whether the *incoming* object
    is in the ghost list B2.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p

    victim_key = None
    
    len_t1 = len(m_t1)
    incoming_key = obj.key
    
    # ARC Logic:
    # We evict from T1 if:
    # 1. T1 contains items AND it is exceeding its target `m_p`.
    # 2. OR, the incoming item is in B2 (indicating we need to grow T2/shrink T1), 
    #    and T1 is essentially "full" relative to `m_p`.
    
    # Note: We check if `incoming_key` is in B2 to perform the logic correctly.
    # Logic: if (T1 is not empty) and ((len(T1) > p) or (x in B2 and len(T1) == p))
    # In ARC, condition `len(T1) == p` with `x in B2` also triggers T1 eviction.
    
    # Condition to evict from T1
    evict_t1 = False
    
    if m_t1:
        if len_t1 > m_p:
            evict_t1 = True
        elif incoming_key in m_b2 and len_t1 == int(m_p):
             # If strictly equal, and we hit B2, we favor evicting T1 to allow P to shrink later
            evict_t1 = True

    if evict_t1:
        victim_key = m_t1[0] # LRU of T1
    else:
        # Otherwise evict from T2
        if m_t2:
            victim_key = m_t2[0] # LRU of T2
        elif m_t1:
            # Fallback: if T2 is empty but logic said don't evict T1 (e.g. p is high),
            # but we MUST evict something, we evict from T1.
            victim_key = m_t1[0]
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    Move the object to the MRU position of T2 (Frequent).
    '''
    global m_t1, m_t2
    
    key = obj.key
    
    if key in m_t1:
        m_t1.remove(key)
        m_t2.append(key) # Promote Recent -> Frequent
    elif key in m_t2:
        m_t2.remove(key)
        m_t2.append(key) # Refresh Frequent

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    1. Check Ghost lists (B1, B2) to adapt `m_p`.
    2. Insert the new object into T1 or T2 based on history.
    3. Trim ghost lists lazily.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    
    # 1. Adapt P (Target size of T1)
    if key in m_b1:
        # Hit in Ghost Recent: We should have kept T1 larger.
        delta = 1
        if len(m_b1) >= len(m_b2) and len(m_b2) > 0:
            delta = 1
        elif len(m_b2) > len(m_b1):
             # Float division ensures granularity
             delta = len(m_b2) / len(m_b1)
             
        # C (Capacity) is dynamic in a byte-cache. We use current count as proxy.
        # Adding 1 to account for the new item roughly.
        current_c = len(m_t1) + len(m_t2) + 1
        m_p = min(float(current_c), m_p + delta)
        
        # Move from Ghost to Real Frequent
        m_b1.remove(key)
        m_t2.append(key)
        
    elif key in m_b2:
        # Hit in Ghost Frequent: We should have kept T2 larger (T1 smaller).
        delta = 1
        if len(m_b2) >= len(m_b1) and len(m_b1) > 0:
            delta = 1
        elif len(m_b1) > len(m_b2):
            delta = len(m_b1) / len(m_b2)
            
        m_p = max(0.0, m_p - delta)
        
        # Move from Ghost to Real Frequent
        m_b2.remove(key)
        m_t2.append(key)
        
    else:
        # Totally Cold Miss
        m_t1.append(key)

    # 2. Trim Ghosts
    # In a byte-sized cache, strictly enforcing ghost counts based on *current* item count
    # is dangerous (one large item makes capacity=1, wiping out history).
    # We use a relaxed bound. We allow ghosts to hold at least a reasonable history,
    # or max(current_count, some_lower_bound) logic. 
    # Here, we abide by ARC rule: |L1| + |L2| <= 2*C.
    # We approximate C as the current directory size (len_t1 + len_t2).
    
    current_directory_count = len(m_t1) + len(m_t2)
    
    # We enforce |T1| + |B1| <= C
    while len(m_t1) + len(m_b1) > current_directory_count and len(m_b1) > 0:
        m_b1.pop(0) # Remove LRU

    # We enforce |T1| + |T2| + |B1| + |B2| <= 2*C
    # Since |T1|+|T2| = C, this simplifies to |B1| + |B2| <= C usually,
    # but the generalized form is safer.
    total_keys = len(m_t1) + len(m_t2) + len(m_b1) + len(m_b2)
    while total_keys > 2 * current_directory_count and (len(m_b1) > 0 or len(m_b2) > 0):
        # Remove from the longer ghost list usually, or B2 LRU by default standard ARC
        if len(m_b2) > 0:
            m_b2.pop(0)
        elif len(m_b1) > 0:
            m_b1.pop(0)
        total_keys -= 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Move the key from Real Cache (T1/T2) to Ghost Cache (B1/B2).
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    evicted_key = evicted_obj.key
    
    # We must find where the key was to move it correctly.
    # Note: evict() returned the victim, but update_after_evict is where we theoretically 
    # move it to ghost.
    
    if evicted_key in m_t1:
        m_t1.remove(evicted_key)
        # If it was in T1 (Recent), it goes to B1 (Ghost Recent)
        m_b1.append(evicted_key)
    elif evicted_key in m_t2:
        m_t2.remove(evicted_key)
        # If it was in T2 (Frequent), it goes to B2 (Ghost Frequent)
        m_b2.append(evicted_key)
    else:
        # Fallback for synchronization errors:
        # If the framework evicted something we don't track (shouldn't happen),
        # we treat it as a T1 eviction to be safe.
        m_b1.append(evicted_key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp26v3kbsx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2pud5k_t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu3lcxi1t.pickle

Iteration 73: New subsample score 0.937052 is not better than old score 0.937052, skipping
Iteration 74: Selected program 19 score: 0.23256970833333343
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw8hsna9n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp32l4lpq6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprxsfj4r5.pickle

Iteration 74: Proposed new text for program: from collections import OrderedDict

# Metadata Storage
# 1. m_stack_s: OrderedDict (key -> None).
#    Represents the LIRS Stack S (History + LIRs). 
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed.
# 2. m_queue_q: OrderedDict (key -> None).
#    Represents the Resident HIR Queue Q.
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed (Candidate for eviction).
# 3. m_state: dict (key -> 'LIR' or 'HIR'). 
#    Tracks the status of resident items. If a key is in Stack S but not m_state, it is a Non-Resident HIR.
# 4. m_lir_size: int. 
#    Tracks total size of current LIR items to manage capacity dynamically.

m_stack_s = OrderedDict()
m_queue_q = OrderedDict()
m_state = dict()
m_lir_size = 0

# Configuration
# Reserve ~1% of cache for HIR items to act as a filter for scans.
# This maximizes the space for the LIR working set.
LIR_TARGET_RATIO = 0.99

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction.
    Strategy: 
    1. Evict the resident HIR item at the front of Queue Q (oldest).
    2. If Q is empty (all LIR), evict the LIR item at the bottom of Stack S.
    '''
    global m_queue_q, m_stack_s
    
    # 1. Try to evict from HIR Queue (Resident HIRs)
    if m_queue_q:
        victim_key = next(iter(m_queue_q))
        return victim_key
        
    # 2. Fallback: If Queue Q is empty, the cache is filled entirely with LIRs.
    # The bottom of Stack S is guaranteed to be an LIR item due to pruning.
    if m_stack_s:
        victim_key = next(iter(m_stack_s))
        return victim_key

    # 3. Last resort (should not be reached)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    state = m_state.get(key, 'HIR')
    
    if state == 'LIR':
        # LIR Case:
        # Move to top of Stack S (Recency update)
        if key in m_stack_s:
            m_stack_s.move_to_end(key)
        else:
            # Should not happen for resident LIR, but safe fallback
            m_stack_s[key] = None
        
        # Pruning is vital here: moving an item to the top might reveal 
        # a non-LIR item at the bottom of the stack.
        _prune_stack()
        
    elif state == 'HIR':
        # HIR Case:
        if key in m_stack_s:
            # HIR hit inside Stack S -> Hot access (within LIR reuse distance).
            # Promote to LIR.
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            
            # Remove from HIR Queue Q
            if key in m_queue_q:
                del m_queue_q[key]
            
            # Move to top of Stack S
            m_stack_s.move_to_end(key)
            
            # Enforce LIR capacity (demote others if we grew too big)
            _enforce_lir_capacity(cache_snapshot)
        else:
            # HIR hit outside Stack S -> Cold access (Resident but old).
            # Stays HIR, but moves to top of S and end of Q (fresh chance).
            m_stack_s[key] = None
            if key in m_queue_q:
                m_queue_q.move_to_end(key)
            else:
                m_queue_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # 1. Check if this key exists in Stack S (Non-Resident History Hit)
    if key in m_stack_s:
        # It was evicted recently but is still in our "Hot" history scope.
        # Restore as LIR.
        m_state[key] = 'LIR'
        m_lir_size += obj.size
        m_stack_s.move_to_end(key)
        _enforce_lir_capacity(cache_snapshot)
    else:
        # 2. Cold Miss.
        # Bootstrapping optimization: If LIR set is small (cache filling up),
        # treat new items as LIRs to populate the stack history quickly.
        # Otherwise, standard LIRS logic treats new items as HIR.
        if m_lir_size + obj.size <= capacity * LIR_TARGET_RATIO:
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            m_stack_s[key] = None
        else:
            # Start as HIR
            m_state[key] = 'HIR'
            m_stack_s[key] = None
            m_queue_q[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = evicted_obj.key
    
    # 1. Remove from Queue Q
    if key in m_queue_q:
        del m_queue_q[key]
        
    # 2. Update State and Size
    if key in m_state:
        if m_state[key] == 'LIR':
            m_lir_size -= evicted_obj.size
        del m_state[key]
        
    # 3. Stack S Maintenance
    # If the evicted item was in Stack S, it stays there! 
    # It becomes a "Non-Resident HIR" entry, providing history for future hits.
    # However, if it sits at the bottom, _prune_stack will remove it.
    _prune_stack()

def _prune_stack():
    '''
    Ensures the bottom of Stack S is always an LIR item.
    Removes HIR items (resident or non-resident) from the bottom.
    '''
    global m_stack_s, m_state
    
    while m_stack_s:
        bottom_key = next(iter(m_stack_s))
        
        # If the bottom item is LIR and Resident, the stack is valid.
        if bottom_key in m_state and m_state[bottom_key] == 'LIR':
            break
            
        # Otherwise, the bottom is HIR (Resident or Non-Resident).
        # It is no longer useful for defining the LIR stack distance.
        m_stack_s.popitem(last=False)

def _enforce_lir_capacity(cache_snapshot):
    '''
    Prevents LIR set from consuming too much cache.
    Demotes the LRU LIR (bottom of S) to HIR if budget exceeded.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    target_capacity = cache_snapshot.capacity * LIR_TARGET_RATIO
    
    while m_lir_size > target_capacity and m_stack_s:
        # The LIR to demote is at the bottom of Stack S
        victim_key = next(iter(m_stack_s))
        
        # Verify it is LIR
        if m_state.get(victim_key) == 'LIR':
            # Demote LIR -> HIR
            m_state[victim_key] = 'HIR'
            
            # Reduce LIR size. Since it is LIR, it must be in cache.
            if victim_key in cache_snapshot.cache:
                m_lir_size -= cache_snapshot.cache[victim_key].size
            
            # Move to Queue Q (Resident HIR)
            m_queue_q[victim_key] = None
            
        # Trigger pruning to remove the newly demoted HIR from bottom of S
        _prune_stack()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpby58916f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm03lcbpw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn51anh6j.pickle

Iteration 74: New subsample score 0.8983120000000001 is better than old score 0.874361. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfa3_hp1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg3ww2mvh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyicjqsow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqmnhh9aw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm_nz_zsp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfl_4sire.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1bk37j7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp115j3mkb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpln1_0qs2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf5pi0_p2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp64tzyxw7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa2y7fht2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdtwk25y3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbbxndpvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3s0e6_6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvgx1jh51.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvle7f2t9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0ri2kvmi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2in9vlrr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpilna50x3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp40gv9o5_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv8_dcfl9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcwhkk3ar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzkrjfhi6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt5jwg4pj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpttak72ef.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpacfnech3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe88mj637.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzq3i1clh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5955c3l9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpel4ckt88.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqzqujegn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2gj8oags.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6yiwc9k6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp75reqeba.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4yfahq8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1saqiwkj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp57hb61r3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk9g1fpkz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuivc4xr6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphm0dibpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvq4v9aef.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6pn8zthv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0gdrk2qu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_s9d49_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6maddjal.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpplbe5kgi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_pb32u0m.pickle

Iteration 74: New program is on the linear pareto front
Iteration 74: Full valset score for new program: 0.2627519583333333
Iteration 74: Full train_val score for new program: 0.2627519583333333
Iteration 74: Individual valset scores for new program: [0.5113, 0.482372, 0.490273, 0.448119, 0.501845, 0.494754, 0.266746, 0.494101, 0.526278, 0.531017, 0.116667, 0.431172, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.270394, 0.423795, 0.02489, 0.060606, 0.060606, 0.269802, 0.393145, 0.856008, 0.893548, 0.139563, 0.034091, 0.052392, 0.080119, 0.08192, 0.775357, 0.078947, 0.120918, 0.14712, 0.639361, 0.125461, 0.086776, 0.094075, 0.103816, 0.067434, 0.333333, 0.159945, 0.098177, 0.466258, 0.176471]
Iteration 74: New valset pareto front scores: [0.5113, 0.482372, 0.493182, 0.448119, 0.50361, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.139563, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.14712, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 74: Full valset pareto front score: 0.27095224999999995
Iteration 74: Updated valset pareto front programs: [{22}, {22}, {2}, {22}, {14}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21}, {22}, {2, 4, 8, 9, 10, 17, 19}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {22}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21}, {22}, {22}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22}, {22}]
Iteration 74: Best valset aggregate score so far: 0.2627519583333333
Iteration 74: Best program as per aggregate score on train_val: 22
Iteration 74: Best program as per aggregate score on valset: 22
Iteration 74: Best score on valset: 0.2627519583333333
Iteration 74: Best score on train_val: 0.2627519583333333
Iteration 74: Linear pareto front program index: 22
Iteration 74: New program candidate index: 22
Iteration 75: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_47yxp5d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdwihcret.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzphm4pap.pickle

Iteration 75: Proposed new text for program: from collections import OrderedDict

# =============================================================================
# Size-Aware Adaptive Replacement Cache (ARC)
# =============================================================================
#
# Concepts:
# 1. m_t1: Resident Recency Set. Contains objects accessed once recently.
# 2. m_t2: Resident Frequency Set. Contains objects accessed at least twice.
# 3. m_b1: Ghost Recency Set. Keys evicted from T1.
# 4. m_b2: Ghost Frequency Set. Keys evicted from T2.
# 5. m_p:  Target size (in bytes) for T1. Adapts based on Ghost hits.
#
# Operation:
# - Hits in B1 suggest T1 is too small -> Increase m_p.
# - Hits in B2 suggest T2 is too small -> Decrease m_p.
# - Eviction Victim is chosen from T1 or T2 based on current T1 size vs m_p.

m_t1 = OrderedDict()
m_t2 = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()

# Metadata Size Tracking (Bytes)
m_t1_size = 0
m_t2_size = 0
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using Size-Aware ARC logic.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_t1_size

    # We determine if we should evict from T1 (Recency) or T2 (Frequency).
    # Logic:
    # 1. If T1 size exceeds its target `m_p`, we usually evict from T1.
    # 2. If the incoming object `obj` (which caused the eviction need) is in B2,
    #    it implies we are about to grow T2. In this specific case, standard ARC
    #    allows T1 to be evicted even if it equals `m_p` to make room.
    
    incoming_key = obj.key
    t1_over_target = m_t1_size > m_p
    in_b2 = incoming_key in m_b2
    
    # Prefer evicting from T1 if it is over budget, or if the incoming item
    # indicates a shift towards Frequency (B2 hit) and T1 is saturating its target.
    if m_t1:
        if t1_over_target or (in_b2 and m_t1_size == m_p):
            return next(iter(m_t1)) # LRU of T1
            
    # Otherwise, evict from T2
    if m_t2:
        return next(iter(m_t2)) # LRU of T2
        
    # Fail-safe (should not be reached if cache is full)
    return next(iter(m_t1)) if m_t1 else next(iter(m_t2))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    A hit on T1 promotes to T2 (Frequency). A hit on T2 updates MRU.
    '''
    global m_t1, m_t2, m_t1_size, m_t2_size
    
    key = obj.key
    
    if key in m_t1:
        # Hit in T1 (Recency) -> Move to T2 (Frequency)
        # The object has now been accessed twice (Entry + this hit).
        del m_t1[key]
        m_t1_size -= obj.size
        
        m_t2[key] = None
        m_t2_size += obj.size
        m_t2.move_to_end(key)
        
    elif key in m_t2:
        # Hit in T2 -> It stays in Frequency, just update position (MRU)
        m_t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Insert (Miss).
    Checks Ghost lists to adapt the target size `m_p`.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_t1_size, m_t2_size
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # 1. Check Ghost Lists (History of recent evictions)
    if key in m_b1:
        # Ghost Hit in B1 (Recency History)
        # We evicted a recent item too soon. Increase target P for T1.
        delta = obj.size
        if len(m_b1) < len(m_b2):
             delta = int(obj.size * (len(m_b2) / len(m_b1)))
             
        m_p = min(capacity, m_p + delta)
        
        # Remove from ghost and promote to T2 (it's now frequent)
        del m_b1[key]
        m_t2[key] = None
        m_t2_size += obj.size
        
    elif key in m_b2:
        # Ghost Hit in B2 (Frequency History)
        # We evicted a frequent item too soon. Decrease target P for T1 (Grow T2).
        delta = obj.size
        if len(m_b2) < len(m_b1):
            delta = int(obj.size * (len(m_b1) / len(m_b2)))
            
        m_p = max(0, m_p - delta)
        
        # Remove from ghost and promote to T2
        del m_b2[key]
        m_t2[key] = None
        m_t2_size += obj.size
        
    else:
        # 2. Total Cold Miss
        # Insert into T1 (Recency)
        m_t1[key] = None
        m_t1_size += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction. Moves victims to Ghost lists.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_t1_size, m_t2_size
    
    key = evicted_obj.key
    
    # We must determine if the evicted object came from T1 or T2.
    if key in m_t1:
        # Evicted from T1 -> Move to B1
        del m_t1[key]
        m_t1_size -= evicted_obj.size
        m_b1[key] = None
        
    elif key in m_t2:
        # Evicted from T2 -> Move to B2
        del m_t2[key]
        m_t2_size -= evicted_obj.size
        m_b2[key] = None

    # Housekeeping: Prevent Ghost lists from growing infinitely.
    # While standard ARC bounds B1+B2 by count, for variable sizes we allow
    # a generous count limit relative to the number of cached items to maintain history.
    max_ghosts = 2 * len(cache_snapshot.cache) + 100
    if len(m_b1) > max_ghosts:
        m_b1.popitem(last=False)
    if len(m_b2) > max_ghosts:
        m_b2.popitem(last=False)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjufil6cr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpshix_6_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpycealhsf.pickle

Iteration 75: New subsample score 1.4664069999999998 is not better than old score 1.550197, skipping
Iteration 76: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo4m9bedh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnn7sgn1o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4ffvw05l.pickle

Iteration 76: Proposed new text for program: import math

# Metadata storage
# 1. key -> last_access_time (Used for LRU tie-breaking)
# 2. key -> frequency_score (The primary metric for eviction, represents value)
# 3. key -> bool (Ghost presence check)
m_key_last_access = dict()
m_key_frequency = dict()
m_ghost_entries = set()

# Configuration
# Capacity for the ghost history (usually 0.5x to 1x of cache size works best, 
# but since we don't know N, we set a safe fixed limit or manage dynamically).
MAX_GHOSTS = 5000 

# The global "inflation" counter.
# This technique (Dynamic Aging) prevents old high-frequency items from 
# staying forever. New items enter with this base value.
global_min_freq = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU with Dynamic Aging (LFU-DA) / GreedyDual-Size.
    
    We calculate a value for every object: H = Frequency + Arrival_Time_Base.
    We evict the object with the smallest H value. 
    
    If there is a tie in H value, we evict the Least Recently Used (LRU) among them.
    
    When an object is evicted, we update the global_min_freq to that object's value.
    This ensures that future insertions start at a higher baseline, effectively
    "aging out" objects that sit in the cache with static frequency counts.
    '''
    global global_min_freq, m_key_frequency, m_key_last_access
    
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # We want to find the victim with the minimum Priority Score.
    # Score = Frequency Value.
    # Tie-breaker = Last Access Time (Ascending -> Older is victim).
    
    victim_key = None
    min_freq = float('inf')
    min_recency = float('inf')
    
    # Linear scan to find the minimum. 
    # (In a production system, a Min-Heap would be O(1), but O(N) is acceptable here).
    for key in current_keys:
        freq = m_key_frequency.get(key, 0.0)
        recency = m_key_last_access.get(key, 0)
        
        # We are looking for the smallest Frequency.
        # If Frequencies are equal, we look for the smallest Recency (LRU).
        if freq < min_freq:
            min_freq = freq
            min_recency = recency
            victim_key = key
        elif freq == min_freq:
            if recency < min_recency:
                min_recency = recency
                victim_key = key
    
    # Crucial Step for LFU-DA: 
    # The 'value' of the cache rises to the level of the item we just evicted.
    # New items must eventually beat this threshold to survive.
    if victim_key is not None:
        global_min_freq = min_freq

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_key_last_access, m_key_frequency
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Additive increase.
    # We update the frequency relative to its current state.
    # Note: We do NOT reset it to global_min_freq on a hit; we accumulate value.
    current_val = m_key_frequency.get(obj.key, global_min_freq)
    m_key_frequency[obj.key] = current_val + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost cache (ARC-like behavior).
    2. Set initial frequency based on Ghost status + Global Aging Base.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries, global_min_freq
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Determine initial frequency value
    if obj.key in m_ghost_entries:
        # Ghost Hit: This item was recently needed but evicted.
        # It implies the cache is thrashing on this item.
        # We re-insert it with a "bonus" so it survives longer than a standard new item.
        # We give it the current inflation base + 2.0 (equivalent to 2 hits immediately).
        m_key_frequency[obj.key] = global_min_freq + 2.0
        m_ghost_entries.remove(obj.key)
    else:
        # Standard Insert:
        # Start at the current global minimum. This is the "Aging" mechanic.
        # It gives the new item a fair fighting chance against currently cached items,
        # but places it at the bottom of the hierarchy to be evicted if not used soon.
        # We add 1.0 representing the current access.
        m_key_frequency[obj.key] = global_min_freq + 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost entries to remember we saw this pattern.
    '''
    global m_key_last_access, m_key_frequency, m_ghost_entries
    
    key = evicted_obj.key
    
    # Clean up
    if key in m_key_last_access:
        del m_key_last_access[key]
        
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    # Add to Ghost (Shadow) cache
    m_ghost_entries.add(key)
    
    # Maintain Ghost size limits
    # If the ghost cache grows too large, remove arbitrary/oldest entries.
    if len(m_ghost_entries) > MAX_GHOSTS:
        # Pop an arbitrary element (Set doesn't guarantee order, but keeps O(1))
        m_ghost_entries.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpua5pibra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg19cv6bd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpma6s6n3o.pickle

Iteration 76: New subsample score 1.515495 is not better than old score 1.596787, skipping
Iteration 77: Selected program 5 score: 0.2508065
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcsw9d1mz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplkl_dam9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps4kzfzmr.pickle

Iteration 77: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency (int): Number of hits while in cache
m_key_frequency = dict()
# 2. key -> priority (float): The GDSF 'H' value
m_key_priority = dict()
# 3. key -> last_access (int): Used for tie-breaking
m_key_last_access = dict()

# Global inflation factor 'L' for GDSF
# This acts as the "watermark" for the cache's value. 
# New items must beat this value (plus their specific merit) to survive.
g_inflation_value = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Greedy-Dual-Size-Frequency (GDSF).
    
    We evict the object with the lowest Priority (H).
    
    Formula: H = L + (Frequency / Size)
    
    1. Small objects get high H (harder to evict).
    2. Frequent objects get high H.
    3. Recently accessed objects get high H (because L increases over time).
    '''
    global g_inflation_value
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    # Helper to retrieve comparison tuple: (Priority, Last_Access)
    # We want to evict the item with the smallest Priority.
    # If Priorities are equal, we evict the Least Recently Used (smallest last_access).
    def get_victim_score(k):
        # Default priority to -1 if missing to ensure it gets evicted if something is wrong
        return (m_key_priority.get(k, -1.0), m_key_last_access.get(k, 0))

    # Find the victim
    victim_key = min(current_keys, key=get_victim_score)
    
    # GDSF Critical Step: Update the global inflation factor L.
    # L becomes the priority of the item we just evicted.
    # This implies that for a new item to stay in the cache later, 
    # it needs to accrue enough "merit" to beat this new baseline.
    if victim_key in m_key_priority:
        g_inflation_value = m_key_priority[victim_key]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Re-calculate Priority using the CURRENT global L.
       This "restores" the object's value, effectively marking it as recently used.
    '''
    global g_inflation_value, m_key_frequency, m_key_priority, m_key_last_access
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency
    new_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = new_freq
    
    # Calculate Priority
    # H = L + (Frequency / Size)
    # Protection against size 0 (though unlikely in valid traces)
    safe_size = obj.size if obj.size > 0 else 1
    
    priority = g_inflation_value + (new_freq / safe_size)
    m_key_priority[obj.key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency to 1.
    2. Calculate initial Priority based on current L.
    '''
    global g_inflation_value, m_key_frequency, m_key_priority, m_key_last_access
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Initialize Frequency
    m_key_frequency[obj.key] = 1
    
    # Initialize Priority
    # H = L + (1 / Size)
    safe_size = obj.size if obj.size > 0 else 1
    
    priority = g_inflation_value + (1.0 / safe_size)
    m_key_priority[obj.key] = priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up metadata to prevent memory leaks.
    Note: GDSF does not strictly require a "Ghost List" because the 'L' factor
    serves as the system's memory of past eviction costs.
    '''
    global m_key_frequency, m_key_priority, m_key_last_access
    
    key = evicted_obj.key
    
    if key in m_key_frequency:
        del m_key_frequency[key]
    if key in m_key_priority:
        del m_key_priority[key]
    if key in m_key_last_access:
        del m_key_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaeqe1z7o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi8oru83h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4am17_u7.pickle

Iteration 77: New subsample score 0.8102079999999999 is better than old score 0.7903340000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2_dunjf0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplr5lu9am.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0z4yzskk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv1z7_iyb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5pq2f8iy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5wkiknar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjcpqn6sg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptha4yzf4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3oqu7nz1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu0hr7t7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc8hbwjw1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprj2aqenm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3zr4flk6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2cvzo89r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyrcy2o6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpawit6zwb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzy9nbszv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjqy5peg7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2amcqj2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnxyo_mt6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxj1ibe7x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0rqx2trz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdi_ykvkg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppe9ud43l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx1qsxvu4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpserqa8gu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_stgs02a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplo74_7ol.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5h2hmlel.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa2oygf7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8evc88bj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy1wjuwzb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk_yl8vpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5r5paf23.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnq_0ioi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxbh4qc43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr4e91crc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt_b0v9dj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbcoe2lfa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9_ikwv4m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcgq0xdq5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf9bjkfa4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmmsiuo_5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0l2v9_2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpexq8_edu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqh06z33u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp38vy7wjb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppkj2uyw9.pickle

Iteration 77: Full valset score for new program: 0.21743841666666666
Iteration 77: Full train_val score for new program: 0.21743841666666666
Iteration 77: Individual valset scores for new program: [0.457329, 0.435559, 0.443306, 0.388904, 0.45387, 0.444944, 0.264354, 0.498624, 0.537719, 0.531017, 0.066667, 0.332149, 0.023893, 0.0, 0.01968, 0.019442, 0.018581, 0.022069, 0.021375, 0.266728, 0.33825, 0.025282, 0.057382, 0.057382, 0.269776, 0.254032, 0.764647, 0.883964, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.747447, 0.072368, 0.062665, 0.009162, 0.618238, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.183333, 0.020965, 0.021879, 0.441718, 0.03268]
Iteration 77: New valset pareto front scores: [0.5113, 0.482372, 0.493182, 0.448119, 0.50361, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.139563, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.14712, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 77: Full valset pareto front score: 0.27095224999999995
Iteration 77: Updated valset pareto front programs: [{22}, {22}, {2}, {22}, {14}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21}, {22}, {2, 4, 8, 9, 10, 17, 19}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {22}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21}, {22}, {22}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22}, {22}]
Iteration 77: Best valset aggregate score so far: 0.2627519583333333
Iteration 77: Best program as per aggregate score on train_val: 22
Iteration 77: Best program as per aggregate score on valset: 22
Iteration 77: Best score on valset: 0.2627519583333333
Iteration 77: Best score on train_val: 0.2627519583333333
Iteration 77: Linear pareto front program index: 22
Iteration 77: New program candidate index: 23
Iteration 78: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkr___gfo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyxeayuki.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo14lkx6s.pickle

Iteration 78: Proposed new text for program: import math

# Metadata storage
# 1. key -> frequency_score (The value used to determine eviction)
# 2. key -> last_access_time (Used for LRU tie-breaking)
m_key_score = dict()
m_key_last_access = dict()

# Global state for LFU-DA (Dynamic Aging)
# This represents the "minimum frequency" currently in the cache.
# It effectively ages existing items by raising the bar for new items.
min_frequency_baseline = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: LFU-DA (Dynamic Aging) with LRU Tie-Breaker.
    
    Logic:
    1. Find the item with the lowest Frequency Score.
    2. If there are ties (multiple items with same lowest score), 
       evict the one with the oldest Last_Access_Time (LRU).
    3. Update the global 'min_frequency_baseline' to the score of the evicted item.
       This is the "Aging" mechanism: it raises the floor for future insertions.
    '''
    global min_frequency_baseline
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    
    # We want to minimize (Score, Last_Access_Time)
    # Initialize with infinity
    lowest_metric = (float('inf'), float('inf'))
    
    # Iterate to find victim
    for key in current_keys:
        score = m_key_score.get(key, 0.0)
        recency = m_key_last_access.get(key, 0)
        
        # Metric tuple: (Frequency Score, Recency)
        # Python compares tuples element by element.
        metric = (score, recency)
        
        if metric < lowest_metric:
            lowest_metric = metric
            victim_key = key

    # LFU-DA Logic: The evicted value becomes the new baseline.
    # This effectively "ages" all other items because new items will 
    # enter at this higher baseline level.
    if victim_key is not None:
        min_frequency_baseline = m_key_score.get(victim_key, 0.0)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (for LRU tie-breaking).
    2. Increment Frequency Score.
    '''
    global m_key_score, m_key_last_access
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # Standard LFU increment.
    # We use get() with default 0, though it should exist on a hit.
    current_score = m_key_score.get(obj.key, min_frequency_baseline)
    m_key_score[obj.key] = current_score + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Last Access Time.
    2. Initialize Score using LFU-DA logic:
       New Score = Global Baseline + Initial Weight (1.0).
       
    By adding min_frequency_baseline, we ensure new items have a fighting chance
    against old items, while implicitly penalizing items that haven't been 
    accessed since the baseline rose.
    '''
    global m_key_score, m_key_last_access, min_frequency_baseline
    
    current_time = cache_snapshot.access_count
    m_key_last_access[obj.key] = current_time
    
    # LFU-DA Initialization
    # New items inherit the history of the cache (baseline) + 1 hit.
    m_key_score[obj.key] = min_frequency_baseline + 1.0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata for the evicted object to free memory.
    '''
    global m_key_score, m_key_last_access
    
    key = evicted_obj.key
    
    if key in m_key_score:
        del m_key_score[key]
    
    if key in m_key_last_access:
        del m_key_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpknghgitp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfkyj8mow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphuxh9nfn.pickle

Iteration 78: New subsample score 0.089546 is not better than old score 0.428852, skipping
Iteration 79: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_rawx8vm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvmsl91n7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5zk1uck6.pickle

Iteration 79: Proposed new text for program: import random

# Global Metadata
# 1. Frequency History: Tracks rough frequency of items (both inside and outside cache).
#    This serves as our "Count-Min Sketch" or history keeper.
#    We periodically halve these values to adapt to changing patterns (Aging).
m_freq_sketch = {}

# 2. Key -> Access Time: Used for LRU logic within segments.
m_last_access = {}

# Configuration
# Max size for the frequency history map to prevent memory leaks.
# Should be larger than cache capacity to remember history of evicted items.
MAX_HISTORY_KEYS = 10000 

# Aging interval: Halve all frequencies after this many accesses.
# Keeps the history relevant to the recent window.
AGING_INTERVAL = 2000
access_counter_since_aging = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Admission with LFU-hinted LRU.
    
    When cache is full, we identify a candidate victim from the cache (LRU).
    We then compare the candidate's historical frequency with the new object's frequency.
    
    1. Find the LRU item (the standard victim).
    2. Compare freq(LRU_item) vs freq(new_obj).
    3. If freq(new_obj) < freq(LRU_item) and the LRU item has significant history,
       we prefer to "evict" the new object immediately (reject admission) 
       to preserve the valuable cache content.
       
    However, strictly rejecting the new object requires the framework to support it. 
    Assuming 'obj' is already pending insertion, returning obj.key implies 
    we discard the new data. If the framework inserts first then calls evict, 
    returning obj.key works as "reject".
    '''
    global m_freq_sketch, m_last_access
    
    # 1. Identify the LRU candidate from existing cache items
    # We want the item with the oldest access time.
    candidate_key = None
    min_time = float('inf')
    
    # Sampling Optimization:
    # Iterating the whole dict is O(N). For large caches, we sample N keys 
    # and pick the LRU among them to keep decision time fast.
    # If cache is small (<500), scan all.
    keys_to_check = list(cache_snapshot.cache.keys())
    if len(keys_to_check) > 100:
        keys_to_check = random.sample(keys_to_check, 50)
        
    for k in keys_to_check:
        # Skip the object currently being inserted if it's already in the dict 
        # (depends on framework order, safety check)
        if k == obj.key: 
            continue
            
        t = m_last_access.get(k, 0)
        if t < min_time:
            min_time = t
            candidate_key = k

    if candidate_key is None:
        return None

    # 2. Admission Control (The "Doorman")
    # Compare frequency of the incoming object vs the eviction candidate.
    candidate_freq = m_freq_sketch.get(candidate_key, 0)
    new_obj_freq = m_freq_sketch.get(obj.key, 0)
    
    # Rules:
    # A. If the candidate has high frequency (popular) and the new object has very low frequency (new/rare),
    #    we prefer to drop the new object to protect the popular one.
    #    (Only applies if candidate has been seen at least more than once).
    if candidate_freq > new_obj_freq and candidate_freq > 1:
        # We sacrifice the new object.
        # Note: We must return the key of the object to be evicted.
        return obj.key
    
    # B. Otherwise, evict the LRU candidate.
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (LRU).
    2. Increment Frequency Sketch (LFU).
    '''
    global m_last_access, m_freq_sketch
    
    current_time = cache_snapshot.access_count
    m_last_access[obj.key] = current_time
    
    # Increment frequency with a cap to prevent overflow or excessive dominance
    m_freq_sketch[obj.key] = m_freq_sketch.get(obj.key, 0) + 1
    if m_freq_sketch[obj.key] > 15: # Soft cap for small history window
        m_freq_sketch[obj.key] = 15
        
    _check_aging()

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize access time.
    2. Increment frequency (it was seen).
    '''
    global m_last_access, m_freq_sketch
    
    current_time = cache_snapshot.access_count
    m_last_access[obj.key] = current_time
    
    # It counts as a "view" even if it's new
    m_freq_sketch[obj.key] = m_freq_sketch.get(obj.key, 0) + 1
    
    _check_aging()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from LRU tracking.
    2. Keep in Frequency Sketch (Ghost), but ensure size limits.
    '''
    global m_last_access, m_freq_sketch
    
    key = evicted_obj.key
    
    if key in m_last_access:
        del m_last_access[key]
        
    # We intentionally DO NOT delete from m_freq_sketch immediately.
    # This allows the algorithm to remember "Oh, I saw this recently" 
    # if it comes back (Ghost Hits).
    
    # Clean up frequency sketch if it gets too big
    if len(m_freq_sketch) > MAX_HISTORY_KEYS:
        # Remove items not in cache to save space. 
        # If still too big, random evict.
        
        # Simple heuristic: remove keys with frequency 1 (noise) first
        keys_to_remove = [k for k, v in m_freq_sketch.items() if v <= 1 and k not in cache_snapshot.cache]
        
        # If no low freq items, just prune excess random keys not in cache
        if not keys_to_remove:
             keys_to_remove = [k for k in m_freq_sketch.keys() if k not in cache_snapshot.cache][:100]
             
        for k in keys_to_remove:
            del m_freq_sketch[k]

def _check_aging():
    '''
    Periodically divides all frequencies by 2.
    This simulates a sliding window, ensuring that 'old' popularity fades away 
    and allows new popular items to take over.
    '''
    global access_counter_since_aging, m_freq_sketch, AGING_INTERVAL
    
    access_counter_since_aging += 1
    
    if access_counter_since_aging >= AGING_INTERVAL:
        access_counter_since_aging = 0
        # Decay process: Divide all counts by 2
        # Use list(keys) to allow modification during iteration if necessary
        # filtering out 0s helps cleanup.
        keys_to_del = []
        for k, v in m_freq_sketch.items():
            new_val = v // 2
            if new_val == 0:
                keys_to_del.append(k)
            else:
                m_freq_sketch[k] = new_val
        
        for k in keys_to_del:
            del m_freq_sketch[k]
Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvp0iisrl.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsdu5613d.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgdduvf1y.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Iteration 79: New subsample score 0.0 is not better than old score 0.614625, skipping
Iteration 80: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp82xpvud0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdqoi1h7v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpisiv8c9t.pickle

Iteration 80: Proposed new text for program: from collections import OrderedDict

# Global Metadata for ARC (Adaptive Replacement Cache)
# T1: Recent Resident (Keys in cache, accessed once recently). 
#     OrderedDict: LRU at index 0 (iter start), MRU at end.
# T2: Frequent Resident (Keys in cache, accessed >= 2 times).
#     OrderedDict: LRU at index 0 (iter start), MRU at end.
# B1: Recent Ghost (Keys evicted from T1). Tracks recency history.
# B2: Frequent Ghost (Keys evicted from T2). Tracks frequency history.
# p:  Target size for T1 (Adaptation parameter).

m_t1 = OrderedDict()
m_t2 = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()
m_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Policy.
    Decides whether to evict from T1 (Recent) or T2 (Frequent) based on the target 'p'.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    # Safety check for empty cache (unlikely when full)
    if not m_t1 and not m_t2:
        return None

    # Logic to select victim:
    # We prefer to maintain 'p' items in T1.
    # However, if the incoming item 'obj' is in B2 (Ghost Frequent), it implies we
    # should have favored T2 more. In standard ARC, this relaxes the condition to evict T1.
    
    incoming_in_b2 = (obj.key in m_b2)
    
    # Standard ARC Replace Logic:
    # If len(T1) > p, we usually evict T1.
    # Exception/Extension: If incoming is in B2, we are even more aggressive against T1
    # (effectively treating len(T1) == p as "too full" for T1).
    
    evict_t1 = False
    if len(m_t1) > m_p:
        evict_t1 = True
    elif incoming_in_b2 and len(m_t1) == m_p:
        evict_t1 = True
        
    # Perform selection ensuring we don't pick from an empty list
    if evict_t1:
        if m_t1:
            return next(iter(m_t1)) # LRU of T1
        else:
            return next(iter(m_t2)) # Fallback to T2
    else:
        if m_t2:
            return next(iter(m_t2)) # LRU of T2
        else:
            return next(iter(m_t1)) # Fallback to T1

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    The item is "hot". Whether it was in T1 or T2, it moves to the MRU position of T2.
    '''
    global m_t1, m_t2
    key = obj.key
    
    if key in m_t1:
        # Promote Recent -> Frequent
        del m_t1[key]
        m_t2[key] = None
    elif key in m_t2:
        # Update Recency within Frequent
        m_t2.move_to_end(key)
    else:
        # Edge case: Hit but not in metadata (e.g., initial sync issue), treat as T2
        m_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Handles placement of new items into T1 (Cold Miss) or T2 (Ghost Hit).
    Adapts the parameter 'p' based on Ghost Hits.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    key = obj.key
    
    # Calculate current cache count (approximate capacity C)
    # We use current resident count + 1 (for the new item)
    c = len(m_t1) + len(m_t2) + 1
    
    if key in m_b1:
        # Hit in Ghost Recent (B1).
        # Meaning: We should have made T1 larger to keep this item.
        # Action: Increase p.
        delta = 1
        if len(m_b1) < len(m_b2):
            delta = len(m_b2) // len(m_b1)
            
        m_p = min(c, m_p + delta)
        
        # Promote to T2 (it has been seen twice now: once T1, now B1->Insert)
        del m_b1[key]
        m_t2[key] = None
        
    elif key in m_b2:
        # Hit in Ghost Frequent (B2).
        # Meaning: We should have made T2 larger (and T1 smaller).
        # Action: Decrease p.
        delta = 1
        if len(m_b2) < len(m_b1):
            delta = len(m_b1) // len(m_b2)
            
        m_p = max(0, m_p - delta)
        
        # Promote to T2
        del m_b2[key]
        m_t2[key] = None
        
    else:
        # Cold Miss. New item goes to T1 (Probation).
        m_t1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Move the metadata of the evicted item to the corresponding Ghost list (B1 or B2).
    Ensure Ghost lists do not grow indefinitely.
    '''
    global m_t1, m_t2, m_b1, m_b2
    key = evicted_obj.key
    
    # Move resident -> ghost
    if key in m_t1:
        del m_t1[key]
        m_b1[key] = None # Add to MRU of B1
    elif key in m_t2:
        del m_t2[key]
        m_b2[key] = None # Add to MRU of B2
        
    # Clean up Ghost Lists to prevents memory leaks.
    # ARC typically suggests keeping ghosts up to Size C or 2C.
    # Since capacity varies, we use current resident count as a proxy.
    max_ghost_size = max(50, len(cache_snapshot.cache) * 2)
    
    if len(m_b1) > max_ghost_size:
        m_b1.popitem(last=False) # Remove LRU
        
    if len(m_b2) > max_ghost_size:
        m_b2.popitem(last=False) # Remove LRU
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8zukymah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps4bo4b26.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoz1jnqhw.pickle

Iteration 80: New subsample score 0.5689970000000001 is not better than old score 0.597866, skipping
Iteration 81: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb5jc7mye.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt23gobwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_o3kgqse.pickle

Iteration 81: Proposed new text for program: from collections import OrderedDict

# ==============================================================================
# Metadata Storage
# ==============================================================================
# We divide the cache into two segments: Probation and Protected.
# 1. m_probation: OrderedDict (key -> size).
#    Holds new items or items demoted from Protected.
#    - Right (End): MRU (Most Recently Used)
#    - Left (Start): LRU (Least Recently Used) -> Candidates for eviction.
#
# 2. m_protected: OrderedDict (key -> size).
#    Holds items that have been accessed more than once (proven popular).
#    - Right (End): MRU
#    - Left (Start): LRU -> Candidates for demotion to Probation.
#
# 3. m_ghost: OrderedDict (key -> None).
#    Holds keys of items recently evicted from Probation.
#    Used to detect if a "new" item is actually a false-negative eviction.
#
# 4. m_prot_size, m_prob_size: int.
#    Track total bytes in each segment to enforce ratios.

m_probation = OrderedDict()
m_protected = OrderedDict()
m_ghost = OrderedDict()

m_prob_size = 0
m_prot_size = 0

# ==============================================================================
# Configuration
# ==============================================================================
# Protected segment target ratio. 80% allows a large working set to stay resident,
# while 20% probation filters out one-time scans.
PROTECTED_RATIO = 0.80 

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction.
    Strategy: SLRU
    1. Evict from Probation LRU (Left) first. This filters scans.
    2. If Probation is empty, evict from Protected LRU (Left).
    '''
    global m_probation, m_protected
    
    # 1. Prefer evicting from Probation (filter out cold/scan items)
    if m_probation:
        return next(iter(m_probation))
        
    # 2. Fallback: Evict from Protected if Probation is empty
    if m_protected:
        return next(iter(m_protected))

    # 3. Safety fallback (should rarely happen if metadata is synced)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    Logic:
    - If in Protected: Move to MRU.
    - If in Probation: Promote to Protected (it proved its worth).
    '''
    global m_probation, m_protected, m_prob_size, m_prot_size
    
    key = obj.key
    size = obj.size
    
    if key in m_protected:
        # Hit in Protected: Just refresh recency
        m_protected.move_to_end(key)
        
    elif key in m_probation:
        # Hit in Probation: Promote to Protected
        del m_probation[key]
        m_prob_size -= size
        
        m_protected[key] = size
        m_prot_size += size
        
        # Balance segments: If Protected is too big, demote LRU to Probation
        _balance_segments(cache_snapshot.capacity)

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    Logic:
    - If in Ghost: It was useful recently. Resurrect to Protected.
    - Else: Insert into Probation (New/Cold).
    '''
    global m_probation, m_protected, m_ghost, m_prob_size, m_prot_size
    
    key = obj.key
    size = obj.size
    
    if key in m_ghost:
        # Ghost Hit: The item was evicted recently but requested again.
        # This implies our cache was too small or it's a looping pattern.
        # Promote directly to Protected to keep it safe this time.
        del m_ghost[key]
        
        m_protected[key] = size
        m_prot_size += size
    else:
        # Standard Miss: Insert into Probation
        m_probation[key] = size
        m_prob_size += size
        
    # Ensure Protected segment doesn't hog all space
    _balance_segments(cache_snapshot.capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    Logic:
    - Remove from segments.
    - Add to Ghost list to track history.
    '''
    global m_probation, m_protected, m_ghost, m_prob_size, m_prot_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # Remove from Probation
    if key in m_probation:
        del m_probation[key]
        m_prob_size -= size
        # Add to Ghost (only track probation evictions usually, but tracking all is safer for traces)
        m_ghost[key] = None
        
    # Remove from Protected
    elif key in m_protected:
        del m_protected[key]
        m_prot_size -= size
        # Even if evicted from Protected (rare), track in Ghost
        m_ghost[key] = None
        
    # Ghost Management: Prevent unbounded growth of metadata
    # Limit ghost count to roughly the number of items in the cache
    max_ghost_items = len(m_protected) + len(m_probation) + 10 # Buffer
    if len(m_ghost) > max_ghost_items:
        m_ghost.popitem(last=False) # Remove oldest ghost

def _balance_segments(capacity):
    '''
    Maintains the ratio between Protected and Probation segments.
    If Protected grows too large (byte size), demote LRU items to Probation.
    '''
    global m_probation, m_protected, m_prob_size, m_prot_size
    
    target_prot_size = capacity * PROTECTED_RATIO
    
    # While Protected exceeds target, demote its LRU to Probation.
    # We move it to Probation MRU (give it a second chance in the filtered pool).
    while m_prot_size > target_prot_size and m_protected:
        # Pop LRU from Protected (Left)
        key, size = m_protected.popitem(last=False)
        m_prot_size -= size
        
        # Push to Probation MRU (Right)
        m_probation[key] = size
        m_prob_size += size
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn4066xs2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfr4zrsfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaiuaosmw.pickle

Iteration 81: New subsample score 0.321516 is better than old score 0.305286. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbf6xfi8j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg43ue30m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptghgwhaj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk98nbe5k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkc9t_ddl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkz5ypahr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzlcr2fi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp1sf67tl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph5y_8c7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfy7yhkfa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpillb9_jd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiqrdw_u_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyoh37_n4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0_5ssu7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpug5bcxpl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwebphlfy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplk1wafhr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpchulpsmg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqmlqz4gn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfnpj9jcp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp32_g3dsk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9v713ngo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcxtmm2p4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmlknyc3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpslxisf64.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppgizt6_q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpew_c378n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcprb7f7e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz6ib1eez.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprdiql369.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy2mo1g5p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc9gqk9g6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3wzscbar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqo9mzdtf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4x3hs3ir.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpti6oggsc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8mqgtzpp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpk5qo28j9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf81q8wx6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3i2bvrvf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpch5rd4_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4nu7cmyx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7isa6kh6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsh9pkdho.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpddouxeen.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf15q1ip2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnw9mbbw7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr1dmyfxh.pickle

Iteration 81: Full valset score for new program: 0.24984599999999993
Iteration 81: Full train_val score for new program: 0.24984599999999993
Iteration 81: Individual valset scores for new program: [0.498709, 0.470348, 0.480395, 0.433093, 0.494946, 0.480802, 0.272727, 0.498624, 0.539864, 0.531017, 0.083333, 0.332149, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.366765, 0.026164, 0.058672, 0.058672, 0.269798, 0.384073, 0.825223, 0.888242, 0.142258, 0.038636, 0.045558, 0.029555, 0.029983, 0.754595, 0.083333, 0.067961, 0.149856, 0.640392, 0.125461, 0.095925, 0.10507, 0.104279, 0.052632, 0.333333, 0.127316, 0.104348, 0.466258, 0.081699]
Iteration 81: New valset pareto front scores: [0.5113, 0.482372, 0.493182, 0.448119, 0.50361, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.142258, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.149856, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 81: Full valset pareto front score: 0.2710653958333333
Iteration 81: Updated valset pareto front programs: [{22}, {22}, {2}, {22}, {14}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23, 24}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 24}, {22}, {2, 4, 8, 9, 10, 17, 19}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {24}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21, 24}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21, 24}, {22}, {24}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22, 24}, {22}]
Iteration 81: Best valset aggregate score so far: 0.2627519583333333
Iteration 81: Best program as per aggregate score on train_val: 22
Iteration 81: Best program as per aggregate score on valset: 22
Iteration 81: Best score on valset: 0.2627519583333333
Iteration 81: Best score on train_val: 0.2627519583333333
Iteration 81: Linear pareto front program index: 22
Iteration 81: New program candidate index: 24
Iteration 82: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9g8mmhu1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptbarhkyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuuyuh8f9.pickle

Iteration 82: Proposed new text for program: from collections import OrderedDict

# =============================================================================
# Global Metadata Storage
# =============================================================================
# m_stack_s: OrderedDict (key -> None)
#   Represents the LIRS Stack S.
#   - Right (End): Most Recently Accessed.
#   - Left (Start): Least Recently Accessed (Bottom of stack).
#   - Contains LIR items and non-resident HIR items (history).
m_stack_s = OrderedDict()

# m_queue_q: OrderedDict (key -> None)
#   Represents the Resident HIR Queue Q.
#   - Right (End): Most Recently Accessed.
#   - Left (Start): Least Recently Accessed (Candidate for eviction).
#   - Contains only resident HIR items.
m_queue_q = OrderedDict()

# m_state: dict (key -> 'LIR' or 'HIR')
#   Tracks the status of resident items. 
#   If a key is in cache_snapshot but not here, it's an error state.
#   If a key is in m_stack_s but not here, it is a Non-Resident HIR.
m_state = dict()

# m_lir_size: int
#   Tracks the total size (in bytes) of all resident LIR items.
m_lir_size = 0

# =============================================================================
# Configuration
# =============================================================================
# We reserve 10% of the cache for HIR items. This provides a larger buffer
# for new items to prove their utility before being evicted.
# A 99% LIR ratio (as in the previous version) causes thrashing on scans.
LIR_TARGET_RATIO = 0.90

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction.
    1. Prefer evicting from the HIR Queue (Queue Q).
    2. If Q is empty, evict the LRU LIR item (Bottom of Stack S).
    '''
    global m_queue_q, m_stack_s

    # 1. Primary Eviction: Resident HIR (Front of Queue Q)
    if m_queue_q:
        return next(iter(m_queue_q))
        
    # 2. Secondary Eviction: Resident LIR (Bottom of Stack S)
    # Due to _prune_stack(), the bottom of S is guaranteed to be a resident LIR
    # if Q is empty (meaning all cached items are LIR).
    if m_stack_s:
        return next(iter(m_stack_s))

    # 3. Fail-safe (Should not happen in a correct LIRS implementation)
    # If both metadata structures are empty but cache is full.
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    # If for some reason state is missing, assume HIR to be safe
    state = m_state.get(key, 'HIR')
    
    if state == 'LIR':
        # --- LIR HIT ---
        # LIR items must be in Stack S. Move to top (Recency).
        if key in m_stack_s:
            m_stack_s.move_to_end(key)
        else:
            # Recovery for inconsistent state (shouldn't happen)
            m_stack_s[key] = None
        
        # If the LIR item was at the bottom, moving it up might expose
        # non-resident HIRs at the new bottom. Prune them.
        _prune_stack()
        
    elif state == 'HIR':
        # --- HIR HIT ---
        if key in m_stack_s:
            # Case 1: HIR Hit inside Stack S (Hot).
            # This indicates the item was referenced twice within the LIR reuse distance.
            # Promote HIR -> LIR.
            
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            
            # Remove from HIR Queue
            if key in m_queue_q:
                del m_queue_q[key]
            
            # Move to top of Stack S
            m_stack_s.move_to_end(key)
            
            # Promoting increases LIR size, potentially violating capacity.
            _enforce_lir_capacity(cache_snapshot)
        else:
            # Case 2: HIR Hit outside Stack S (Cold).
            # The item is resident but hasn't been seen "recently enough" in the stack history.
            # It remains HIR, but gets a fresh chance in Queue Q and Stack S.
            
            # Add to top of Stack S
            m_stack_s[key] = None
            
            # Move to MRU position in Queue Q
            if key in m_queue_q:
                m_queue_q.move_to_end(key)
            else:
                m_queue_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # 1. Check if key exists in Stack S (Non-Resident History Hit)
    if key in m_stack_s:
        # We evicted it recently, but it's back. It falls within the LIR stack distance.
        # Promote to LIR immediately.
        m_state[key] = 'LIR'
        m_lir_size += obj.size
        m_stack_s.move_to_end(key)
        
        # We might have exceeded LIR budget
        _enforce_lir_capacity(cache_snapshot)
        
    else:
        # 2. Pure Cold Miss
        # Heuristic: If the LIR set is very small (cache is filling up), 
        # mark new items as LIR to populate history quickly.
        # Otherwise, standard LIRS marks new items as HIR.
        
        if m_lir_size + obj.size <= capacity * LIR_TARGET_RATIO:
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            m_stack_s[key] = None
        else:
            m_state[key] = 'HIR'
            m_stack_s[key] = None # Add to Stack S (Recency)
            m_queue_q[key] = None # Add to Queue Q (Resident Candidate)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = evicted_obj.key
    
    # 1. Remove from Resident HIR Queue
    if key in m_queue_q:
        del m_queue_q[key]
        
    # 2. Update Resident State
    if key in m_state:
        if m_state[key] == 'LIR':
            m_lir_size -= evicted_obj.size
        del m_state[key]
        
    # 3. Stack S Maintenance
    # If the evicted item is in Stack S, it becomes a "Non-Resident HIR".
    # It STAYS in m_stack_s to provide history. 
    # However, if it was at the bottom of S, we prune.
    _prune_stack()

def _prune_stack():
    '''
    Removes items from the bottom of Stack S if they are not resident LIRs.
    This limits the stack size and ensures the bottom is always a hot LIR item.
    '''
    global m_stack_s, m_state
    
    while m_stack_s:
        bottom_key = next(iter(m_stack_s))
        
        # We stop pruning only if the bottom item is a Resident LIR.
        # Non-Resident items (not in m_state) and Resident HIRs (in m_state as HIR)
        # are removed from the bottom of the stack logic.
        if bottom_key in m_state and m_state[bottom_key] == 'LIR':
            break
            
        m_stack_s.popitem(last=False)

def _enforce_lir_capacity(cache_snapshot):
    '''
    Demotes LIR items to HIR if the LIR set exceeds its target size.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    target_capacity = cache_snapshot.capacity * LIR_TARGET_RATIO
    
    # While LIRs take up too much space and we have candidates to demote
    while m_lir_size > target_capacity and m_stack_s:
        # The candidate for demotion is the bottom of Stack S (LRU LIR)
        victim_key = next(iter(m_stack_s))
        
        # Sanity check: ensure it is actually LIR
        if m_state.get(victim_key) == 'LIR':
            # Demote LIR -> HIR
            m_state[victim_key] = 'HIR'
            
            # Reduce LIR size tracking
            # Note: We must fetch size from cache_snapshot because we don't have the object obj here
            if victim_key in cache_snapshot.cache:
                m_lir_size -= cache_snapshot.cache[victim_key].size
            
            # Move to the Most Recently Used end of Queue Q
            # (It was an LIR, so it's more valuable than a cold HIR)
            m_queue_q[victim_key] = None
            
        # After demotion (it becomes HIR) or if it was already HIR/Non-Resident,
        # the Stack Bottom is no longer a valid LIR. Prune it.
        _prune_stack()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmc79w9mu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkiq6_uzw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpibvz1y6b.pickle

Iteration 82: New subsample score 1.36139 is not better than old score 1.407943, skipping
Iteration 83: Selected program 15 score: 0.24230047916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_xb318lm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqrf2s4dq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgmywj_p0.pickle

Iteration 83: Proposed new text for program: import math

# Metadata storage
# Global dictionary to store the "H" value (priority) for each key.
# Priority H is calculated as: H = L + (Frequency / Size)
# Where L is an aging factor that increases over time as items are evicted.
m_priority = dict()

# We also track frequency to compute the H value.
m_frequency = dict()

# Global inflation factor 'L'.
# This acts as the "current time" of the priority system.
# When an item is evicted, L is updated to the evicted item's priority.
# This ensures that new items (which start with priority based on current L) 
# have a fighting chance against old, high-frequency items.
global_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GDSF-like (Greedy Dual Size Frequency)
    
    The goal is to minimize the cost of misses per byte of cache.
    We calculate a priority value H(p) for each object p:
        H(p) = L + (Frequency(p) / Size(p))
        
    Where L is a running aging factor.
    
    When we must evict, we remove the item with the LOWEST H(p).
    After eviction, we update L = H(evicted_item).
    
    This mechanism naturally handles:
    1. Size: Larger items have smaller (Freq/Size), lowering priority.
    2. Frequency: Frequently accessed items have higher priority.
    3. Recency/Aging: L increases over time. Old items that haven't been re-accessed
       retain their old H values, while L grows, effectively making them "colder" 
       relative to new items entering with the current L.
    '''
    global m_priority
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    
    # We also track secondary tie-breakers to ensure deterministic behavior.
    # If priorities are identical, we prefer evicting the one with the lowest frequency,
    # and then the largest size (to clear space).
    
    # Note: In a production system, a Min-Heap would be used to find the victim in O(1).
    # Here we iterate O(N), which is acceptable for simulation constraints.
    
    for key in current_keys:
        # Get stored priority. If for some reason it's missing, calculate a default.
        # Default fallback: strictly based on current global L
        p = m_priority.get(key, 0.0)
        
        if p < min_priority:
            min_priority = p
            victim_key = key
        elif p == min_priority:
            # Tie-breaker logic
            # 1. Break ties by evicting lower frequency (less valuable history)
            curr_freq = m_frequency.get(key, 1)
            best_freq = m_frequency.get(victim_key, 1)
            
            if curr_freq < best_freq:
                victim_key = key
            elif curr_freq == best_freq:
                # 2. Break ties by evicting larger size (freed more space)
                curr_size = cache_snapshot.cache[key].size
                best_size = cache_snapshot.cache[victim_key].size
                if curr_size > best_size:
                    victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Re-calculate Priority H.
    3. Restore 'L' into the priority (Aging reset).
    
    Formula: H(p) = L + (Frequency(p) / Size(p))
    '''
    global m_priority, m_frequency, global_L
    
    # 1. Update Frequency
    m_frequency[obj.key] = m_frequency.get(obj.key, 0) + 1
    
    # 2. Update Priority
    # We use the current global_L. This effectively "brings the item to the present".
    # Items not accessed recently will have H values based on older (smaller) L values,
    # making them candidates for eviction.
    freq = m_frequency[obj.key]
    size = obj.size # Using the object's size directly
    
    # Avoid division by zero
    if size <= 0: size = 1
        
    m_priority[obj.key] = global_L + (freq / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency (usually 1).
    2. Calculate Initial Priority.
    '''
    global m_priority, m_frequency, global_L
    
    # 1. Init Frequency
    m_frequency[obj.key] = 1
    
    # 2. Calculate Priority
    # New items enter with the current base inflation value L.
    size = obj.size
    if size <= 0: size = 1
    
    m_priority[obj.key] = global_L + (1.0 / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Update the global aging factor L.
       L becomes the priority of the item we just evicted.
       This ensures that subsequent items must have a value > old_L to survive.
    2. Clean up metadata.
    '''
    global m_priority, m_frequency, global_L
    
    key = evicted_obj.key
    
    # 1. Update Global L
    if key in m_priority:
        # The inflation factor L ratchets up to the priority of the evicted item.
        # This is the core mechanism of GDSF/GDS.
        global_L = m_priority[key]
        
        # Cleanup
        del m_priority[key]
    
    if key in m_frequency:
        del m_frequency[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpku2eu435.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvzl8nux8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0bffwtg1.pickle

Iteration 83: New subsample score 0.561111 is not better than old score 0.569194, skipping
Iteration 84: Selected program 10 score: 0.23939320833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp64ls3fa9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkyhhyr6t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprdkt85f2.pickle

Iteration 84: Proposed new text for program: import math

# Metadata storage
# 1. key -> integer frequency (The number of times accessed)
# 2. key -> integer priority (The calculated eviction priority score)
# 3. key -> integer last_access (Logical timestamp for tie-breaking)
# 4. ghosts -> set (Keys recently evicted, used to detect working sets larger than cache)

m_frequency = dict()
m_priority = dict()
m_last_access = dict()
m_ghosts = set()

# Optimization parameters
MAX_GHOST_RATIO = 2.0  # Allow ghost list to be up to 2x cache capacity (heuristic)
INFLATION_L = 0.0      # The "inflation" value, modeled after GDSF 'L' value.
                       # This acts as the minimum priority in the cache, causing aging.

def get_priority(freq, access_time, L):
    '''
    Calculates the Greedy Dual-Size Frequency (GDSF) priority.
    Priority = L + Frequency
    
    In standard GDSF, cost/size is often used. Here, since we want to maximize 
    Hit Rate specifically, we focus purely on Frequency + Aging (L).
    
    The 'L' factor ensures that new items (low freq) eventually gain higher priority 
    than old items (high freq) if the old items are never accessed again, because 
    L rises over time as items are evicted.
    '''
    return L + freq

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: GDSF-like (Greedy Dual-Size Frequency).
    
    We evict the item with the LOWEST Priority value.
    If priorities are equal, we evict the one with the oldest Last Access Time (LRU tie-breaker).
    
    Crucially, when we evict an item, we update the global inflation parameter 'L'
    to the priority of the evicted item. This effectively "ages" all other items
    in the cache relatively.
    '''
    global INFLATION_L
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    # We want to minimize (Priority, Last_Access_Time)
    # Initialize min_score with infinity
    min_priority = float('inf')
    min_time = float('inf')
    
    for key in current_keys:
        p = m_priority.get(key, 0.0)
        t = m_last_access.get(key, 0)
        
        # Check if this item is a better victim candidate
        if p < min_priority:
            min_priority = p
            min_time = t
            victim_key = key
        elif p == min_priority:
            # Tie-breaker: LRU
            if t < min_time:
                min_time = t
                victim_key = key

    # GDSF Logic: The system 'ages' by raising the floor (L) to the evicted item's priority.
    if victim_key is not None:
        INFLATION_L = m_priority.get(victim_key, 0.0)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency.
    3. Re-calculate Priority based on current L and new Frequency.
    '''
    global m_last_access, m_frequency, m_priority, INFLATION_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Access Time
    m_last_access[key] = current_time
    
    # Update Frequency
    current_freq = m_frequency.get(key, 0) + 1
    m_frequency[key] = current_freq
    
    # Update Priority
    # Note: On a hit, we reset the priority base to the current global L.
    # This brings the item "forward" in time.
    m_priority[key] = get_priority(current_freq, current_time, INFLATION_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if it is a Ghost (recently evicted).
    2. Determine initial frequency (1 for new, restored for ghosts).
    3. Calculate initial Priority.
    '''
    global m_last_access, m_frequency, m_priority, m_ghosts, INFLATION_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_last_access[key] = current_time
    
    if key in m_ghosts:
        # Ghost Hit: This item was recently needed but evicted. 
        # It implies our cache is too small or the item is part of a cyclic working set.
        # We restore its previous frequency (or a base ghost freq) to give it resistance.
        # We remove it from ghosts.
        freq = m_frequency.get(key, 1) # Retrieve old freq if still tracked, else 1
        # Boost it slightly so it doesn't immediately die again
        freq = max(freq, 2) 
        m_ghosts.remove(key)
    else:
        # Brand new item. Start with frequency 1.
        freq = 1
        
    m_frequency[key] = freq
    m_priority[key] = get_priority(freq, current_time, INFLATION_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata for the evicted key.
    2. Add to Ghost list to track "misses on recently evicted".
    3. Manage Ghost list capacity.
    '''
    global m_priority, m_ghosts, m_last_access, m_frequency
    
    key = evicted_obj.key
    
    # We maintain frequency in m_frequency even after eviction for a while (metadata overhead is usually low vs object size),
    # but we MUST remove priority and access time to ensure clean state if re-inserted.
    if key in m_priority:
        del m_priority[key]
    
    # We DO NOT delete from m_frequency immediately, so that if it returns as a ghost,
    # we remember it was popular. However, we must bound this memory eventually.
    
    if key in m_last_access:
        del m_last_access[key]

    m_ghosts.add(key)
    
    # Cleanup Ghosts and Frequency map to prevent memory leaks
    # Heuristic: Keep ghosts proportional to the number of items the cache CAN hold.
    # Since we don't know exact object count capacity, we use a loose bound or len(cache).
    # If the cache is very small (few large items), we still want a decent history.
    
    current_cache_len = len(cache_snapshot.cache)
    target_ghost_len = max(current_cache_len * MAX_GHOST_RATIO, 100)
    
    if len(m_ghosts) > target_ghost_len:
        # Remove an arbitrary ghost (set pop is arbitrary)
        # In a perfect world we'd use a FIFO queue for ghosts, but sets are O(1) lookup.
        # We sacrifice perfect LRU eviction of ghosts for speed.
        popped = m_ghosts.pop()
        
        # Once it leaves the ghost list, we finally forget its frequency
        if popped in m_frequency:
            del m_frequency[popped]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpca6aq4cw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2qu988r4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoltmk2kz.pickle

Iteration 84: New subsample score 0.8163259999999999 is not better than old score 0.8575490000000001, skipping
Iteration 85: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvciiniyw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuizgd7zp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8cjt2i1q.pickle

Iteration 85: Proposed new text for program: from collections import OrderedDict

# Metadata Storage
# 1. m_new: OrderedDict (key -> None).
#    FIFO Queue for new items. Acts as a filter for scans.
#    Items here are candidates for early eviction.
# 2. m_main: OrderedDict (key -> None).
#    LRU Queue for hot items (frequency > 1 or re-referenced recently).
# 3. m_ghost: OrderedDict (key -> None).
#    FIFO History of keys recently evicted.
# 4. m_size_new: int. Total size (bytes) of items in m_new.
# 5. m_size_main: int. Total size (bytes) of items in m_main.

m_new = OrderedDict()
m_main = OrderedDict()
m_ghost = OrderedDict()

m_size_new = 0
m_size_main = 0

# Configuration
# Reserve ~25% of cache capacity for the "New" queue to absorb scans.
# The remaining 75% is dedicated to the "Main" working set.
NEW_SECTION_RATIO = 0.25

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using a Size-Aware 2Q strategy.
    '''
    global m_new, m_main, m_size_new
    
    # Calculate the byte limit for the 'New' section
    target_new_capacity = cache_snapshot.capacity * NEW_SECTION_RATIO
    
    # Strategy:
    # 1. If the 'New' section is larger than its allocated ratio, evict from 'New'.
    #    This prevents scans or large cold objects from polluting the whole cache.
    if m_new and m_size_new > target_new_capacity:
        return next(iter(m_new))
        
    # 2. If 'New' is within budget, we generally prefer to evict from 'Main' (LRU)
    #    to make space for new items, ensuring flow.
    if m_main:
        return next(iter(m_main))
        
    # 3. Fallback: If 'Main' is empty (rare), evict from 'New'.
    if m_new:
        return next(iter(m_new))

    # 4. Last resort safety
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    Promotes items from New -> Main or updates LRU in Main.
    '''
    global m_new, m_main, m_size_new, m_size_main
    
    key = obj.key
    
    if key in m_main:
        # Hit in Main: It's a hot item. Move to MRU (Right).
        m_main.move_to_end(key)
        
    elif key in m_new:
        # Hit in New: The item has proven it's not a one-time scan.
        # Promote to Main.
        del m_new[key]
        m_size_new -= obj.size
        
        m_main[key] = None
        m_size_main += obj.size
        # No need to move_to_end, insertion makes it MRU.

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    '''
    global m_new, m_main, m_ghost, m_size_new, m_size_main
    
    key = obj.key
    
    # Check if this key is in our Ghost history
    if key in m_ghost:
        # Ghost Hit: We evicted this recently, and now it's back.
        # This implies it's part of a loop or a larger working set.
        # Promote directly to Main.
        del m_ghost[key]
        m_main[key] = None
        m_size_main += obj.size
    else:
        # Cold Insert: Treat as new. Add to New queue.
        m_new[key] = None
        m_size_new += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup and Ghost history updates.
    '''
    global m_new, m_main, m_ghost, m_size_new, m_size_main
    
    key = evicted_obj.key
    
    # 1. Update Resident Queues and Sizes
    if key in m_new:
        del m_new[key]
        m_size_new -= evicted_obj.size
    elif key in m_main:
        del m_main[key]
        m_size_main -= evicted_obj.size
        
    # 2. Add to Ghost History
    # Both New and Main evictions go to Ghost to catch "return soon" items.
    m_ghost[key] = None
    
    # 3. Manage Ghost Size
    # If Ghost grows too large, prune it. 
    # A limit proportional to the resident count (e.g., 2x) allows catching loops 
    # that are larger than the cache.
    resident_count = len(m_new) + len(m_main)
    # Ensure a minimum buffer (e.g., 100) to handle startup or sparse caches.
    max_ghost_size = max(resident_count * 2, 100)
    
    if len(m_ghost) > max_ghost_size:
        m_ghost.popitem(last=False) # Remove oldest ghost
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbw1yrugl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdumfozh8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpchz2z1cu.pickle

Iteration 85: New subsample score 1.033083 is not better than old score 1.068546, skipping
Iteration 86: Selected program 14 score: 0.24068325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkxyie1xl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpydpmmxoh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpseheu9sk.pickle

Iteration 86: Proposed new text for program: # Metadata storage
# meta_freq: Dictionary mapping object keys to their access frequency.
# meta_last_access: Dictionary mapping object keys to the logical time they were last accessed (for LRU tie-breaking).
# gdsf_L: The global 'aging' factor for the Greedy Dual-Size Frequency algorithm.
meta_freq = {}
meta_last_access = {}
gdsf_L = 0.0

def evict(cache_snapshot, obj):
    '''
    GDSF (Greedy Dual-Size Frequency) Eviction Strategy.
    
    We identify the victim with the lowest Priority Score:
        Score = L + (Frequency / Size)
        
    - L: Global aging factor (Priority of the last evicted object).
    - Frequency: Number of times the object has been hit.
    - Size: Size of the object in bytes.
    
    Rationale:
    - Higher Frequency -> Keep (High Score)
    - Smaller Size -> Keep (High Score, because 1/Size is larger). Keeping small objects increases 
      the density of objects in the cache, statistically increasing the hit rate.
    
    Tie-breaking:
    - If scores are equal, we evict the Least Recently Used (LRU) object.
    '''
    global meta_freq, meta_last_access, gdsf_L
    
    victim_key = None
    min_priority = float('inf')
    
    # We iterate over the current cache to find the object with the minimum GDSF score.
    # While iterating, we also handle the LRU tie-breaker logic.
    for key, cached_obj in cache_snapshot.cache.items():
        # Retrieve metadata, defaulting to safe values if not found
        freq = meta_freq.get(key, 1)
        size = cached_obj.size
        last_access = meta_last_access.get(key, 0)
        
        # Calculate GDSF Priority
        # P = L + (Frequency / Size)
        # Note: Size is guaranteed positive by constraints.
        priority = gdsf_L + (freq / size)
        
        # Check if this object is the new minimum candidate
        if priority < min_priority:
            min_priority = priority
            victim_key = key
            victim_last_access = last_access
        elif priority == min_priority:
            # Tie-breaker: If priorities are identical, evict the older one (LRU)
            if last_access < victim_last_access:
                victim_key = key
                victim_last_access = last_access
                
    # Update the global aging factor L.
    # In GDSF, L advances to the priority value of the evicted object.
    # This prevents the cache from stagnating by ensuring new items must eventually 
    # exceed this threshold (via repeated hits) to persist.
    if min_priority != float('inf'):
        gdsf_L = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    1. Increment the frequency counter (increasing its GDSF score).
    2. Update the last access time (for LRU tie-breaking).
    '''
    global meta_freq, meta_last_access
    
    key = obj.key
    meta_freq[key] = meta_freq.get(key, 0) + 1
    meta_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Insert (Miss):
    1. Initialize frequency to 1.
    2. Set access time.
    '''
    global meta_freq, meta_last_access
    
    key = obj.key
    meta_freq[key] = 1
    meta_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up metadata to prevent memory leaks.
    Note: In GDSF, we typically do not keep history of evicted items in the frequency map
    because the 'aging factor' L implicitly handles the history/value threshold.
    Resetting frequency prevents 'ghost' popularity from polluting decisions if access patterns change.
    '''
    global meta_freq, meta_last_access
    
    evicted_key = evicted_obj.key
    
    if evicted_key in meta_freq:
        del meta_freq[evicted_key]
    if evicted_key in meta_last_access:
        del meta_last_access[evicted_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0xve_p66.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_ppe0abb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy0xzu4qh.pickle

Iteration 86: New subsample score 0.561264 is not better than old score 0.58707, skipping
Iteration 87: Selected program 24 score: 0.24984599999999993
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbt52zzs0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbf91cgv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9mi6x1ja.pickle

Iteration 87: Proposed new text for program: from collections import OrderedDict

# ==============================================================================
# Metadata Storage: Adaptive Replacement Cache (ARC) Strategy
# ==============================================================================
# We maintain four lists (OrderedDicts) to track cache state and history.
# T1: Recent Cache (Probation) - Objects accessed once recently.
# T2: Frequent Cache (Protected) - Objects accessed at least twice.
# B1: Ghost Recent - Keys evicted from T1 (Metadata only).
# B2: Ghost Frequent - Keys evicted from T2 (Metadata only).
#
# p: Target size for T1. 
#    - If p increases, we favor Recency (T1).
#    - If p decreases, we favor Frequency (T2).

t1 = OrderedDict()  # Key -> Size
t2 = OrderedDict()  # Key -> Size
b1 = OrderedDict()  # Key -> Size (Ghost)
b2 = OrderedDict()  # Key -> Size (Ghost)

p = 0  # Adaptive parameter (target size for T1 in bytes)

current_t1_size = 0
current_t2_size = 0

def replace(cache_snapshot, obj_key):
    '''
    Subroutine to evict an item to make space.
    Decides whether to evict from T1 or T2 based on the adaptive parameter 'p'.
    '''
    global t1, t2, b1, b2, current_t1_size, current_t2_size, p

    # If T1 is not empty and its size exceeds the target 'p', 
    # OR if T1 is holding the specific item we want to evict (implied by context),
    # OR if T2 is empty (must evict T1).
    # Logic: If T1 is too big relative to p, evict from T1.
    
    # Condition to evict from T1 (LRU end):
    # We evict from T1 if T1 has items AND (T1 size > p OR (T1 hit B2 logic implies we need space in T2 but T1 is the only source)).
    # Simplified ARC logic for variable size:
    
    if t1 and ( (current_t1_size > p) or (obj_key in b2 and current_t1_size == p) ):
        # Evict from T1 LRU (pop first)
        key, size = t1.popitem(last=False)
        current_t1_size -= size
        
        # Add to B1 (Ghost Recent)
        b1[key] = size
        return key
    else:
        # Evict from T2 LRU (pop first)
        # Fallback to T1 if T2 is empty
        if t2:
            key, size = t2.popitem(last=False)
            current_t2_size -= size
            
            # Add to B2 (Ghost Frequent)
            b2[key] = size
            return key
        elif t1:
             # Safety fallback: if logic demanded T2 eviction but T2 is empty
            key, size = t1.popitem(last=False)
            current_t1_size -= size
            b1[key] = size
            return key
            
    return None

def evict(cache_snapshot, obj):
    '''
    Main eviction entry point.
    Delegates to the ARC 'replace' logic, but verifies the key exists in cache.
    '''
    # We simulate the replace logic to find the victim key.
    # The 'replace' function usually modifies metadata, but here we just need to identify
    # the victim. However, in this framework, 'evict' is called immediately before 
    # 'update_after_evict', so we can rely on our metadata state.
    
    # Warning: The standard ARC 'replace' is usually called inside the insert loop.
    # Since we have a separated 'evict' function, we must peek at what 'replace' WOULD do.
    
    global t1, t2, p, current_t1_size, b2
    
    # Logic: Mirror the conditions in update_after_insert/replace
    # If T1 is "too fat" (size > p), we kill T1's LRU.
    # Otherwise, we kill T2's LRU.
    
    # Specific edge case handling for B2 hits is tricky here without knowing if it's a B2 hit,
    # but generally:
    if t1 and (current_t1_size > p):
        return next(iter(t1))
    
    # If T1 is fine, we check T2
    if t2:
        return next(iter(t2))
        
    # If T2 is empty, we must evict T1
    if t1:
        return next(iter(t1))
        
    # Should not happen if cache is full
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Object found in cache (in T1 or T2).
    '''
    global t1, t2, current_t1_size, current_t2_size
    
    key = obj.key
    size = obj.size
    
    # Case 1: Hit in T1 (Recent) -> Move to T2 (Frequent)
    if key in t1:
        del t1[key]
        current_t1_size -= size
        
        t2[key] = size
        current_t2_size += size
        
    # Case 2: Hit in T2 (Frequent) -> Move to MRU of T2
    elif key in t2:
        t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    Object not in cache. Could be a new insert, or a ghost hit.
    '''
    global t1, t2, b1, b2, p, current_t1_size, current_t2_size
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity

    # -----------------------------
    # Case 1: Hit in B1 (Ghost Recent)
    # -----------------------------
    if key in b1:
        # Adaptation: We evicted from T1 too soon. Increase p (target T1 size).
        # Delta calculation: usually 1, but we can weigh by size ratios
        delta = 1
        if len(b1) >= len(b2) and len(b1) > 0:
             delta = 1
        elif len(b2) > 0:
             delta = len(b2) / len(b1)
             
        # Increment p, capped at capacity
        p = min(capacity, p + delta * size) # Weight by size for byte-addressed cache
        
        # Remove from ghost
        del b1[key]
        
        # Insert into T2 (Frequent) because it was seen recently (in T1) and now again
        t2[key] = size
        current_t2_size += size

    # -----------------------------
    # Case 2: Hit in B2 (Ghost Frequent)
    # -----------------------------
    elif key in b2:
        # Adaptation: We evicted from T2 too soon. Decrease p (target T1 size).
        delta = 1
        if len(b2) >= len(b1) and len(b2) > 0:
            delta = 1
        elif len(b1) > 0:
            delta = len(b1) / len(b2)
            
        # Decrement p, floored at 0
        p = max(0, p - delta * size)
        
        # Remove from ghost
        del b2[key]
        
        # Insert into T2 (Frequent) - it was frequent before eviction, still frequent
        t2[key] = size
        current_t2_size += size

    # -----------------------------
    # Case 3: Completely New Item
    # -----------------------------
    else:
        # Insert into T1 (Recent)
        t1[key] = size
        current_t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata after the system performs the actual eviction.
    We need to ensure our T1/T2 lists match the reality of the cache.
    '''
    global t1, t2, b1, b2, current_t1_size, current_t2_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # The 'evict' function identified the victim, and the system removed it.
    # Now we move it to Ghost lists.
    
    if key in t1:
        del t1[key]
        current_t1_size -= size
        b1[key] = size # Add to Ghost Recent
        # Enforce ghost limits (FIFO for ghosts)
        # Heuristic: Keep ghost size roughly bounded by capacity or count
        if len(b1) > cache_snapshot.capacity / (size + 1) * 2: # Loose bound
             b1.popitem(last=False)

    elif key in t2:
        del t2[key]
        current_t2_size -= size
        b2[key] = size # Add to Ghost Frequent
        if len(b2) > cache_snapshot.capacity / (size + 1) * 2:
             b2.popitem(last=False)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps7wworl4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7rwrgkvo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaidv4uz_.pickle

Iteration 87: New subsample score 0.7905629999999999 is better than old score 0.7716259999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvi21binn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc68wo9yg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp4b0hkdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2c9qlxqr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxj3m_7gk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcv4g8oys.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkbmv9wdy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1ne9fkor.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp25y8hiy6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4tss6h55.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyb_ix0ik.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu5vzcvlp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf45w4dcy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxrw5isk7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaecufz0r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2mopzff4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpakfdnfey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0p2a24bk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph57hq3q4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5n3vehrq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxn2cbadu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv4vdpz92.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwcfruy76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxwv3a7r6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpit3wd2n2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkj6yd9ma.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp63satprx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw7cqapr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdls4ex61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxjslwxzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpis7ll6rp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_mzfkk8f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphev01bzm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptvz2oe1m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzkgsz9ri.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmponv35deg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuyjta82b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzvm49ku1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq5h5i1gh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjkamosnd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwjejdhrc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf28db9gf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphgv4aqtv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpscqp7pfz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf5uc9j6_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuigpvk26.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpondhn22s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp87pcklvf.pickle

Iteration 87: Full valset score for new program: 0.24445077083333333
Iteration 87: Full train_val score for new program: 0.24445077083333333
Iteration 87: Individual valset scores for new program: [0.507856, 0.483189, 0.491182, 0.441451, 0.505643, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.334813, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.269811, 0.372984, 0.849057, 0.891494, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.077904, 0.124007, 0.054071, 0.052632, 0.3, 0.09278, 0.060028, 0.466258, 0.081699]
Iteration 87: New valset pareto front scores: [0.5113, 0.483189, 0.493182, 0.448119, 0.505643, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.142258, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.149856, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 87: Full valset pareto front score: 0.2711247708333333
Iteration 87: Updated valset pareto front programs: [{22}, {25}, {2}, {22}, {25}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23, 24}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 24, 25}, {22}, {2, 4, 8, 9, 10, 17, 19}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {24}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21, 24, 25}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21, 24, 25}, {22}, {24}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22, 24, 25}, {22}]
Iteration 87: Best valset aggregate score so far: 0.2627519583333333
Iteration 87: Best program as per aggregate score on train_val: 22
Iteration 87: Best program as per aggregate score on valset: 22
Iteration 87: Best score on valset: 0.2627519583333333
Iteration 87: Best score on train_val: 0.2627519583333333
Iteration 87: Linear pareto front program index: 22
Iteration 87: New program candidate index: 25
Iteration 88: Selected program 24 score: 0.24984599999999993
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_npnwi7z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfrt2_yx2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0x_3ceb5.pickle

Iteration 88: Proposed new text for program: from collections import OrderedDict

# ==============================================================================
# Adaptive Replacement Cache (ARC) - Byte Aware Implementation
# ==============================================================================
# We implement the ARC algorithm logic adapted for byte-sized caches.
# ARC dynamically tunes the balance between "Recent" (T1) and "Frequent" (T2)
# based on hits in their respective "Ghost" lists (B1 and B2).
#
# Segments:
# t1: Recent (Probation) - OrderedDict (key -> size). New items enter here.
# t2: Frequent (Protected) - OrderedDict (key -> size). Popular items live here.
# b1: Ghost Recent - OrderedDict (key -> None). History of evictions from t1.
# b2: Ghost Frequent - OrderedDict (key -> None). History of evictions from t2.

t1 = OrderedDict()
t2 = OrderedDict()
b1 = OrderedDict()
b2 = OrderedDict()

# Track byte sizes of resident segments
t1_size = 0
t2_size = 0

# The adaptation parameter 'p' represents the target size (in bytes) for T1.
# It shifts dynamically:
# - Increases when we get a hit in b1 (implies we needed a larger t1).
# - Decreases when we get a hit in b2 (implies we needed a larger t2).
# 0 <= p <= capacity
p = 0

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction based on ARC logic.
    We compare the current size of T1 against the target 'p'.
    '''
    global t1, t2, p, t1_size
    
    # Strategy:
    # 1. If T1 (Recent/Probation) is larger than its target 'p', we evict from T1 
    #    to move towards the target balance.
    #    Note: We also prioritize evicting T1 if T2 is empty (can't evict from nothing).
    if t1 and ((t1_size > p) or not t2):
        return next(iter(t1)) # Evict LRU from T1 (Left side of OrderedDict)
    
    # 2. Otherwise, evict from T2 (Frequent/Protected).
    if t2:
        return next(iter(t2)) # Evict LRU from T2
        
    # 3. Fallback (Safety mechanism, though should not be reached in normal full state)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    Logic:
    - If in T1: It's proved its worth. Move to T2 (Promote).
    - If in T2: It's still popular. Move to MRU (Refresh).
    '''
    global t1, t2, t1_size, t2_size
    
    key = obj.key
    size = obj.size
    
    if key in t1:
        # Promote from T1 (Probation) to T2 (Protected)
        del t1[key]
        t1_size -= size
        
        t2[key] = size
        t2_size += size
        t2.move_to_end(key)
        
    elif key in t2:
        # Refresh in T2 (Protected)
        t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    Logic:
    - Check Ghost lists (B1/B2) to trigger adaptation of 'p'.
    - If Ghost Hit: Resurrect to T2.
    - If Cold Miss: Insert into T1.
    '''
    global t1, t2, b1, b2, p, t1_size, t2_size
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # Case 1: Hit in B1 (Ghost Recent)
    # This item was in T1 recently but we evicted it too soon.
    # Conclusion: T1 is too small. Increase p (target for T1).
    if key in b1:
        # Adaptation magnitude: By the size of the object
        delta = size
        p = min(capacity, p + delta)
        
        del b1[key]
        
        # Resurrect directly to T2 (Frequent) because it has been seen twice (once recently)
        t2[key] = size
        t2_size += size
        
    # Case 2: Hit in B2 (Ghost Frequent)
    # This item was in T2 recently but we evicted it.
    # Conclusion: T2 is too small (implies T1 is too big). Decrease p.
    elif key in b2:
        delta = size
        p = max(0, p - delta)
        
        del b2[key]
        
        # Resurrect to T2
        t2[key] = size
        t2_size += size
        
    # Case 3: Totally Cold Miss
    # Never seen recently. Insert into T1 (Recent/Probation).
    else:
        t1[key] = size
        t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    Logic:
    - Remove from the segment it belonged to (T1 or T2).
    - Add key to the corresponding Ghost list (B1 or B2).
    - Trim Ghost lists to prevent unbounded memory usage.
    '''
    global t1, t2, b1, b2, t1_size, t2_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # 1. Remove from Resident Lists and add to Ghosts
    if key in t1:
        del t1[key]
        t1_size -= size
        b1[key] = None # Add to Ghost Recent
        
    elif key in t2:
        del t2[key]
        t2_size -= size
        b2[key] = None # Add to Ghost Frequent

    # 2. Ghost Management (Capacity Bound)
    # We restrict the ghost history to roughly 2x the count of items in the cache.
    # This ensures O(N) space complexity relative to cache size.
    max_ghosts = (len(t1) + len(t2)) * 2 + 10 # +10 buffer for empty starts
    
    if len(b1) > max_ghosts:
        b1.popitem(last=False) # Remove oldest ghost from B1
        
    if len(b2) > max_ghosts:
        b2.popitem(last=False) # Remove oldest ghost from B2
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph4e_b38c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8k55ud9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1li06pel.pickle

Iteration 88: New subsample score 0.691276 is better than old score 0.6818690000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplztnfvxu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpij8ou4d1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpycyj459k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppc9nsbxy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxjgzmyzk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg135z0he.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbbpfclbv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe9_v4_l3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp89n33inb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpflavv1mi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdxf7zg60.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp07zhhpyx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpm4p5l4xy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcsctyc03.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsz0jff3k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph4fseemo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbwga8wdl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphmew90v8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkzx3jl9d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpif69ma1a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpcpmkkqi3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpivd81tlj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp3xe4u635.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzr01ls0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1xn5b3hs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptgfjnxxr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1_pvs7yr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyegsk0jh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpi5w5okl3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsu5dzaxx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv6l0cgn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpihnrprwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyokgvrli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwkew599q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsf4a18v4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo0qrcc96.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_ncsp4_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppgcptzpk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgmlvsdbg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplq7sjwuo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp__mi8v5m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6vt4n3zc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxad90b19.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpis6wmusy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvexvas8e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptmo2jnul.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw3t_wfa9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp4nd_0xlx.pickle

Iteration 88: Full valset score for new program: 0.23405064583333332
Iteration 88: Full train_val score for new program: 0.23405064583333332
Iteration 88: Individual valset scores for new program: [0.49591, 0.46813, 0.480516, 0.428203, 0.489918, 0.476882, 0.270335, 0.498034, 0.540937, 0.531017, 0.091667, 0.326377, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.352016, 0.026556, 0.058672, 0.058672, 0.269807, 0.356855, 0.849057, 0.891152, 0.020964, 0.038636, 0.045558, 0.039527, 0.040325, 0.721409, 0.083333, 0.067961, 0.010922, 0.640392, 0.125461, 0.030773, 0.031765, 0.032288, 0.052632, 0.266667, 0.022947, 0.0331, 0.466258, 0.081699]
Iteration 88: New valset pareto front scores: [0.5113, 0.483189, 0.493182, 0.448119, 0.505643, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.142258, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.149856, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 88: Full valset pareto front score: 0.2711247708333333
Iteration 88: Updated valset pareto front programs: [{22}, {25}, {2}, {22}, {25}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23, 24}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 24, 25, 26}, {22}, {2, 4, 8, 9, 10, 17, 19, 26}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {24}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21, 24, 25, 26}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21, 24, 25, 26}, {22}, {24}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22, 24, 25, 26}, {22}]
Iteration 88: Best valset aggregate score so far: 0.2627519583333333
Iteration 88: Best program as per aggregate score on train_val: 22
Iteration 88: Best program as per aggregate score on valset: 22
Iteration 88: Best score on valset: 0.2627519583333333
Iteration 88: Best score on train_val: 0.2627519583333333
Iteration 88: Linear pareto front program index: 22
Iteration 88: New program candidate index: 26
Iteration 89: Selected program 2 score: 0.23265512500000005
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy54ty_n3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpidj5fdwa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwak3okae.pickle

Iteration 89: Proposed new text for program: # Metadata storage
# 1. m_last_access: key -> access_count (Recency tracking)
# 2. m_frequency: key -> int (Frequency tracking to distinguish probationary vs protected)
# 3. m_protected_set: key -> bool (Explicit membership set for the protected segment)

m_last_access = dict()
m_frequency = dict()
m_protected_set = set()

def evict(cache_snapshot, obj):
    '''
    Segmented LRU (SLRU) Eviction Strategy.
    
    Concept:
    The cache is logically divided into two segments:
    1. Probationary Segment: Items accessed only once recently. These are candidates for quick eviction.
    2. Protected Segment: Items accessed at least twice. These are kept longer.
    
    Eviction Logic:
    1. Preferentially evict from the Probationary Segment (LRU within this group).
       - This filters out "scans" (one-time sequential reads) effectively.
    2. If the Probationary Segment is empty, evict from the Protected Segment (LRU within this group).
       - In this case, the victim is effectively demoted back to probationary status (conceptually), 
         but since it's leaving the cache, we just remove it.
    '''
    global m_last_access, m_frequency, m_protected_set
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None
    
    # Identify candidates in the Probationary segment (not in m_protected_set)
    # Note: We rely on the set for O(1) checks, which is faster than checking frequency dicts
    probationary_candidates = [k for k in current_keys if k not in m_protected_set]
    
    victim_key = None
    
    if probationary_candidates:
        # Evict LRU from Probationary segment
        victim_key = min(probationary_candidates, key=lambda k: m_last_access.get(k, 0))
    else:
        # If no probationary items, we must evict from Protected segment (LRU)
        # This usually happens if the working set is smaller than cache size but highly active
        victim_key = min(current_keys, key=lambda k: m_last_access.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency.
    2. Promote to Protected Segment if not already there.
    '''
    global m_last_access, m_frequency, m_protected_set
    
    # Update recency
    m_last_access[obj.key] = cache_snapshot.access_count
    
    # Update frequency
    current_freq = m_frequency.get(obj.key, 0)
    m_frequency[obj.key] = current_freq + 1
    
    # Promotion Logic:
    # If an item is hit, it has proven its utility. Move to Protected set.
    if obj.key not in m_protected_set:
        m_protected_set.add(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize recency.
    2. Initialize frequency.
    3. Place in Probationary Segment initially.
    '''
    global m_last_access, m_frequency, m_protected_set
    
    m_last_access[obj.key] = cache_snapshot.access_count
    m_frequency[obj.key] = 1
    
    # New insertions always start in Probationary (Not in protected set)
    if obj.key in m_protected_set:
        m_protected_set.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Clean up metadata to prevent memory leaks.
    '''
    global m_last_access, m_frequency, m_protected_set
    
    key = evicted_obj.key
    
    if key in m_last_access:
        del m_last_access[key]
    if key in m_frequency:
        del m_frequency[key]
    if key in m_protected_set:
        m_protected_set.remove(key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt248p5sy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplawvfrog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmppjhx58_r.pickle

Iteration 89: New subsample score 1.1854419999999999 is not better than old score 1.398946, skipping
Iteration 90: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptniq84po.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp29n2tdwr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpya_hgkli.pickle

Iteration 90: Proposed new text for program: # Metadata storage
# We implement a Segmented LRU (SLRU) with Ghost lists and Size-Aware eviction.
# This approach is robust against scans (via Probation segment) and thrashing (via Ghost list).
# The Size-Aware eviction in the Probation segment helps minimize object miss rates by 
# penalizing large, cold objects that pollute the cache.

# 1. m_probation: dict (Ordered)
#    - Stores keys in the "Probationary" segment.
#    - New items enter here.
#    - Order: Insertion order (LRU -> MRU).
# 2. m_protected: dict (Ordered)
#    - Stores keys in the "Protected" segment.
#    - Items promoted from Probation enter here.
#    - Order: Insertion order (LRU -> MRU).
# 3. m_ghost: dict (Ordered)
#    - Stores keys of recently evicted items.
#    - If a ghost item is re-inserted, it is promoted to Protected immediately.

m_probation = dict()
m_protected = dict()
m_ghost = dict()

# Constants
# 80% of items are protected, 20% are probationary.
# This ensures the working set stays resident while new items (scans) cycle through Probation.
PROTECTED_RATIO = 0.8
# Keep metadata for evicted items up to ~2x the cache size.
GHOST_RATIO = 2.0
# Number of LRU candidates to inspect in Probation to find a large victim.
# Inspecting 5 items balances O(1) performance with finding a good victim.
CANDIDATE_WINDOW = 5

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Policy with Size Penalty.
    
    Strategy:
    1. Maintain a target balance between Protected and Probation items.
    2. If Protected segment is too large, evict its LRU item to maintain flow.
    3. Otherwise, evict from Probation.
       - Optimization: Instead of blindly evicting the absolute LRU, look at the bottom 
         CANDIDATE_WINDOW items and evict the one with the Largest Size.
       - This penalizes large objects that consume capacity but haven't proven frequent enough 
         to be promoted to Protected.
    '''
    global m_probation, m_protected
    
    current_keys = cache_snapshot.cache
    total_items = len(current_keys)
    if total_items == 0:
        return None
        
    target_protected = int(total_items * PROTECTED_RATIO)
    
    # 1. Enforce Protected Segment Limit
    # If we have too many protected items, we prune the oldest protected item.
    if len(m_protected) > target_protected:
        if m_protected:
            # Return LRU of Protected
            return next(iter(m_protected))
            
    # 2. Evict from Probation Segment
    if m_probation:
        # Collect the bottom N candidates from Probation
        candidates = []
        iterator = iter(m_probation)
        count = 0
        
        while count < CANDIDATE_WINDOW:
            try:
                k = next(iterator)
                # Verify key is in cache snapshot (sync safety)
                item = current_keys.get(k)
                if item:
                    candidates.append((k, item.size))
                    count += 1
                else:
                    # Key in metadata but not in cache? Evict/Return to clean up.
                    return k
            except StopIteration:
                break
        
        # Pick the victim with the maximum size
        if candidates:
            # max() finds the item with largest size.
            # Since candidates are ordered by LRU, ties in size are broken by LRU 
            # (max returns the first occurrence of the max value).
            victim_key, _ = max(candidates, key=lambda x: x[1])
            return victim_key
            
        # Fallback (rare)
        return next(iter(m_probation))
    
    # 3. Fallback: If Probation is empty but we need to evict (e.g. Protected not full by count but cache full by bytes)
    if m_protected:
        return next(iter(m_protected))
        
    # 4. Absolute Fallback
    return next(iter(current_keys))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If item is in Probation: Promote to Protected (MRU).
    - If item is in Protected: Update Recency (Move to MRU).
    '''
    global m_probation, m_protected
    key = obj.key
    
    if key in m_protected:
        # Move to MRU in Protected
        val = m_protected.pop(key)
        m_protected[key] = val
    elif key in m_probation:
        # Promote to Protected
        del m_probation[key]
        m_protected[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    - Check Ghost list (History).
    - If present (Ghost Hit): It's a returning item. Insert directly into Protected.
    - Else: Insert into Probation.
    '''
    global m_probation, m_protected, m_ghost
    key = obj.key
    
    if key in m_ghost:
        # Resurrect to Protected
        del m_ghost[key]
        m_protected[key] = None
    else:
        # Insert new item to Probation
        m_probation[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Remove from segments.
    - Add to Ghost list to track history.
    - Prune Ghost list to prevent memory leaks.
    '''
    global m_probation, m_protected, m_ghost
    key = evicted_obj.key
    
    # Cleanup Resident Metadata
    if key in m_probation:
        del m_probation[key]
    if key in m_protected:
        del m_protected[key]
        
    # Add to Ghost Metadata
    m_ghost[key] = None
    
    # Limit Ghost Size
    # Use current cache count as a proxy for capacity
    current_count = len(cache_snapshot.cache)
    # Ensure a minimum baseline (e.g., 100) to avoid wiping ghost on empty cache
    limit_base = current_count if current_count > 0 else 100
    max_ghost_size = int(limit_base * GHOST_RATIO)
    
    if len(m_ghost) > max_ghost_size:
        # Remove oldest ghost entry (FIFO)
        oldest = next(iter(m_ghost))
        del m_ghost[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp76yz8xv1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaj3q2x2c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgmg3ftfk.pickle

Iteration 90: New subsample score 0.144526 is not better than old score 0.15553299999999998, skipping
Iteration 91: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_osrn0jo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpy5evpgl0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt7bkal8l.pickle

Iteration 91: Proposed new text for program: from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Globals
# T1: Recent Resident (Key -> Size). MRU is right (end), LRU is left (start).
m_t1 = OrderedDict()
# T2: Frequent Resident (Key -> Size). MRU is right (end), LRU is left (start).
m_t2 = OrderedDict()
# B1: Recent Ghost (Key -> None). History of evicted T1.
m_b1 = OrderedDict()
# B2: Frequent Ghost (Key -> None). History of evicted T2.
m_b2 = OrderedDict()

# ARC Tuning Parameters
m_p = 0          # Target size for T1 (Recency set)
m_t1_size = 0    # Current size of T1 in bytes

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using ARC logic.
    Decides whether to evict from T1 (Recency) or T2 (Frequency)
    based on the target size 'p' and the state of the incoming object.
    '''
    global m_t1, m_t2, m_b2, m_p, m_t1_size

    # ARC Replace Logic:
    # We evict from T1 if:
    # 1. T1 contains items AND
    # 2. (T1 size exceeds target p) OR (The incoming item is in B2 and T1 is exactly at target p)
    #
    # Logic derivation:
    # If incoming is in B2, ARC logic says we should decrement p. 
    # This implies T1 is too big relative to the new target, favoring eviction from T1.
    
    # Check if we should evict from T1
    if m_t1 and (m_t1_size > m_p or (obj.key in m_b2 and m_t1_size == m_p)):
        return next(iter(m_t1)) # Return LRU of T1
    
    # Otherwise, evict from T2
    if m_t2:
        return next(iter(m_t2)) # Return LRU of T2
        
    # Fallback (should typically not reach here if cache is full)
    if m_t1:
        return next(iter(m_t1))
    
    # Absolute fallback
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles Cache Hits. 
    Moves items to T2 (Frequency set) or updates their position in T2.
    '''
    global m_t1, m_t2, m_t1_size
    
    key = obj.key
    
    # Case 1: Hit in T2 (Frequent Set)
    if key in m_t2:
        m_t2.move_to_end(key) # Update Recency in T2
        
    # Case 2: Hit in T1 (Recent Set) -> Promote to T2
    elif key in m_t1:
        # Remove from T1
        size = m_t1.pop(key)
        m_t1_size -= size
        
        # Add to T2 (MRU)
        m_t2[key] = size
        
def update_after_insert(cache_snapshot, obj):
    '''
    Handles Cache Misses (Inserts).
    Performs the core ARC adaptation (adjusting p) based on Ghost lists (B1/B2).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_t1_size
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # Case 1: Ghost Hit in B1 (Was recently in T1)
    # Interpretation: We evicted it from T1 too soon. We need a larger T1.
    if key in m_b1:
        delta = 1
        if len(m_b1) < len(m_b2):
            delta = len(m_b2) // len(m_b1)
        
        # Increase target p (bounded by capacity)
        # We scale the adaptation by object size to fit the byte-based p
        m_p = min(capacity, m_p + delta * size)
        
        # Remove from ghost list
        del m_b1[key]
        
        # Move to T2 (it has been accessed twice now: once effectively in T1, now again)
        m_t2[key] = size

    # Case 2: Ghost Hit in B2 (Was recently in T2)
    # Interpretation: We evicted it from T2 too soon. We need a larger T2 (smaller T1).
    elif key in m_b2:
        delta = 1
        if len(m_b2) < len(m_b1):
            delta = len(m_b1) // len(m_b2)
            
        # Decrease target p (bounded by 0)
        m_p = max(0, m_p - delta * size)
        
        # Remove from ghost list
        del m_b2[key]
        
        # Re-insert into T2
        m_t2[key] = size
        
    # Case 3: Totally Cold Miss
    else:
        # Insert into T1 (MRU)
        m_t1[key] = size
        m_t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles Metadata updates after an eviction.
    Moves the victim to the corresponding Ghost List (B1 or B2).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_t1_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # 1. Identify where it was evicted from and update sizes/lists
    if key in m_t1:
        del m_t1[key]
        m_t1_size -= size
        # Move to Ghost B1 (Recent History)
        m_b1[key] = None
    elif key in m_t2:
        del m_t2[key]
        # Move to Ghost B2 (Frequent History)
        m_b2[key] = None
        
    # 2. Ghost List Management (Pruning)
    # ARC requires ghost lists to effectively double the cache directory size.
    # Since we work with variable sizes, we use the current object count 
    # as a heuristic to keep ghost lists from growing indefinitely.
    target_len = len(cache_snapshot.cache) + 1
    
    # If we have too many ghosts, trim the oldest ones.
    # Standard ARC: len(T1) + len(B1) <= C
    while len(m_b1) > target_len:
        m_b1.popitem(last=False) # Remove oldest
        
    # Standard ARC: len(T2) + len(B2) <= 2C
    while len(m_b2) > target_len * 2:
        m_b2.popitem(last=False) # Remove oldest
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfl5yrvf4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyn6nh3md.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmps_7sfv2f.pickle

Iteration 91: New subsample score 0.7307810000000001 is not better than old score 0.8233790000000001, skipping
Iteration 92: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpa41tklym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn_gmxssp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn47rruqs.pickle

Iteration 92: Proposed new text for program: from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Implementation
# Adapted for variable object sizes.

# Metadata Storage
# 1. m_t1: Resident Recency (T1). OrderedDict (key -> size).
#    Contains items that have been accessed recently (usually once).
#    - Right: MRU, Left: LRU.
# 2. m_t2: Resident Frequency (T2). OrderedDict (key -> size).
#    Contains items that have been accessed at least twice.
#    - Right: MRU, Left: LRU.
# 3. m_b1: Ghost Recency (B1). OrderedDict (key -> size).
#    History of items recently evicted from T1.
# 4. m_b2: Ghost Frequency (B2). OrderedDict (key -> size).
#    History of items recently evicted from T2.

m_t1 = OrderedDict()
m_t2 = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()

# Global State
# m_p: Target size (in bytes) for T1.
# m_size_t1: Current total size of T1 in bytes.
# m_size_t2: Current total size of T2 in bytes.

m_p = 0
m_size_t1 = 0
m_size_t2 = 0

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using ARC logic.
    Decides whether to evict from T1 or T2 based on the current sizes relative to target m_p.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_size_t1, m_size_t2
    
    # 1. Simulate ARC Adaptation Logic to decide eviction source.
    # If the incoming object is in a Ghost list, it means our previous cache sizing 
    # was suboptimal. We calculate a temporary target_p to decide the best victim.
    
    target_p = m_p
    key = obj.key
    
    if key in m_b1:
        # Hit in B1 (Ghost Recency) -> We should have had a larger T1.
        # Adaptation: Increase target_p.
        delta = obj.size
        target_p = m_p + delta
        
    elif key in m_b2:
        # Hit in B2 (Ghost Frequency) -> We should have had a larger T2 (smaller T1).
        # Adaptation: Decrease target_p.
        delta = obj.size
        target_p = m_p - delta
        
    # Clamp target_p within bounds
    capacity = cache_snapshot.capacity
    if target_p < 0: target_p = 0
    if target_p > capacity: target_p = capacity
    
    # 2. ARC Replacement Decision
    # Evict from T1 if it is "too big" (size > target_p) OR if we must (T2 is empty).
    # Otherwise, evict from T2.
    if m_t1 and (m_size_t1 > target_p or not m_t2):
        return next(iter(m_t1)) # Return LRU of T1
    else:
        # Fallback: Evict from T2
        if m_t2:
            return next(iter(m_t2)) # Return LRU of T2
        # Should technically not reach here if cache is full
        return next(iter(m_t1))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    Moves hits to T2 (Frequency) as they have proven their utility.
    '''
    global m_t1, m_t2, m_size_t1, m_size_t2
    
    key = obj.key
    size = obj.size
    
    if key in m_t1:
        # Promote T1 -> T2
        del m_t1[key]
        m_size_t1 -= size
        
        m_t2[key] = size
        m_size_t2 += size
        
    elif key in m_t2:
        # Update MRU position in T2
        m_t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    - Updates m_p (adaptation) if hit in Ghost lists.
    - Inserts new item into T1 (Recency) or T2 (if promoted from Ghost).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_size_t1, m_size_t2
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Check Ghost Lists (Adaptation)
    if key in m_b1:
        # Ghost Hit B1 -> Increase m_p
        delta = size
        m_p = min(capacity, m_p + delta)
        
        # Move from Ghost B1 to Resident T2 (Promote)
        del m_b1[key]
        m_t2[key] = size
        m_size_t2 += size
        
    elif key in m_b2:
        # Ghost Hit B2 -> Decrease m_p
        delta = size
        m_p = max(0, m_p - delta)
        
        # Move from Ghost B2 to Resident T2 (Reclaim)
        del m_b2[key]
        m_t2[key] = size
        m_size_t2 += size
        
    else:
        # 2. Cold Miss -> Insert into T1 (Recency)
        m_t1[key] = size
        m_size_t1 += size

    # 3. Metadata Maintenance (Pruning Ghosts)
    # Prevent ghost lists from growing indefinitely.
    # Heuristic: Keep ghost count roughly 2x resident count.
    resident_count = len(m_t1) + len(m_t2)
    max_ghosts = 2 * resident_count + 1000
    
    if len(m_b1) + len(m_b2) > max_ghosts:
        if len(m_b1) > len(m_b2):
            m_b1.popitem(last=False)
        elif m_b2:
            m_b2.popitem(last=False)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    Moves the evicted resident item to the corresponding Ghost list.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_size_t1, m_size_t2
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    if key in m_t1:
        del m_t1[key]
        m_size_t1 -= size
        m_b1[key] = size # Add to B1 (Ghost Recency)
        
    elif key in m_t2:
        del m_t2[key]
        m_size_t2 -= size
        m_b2[key] = size # Add to B2 (Ghost Frequency)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbkb__8xz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxtpuh879.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8pwxo3_h.pickle

Iteration 92: New subsample score 0.952843 is not better than old score 1.007758, skipping
Iteration 93: Selected program 15 score: 0.24230047916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpr_2n7s70.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6x10qev_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpefhz95lp.pickle

Iteration 93: Proposed new text for program: import math

# Metadata storage
# key -> frequency (int)
m_key_frequency = dict()

# key -> priority_score (float)
# The "H" value in GDSF algorithm.
m_key_priority = dict()

# Global inflation factor 'L'.
# This increases monotonically as items are evicted, acting as a dynamic "current time" 
# or "aging floor".
inflation_L = 0.0

def get_priority(obj_key, size, freq, L_value):
    '''
    GDSF Priority Function:
    Priority = L + (Frequency / Size)
    
    However, strictly using Freq/Size favors very small objects too heavily.
    We use a balanced approach:
    Priority = L + (Frequency * Weight) / Size
    
    Where we dampen the frequency effect slightly to prevent old items from sticking forever,
    and we dampen the size penalty slightly so massive items aren't evicted immediately if useful.
    '''
    # Safety for 0 size (though spec says positive integers)
    safe_size = size if size > 0 else 1
    
    # We use log-frequency to prevent count explosion and allow new trends to surface.
    # We add 1 to freq for the log to work on the first hit.
    freq_factor = math.log2(freq + 1) + 1
    
    return L_value + (freq_factor / safe_size)

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Greedy Dual-Size Frequency (GDSF).
    
    We evict the item with the lowest priority score (H).
    After eviction, we update the global inflation factor L to the priority of the evicted item.
    This ensures that future inserted items start with at least this priority, 
    effectively "aging" the existing items in the cache relative to new ones.
    '''
    global m_key_priority, inflation_L
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    
    # Search for the item with the lowest H value (Priority)
    for key in current_keys:
        p = m_key_priority.get(key, 0.0)
        
        # Tie-breaker: If priorities are extremely close, pick the one 
        # accessed longest ago (LRU-like tie breaking). 
        # Since we don't have explicit LRU list, we rely on the floating point values.
        # Usually, the math is distinct enough.
        
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # Update global aging factor L
    # This is the crucial step of GDSF: The system "price" rises to the eviction victim's value.
    if victim_key is not None:
        inflation_L = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on current L and new Frequency.
       This "refreshes" the object, pushing it above the current eviction floor.
    '''
    global m_key_frequency, m_key_priority, inflation_L
    
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1
    
    # Restore the priority using the CURRENT global inflation L
    m_key_priority[obj.key] = get_priority(
        obj.key, 
        obj.size, 
        m_key_frequency[obj.key], 
        inflation_L
    )

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency.
    2. Calculate Initial Priority using current global L.
    '''
    global m_key_frequency, m_key_priority, inflation_L
    
    # Start with frequency 1
    m_key_frequency[obj.key] = 1
    
    # Set priority. Because it uses the current 'inflation_L', 
    # it starts "safe" from immediate eviction (unless it's huge).
    m_key_priority[obj.key] = get_priority(
        obj.key, 
        obj.size, 
        1, 
        inflation_L
    )

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata.
    '''
    global m_key_frequency, m_key_priority
    
    key = evicted_obj.key
    
    # We remove metadata to save memory. 
    # Unlike LFU with Ghost lists, GDSF relies on the global 'inflation_L' 
    # to handle the history/context, so we don't need to track evicted keys.
    if key in m_key_frequency:
        del m_key_frequency[key]
        
    if key in m_key_priority:
        del m_key_priority[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp17b64fpb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjcvppm7w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjpx3p3_o.pickle

Iteration 93: New subsample score 0.11654400000000001 is not better than old score 0.448247, skipping
Iteration 94: Selected program 15 score: 0.24230047916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoot7xxop.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_a3fiht4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphbej_7fq.pickle

Iteration 94: Proposed new text for program: import math

# Metadata storage
# key -> last_access_time (int)
m_key_last_access = dict()
# key -> frequency (int)
m_key_frequency = dict()
# key -> priority_score (float) - This represents the 'H' value in GDSF
m_key_priority = dict()

# Global inflation factor 'L'.
# In GDS/GDSF, 'L' is updated to the priority of the evicted object.
# This acts as the aging mechanism: new objects start with priority L + utility.
# Old objects that aren't accessed effectively have their relative priority dropped.
L_current = 0.0

def calculate_priority(freq, size, L_val):
    '''
    GDSF (Greedy Dual-Size Frequency) Priority Calculation.
    Score = L + (Frequency / Size)
    
    We add a tiny epsilon or standard weight to size to prevent division issues
    and balance very small objects.
    '''
    # Using a slightly modified cost function:
    # Priority = L + (Frequency * Cost) / Size
    # Here cost is 1 (uniform). 
    # We maintain 1.0 multiplier to ensure float division.
    return L_val + (float(freq) / float(size))

def evict(cache_snapshot, obj):
    '''
    Policy: Evict the object with the lowest GDSF Priority Score.
    
    GDSF combines:
    1. Frequency (Keep popular items)
    2. Size (Evict large items to make room for many small ones)
    3. Recency/Aging (The 'L' factor naturally ages out stale entries)
    
    Tie-breaker: LRU (Least Recently Used)
    '''
    global m_key_priority, L_current, m_key_last_access
    
    current_keys = list(cache_snapshot.cache.keys())
    if not current_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    min_access = float('inf')
    
    # Search for the item with the minimum Priority score
    for key in current_keys:
        # If for some reason metadata is missing, default to safe values
        p_val = m_key_priority.get(key, 0.0)
        acc_time = m_key_last_access.get(key, 0)
        
        # We look for the smallest priority value
        if p_val < min_priority:
            min_priority = p_val
            min_access = acc_time
            victim_key = key
        elif p_val == min_priority:
            # Tie-breaker: Evict the older one (LRU logic)
            if acc_time < min_access:
                min_access = acc_time
                victim_key = key
    
    # Update the global aging factor L to the priority of the evicted item.
    # This ensures future items enter the cache with a higher baseline,
    # effectively "aging" existing items relative to new ones.
    if victim_key is not None:
        L_current = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Update Last Access Time.
    3. Re-calculate Priority (restoring it to L_current + Utility).
       This "resets" its age, keeping it in the cache longer.
    '''
    global m_key_last_access, m_key_frequency, m_key_priority, L_current
    
    current_time = cache_snapshot.access_count
    
    # Update Metadata
    m_key_last_access[obj.key] = current_time
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1
    
    # Re-calculate priority
    # Note: In GDSF, when an object is hit, we reset its 'L' component to the CURRENT L.
    # This brings it "forward" in time.
    freq = m_key_frequency[obj.key]
    m_key_priority[obj.key] = calculate_priority(freq, obj.size, L_current)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Metadata.
    2. Calculate initial Priority based on current L.
    '''
    global m_key_last_access, m_key_frequency, m_key_priority, L_current
    
    current_time = cache_snapshot.access_count
    
    m_key_last_access[obj.key] = current_time
    m_key_frequency[obj.key] = 1 # Start with frequency 1
    
    # Calculate initial priority
    # New object enters with base priority of current inflation factor L
    m_key_priority[obj.key] = calculate_priority(1, obj.size, L_current)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata for the evicted object to free memory.
    '''
    global m_key_last_access, m_key_frequency, m_key_priority
    
    key = evicted_obj.key
    
    if key in m_key_last_access:
        del m_key_last_access[key]
    if key in m_key_frequency:
        del m_key_frequency[key]
    if key in m_key_priority:
        del m_key_priority[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6pb413_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw4xfq5f8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpffqshqgd.pickle

Iteration 94: New subsample score 0.41949600000000004 is not better than old score 0.48809199999999997, skipping
Iteration 95: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpekvj9gxq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp4ikfg4h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpividp2o9.pickle

Iteration 95: Proposed new text for program: from collections import OrderedDict

# Metadata storage
# 1. lirs_stack: OrderedDict (key -> access_time)
#    Represents the LIRS "Stack S". Top is MRU. Bottom is LRU.
#    Contains both LIR and HIR (resident and non-resident) blocks.
#    Used to calculate Inter-Reference Recency (IRR).
# 2. hir_queue: OrderedDict (key -> access_time)
#    Represents the LIRS "Queue Q".
#    Contains only Resident HIR blocks. Used to pick eviction victims.
# 3. block_state: dict (key -> str)
#    Tracks state: 'LIR', 'HIR_RESIDENT', 'HIR_NON_RESIDENT'

lirs_stack = OrderedDict()
hir_queue = OrderedDict()
block_state = dict()

# Constants
# We reserve a small portion of the cache for HIR blocks to allow new data 
# to prove itself. 1% to 5% is standard.
HIR_PERCENTAGE = 1.0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy:
    Remove the item at the front (LRU end) of the HIR Queue (Queue Q).
    '''
    global hir_queue, block_state, lirs_stack
    
    if not hir_queue:
        # Fallback: If Queue is empty (rare, implies cache is 100% LIR),
        # we must evict an LIR block. In LIRS, this means the bottom of Stack S.
        # However, to be safe, we look at the cache keys directly if needed,
        # but logically we should demote the bottom LIR.
        
        # Find the LIR item at the bottom of the stack
        victim_key = None
        for key in lirs_stack:
            if block_state.get(key) == 'LIR':
                victim_key = key
                break
        
        if victim_key is None:
            # Absolute fallback if metadata is desync
            return list(cache_snapshot.cache.keys())[0]
            
        return victim_key

    # Standard LIRS eviction: Head of Queue Q (LRU of HIRs)
    victim_key, _ = hir_queue.popitem(last=False)
    
    # We return the key. The caller removes it from cache_snapshot.cache.
    # We must update our metadata in `update_after_evict`.
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    LIRS Hit Logic:
    1. LIR Hit: Move to top of Stack S. Prune bottom if needed.
    2. HIR Hit:
       - If inside Stack S (Non-Resident or Resident): It becomes hot (LIR).
       - If NOT inside Stack S: Remains HIR, moves to end of Queue Q.
    '''
    global lirs_stack, hir_queue, block_state
    
    key = obj.key
    state = block_state.get(key)
    current_time = cache_snapshot.access_count

    if state == 'LIR':
        # Accessed an LIR block.
        # Move to top of Stack S.
        if key in lirs_stack:
            del lirs_stack[key]
        lirs_stack[key] = current_time
        
        # Pruning: If the moved LIR block was at the bottom, the new bottom
        # might be an HIR block, which is invalid for Stack S. Prune it.
        _prune_stack()
        
    elif state == 'HIR_RESIDENT':
        # Accessed a Resident HIR block.
        
        if key in lirs_stack:
            # It is in Stack S. This means its recency is low enough.
            # Promote to LIR.
            
            # 1. Change state
            block_state[key] = 'LIR'
            
            # 2. Remove from Queue Q (it's no longer HIR)
            if key in hir_queue:
                del hir_queue[key]
                
            # 3. Move to top of Stack S
            del lirs_stack[key]
            lirs_stack[key] = current_time
            
            # 4. We promoted an HIR to LIR. We might need to demote an LIR to maintain ratio.
            _enforce_lir_limit(cache_snapshot)
            
        else:
            # It is NOT in Stack S.
            # It remains HIR. Move to top of Stack S and end of Queue Q.
            lirs_stack[key] = current_time
            
            if key in hir_queue:
                del hir_queue[key]
            hir_queue[key] = current_time

    # Note: 'HIR_NON_RESIDENT' hits technically happen in 'update_after_insert' 
    # because the system sees them as misses first, then inserts.
    
def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was a Non-Resident HIR (Ghost).
    '''
    global lirs_stack, hir_queue, block_state
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Check if this key exists in our metadata (Ghost hit)
    if key in lirs_stack and block_state.get(key) == 'HIR_NON_RESIDENT':
        # It was in the stack as non-resident.
        # This implies it's a "hot" miss. Promote to LIR.
        
        block_state[key] = 'LIR'
        
        # Move to top of stack
        del lirs_stack[key]
        lirs_stack[key] = current_time
        
        _enforce_lir_limit(cache_snapshot)
    else:
        # Cold miss. New resident HIR.
        block_state[key] = 'HIR_RESIDENT'
        
        # Add to top of stack
        lirs_stack[key] = current_time
        # Add to end of Queue Q
        hir_queue[key] = current_time
        
        # Note: We do NOT remove it from lirs_stack immediately even if it's new.
        # It gets pruned later if it drifts to the bottom.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Post-Eviction Cleanup.
    The evicted object is no longer resident.
    '''
    global lirs_stack, hir_queue, block_state
    
    key = evicted_obj.key
    
    # Logic:
    # If it was in Stack S, it becomes Non-Resident HIR (Ghost).
    # If it was NOT in Stack S, we discard metadata entirely.
    
    if key in lirs_stack:
        block_state[key] = 'HIR_NON_RESIDENT'
        # It stays in lirs_stack to track recency for potential future promotion.
        # It is strictly removed from hir_queue (done in evict, but safety check here)
        if key in hir_queue:
            del hir_queue[key]
    else:
        # Not in stack, not in cache -> completely forget it.
        if key in lirs_stack:
            del lirs_stack[key]
        if key in hir_queue:
            del hir_queue[key]
        if key in block_state:
            del block_state[key]

def _prune_stack():
    '''
    Maintains the LIRS property:
    The bottom of Stack S must always be an LIR block.
    If we find an HIR block (Resident or Non-Resident) at the bottom, 
    we remove it. We repeat this until the bottom is LIR or stack empty.
    '''
    global lirs_stack, block_state
    
    # lirs_stack is OrderedDict. items() iterates in order. 
    # But we want to check the bottom (first item).
    
    while lirs_stack:
        # Peek at bottom key (next(iter) is O(1))
        bottom_key = next(iter(lirs_stack))
        state = block_state.get(bottom_key)
        
        if state == 'LIR':
            # Stop pruning, bottom is valid.
            break
        
        # It is HIR (Resident or Non-Resident). Remove from Stack S.
        del lirs_stack[bottom_key]
        
        # If it was Non-Resident, we can now forget it completely 
        # (it has fallen out of the "hot" history window).
        if state == 'HIR_NON_RESIDENT':
            del block_state[bottom_key]
        # If it is HIR_RESIDENT, it stays in Cache and Queue Q, 
        # just loses its place in the history stack.

def _enforce_lir_limit(cache_snapshot):
    '''
    If LIR set grows too large, convert the least recently used LIR to HIR.
    The LRU LIR is always at the bottom of Stack S.
    '''
    global lirs_stack, hir_queue, block_state
    
    # Calculate Max LIR count allowed.
    # Heuristic: 99% of capacity can be LIR.
    total_objects = len(cache_snapshot.cache) 
    if total_objects == 0: return

    # We allow the cache to fill. This logic triggers when we promote items.
    # If the number of LIR blocks exceeds (Total Cache Slots - HIR_Buffer), demote.
    # Since we don't know exact slots, we use current cache size as proxy for capacity usage.
    
    # Count current LIRs
    lir_count = sum(1 for k, v in block_state.items() if v == 'LIR')
    
    # Target HIR size (e.g., 1% of the cache capacity should be kept for rotation)
    # If cache is not full, we don't need to force demotion yet strictly, 
    # but LIRS usually maintains a ratio.
    max_lir = max(1, int(total_objects * (1.0 - (HIR_PERCENTAGE / 100.0))))
    
    if lir_count > max_lir:
        # Demote the bottom-most LIR block in Stack S.
        # Because we prune the stack, the bottom-most item IS the LRU LIR.
        
        # 1. Find bottom LIR (It is the first item in OrderedDict due to pruning)
        victim_lir = next(iter(lirs_stack))
        
        # 2. Change state to HIR Resident
        block_state[victim_lir] = 'HIR_RESIDENT'
        
        # 3. Add to Queue Q (Resident HIRs must be in Q)
        # Note: It effectively goes to the 'mru' end of Q usually, or we can simply add.
        hir_queue[victim_lir] = lirs_stack[victim_lir]
        
        # 4. Trigger pruning. Since the bottom is now HIR, it will be removed from Stack S.
        _prune_stack()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdsgxp_8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7fywiex3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpn3y_w1q_.pickle

Iteration 95: New subsample score 0.530733 is not better than old score 0.577967, skipping
Iteration 96: Selected program 24 score: 0.24984599999999993
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzxi5n9e8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_0_41h06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuk35nu0u.pickle

Iteration 96: Proposed new text for program: from collections import OrderedDict

# ==============================================================================
# Byte-Weighted Adaptive Replacement Cache (ARC)
# ==============================================================================
# We maintain four segments:
# 1. m_t1: Recent (Probation) - Objects accessed once.
# 2. m_t2: Frequent (Protected) - Objects accessed >= 2 times.
# 3. m_b1: Ghost Recent - Keys evicted from T1.
# 4. m_b2: Ghost Frequent - Keys evicted from T2.
#
# m_p: Target size (in bytes) for m_t1.
#      - Increases on B1 hits (Recency needed).
#      - Decreases on B2 hits (Frequency needed).

m_t1 = OrderedDict()
m_t2 = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()

m_t1_size = 0
m_t2_size = 0
m_b1_size = 0
m_b2_size = 0

m_p = 0.0

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using ARC logic.
    Since this function is called *before* the insert logic updates 'p',
    we simulate the adaptation to make the optimal eviction decision.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p, m_t1_size, m_b1_size, m_b2_size
    
    # 1. Simulate Adaptation based on incoming object to decide victim
    target_p = m_p
    key = obj.key
    size = obj.size
    capacity = float(cache_snapshot.capacity)
    
    if key in m_b1:
        # B1 hit implies T1 was too small. 
        delta = float(size)
        if m_b1_size < m_b2_size:
            # Adaptation step size logic borrowed from ARC, scaled for bytes
            delta *= (float(m_b2_size) / m_b1_size)
        target_p = min(capacity, m_p + delta)
        
    elif key in m_b2:
        # B2 hit implies T2 was too small (T1 too big).
        delta = float(size)
        if m_b2_size < m_b1_size:
            delta *= (float(m_b1_size) / m_b2_size)
        target_p = max(0.0, m_p - delta)

    # 2. Choose Victim
    # Safety fallback if segments are empty
    if not m_t1:
        return next(iter(m_t2))
    if not m_t2:
        return next(iter(m_t1))

    # ARC Rule: Evict from T1 if T1 size exceeds target 'p'.
    # This keeps T1 constrained to the learned ideal size.
    if m_t1_size > target_p:
        return next(iter(m_t1)) # LRU of T1
    
    # Otherwise, evict from T2
    return next(iter(m_t2)) # LRU of T2

def update_after_hit(cache_snapshot, obj):
    '''
    Cache Hit: Item is in T1 or T2.
    Move item to MRU of T2 (Protected).
    '''
    global m_t1, m_t2, m_t1_size, m_t2_size
    
    key = obj.key
    size = obj.size
    
    if key in m_t1:
        # Promote from T1 -> T2
        del m_t1[key]
        m_t1_size -= size
        
        m_t2[key] = size
        m_t2_size += size
        
    elif key in m_t2:
        # Update recency in T2
        m_t2.move_to_end(key)
        # Refresh size in case it changed (though obj is read-only)
        m_t2[key] = size

def update_after_insert(cache_snapshot, obj):
    '''
    Cache Miss (Insert):
    1. Check Ghosts (B1/B2) to adapt 'm_p'.
    2. Insert new object into T1 or promote ghost to T2.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    global m_t1_size, m_t2_size, m_b1_size, m_b2_size
    
    key = obj.key
    size = obj.size
    capacity = float(cache_snapshot.capacity)
    
    # 1. Check if it's a Ghost Hit (Recency/Frequency Miss)
    if key in m_b1:
        # Hit in Ghost T1 -> We should have kept T1 larger. Increase p.
        delta = float(size)
        if m_b1_size < m_b2_size:
            delta *= (float(m_b2_size) / m_b1_size)
        m_p = min(capacity, m_p + delta)
        
        # Move B1 -> T2
        del m_b1[key]
        m_b1_size -= size
        
        m_t2[key] = size
        m_t2_size += size
        
    elif key in m_b2:
        # Hit in Ghost T2 -> We should have kept T2 larger. Decrease p.
        delta = float(size)
        if m_b2_size < m_b1_size:
            delta *= (float(m_b1_size) / m_b2_size)
        m_p = max(0.0, m_p - delta)
        
        # Move B2 -> T2
        del m_b2[key]
        m_b2_size -= size
        
        m_t2[key] = size
        m_t2_size += size
        
    else:
        # 2. Completely New Item -> Insert into T1 (MRU)
        m_t1[key] = size
        m_t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handle cleanup after eviction.
    Move evicted item to corresponding Ghost list (B1 or B2).
    '''
    global m_t1, m_t2, m_b1, m_b2
    global m_t1_size, m_t2_size, m_b1_size, m_b2_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # 1. Move to Ghost
    if key in m_t1:
        del m_t1[key]
        m_t1_size -= size
        m_b1[key] = size
        m_b1_size += size
    elif key in m_t2:
        del m_t2[key]
        m_t2_size -= size
        m_b2[key] = size
        m_b2_size += size
        
    # 2. Limit Ghost Size
    # ARC typically suggests keeping total ghost directory size bounded (e.g., L1+B1 <= C).
    # We limit total ghost bytes to Cache Capacity to prevent memory bloat.
    max_ghost_bytes = cache_snapshot.capacity
    
    while (m_b1_size + m_b2_size) > max_ghost_bytes:
        # Remove oldest from the Ghost list. 
        # Prefer removing from the larger ghost list to maintain balance, or simple FIFO.
        if m_b1 and (m_b1_size > m_b2_size or not m_b2):
            k, s = m_b1.popitem(last=False)
            m_b1_size -= s
        elif m_b2:
            k, s = m_b2.popitem(last=False)
            m_b2_size -= s
        else:
            break
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpatzqu6yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp61do6x2k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjh5kh9ml.pickle

Iteration 96: New subsample score 1.038849 is better than old score 1.02484. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfqc_vqf0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp8idtn63g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1dfxa1id.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9tb25he1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2bhj5n04.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7a6bjs3q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprrwasn26.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpksmpibt0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgftlk9ug.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpldrea6bp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuk43lw01.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxfndp144.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyiue6fj8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg4hx08rc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp84v2cy4x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxoya5_hh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0bwzpwt5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0uusefn3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0mufzv3r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgggidlvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz5solhzv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_vrt8ie9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp04prsyih.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp93xka77t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmfq8lpgd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpde7c6b8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe8x1p56w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpixv0z0zd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6gwdilv5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplu_iw_q7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpodab4w46.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkvn13edc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_fezbffv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf4xxqv07.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiim3fl3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo8p1t8lr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6zxru73g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpct8waudw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpen8b0igc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzgogeu6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyqkoj0x1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2i7r1yb5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5ia0ne43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprvtsrds1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpf9akzr7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmn67myi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpq3o3u_a2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphupeewts.pickle

Iteration 96: Full valset score for new program: 0.2468353333333333
Iteration 96: Full train_val score for new program: 0.2468353333333333
Iteration 96: Individual valset scores for new program: [0.508448, 0.484357, 0.490879, 0.441362, 0.505429, 0.490545, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.341918, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.342183, 0.026164, 0.058672, 0.058672, 0.269833, 0.358871, 0.848064, 0.891494, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.084003, 0.123396, 0.098718, 0.052632, 0.3, 0.15133, 0.071529, 0.466258, 0.081699]
Iteration 96: New valset pareto front scores: [0.5113, 0.484357, 0.493182, 0.448119, 0.505643, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.142258, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.149856, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 96: Full valset pareto front score: 0.27114910416666665
Iteration 96: Updated valset pareto front programs: [{22}, {27}, {2}, {22}, {25}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23, 24}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 24, 25, 26, 27}, {22}, {2, 4, 8, 9, 10, 17, 19, 26}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {24}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21, 24, 25, 26, 27}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21, 24, 25, 26, 27}, {22}, {24}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27}, {22}]
Iteration 96: Best valset aggregate score so far: 0.2627519583333333
Iteration 96: Best program as per aggregate score on train_val: 22
Iteration 96: Best program as per aggregate score on valset: 22
Iteration 96: Best score on valset: 0.2627519583333333
Iteration 96: Best score on train_val: 0.2627519583333333
Iteration 96: Linear pareto front program index: 22
Iteration 96: New program candidate index: 27
Iteration 97: Selected program 17 score: 0.2406433541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfput5jsj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpx4hzbpgm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpif5u20jp.pickle

Iteration 97: Proposed new text for program: # ARC (Adaptive Replacement Cache) Global State

# Resident Lists (Subset of actual cache)
# t1: Recent (LRU at start, MRU at end)
t1 = dict()
# t2: Frequent (LRU at start, MRU at end)
t2 = dict()

# Ghost Lists (History of evicted items)
# b1: Ghost Recent (LRU at start, MRU at end)
b1 = dict()
# b2: Ghost Frequent (LRU at start, MRU at end)
b2 = dict()

# Adaptation Parameter
# p: Target size for the recent list t1
p = 0

def _move_to_mru(container, key):
    """Moves a key to the end (MRU position) of the dictionary."""
    val = container.pop(key)
    container[key] = val

def evict(cache_snapshot, obj):
    """
    Decides which object to evict when the cache is full.
    ARC Logic: 
    - If t1 has more items than its target `p`, evict from t1 (Recency).
    - Otherwise, evict from t2 (Frequency).
    """
    global t1, t2, p
    
    # Safety fallback if algorithm state is empty but cache is not
    if not t1 and not t2:
        return next(iter(cache_snapshot.cache)) if cache_snapshot.cache else None

    # Logic to choose victim
    # If we have "too much" recency (len(t1) > p), we shed from t1.
    # Otherwise we shed from t2.
    if t1 and len(t1) > p:
        victim_key = next(iter(t1)) # LRU of t1
        return victim_key
    elif t2:
        victim_key = next(iter(t2)) # LRU of t2
        return victim_key
    
    # Absolute fallback (should ideally not be reached)
    return next(iter(t1)) if t1 else next(iter(t2))

def update_after_hit(cache_snapshot, obj):
    """
    Called when obj is found in the cache.
    ARC Logic:
    - If in t1, move to t2 (Recency -> Frequency).
    - If in t2, move to t2 MRU (Frequency -> Frequency).
    """
    global t1, t2
    key = obj.key
    
    # Case 1: Hit in T1 (Recent -> Frequent)
    if key in t1:
        del t1[key]
        t2[key] = None # Add to T2 MRU
        
    # Case 2: Hit in T2 (Frequent -> Frequent)
    elif key in t2:
        _move_to_mru(t2, key)
    
    # Note: If key is in cache but not in t1/t2 (sync issue), add to t2
    else:
        t2[key] = None

def update_after_insert(cache_snapshot, obj):
    """
    Called after a new object is inserted into the cache.
    This handles:
    1. New insertions (Miss).
    2. "Ghost" hits (Miss in cache, but Hit in b1/b2 history).
    """
    global t1, t2, b1, b2, p
    key = obj.key
    
    # Since the cache size isn't fixed in this API, we infer capacity context
    # roughly from the current population for the adaptation logic limits.
    # However, strictly for the adaptation formula, we handle the math carefully.

    # Check Ghost Lists
    if key in b1:
        # Hit in Ghost Recency (b1) -> We should have kept it in t1 longer.
        # Increase p (Target size of t1).
        delta = 1
        if len(b1) < len(b2):
            delta = len(b2) / len(b1)
        
        # We roughly estimate capacity C as current cache size (it's full or close to it)
        # Note: In standard ARC, C is fixed. Here we clamp dynamically.
        current_capacity = len(cache_snapshot.cache) 
        p = min(current_capacity, p + delta)
        
        del b1[key]
        t2[key] = None # Promote to t2
        
    elif key in b2:
        # Hit in Ghost Frequency (b2) -> We should have kept it in t2 longer.
        # Decrease p (Target size of t1, implicitly increasing t2).
        delta = 1
        if len(b2) < len(b1):
            delta = len(b1) / len(b2)
        
        p = max(0, p - delta)
        
        del b2[key]
        t2[key] = None # Promote to t2 (restore)
        
    else:
        # Totally new item (L1 miss).
        # Insert into t1 (Recency list).
        t1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Called after an object is evicted.
    ARC Logic:
    - The evicted resident object becomes a ghost.
    - If evicted from t1, move to b1.
    - If evicted from t2, move to b2.
    - Ensure ghost lists don't grow infinitely (Limit to roughly Capacity).
    """
    global t1, t2, b1, b2
    
    key = evicted_obj.key
    
    # Determine where it was evicted from based on our tracking
    # Note: The 'evict' function chose the victim, but strictly speaking
    # the system removed it. We check our own dictionaries to see where it was.
    
    if key in t1:
        del t1[key]
        b1[key] = None # Add to b1 MRU
    elif key in t2:
        del t2[key]
        b2[key] = None # Add to b2 MRU
    
    # Prune Ghost Lists
    # ARC generally keeps |b1| + |b2| <= Capacity.
    # Since we don't have a static Capacity constant, we use the current cache size.
    capacity = len(cache_snapshot.cache) + 1 # +1 accounts for the just evicted item usually
    
    total_ghosts = len(b1) + len(b2)
    if total_ghosts > capacity * 2: # Keep a safe buffer (2x capacity is standard ARC)
        if len(b1) > capacity:
            # Remove LRU from b1
            del b1[next(iter(b1))]
        elif b2:
             # Remove LRU from b2
             del b2[next(iter(b2))]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsvspp3i_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgryn2tqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptm3fqv7r.pickle

Iteration 97: New subsample score 0.8044899999999999 is better than old score 0.736191. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdhpbxd0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptkdbc84z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpir96dred.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpksquvl6m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpma_uhp32.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp56h18hhf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9jw8xf2v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprxro7ylw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpv5fj77o4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptkiw7zy6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpj562vqs6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpg4rrezb8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd9m2xwd7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmfvru00m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7ti_6ggc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd8w1w2hc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6f2rboj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpexr428sx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7q4pa3ee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfvkx675c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpdkxl_ihl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpz42ixy42.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmkwg_56n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprxg_v3fd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9uydtxw5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpve3uublt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzz8l7t94.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvvz4m_gt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2_0v_e89.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp50t9s1fe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp32fmh_qi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6qahl2dl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo9uo7xfk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbcaiuxup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpen5yt3yl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjzgqrn07.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb8vd9jue.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpiganyed9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp34ntb88l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpypxw95pd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpd45yt1cj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp64y9ai05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9gbr84rg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnfavx8ec.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplqxm6st8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkjkdj7rp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt9bt3rhd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5o_c5s1o.pickle

Iteration 97: Full valset score for new program: 0.2415486041666667
Iteration 97: Full train_val score for new program: 0.2415486041666667
Iteration 97: Individual valset scores for new program: [0.509363, 0.48459, 0.493485, 0.444207, 0.50484, 0.492275, 0.272727, 0.498034, 0.540937, 0.531017, 0.091667, 0.340142, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.3412, 0.026556, 0.058672, 0.058672, 0.269802, 0.391129, 0.849057, 0.892521, 0.096137, 0.038636, 0.045558, 0.029555, 0.029983, 0.723111, 0.083333, 0.067961, 0.053444, 0.640392, 0.125461, 0.064597, 0.035431, 0.046037, 0.052632, 0.316667, 0.038042, 0.047405, 0.466258, 0.081699]
Iteration 97: New valset pareto front scores: [0.5113, 0.48459, 0.493485, 0.448119, 0.505643, 0.494754, 0.273923, 0.498624, 0.541294, 0.531017, 0.116667, 0.431616, 0.087309, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.272227, 0.423795, 0.026556, 0.060606, 0.060606, 0.332169, 0.395161, 0.857994, 0.894232, 0.142258, 0.038636, 0.052392, 0.080119, 0.08192, 0.775357, 0.083333, 0.120918, 0.149856, 0.641937, 0.125461, 0.154422, 0.128283, 0.167928, 0.067434, 0.366667, 0.184265, 0.154278, 0.466258, 0.176471]
Iteration 97: Full valset pareto front score: 0.27116027083333333
Iteration 97: Updated valset pareto front programs: [{22}, {28}, {28}, {22}, {25}, {22}, {8, 2}, {0, 3, 7, 9, 12, 15, 16, 18, 19, 20, 23, 24}, {15}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}, {22}, {17}, {22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}, {22}, {22}, {22}, {22}, {22}, {1, 2, 4, 5, 6, 8, 9, 10, 11, 13, 14, 15, 16, 17, 20, 21, 24, 25, 26, 27, 28}, {22}, {2, 4, 8, 9, 10, 17, 19, 26, 28}, {22}, {22}, {10, 21}, {9}, {9}, {10, 4, 6, 15}, {24}, {1, 4, 5, 6, 10, 13, 14, 15, 16, 20, 21, 24, 25, 26, 27, 28}, {22}, {22}, {22}, {22}, {1, 4, 5, 6, 9, 10, 11, 13, 14, 16, 20, 21, 24, 25, 26, 27, 28}, {22}, {24}, {2, 4, 6, 8, 10, 15}, {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}, {5}, {5}, {5}, {22}, {4, 5, 6, 10, 15}, {5}, {5}, {1, 2, 4, 5, 6, 8, 10, 11, 13, 14, 15, 16, 20, 21, 22, 24, 25, 26, 27, 28}, {22}]
Iteration 97: Best valset aggregate score so far: 0.2627519583333333
Iteration 97: Best program as per aggregate score on train_val: 22
Iteration 97: Best program as per aggregate score on valset: 22
Iteration 97: Best score on valset: 0.2627519583333333
Iteration 97: Best score on train_val: 0.2627519583333333
Iteration 97: Linear pareto front program index: 22
Iteration 97: New program candidate index: 28
Iteration 98: Selected program 22 score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpkzcfk4uy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5nz82_60.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpw3mlmazw.pickle

Iteration 98: Proposed new text for program: from collections import OrderedDict

# Global Metadata for S3-FIFO
# ---------------------------
# m_small_q: OrderedDict (FIFO). Represents the "Small" queue (S).
#            Contains new items under probation.
# m_main_q:  OrderedDict (FIFO). Represents the "Main" queue (M).
#            Contains items that have proven their frequency.
# m_ghost_q: OrderedDict (FIFO). Represents the "Ghost" queue (G).
#            Tracks keys of recently evicted items to distinguish recurrence from newness.
# m_freq:    Dict. Tracks access frequency (hit bits) for resident items.

m_small_q = OrderedDict()
m_main_q = OrderedDict()
m_ghost_q = OrderedDict()
m_freq = {}

# Track total size in bytes for each queue to handle variable object sizes accurately
m_small_size = 0
m_main_size = 0

# Configuration
# 10% of capacity is reserved for the probationary Small Queue.
# This allows the Main Queue to hold 90% of the working set.
SMALL_RATIO = 0.1

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction using S3-FIFO logic.
    Strategy:
    1. Prefer evicting from Small Queue if it exceeds its size budget OR if Main Queue is empty.
       - If the candidate at the head of Small Queue was accessed: Promote to Main Queue.
       - Else: Evict it.
    2. Fallback to Main Queue.
       - If the candidate at the head of Main Queue was accessed: Give "Second Chance" (Reinsert at tail).
       - Else: Evict it.
    '''
    global m_small_q, m_main_q, m_ghost_q, m_freq, m_small_size, m_main_size
    
    capacity = cache_snapshot.capacity
    target_small_cap = capacity * SMALL_RATIO
    
    while True:
        # Decision: Evict from Small (S) or Main (M)?
        # We target S if it's too big, or if M is empty (bootstrapping).
        if m_small_q and (m_small_size > target_small_cap or not m_main_q):
            # --- Process Small Queue ---
            candidate_key = next(iter(m_small_q))
            
            # Lazy Promotion Check
            if m_freq.get(candidate_key, 0) > 0:
                # Item was hit while in S. It's not a scan. Promote to M.
                # Retrieve object reference to get size
                obj_ref = cache_snapshot.cache[candidate_key]
                size = obj_ref.size
                
                # Move from S to M
                del m_small_q[candidate_key]
                m_small_size -= size
                
                m_main_q[candidate_key] = None
                m_main_size += size
                
                # Reset hit bit (it used its coupon)
                m_freq[candidate_key] = 0
                
                # Continue loop to find a real victim
                continue
            else:
                # Item was NOT hit. It's a cold item/scan. Evict it.
                return candidate_key
        
        elif m_main_q:
            # --- Process Main Queue ---
            candidate_key = next(iter(m_main_q))
            
            # Second Chance Check
            if m_freq.get(candidate_key, 0) > 0:
                # Item was hit. It's still hot. Reinsert at tail of M.
                m_main_q.move_to_end(candidate_key)
                
                # Reset hit bit
                m_freq[candidate_key] = 0
                
                # Continue loop
                continue
            else:
                # Item became cold. Evict it.
                return candidate_key
        
        else:
            # Failsafe: Should not happen if cache is full.
            # Just return an arbitrary item to avoid infinite loops if state is corrupted.
            return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    Increment frequency counter (capped at small integer).
    Note: We do NOT move items here. S3-FIFO does lazy movement during eviction.
    This keeps hit processing O(1) and very fast.
    '''
    global m_freq
    key = obj.key
    # Cap at 3. We really only care if it's > 0, but counters help robustness.
    m_freq[key] = min(m_freq.get(key, 0) + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    Determine if this is a "new" object or a "returning" object (Ghost Hit).
    '''
    global m_small_q, m_main_q, m_ghost_q, m_freq, m_small_size, m_main_size
    
    key = obj.key
    m_freq[key] = 0
    
    if key in m_ghost_q:
        # Ghost Hit: We saw this recently evicted. It's likely a larger loop or working set.
        # Promote directly to Main Queue (M).
        m_main_q[key] = None
        m_main_size += obj.size
        
        # Remove from Ghost history
        del m_ghost_q[key]
    else:
        # First time seeing this (or forgotten).
        # Insert into Small Queue (S) for probation.
        m_small_q[key] = None
        m_small_size += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Clean up queues and Add to Ghost Queue history.
    '''
    global m_small_q, m_main_q, m_ghost_q, m_freq, m_small_size, m_main_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    # 1. Remove from Resident Queues and update sizes
    if key in m_small_q:
        del m_small_q[key]
        m_small_size -= size
    elif key in m_main_q:
        del m_main_q[key]
        m_main_size -= size
    
    # 2. Clear frequency tracking
    if key in m_freq:
        del m_freq[key]
        
    # 3. Add to Ghost Queue
    m_ghost_q[key] = None
    
    # 4. Cap Ghost Queue Size
    # To prevent metadata explosion, limit ghost entries roughly to the number of items in cache.
    # Since we don't have a hard count limit, we use a dynamic heuristic.
    current_cache_count = len(cache_snapshot.cache)
    if len(m_ghost_q) > current_cache_count + 100:
        m_ghost_q.popitem(last=False) # Remove oldest ghost
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7pvof109.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_sla73o7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpadsd48qj.pickle

Iteration 98: New subsample score 0.9218839999999999 is not better than old score 0.991541, skipping
Iteration 99: Selected program 9 score: 0.2466894583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpu90rfb3p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp6qq2ovhr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0tu7urpy.pickle

Iteration 99: Proposed new text for program: import collections

# Metadata storage
# 1. m_access_time: key -> int (Last access timestamp)
# 2. m_recency_state: key -> str ('LIR' or 'HIR')
# 3. m_stack_s: collections.OrderedDict
#    - Maps key -> access_time. Represents the LIRS Stack S.
#    - Used to determine recency and inter-reference recency (IRR).
#    - Bottom of stack is oldest.
# 4. m_queue_q: collections.OrderedDict
#    - Maps key -> access_time. Represents Queue Q.
#    - Stores resident HIR items to define eviction candidates.
#    - Bottom of queue is the eviction candidate.
# 5. m_lir_count: int (Counter for resident LIR items)

m_access_time = dict()
m_recency_state = dict()
m_stack_s = collections.OrderedDict()
m_queue_q = collections.OrderedDict()
m_lir_count = 0

# Constants
# Target ratio of LIR items (Protected) vs HIR items (Probationary).
# 99% LIR maximizes the stable working set, while 1% HIR allows filtering of one-time scans.
LIR_CAPACITY_RATIO = 0.99

def evict(cache_snapshot, obj):
    '''
    LIRS Eviction Policy.
    
    Goal: Evict a Resident HIR item.
    
    Logic:
    1. The primary candidate for eviction is the HIR item at the front (oldest) of Queue Q.
    2. If Q is empty (rare), we fall back to evicting an LIR item, effectively treating the
       cache as LRU for that moment.
    '''
    global m_queue_q, m_recency_state
    
    # Prune Q if necessary to ensure it contains valid candidates
    # (Though logic elsewhere should keep it clean)
    
    if m_queue_q:
        # The bottom of Queue Q is the first item in the OrderedDict
        victim_key, _ = next(iter(m_queue_q.items()))
        return victim_key
    
    # Fallback: If no HIR items in Q, evict the bottom of Stack S (LRU LIR)
    # This happens if the cache is full of LIRs.
    if m_stack_s:
        # Iterate to find the first resident item in stack S
        for key in m_stack_s:
             if key in cache_snapshot.cache:
                 return key

    # Absolute fallback (should not be reached if cache is not empty)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If LIR: 
       - Move to top of Stack S.
       - Prune bottom of Stack S if necessary.
    2. If HIR:
       - If inside Stack S: Becomes LIR (Hot). Move to top of Stack S. Remove from Queue Q.
       - If NOT inside Stack S: Remains HIR. Move to top of Queue Q. Add to top of Stack S.
    '''
    global m_access_time, m_recency_state, m_stack_s, m_queue_q, m_lir_count
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    state = m_recency_state.get(key)
    
    if state == 'LIR':
        # LIR item accessed: move to top of Stack S
        if key in m_stack_s:
            del m_stack_s[key] # Remove from current position
        m_stack_s[key] = current_time # Re-insert at top
        
        # Prune the stack bottom to ensure it remains an LIR item
        _prune_stack_bottom(cache_snapshot)
        
    elif state == 'HIR':
        # HIR item accessed
        is_in_stack = (key in m_stack_s)
        
        if is_in_stack:
            # It was in the stack (recency < IRR of some LIR), so it becomes LIR
            m_recency_state[key] = 'LIR'
            m_lir_count += 1
            
            # Remove from Queue Q as it is no longer an eviction candidate
            if key in m_queue_q:
                del m_queue_q[key]
            
            # Move to top of Stack S
            del m_stack_s[key]
            m_stack_s[key] = current_time
            
            # We added a new LIR, check if we need to demote another LIR
            _enforce_lir_capacity(cache_snapshot)
            _prune_stack_bottom(cache_snapshot)
            
        else:
            # Not in stack (Cold access or very old metadata)
            # Stays HIR.
            # Push to Stack S (it has a new recency)
            m_stack_s[key] = current_time
            
            # Update position in Queue Q (it is now the MRU of the HIRs)
            if key in m_queue_q:
                del m_queue_q[key]
            m_queue_q[key] = current_time

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cold Miss):
    1. Always insert as HIR initially.
    2. Add to top of Stack S.
    3. Add to top of Queue Q.
    '''
    global m_access_time, m_recency_state, m_stack_s, m_queue_q
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    # New items start as HIR
    m_recency_state[key] = 'HIR'
    
    # Add to Stack S
    m_stack_s[key] = current_time
    
    # Add to Queue Q
    m_queue_q[key] = current_time
    
    # Note: We don't check for LIR promotion here usually. 
    # Even if it was a non-resident HIR, standard LIRS treats a miss as 
    # bringing it in as HIR first. It becomes LIR on *next* access if still in stack.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from Queue Q.
    2. Decrease LIR count if applicable (though we usually evict HIRs).
    3. Remove from Recency State.
    4. Note: We DO NOT remove from Stack S immediately. This constitutes the 
       "Non-Resident HIR" metadata that helps LIRS detect looping patterns.
    '''
    global m_queue_q, m_recency_state, m_lir_count, m_access_time, m_stack_s
    
    key = evicted_obj.key
    
    if key in m_queue_q:
        del m_queue_q[key]
        
    if m_recency_state.get(key) == 'LIR':
        m_lir_count -= 1
        
    if key in m_recency_state:
        del m_recency_state[key]
        
    if key in m_access_time:
        del m_access_time[key]
    
    # Limit Stack S size to prevent unbounded metadata growth
    # A common heuristic is 2x to 3x cache capacity
    max_stack_size = 4000 # Heuristic, or based on capacity
    if len(m_stack_s) > max_stack_size:
        # Trim oldest from stack S if it's not an LIR (which shouldn't happen due to pruning, but good for safety)
        # or simply trim the absolute oldest to cap memory.
        oldest_key, _ = next(iter(m_stack_s.items()))
        # Only remove if not resident LIR (Resident LIRs define the stack bottom)
        # But for simple size capping, we just pop.
        if oldest_key not in cache_snapshot.cache:
            m_stack_s.popitem(last=False)

def _prune_stack_bottom(cache_snapshot):
    '''
    LIRS Pruning:
    The bottom of Stack S determines the maximum recency allowed for an LIR block.
    If the bottom of the stack is NOT an LIR block, it is removed.
    This effectively removes "Non-Resident HIR" blocks that are older than the oldest LIR.
    '''
    global m_stack_s, m_recency_state
    
    while m_stack_s:
        # Peek at the bottom (first item)
        bottom_key, _ = next(iter(m_stack_s.items()))
        
        # If it is an LIR item, the stack is valid. Stop pruning.
        if m_recency_state.get(bottom_key) == 'LIR':
            break
            
        # If it is HIR (Resident or Non-Resident), remove it from Stack S.
        del m_stack_s[bottom_key]

def _enforce_lir_capacity(cache_snapshot):
    '''
    If LIR count exceeds target, demote the LIR item at the bottom of Stack S to HIR.
    '''
    global m_stack_s, m_recency_state, m_lir_count, m_queue_q
    
    # Calculate target LIR count based on object count approximation.
    # While cache is bytes, LIRS logic usually operates on block counts.
    # We estimate capacity in items based on current cache population.
    total_items = len(cache_snapshot.cache)
    target_lirs = int(total_items * LIR_CAPACITY_RATIO)
    
    # If cache is very small, allow at least 1 LIR
    if target_lirs < 1 and total_items > 0:
        target_lirs = 1

    while m_lir_count > target_lirs:
        # Find the LIR at the bottom of the stack.
        # Thanks to _prune_stack_bottom, the bottom of Stack S IS the LRU LIR.
        victim_lir, access_time = next(iter(m_stack_s.items()))
        
        # Demote to HIR
        m_recency_state[victim_lir] = 'HIR'
        m_lir_count -= 1
        
        # It is now an HIR item, so it belongs in Queue Q for eviction candidacy
        m_queue_q[victim_lir] = access_time
        
        # Prune stack again, because the bottom is now HIR (the one we just demoted)
        _prune_stack_bottom(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph5jui5zw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp734zq6nu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnc6mhyji.pickle

Iteration 99: New subsample score 1.142776 is not better than old score 1.2883470000000001, skipping
Iteration 100: Selected program 24 score: 0.24984599999999993
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpnhmgett4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpezx2n4_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpc3v1219a.pickle

Iteration 100: Proposed new text for program: from collections import OrderedDict

# ==============================================================================
# Metadata Storage: Adaptive Replacement Cache (ARC) Structures
# ==============================================================================
# T1: Recent cache entries (Probation). Key -> Size.
# T2: Frequent cache entries (Protected). Key -> Size.
# B1: Ghost entries for T1 (Recently evicted from T1). Key -> Size (only for accounting).
# B2: Ghost entries for T2 (Recently evicted from T2). Key -> Size.
#
# p: Target size for T1 (in bytes). Adaptive parameter.
#
# Note: In standard ARC, operations are O(1). We use OrderedDict for LRU behavior.
# LRU is at the 'left' (beginning), MRU is at the 'right' (end).

m_t1 = OrderedDict()
m_t2 = OrderedDict()
m_b1 = OrderedDict()
m_b2 = OrderedDict()

# Current sizes in bytes
m_t1_size = 0
m_t2_size = 0
m_b1_size = 0
m_b2_size = 0

# Adaptive target size for T1.
# Starts at 0, but effectively initializes to capacity/2 logic in the adaptive steps.
m_p = 0 

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction based on ARC logic.
    We compare the size of T1 against the adaptive target `p`.
    '''
    global m_t1, m_t2, m_t1_size, m_t2_size, m_b1, m_b2, m_p
    
    # Standard ARC replace logic adapted for variable object sizes:
    # If len(T1) > p, we evict from T1. Else evict from T2.
    # Specifically, if T1 is holding more bytes than its target 'p', it yields space.
    
    # We also check if B1 has hits. In standard ARC, the condition is:
    # if (T1 is not empty) and ((T1 size > p) or (hit in B2 and T1 size == p))
    # Simplified for bytes:
    
    # If T1 has items and is over its budget 'p', evict from T1 LRU
    if m_t1 and m_t1_size > m_p:
        return next(iter(m_t1))
    
    # Otherwise, if T2 has items, evict from T2 LRU
    if m_t2:
        return next(iter(m_t2))
        
    # Fallback if T2 is empty (implies T1 is huge or p is huge), evict T1
    if m_t1:
        return next(iter(m_t1))
        
    # Safety fallback
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles Cache Hit.
    ARC Logic:
    - If in T1 or T2, move to MRU of T2.
    '''
    global m_t1, m_t2, m_t1_size, m_t2_size
    
    key = obj.key
    size = obj.size
    
    if key in m_t1:
        # Move from T1 (Recent) to T2 (Frequent)
        del m_t1[key]
        m_t1_size -= size
        
        m_t2[key] = size
        m_t2_size += size
        m_t2.move_to_end(key)
        
    elif key in m_t2:
        # Already in T2, just update recency
        m_t2.move_to_end(key)

def update_after_insert(cache_snapshot, obj):
    '''
    Handles Cache Miss (Insert).
    ARC Logic handles adaptation of 'p' based on Ghost hits (B1/B2).
    '''
    global m_t1, m_t2, m_b1, m_b2
    global m_t1_size, m_t2_size, m_b1_size, m_b2_size, m_p
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity

    # 1. Check Ghost Lists (Adaptation)
    if key in m_b1:
        # Hit in B1 (Recency Ghost): We should have kept T1 larger.
        # Increase p.
        # Delta = 1 if |B1| >= |B2|, else |B2|/|B1|. Adapted for bytes.
        delta = size # Default step
        if m_b1_size >= m_b2_size:
            delta = size
        else:
            # Scale based on ratio, capped slightly to prevent massive jumps
            ratio = m_b2_size / max(1, m_b1_size)
            delta = size * ratio
            
        m_p = min(capacity, m_p + delta)
        
        # Remove from B1 and insert into T2 (promoted because it was seen again)
        del m_b1[key]
        m_b1_size -= size
        
        # In ARC, a hit in B1 means it moves to T2 (Frequent)
        m_t2[key] = size
        m_t2_size += size

    elif key in m_b2:
        # Hit in B2 (Frequency Ghost): We should have kept T2 larger.
        # Decrease p (which effectively increases space for T2).
        delta = size
        if m_b2_size >= m_b1_size:
            delta = size
        else:
            ratio = m_b1_size / max(1, m_b2_size)
            delta = size * ratio
            
        m_p = max(0, m_p - delta)
        
        # Remove from B2 and insert into T2
        del m_b2[key]
        m_b2_size -= size
        
        m_t2[key] = size
        m_t2_size += size

    else:
        # 2. Total Miss (Not in T1, T2, B1, B2)
        # Insert into T1 (New/Recent)
        m_t1[key] = size
        m_t1_size += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles metadata updates after an object is physically evicted from cache.
    In ARC, the evicted object moves to the corresponding Ghost list (B1 or B2).
    '''
    global m_t1, m_t2, m_b1, m_b2
    global m_t1_size, m_t2_size, m_b1_size, m_b2_size
    
    key = evicted_obj.key
    size = evicted_obj.size
    capacity = cache_snapshot.capacity

    # Identify where it came from and move to ghost
    if key in m_t1:
        del m_t1[key]
        m_t1_size -= size
        
        # Add to B1 (Ghost of T1)
        m_b1[key] = size
        m_b1_size += size
        
    elif key in m_t2:
        del m_t2[key]
        m_t2_size -= size
        
        # Add to B2 (Ghost of T2)
        m_b2[key] = size
        m_b2_size += size

    # 3. Maintain Ghost List sizes
    # ARC typically keeps |T1|+|B1| <= C and |T2|+|B2| <= 2C.
    # Since we deal with bytes, we simply clamp the ghost lists to not grow indefinitely.
    # A safe heuristic is max(B1) + max(T1) approx capacity.
    
    # Trim B1 if too large
    while m_b1_size > capacity and m_b1:
        k, s = m_b1.popitem(last=False) # Remove LRU
        m_b1_size -= s
        
    # Trim B2 if too large
    while m_b2_size > capacity and m_b2:
        k, s = m_b2.popitem(last=False) # Remove LRU
        m_b2_size -= s
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpxhcpmat0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp7_3sdbl9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoeyw2pp9.pickle

Iteration 100: New subsample score 0.26193900000000003 is not better than old score 0.305343, skipping
Iteration 101: Selected program 28 score: 0.2415486041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpggxx23bw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpzll9d1k8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp9z9_xtvs.pickle

Iteration 101: Proposed new text for program: # GDSF (Greedy Dual-Size Frequency) Global State

# Dictionary to store metadata for cached items
# Key: object key
# Value: dict {'freq': int, 'priority': float, 'size': int}
meta_data = dict()

# Global inflation value (L) acts as the "aging" baseline
# It represents the priority of the most recently evicted item.
L = 0.0

def evict(cache_snapshot, obj):
    """
    Decides which object to evict based on the lowest GDSF priority score.
    Algorithm:
    1. Identify the object in the cache with the minimum priority value.
    2. Update the global aging factor L to this minimum priority.
    3. Return the key of the victim.
    """
    global meta_data, L
    
    # If cache is empty, nothing to evict (should not happen based on context)
    if not cache_snapshot.cache:
        return None

    # Find the object with the lowest priority in the current cache.
    # We iterate over cache_snapshot.cache to ensure we only consider currently resident items.
    # Note: We use a default priority of L for any item missing metadata (defensive fallback).
    
    victim_key = None
    min_priority = float('inf')
    
    # Optimization: A linear scan is O(N). For very large caches, a heap is preferred,
    # but given the read-only constraints and the need to update priorities on Hits,
    # a scan ensures absolute correctness for the GDSF logic.
    for key in cache_snapshot.cache:
        if key in meta_data:
            p = meta_data[key]['priority']
        else:
            # Fallback for sync issues: treat as a new item
            p = L 
        
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # If we couldn't find a victim (e.g., empty loop), fallback to arbitrary
    if victim_key is None:
        return next(iter(cache_snapshot.cache))

    # GDSF Aging Update:
    # The system "ages" by raising the water level (L) to the evicted item's priority.
    # This means future insertions/hits will start from a higher baseline,
    # implicitly lowering the relative priority of older, untouched items.
    L = min_priority
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    Called when obj is found in the cache.
    Action:
    1. Increment Frequency.
    2. Recalculate Priority using the current L (restoring Recency).
    """
    global meta_data, L
    key = obj.key
    
    if key not in meta_data:
        # Re-initialize if missing
        meta_data[key] = {
            'freq': 1,
            'size': obj.size,
            'priority': L + (1.0 / float(obj.size))
        }
    else:
        entry = meta_data[key]
        entry['freq'] += 1
        
        # GDSF Hit Formula: Priority = Current_L + (Frequency / Size)
        # Using Current_L brings the item to the "front" of time (Recency).
        # Frequency/Size accounts for popularity vs cost.
        entry['priority'] = L + (float(entry['freq']) / float(entry['size']))

def update_after_insert(cache_snapshot, obj):
    """
    Called after a new object is inserted into the cache.
    Action:
    1. Initialize metadata with Frequency = 1.
    2. Calculate initial Priority.
    """
    global meta_data, L
    key = obj.key
    
    # GDSF Insert Formula: Priority = Current_L + (1 / Size)
    # New items start above the "water level" L by a margin determined by their size.
    # Large items get a very small margin (likely to be evicted soon if not hit again).
    priority = L + (1.0 / float(obj.size))
    
    meta_data[key] = {
        'freq': 1,
        'size': obj.size,
        'priority': priority
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Called after an object is evicted.
    Action:
    1. Clean up metadata to prevent memory leaks.
    """
    global meta_data
    key = evicted_obj.key
    
    if key in meta_data:
        del meta_data[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpp9hjg_17.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp5agbmqf4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2833qp12.pickle

Iteration 101: New subsample score 1.001515 is not better than old score 1.144223, skipping
Best program from optimization: from collections import OrderedDict

# Metadata Storage
# 1. m_stack_s: OrderedDict (key -> None).
#    Represents the LIRS Stack S (History + LIRs). 
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed.
# 2. m_queue_q: OrderedDict (key -> None).
#    Represents the Resident HIR Queue Q.
#    - Top (Right): Most Recently Accessed.
#    - Bottom (Left): Least Recently Accessed (Candidate for eviction).
# 3. m_state: dict (key -> 'LIR' or 'HIR'). 
#    Tracks the status of resident items. If a key is in Stack S but not m_state, it is a Non-Resident HIR.
# 4. m_lir_size: int. 
#    Tracks total size of current LIR items to manage capacity dynamically.

m_stack_s = OrderedDict()
m_queue_q = OrderedDict()
m_state = dict()
m_lir_size = 0

# Configuration
# Reserve ~1% of cache for HIR items to act as a filter for scans.
# This maximizes the space for the LIR working set.
LIR_TARGET_RATIO = 0.99

def evict(cache_snapshot, obj):
    '''
    Determines the victim for eviction.
    Strategy: 
    1. Evict the resident HIR item at the front of Queue Q (oldest).
    2. If Q is empty (all LIR), evict the LIR item at the bottom of Stack S.
    '''
    global m_queue_q, m_stack_s
    
    # 1. Try to evict from HIR Queue (Resident HIRs)
    if m_queue_q:
        victim_key = next(iter(m_queue_q))
        return victim_key
        
    # 2. Fallback: If Queue Q is empty, the cache is filled entirely with LIRs.
    # The bottom of Stack S is guaranteed to be an LIR item due to pruning.
    if m_stack_s:
        victim_key = next(iter(m_stack_s))
        return victim_key

    # 3. Last resort (should not be reached)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Hit.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    state = m_state.get(key, 'HIR')
    
    if state == 'LIR':
        # LIR Case:
        # Move to top of Stack S (Recency update)
        if key in m_stack_s:
            m_stack_s.move_to_end(key)
        else:
            # Should not happen for resident LIR, but safe fallback
            m_stack_s[key] = None
        
        # Pruning is vital here: moving an item to the top might reveal 
        # a non-LIR item at the bottom of the stack.
        _prune_stack()
        
    elif state == 'HIR':
        # HIR Case:
        if key in m_stack_s:
            # HIR hit inside Stack S -> Hot access (within LIR reuse distance).
            # Promote to LIR.
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            
            # Remove from HIR Queue Q
            if key in m_queue_q:
                del m_queue_q[key]
            
            # Move to top of Stack S
            m_stack_s.move_to_end(key)
            
            # Enforce LIR capacity (demote others if we grew too big)
            _enforce_lir_capacity(cache_snapshot)
        else:
            # HIR hit outside Stack S -> Cold access (Resident but old).
            # Stays HIR, but moves to top of S and end of Q (fresh chance).
            m_stack_s[key] = None
            if key in m_queue_q:
                m_queue_q.move_to_end(key)
            else:
                m_queue_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    Handles metadata updates on Cache Miss (Insert).
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # 1. Check if this key exists in Stack S (Non-Resident History Hit)
    if key in m_stack_s:
        # It was evicted recently but is still in our "Hot" history scope.
        # Restore as LIR.
        m_state[key] = 'LIR'
        m_lir_size += obj.size
        m_stack_s.move_to_end(key)
        _enforce_lir_capacity(cache_snapshot)
    else:
        # 2. Cold Miss.
        # Bootstrapping optimization: If LIR set is small (cache filling up),
        # treat new items as LIRs to populate the stack history quickly.
        # Otherwise, standard LIRS logic treats new items as HIR.
        if m_lir_size + obj.size <= capacity * LIR_TARGET_RATIO:
            m_state[key] = 'LIR'
            m_lir_size += obj.size
            m_stack_s[key] = None
        else:
            # Start as HIR
            m_state[key] = 'HIR'
            m_stack_s[key] = None
            m_queue_q[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Handles cleanup after eviction.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    key = evicted_obj.key
    
    # 1. Remove from Queue Q
    if key in m_queue_q:
        del m_queue_q[key]
        
    # 2. Update State and Size
    if key in m_state:
        if m_state[key] == 'LIR':
            m_lir_size -= evicted_obj.size
        del m_state[key]
        
    # 3. Stack S Maintenance
    # If the evicted item was in Stack S, it stays there! 
    # It becomes a "Non-Resident HIR" entry, providing history for future hits.
    # However, if it sits at the bottom, _prune_stack will remove it.
    _prune_stack()

def _prune_stack():
    '''
    Ensures the bottom of Stack S is always an LIR item.
    Removes HIR items (resident or non-resident) from the bottom.
    '''
    global m_stack_s, m_state
    
    while m_stack_s:
        bottom_key = next(iter(m_stack_s))
        
        # If the bottom item is LIR and Resident, the stack is valid.
        if bottom_key in m_state and m_state[bottom_key] == 'LIR':
            break
            
        # Otherwise, the bottom is HIR (Resident or Non-Resident).
        # It is no longer useful for defining the LIR stack distance.
        m_stack_s.popitem(last=False)

def _enforce_lir_capacity(cache_snapshot):
    '''
    Prevents LIR set from consuming too much cache.
    Demotes the LRU LIR (bottom of S) to HIR if budget exceeded.
    '''
    global m_stack_s, m_queue_q, m_state, m_lir_size
    
    target_capacity = cache_snapshot.capacity * LIR_TARGET_RATIO
    
    while m_lir_size > target_capacity and m_stack_s:
        # The LIR to demote is at the bottom of Stack S
        victim_key = next(iter(m_stack_s))
        
        # Verify it is LIR
        if m_state.get(victim_key) == 'LIR':
            # Demote LIR -> HIR
            m_state[victim_key] = 'HIR'
            
            # Reduce LIR size. Since it is LIR, it must be in cache.
            if victim_key in cache_snapshot.cache:
                m_lir_size -= cache_snapshot.cache[victim_key].size
            
            # Move to Queue Q (Resident HIR)
            m_queue_q[victim_key] = None
            
        # Trigger pruning to remove the newly demoted HIR from bottom of S
        _prune_stack()
Best program validation score: 0.2627519583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpsobv0ore.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_7fjdzg0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmplwaspdd_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmprgq933jx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpuqe_dzaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpeju3gdpp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp_ls2236k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpo8kx_a4e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbok2rn3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp76knff1v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2j_cu7n5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2cyotsab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp0_ff91ir.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpetst02mf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpyky1qwgz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpaxn4ld_l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpawzkprng.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvt_jzwhv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt4m1euza.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp2vqmf23z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpjkfecmfc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbipunzbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph9yy7pf2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpwh92if_1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpepe3r72l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpddbmo7tb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmptfgza0n6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpixiw_qg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpgy6owjen.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpppnksp1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpvcem47i2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpt3jkvqqu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphz4wkzru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpby3b2uuq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmph270ul_q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpoc41ibwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpndrh45zd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpytu3zh86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmphcamcp6h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp1l62p0v0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqkgyvsu0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpe2v8silf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpmjt2m8j6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpb_sakrp9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpfv15jfck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpqb1r2x11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmpbttmed1o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_1/tmp/tmp18tg6dvl.pickle

Optimized program test score (avg over test): 0.2627519583333333
