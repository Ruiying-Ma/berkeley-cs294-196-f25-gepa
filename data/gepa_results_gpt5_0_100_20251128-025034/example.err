2025-11-27 17:56:13,652 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
m_key_timestamp = dict()

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_ts = min(m_key_timestamp.values())
    candid_obj_keys = list(key for key in cache_snapshot.cache if m_key_timestamp[key] == min_ts)
    candid_obj_key = candid_obj_keys[0]
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    assert evicted_obj.key in m_key_timestamp
    m_key_timestamp.pop(evicted_obj.key)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 43
- Hit Rate: 0.05

Example 2:
- Trace ID: 1
- Hit Rate: 0.418

Example 3:
- Trace ID: 28
- Hit Rate: 0.0201

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m17:56:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 17:56:13,663 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m17:57:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 17:57:33,820 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 17:57:33,822 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-Frequency (GDSF) cache eviction algorithm
# - Size-aware, frequency-aware, and self-aging through a global inflation factor.
# - Maintains compact per-resident metadata only.

# Global metadata
m_freq = dict()          # key -> int (bounded frequency)
m_priority = dict()      # key -> float (H score used for eviction)
m_last_access = dict()   # key -> int (timestamp, for tie-breaking)
m_L = 0.0                # global "inflation" factor (aging baseline)

# Tunables
_MAX_FREQ = 32           # cap to prevent runaway counters
_EPS = 1e-9              # numeric safety for divisions

def _compute_priority(freq, size, L):
    # H = L + freq / size
    # size is positive by spec; use max(size, 1) safeguard
    return float(L) + float(freq) / max(int(size), 1)

def _ensure_metadata_for_key(cache_snapshot, key):
    # Lazily initialize metadata for any resident key missing entries.
    # This keeps algorithm robust if the environment preloads items or after resets.
    if key not in m_freq:
        m_freq[key] = 1
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_priority:
        obj = cache_snapshot.cache.get(key)
        size = obj.size if obj is not None else 1
        m_priority[key] = _compute_priority(m_freq[key], size, m_L)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using GDSF:
    - Evict the resident item with the smallest priority H.
    - Tie-break by oldest last access time (LRU-ish).
    '''
    # Ensure all resident keys have computed priority
    for key in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, key)

    # Find victim: min H; tie-break by oldest last_access
    victim_key = None
    victim_H = None
    victim_last_ts = None

    for key in cache_snapshot.cache:
        H = m_priority[key]
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_H = H
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment (bounded) frequency.
    - Recompute priority with current L.
    - Update last access time.
    '''
    key = obj.key
    # Initialize if missing (robustness)
    _ensure_metadata_for_key(cache_snapshot, key)

    # Update frequency (bounded) and recompute priority
    f = min(m_freq.get(key, 1) + 1, _MAX_FREQ)
    m_freq[key] = f
    m_last_access[key] = cache_snapshot.access_count

    # Recompute priority using current L and resident size
    resident_obj = cache_snapshot.cache.get(key, obj)
    m_priority[key] = _compute_priority(f, resident_obj.size, m_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (typically after a miss):
    - Initialize frequency to 1 (first access).
    - Initialize priority to L + 1/size.
    - Set last access time.
    '''
    key = obj.key
    # Start fresh for resident metadata
    m_freq[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_priority[key] = _compute_priority(1, obj.size, m_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Set L to the H of the evicted object (aging step).
    - Remove evicted object's resident metadata.
    '''
    global m_L
    ev_key = evicted_obj.key

    # Ensure we can compute evicted priority even if missing
    f = m_freq.get(ev_key, 1)
    H_evicted = m_priority.get(ev_key, _compute_priority(f, evicted_obj.size, m_L))

    # Age the system: raise L to the evicted priority
    m_L = H_evicted

    # Drop metadata for the evicted resident item
    m_freq.pop(ev_key, None)
    m_priority.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 18:00:20,069 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# GreedyDual-Size-Frequency (GDSF) cache eviction algorithm
# - Size-aware, frequency-aware, and self-aging through a global inflation factor.
# - Maintains compact per-resident metadata only.

# Global metadata
m_freq = dict()          # key -> int (bounded frequency)
m_priority = dict()      # key -> float (H score used for eviction)
m_last_access = dict()   # key -> int (timestamp, for tie-breaking)
m_L = 0.0                # global "inflation" factor (aging baseline)

# Tunables
_MAX_FREQ = 32           # cap to prevent runaway counters
_EPS = 1e-9              # numeric safety for divisions

def _compute_priority(freq, size, L):
    # H = L + freq / size
    # size is positive by spec; use max(size, 1) safeguard
    return float(L) + float(freq) / max(int(size), 1)

def _ensure_metadata_for_key(cache_snapshot, key):
    # Lazily initialize metadata for any resident key missing entries.
    # This keeps algorithm robust if the environment preloads items or after resets.
    if key not in m_freq:
        m_freq[key] = 1
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_priority:
        obj = cache_snapshot.cache.get(key)
        size = obj.size if obj is not None else 1
        m_priority[key] = _compute_priority(m_freq[key], size, m_L)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using GDSF:
    - Evict the resident item with the smallest priority H.
    - Tie-break by oldest last access time (LRU-ish).
    '''
    # Ensure all resident keys have computed priority
    for key in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, key)

    # Find victim: min H; tie-break by oldest last_access
    victim_key = None
    victim_H = None
    victim_last_ts = None

    for key in cache_snapshot.cache:
        H = m_priority[key]
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_H = H
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment (bounded) frequency.
    - Recompute priority with current L.
    - Update last access time.
    '''
    key = obj.key
    # Initialize if missing (robustness)
    _ensure_metadata_for_key(cache_snapshot, key)

    # Update frequency (bounded) and recompute priority
    f = min(m_freq.get(key, 1) + 1, _MAX_FREQ)
    m_freq[key] = f
    m_last_access[key] = cache_snapshot.access_count

    # Recompute priority using current L and resident size
    resident_obj = cache_snapshot.cache.get(key, obj)
    m_priority[key] = _compute_priority(f, resident_obj.size, m_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (typically after a miss):
    - Initialize frequency to 1 (first access).
    - Initialize priority to L + 1/size.
    - Set last access time.
    '''
    key = obj.key
    # Start fresh for resident metadata
    m_freq[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_priority[key] = _compute_priority(1, obj.size, m_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Set L to the H of the evicted object (aging step).
    - Remove evicted object's resident metadata.
    '''
    global m_L
    ev_key = evicted_obj.key

    # Ensure we can compute evicted priority even if missing
    f = m_freq.get(ev_key, 1)
    H_evicted = m_priority.get(ev_key, _compute_priority(f, evicted_obj.size, m_L))

    # Age the system: raise L to the evicted priority
    m_L = H_evicted

    # Drop metadata for the evicted resident item
    m_freq.pop(ev_key, None)
    m_priority.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 14
- Hit Rate: 0.0197

Example 2:
- Trace ID: 36
- Hit Rate: 0.0092

Example 3:
- Trace ID: 12
- Hit Rate: 0.0239

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:00:20 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:00:20,071 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:01:54 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:01:54,366 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:01:54,367 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Segmented LRU with ARC-style adaptation (SLRU-ARC)
# - Two resident segments: Probation (recency) and Protected (recency of promoted items)
# - On hit in Probation: promote to Protected
# - On hit in Protected: refresh recency
# - Eviction victim: LRU in Probation; if empty, LRU in Protected
# - Two non-resident "ghost" sets remember recently evicted keys from each segment:
#     B1: keys evicted from Probation (recency-biased)
#     B2: keys evicted from Protected (frequency-biased)
# - An adaptive target p (count of items) controls the Protected segment size:
#     - If a miss-reinsert hits in B1, shrink Protected (favor recency)
#     - If a miss-reinsert hits in B2, grow Protected (favor frequency)
# - Size-agnostic for hit-rate maximization (avoid penalizing large popular objects)
#
# Complexity:
# - Metadata per resident key: segment label, last-access timestamp, small hit counter
# - Ghost sets are bounded and store only key->timestamp
# - All operations are O(number of resident keys) due to LRU scans (acceptable for this setting)

# Global metadata
m_seg = dict()          # key -> int; 0 = Probation, 1 = Protected
m_last = dict()         # key -> int; last access timestamp
m_hits = dict()         # key -> int; small per-key hit counter (tie-break aid)
g_B1 = dict()           # ghost set for Probation evictions: key -> timestamp
g_B2 = dict()           # ghost set for Protected evictions: key -> timestamp
m_p = 0                 # target size (by item count) of Protected segment

# Tunables
_MAX_HITS = 255         # small, bounded hit counter
_GHOST_MAX = 8192       # max total ghost entries across B1+B2

def _now(cache_snapshot):
    return int(cache_snapshot.access_count)

def _ensure_resident_defaults(cache_snapshot, key):
    # Initialize resident metadata if missing (robust to preloaded items)
    if key not in m_seg:
        m_seg[key] = 0  # default to Probation
    if key not in m_last:
        m_last[key] = 0
    if key not in m_hits:
        m_hits[key] = 0

def _resident_keys(cache_snapshot):
    return cache_snapshot.cache.keys()

def _count_in_segment(cache_snapshot, seg):
    cnt = 0
    for k in _resident_keys(cache_snapshot):
        if m_seg.get(k, 0) == seg:
            cnt += 1
    return cnt

def _find_lru_in_segment(cache_snapshot, seg):
    # Returns (key, ts) of least-recently-used in given segment; (None, None) if empty
    lru_key = None
    lru_ts = None
    for k in _resident_keys(cache_snapshot):
        if m_seg.get(k, 0) != seg:
            continue
        ts = m_last.get(k, 0)
        if lru_key is None or ts < lru_ts:
            lru_key = k
            lru_ts = ts
    return lru_key, lru_ts

def _demote_protected_lru_to_probation(cache_snapshot):
    # Demote oldest protected item to probation to respect target m_p
    key, _ = _find_lru_in_segment(cache_snapshot, 1)
    if key is not None:
        m_seg[key] = 0

def _enforce_protected_target(cache_snapshot):
    # Keep Protected count <= min(m_p, total resident count)
    global m_p
    total = len(cache_snapshot.cache)
    if total <= 0:
        m_p = 0
        return
    target = max(0, min(m_p, total))
    while _count_in_segment(cache_snapshot, 1) > target:
        _demote_protected_lru_to_probation(cache_snapshot)

def _trim_ghosts_if_needed():
    # Bound ghost memory: remove oldest entries across B1 and B2
    total = len(g_B1) + len(g_B2)
    if total <= _GHOST_MAX:
        return

    # Remove oldest items until within bound
    def _pop_oldest(d):
        if not d:
            return
        oldest_k = None
        oldest_ts = None
        for k, ts in d.items():
            if oldest_k is None or ts < oldest_ts:
                oldest_k, oldest_ts = k, ts
        if oldest_k is not None:
            d.pop(oldest_k, None)

    while len(g_B1) + len(g_B2) > _GHOST_MAX:
        # Prefer trimming the larger ghost set
        if len(g_B1) >= len(g_B2):
            _pop_oldest(g_B1)
        else:
            _pop_oldest(g_B2)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim:
    - Evict LRU in Probation if available.
    - Otherwise evict LRU in Protected.
    '''
    # Ensure resident defaults exist
    for key in _resident_keys(cache_snapshot):
        _ensure_resident_defaults(cache_snapshot, key)

    # Prefer LRU in Probation
    key, ts = _find_lru_in_segment(cache_snapshot, 0)
    if key is not None:
        return key

    # If Probation empty, evict LRU in Protected
    key, ts = _find_lru_in_segment(cache_snapshot, 1)
    return key  # may be None if cache empty, but framework shouldn't call in that case

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Update last access time and hit counter.
    - If in Probation, promote to Protected.
    - Enforce Protected target size (may demote oldest Protected back to Probation).
    '''
    key = obj.key
    _ensure_resident_defaults(cache_snapshot, key)

    # Update recency and hits
    m_last[key] = _now(cache_snapshot)
    m_hits[key] = min(m_hits.get(key, 0) + 1, _MAX_HITS)

    # Promote on first hit in Probation
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1  # move to Protected

    # Keep Protected at or below target
    _enforce_protected_target(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
    - Adapt target Protected size m_p using ghost hits (ARC-style):
        * If key in B1 (recently evicted from Probation), decrease m_p (favor recency).
        * If key in B2 (recently evicted from Protected), increase m_p (favor frequency).
    - Insert into Probation with fresh recency.
    - Enforce Protected target (in case promotions elsewhere pushed Protected over target).
    '''
    global m_p
    key = obj.key

    # ARC-style adaptation via ghost sets
    if key in g_B1:
        # Seen again after being evicted from Probation -> increase recency emphasis
        cur_total = max(1, len(cache_snapshot.cache))  # avoid div by zero
        m_p = max(0, m_p - 1)
        g_B1.pop(key, None)
    elif key in g_B2:
        # Seen again after being evicted from Protected -> increase frequency emphasis
        cur_total = max(1, len(cache_snapshot.cache))
        m_p = min(m_p + 1, cur_total)
        g_B2.pop(key, None)

    # Initialize resident state as Probation
    _ensure_resident_defaults(cache_snapshot, key)
    m_seg[key] = 0
    m_last[key] = _now(cache_snapshot)
    m_hits[key] = min(m_hits.get(key, 0) + 1, _MAX_HITS) if m_hits.get(key) else 1

    # Enforce Protected target if needed
    _enforce_protected_target(cache_snapshot)

    # Keep ghost sets bounded
    _trim_ghosts_if_needed()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Record the evicted key in the appropriate ghost set:
        * B1 if evicted from Probation
        * B2 if evicted from Protected
    - Remove resident metadata for the evicted key.
    - Bound ghost sets.
    '''
    ev_key = evicted_obj.key

    # Determine segment at eviction (default Probation if unknown)
    seg = m_seg.get(ev_key, 0)
    ts = _now(cache_snapshot)

    # Record in corresponding ghost set
    if seg == 0:
        g_B1[ev_key] = ts
    else:
        g_B2[ev_key] = ts

    # Drop resident metadata
    m_seg.pop(ev_key, None)
    m_last.pop(ev_key, None)
    m_hits.pop(ev_key, None)

    # Keep ghost sets bounded
    _trim_ghosts_if_needed()
```
2025-11-27 18:02:19,185 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# GreedyDual-Size-Frequency (GDSF) cache eviction algorithm
# - Size-aware, frequency-aware, and self-aging through a global inflation factor.
# - Maintains compact per-resident metadata only.

# Global metadata
m_freq = dict()          # key -> int (bounded frequency)
m_priority = dict()      # key -> float (H score used for eviction)
m_last_access = dict()   # key -> int (timestamp, for tie-breaking)
m_L = 0.0                # global "inflation" factor (aging baseline)

# Tunables
_MAX_FREQ = 32           # cap to prevent runaway counters
_EPS = 1e-9              # numeric safety for divisions

def _compute_priority(freq, size, L):
    # H = L + freq / size
    # size is positive by spec; use max(size, 1) safeguard
    return float(L) + float(freq) / max(int(size), 1)

def _ensure_metadata_for_key(cache_snapshot, key):
    # Lazily initialize metadata for any resident key missing entries.
    # This keeps algorithm robust if the environment preloads items or after resets.
    if key not in m_freq:
        m_freq[key] = 1
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_priority:
        obj = cache_snapshot.cache.get(key)
        size = obj.size if obj is not None else 1
        m_priority[key] = _compute_priority(m_freq[key], size, m_L)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using GDSF:
    - Evict the resident item with the smallest priority H.
    - Tie-break by oldest last access time (LRU-ish).
    '''
    # Ensure all resident keys have computed priority
    for key in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, key)

    # Find victim: min H; tie-break by oldest last_access
    victim_key = None
    victim_H = None
    victim_last_ts = None

    for key in cache_snapshot.cache:
        H = m_priority[key]
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_H = H
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment (bounded) frequency.
    - Recompute priority with current L.
    - Update last access time.
    '''
    key = obj.key
    # Initialize if missing (robustness)
    _ensure_metadata_for_key(cache_snapshot, key)

    # Update frequency (bounded) and recompute priority
    f = min(m_freq.get(key, 1) + 1, _MAX_FREQ)
    m_freq[key] = f
    m_last_access[key] = cache_snapshot.access_count

    # Recompute priority using current L and resident size
    resident_obj = cache_snapshot.cache.get(key, obj)
    m_priority[key] = _compute_priority(f, resident_obj.size, m_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (typically after a miss):
    - Initialize frequency to 1 (first access).
    - Initialize priority to L + 1/size.
    - Set last access time.
    '''
    key = obj.key
    # Start fresh for resident metadata
    m_freq[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_priority[key] = _compute_priority(1, obj.size, m_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Set L to the H of the evicted object (aging step).
    - Remove evicted object's resident metadata.
    '''
    global m_L
    ev_key = evicted_obj.key

    # Ensure we can compute evicted priority even if missing
    f = m_freq.get(ev_key, 1)
    H_evicted = m_priority.get(ev_key, _compute_priority(f, evicted_obj.size, m_L))

    # Age the system: raise L to the evicted priority
    m_L = H_evicted

    # Drop metadata for the evicted resident item
    m_freq.pop(ev_key, None)
    m_priority.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 0
- Hit Rate: 0.4573

Example 2:
- Trace ID: 27
- Hit Rate: 0.884

Example 3:
- Trace ID: 47
- Hit Rate: 0.0327

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:02:19 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:02:19,186 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:03:52 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:03:52,676 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:03:52,677 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```
2025-11-27 18:07:57,490 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 7
- Hit Rate: 0.4986

Example 2:
- Trace ID: 33
- Hit Rate: 0.7376

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:07:57 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:07:57,492 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:09:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:09:33,323 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:09:33,324 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Segmented, size-aware LRU (S-SLRU) with adaptive protected partition
# - Two resident segments:
#     P (probationary): new/recency-only objects
#     Q (protected): objects that have been hit at least once (frequency evidence)
# - Eviction policy:
#     Prefer evicting from P; if P is empty, demote the LRU from Q to P, then evict from P.
#     Within P, pick the object with the largest age/size^beta (oldest and larger first).
# - Adaptive partition:
#     Target bytes for Q (protected) are adapted online:
#       * Hit in P (promotion)  -> increase Q target (favor frequency)
#       * Hit in Q              -> decrease Q target (favor recency)
# - Metadata is resident-only and O(1) per key. No ghost lists.
#
# Advantages:
# - Strong scan resistance (streaming items stay in P and are evicted quickly)
# - Frequency-aware (a second access promotes to Q, making it harder to evict)
# - Size-aware eviction (prefers evicting larger, older items)
# - Online adaptation of recency vs. frequency balance

from math import pow

# Per-resident metadata
m_last_access = dict()   # key -> int, timestamp of last access
m_seg = dict()           # key -> 0 for P (probationary), 1 for Q (protected)

# Segment bookkeeping (bytes)
_g_bytes_P = 0
_g_bytes_Q = 0

# Adaptive target bytes for Q segment (protected)
_g_Q_target_bytes = 0
_g_target_inited = False

# Tunables
_SIZE_EXP = 0.80                   # beta: size penalty exponent
_ADAPT_STEP_FRAC = 0.05            # each adjust step = 5% of capacity (bounded by object size)
_EPS = 1e-12


def _init_target_if_needed(cache_snapshot):
    """Initialize Q target on first use to half of capacity."""
    global _g_target_inited, _g_Q_target_bytes
    if not _g_target_inited:
        _g_Q_target_bytes = int(0.5 * cache_snapshot.capacity)
        _g_target_inited = True


def _clamp_target(cache_snapshot):
    """Clamp Q target within [0, capacity]."""
    global _g_Q_target_bytes
    cap = max(1, int(cache_snapshot.capacity))
    if _g_Q_target_bytes < 0:
        _g_Q_target_bytes = 0
    elif _g_Q_target_bytes > cap:
        _g_Q_target_bytes = cap


def _adapt_step_bytes(cache_snapshot):
    """Compute adaptive step in bytes for partition adjustments."""
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(cap * _ADAPT_STEP_FRAC))


def _size_pow(size, exp):
    return pow(max(1, int(size)), exp)


def _ensure_key_known(cache_snapshot, key):
    """
    Ensure a resident key has segment and last_access metadata.
    If we see a resident key without a segment (e.g., engine initialized mid-run),
    default it to P and account its bytes to P.
    """
    global _g_bytes_P, _g_bytes_Q
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_seg:
        m_seg[key] = 0  # default to probationary
        robj = cache_snapshot.cache.get(key)
        if robj is not None:
            _g_bytes_P += int(robj.size)


def _find_lru_in_segment(cache_snapshot, seg_id):
    """Return (key, last_ts) of the LRU item in the given segment, or (None, None) if none."""
    lru_key = None
    lru_ts = None
    for k in cache_snapshot.cache:
        if m_seg.get(k, 0) != seg_id:
            continue
        ts = m_last_access.get(k, 0)
        if lru_key is None or ts < lru_ts:
            lru_key = k
            lru_ts = ts
    return lru_key, lru_ts


def _demote_one_from_Q(cache_snapshot):
    """
    Demote the LRU item from Q to P if available.
    Returns True if a demotion occurred.
    """
    global _g_bytes_P, _g_bytes_Q
    k, _ = _find_lru_in_segment(cache_snapshot, seg_id=1)
    if k is None:
        return False
    robj = cache_snapshot.cache.get(k)
    if robj is None:
        return False
    size = int(robj.size)
    m_seg[k] = 0
    _g_bytes_Q -= size
    _g_bytes_P += size
    return True


def _pick_victim_from_P(cache_snapshot, now_ts, beta):
    """
    Among P items, pick the one with maximal age/size^beta.
    This prefers evicting older and larger objects first.
    Returns victim key or None if P is empty.
    """
    victim_key = None
    best_score = None  # we maximize age/size^beta
    for k, robj in cache_snapshot.cache.items():
        if m_seg.get(k, 0) != 0:
            continue
        last = m_last_access.get(k, 0)
        age = max(1, now_ts - last)
        denom = _size_pow(robj.size, beta)
        score = float(age) / float(denom)
        if victim_key is None or score > best_score + _EPS:
            victim_key = k
            best_score = score
        elif best_score is not None and abs(score - best_score) <= _EPS:
            # tie-break: evict the older one
            if last < m_last_access.get(victim_key, 0):
                victim_key = k
                best_score = score
    return victim_key


def evict(cache_snapshot, obj):
    """
    Evict strategy:
    - Prefer evicting from P (probationary) using size-aware recency metric.
    - If P empty, demote LRU from Q to P until P has a victim or Q empty.
    - If both segments empty (shouldn't happen), fall back to global oldest/size-aware.
    - Slightly increase size-bias when inserting a very large object.
    """
    _init_target_if_needed(cache_snapshot)
    _clamp_target(cache_snapshot)

    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exist
    for k in cache_snapshot.cache:
        _ensure_key_known(cache_snapshot, k)

    # Adapt size penalty for very large incoming objects
    cap = max(1, int(cache_snapshot.capacity))
    incoming = obj.size if hasattr(obj, "size") else 0
    beta = _SIZE_EXP
    if incoming >= 0.2 * cap:
        beta = min(1.2, _SIZE_EXP + 0.3)  # more size-aware for very large insertions

    # Try to pick from P
    victim = _pick_victim_from_P(cache_snapshot, now_ts, beta)
    if victim is not None:
        return victim

    # If P empty, demote from Q until a P victim exists or Q depleted
    demoted = True
    while victim is None and demoted:
        demoted = _demote_one_from_Q(cache_snapshot)
        victim = _pick_victim_from_P(cache_snapshot, now_ts, beta)

    if victim is not None:
        return victim

    # Fallback: choose global worst by the same metric (should rarely happen)
    victim_key = None
    best_score = None
    for k, robj in cache_snapshot.cache.items():
        last = m_last_access.get(k, 0)
        age = max(1, now_ts - last)
        denom = _size_pow(robj.size, beta)
        score = float(age) / float(denom)
        if victim_key is None or score > best_score + _EPS:
            victim_key = k
            best_score = score
        elif best_score is not None and abs(score - best_score) <= _EPS:
            if last < m_last_access.get(victim_key, 0):
                victim_key = k
                best_score = score
    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update access time.
    - If hit in P, promote to Q and increase Q target (favor frequency).
    - If hit in Q, decrease Q target slightly (favor recency).
    - Keep Q within target by demoting LRU entries if Q exceeds target.
    """
    global _g_bytes_P, _g_bytes_Q, _g_Q_target_bytes

    _init_target_if_needed(cache_snapshot)
    _clamp_target(cache_snapshot)

    now_ts = cache_snapshot.access_count
    key = obj.key
    size = int(obj.size)

    # Ensure metadata
    _ensure_key_known(cache_snapshot, key)

    # Update last access time
    m_last_access[key] = now_ts

    step = _adapt_step_bytes(cache_snapshot)

    if m_seg.get(key, 0) == 0:
        # Promote P -> Q
        m_seg[key] = 1
        _g_bytes_P -= size
        _g_bytes_Q += size

        # Adapt: increase protected target (favor frequency)
        _g_Q_target_bytes = min(cache_snapshot.capacity, _g_Q_target_bytes + min(step, size))
    else:
        # Hit in Q: adapt by decreasing protected target (favor recency)
        _g_Q_target_bytes = max(0, _g_Q_target_bytes - min(step, size))

    _clamp_target(cache_snapshot)

    # Keep Q within target by demoting LRU entries if necessary
    # This also helps when many large items get promoted at once.
    while _g_bytes_Q > _g_Q_target_bytes:
        if not _demote_one_from_Q(cache_snapshot):
            break  # nothing to demote


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after miss):
    - Place new item into P with current timestamp (recency-first admission).
    - Update segment byte counters.
    - Initialize Q target on first use and clamp it.
    """
    global _g_bytes_P, _g_bytes_Q

    _init_target_if_needed(cache_snapshot)
    _clamp_target(cache_snapshot)

    now_ts = cache_snapshot.access_count
    key = obj.key
    size = int(obj.size)

    # Initialize metadata for new resident
    m_last_access[key] = now_ts
    m_seg[key] = 0  # probationary
    _g_bytes_P += size


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Remove metadata and adjust segment byte counters.
    """
    global _g_bytes_P, _g_bytes_Q

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_seg.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)

    if seg == 0:
        _g_bytes_P -= ev_size
    else:
        _g_bytes_Q -= ev_size

    # No need to adapt partition here; it adapts on hits and is clamped elsewhere.
```
2025-11-27 18:10:21,296 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 20
- Hit Rate: 0.3658

Example 2:
- Trace ID: 24
- Hit Rate: 0.2756

Example 3:
- Trace ID: 5
- Hit Rate: 0.4481

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:10:21 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:10:21,297 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:12:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:12:02,724 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:12:02,725 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-Frequency (GDSF) with resident-only metadata
# ------------------------------------------------------------
# Key ideas:
# - Each resident item maintains a priority key H = L + freq / size^beta
#   where:
#     * L is a global aging value (increases to the priority of last evicted item)
#     * freq counts hits on the resident item (insert starts at 1)
#     * size^beta penalizes larger objects (beta in [0,1]; closer to 1 penalizes more)
# - Eviction always removes the item with the smallest stored H. After eviction, L is
#   set to the evicted item's H (aging), naturally blending recency and frequency:
#     * Recency: Newer insertions get higher H due to larger current L.
#     * Frequency: Re-referencing an item increases freq and thus H.
# - Metadata is kept only for resident objects.
#
# Benefits:
# - Strong size-awareness (favoring small items while still protecting large but
#   frequently accessed items).
# - Natural aging prevents stale items from lingering even if they once got high freq.
# - Simple and fast: O(n) victim scan like the baseline; no global decay passes.

# Per-resident metadata
m_freq = dict()          # key -> int, resident hit count (inserted as 1)
m_pri = dict()           # key -> float, stored priority H = L_at_update + freq/size^beta
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Global aging state
_g_L = 0.0               # global aging value (priority of the most recently evicted item)

# Tunables
_SIZE_EXP = 0.85         # beta: 0 => no size penalty; 1 => strong size penalty
_MAX_FREQ = 1_000_000    # cap freq to avoid unbounded growth
_EPS = 1e-12


def _ensure_resident_metadata(cache_snapshot, key, size, now_ts):
    """Ensure resident metadata exist for key; used defensively."""
    if key not in m_freq:
        m_freq[key] = 1
        denom = max(int(size), 1) ** _SIZE_EXP
        m_pri[key] = _g_L + (m_freq[key] / float(denom))
        m_last_access[key] = now_ts
    else:
        # Ensure last_access exists
        if key not in m_last_access:
            m_last_access[key] = now_ts
        # Ensure priority exists
        if key not in m_pri:
            denom = max(int(size), 1) ** _SIZE_EXP
            m_pri[key] = _g_L + (m_freq[key] / float(denom))


def _priority_for(key):
    """Return stored priority H for a resident key."""
    return m_pri.get(key, 0.0)


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest stored priority H.
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Defensive: ensure metadata for all residents
    for k, robj in cache_snapshot.cache.items():
        _ensure_resident_metadata(cache_snapshot, k, robj.size, now_ts)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment resident frequency (capped).
    - Recompute and store H = L + freq / size^beta using current global L.
    - Update last_access timestamp.
    """
    global _g_L

    now_ts = cache_snapshot.access_count
    key = obj.key
    size = obj.size

    # Ensure resident metadata
    _ensure_resident_metadata(cache_snapshot, key, size, now_ts)

    # Update frequency and priority
    m_freq[key] = min(_MAX_FREQ, m_freq.get(key, 0) + 1)
    denom = max(int(size), 1) ** _SIZE_EXP
    m_pri[key] = _g_L + (m_freq[key] / float(denom))
    m_last_access[key] = now_ts


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize freq = 1 (first observed access).
    - Set H = L + 1 / size^beta so that newer insertions benefit from current aging.
    - Set last_access to now.
    """
    global _g_L

    now_ts = cache_snapshot.access_count
    key = obj.key
    size = obj.size

    m_freq[key] = 1
    denom = max(int(size), 1) ** _SIZE_EXP
    m_pri[key] = _g_L + (1.0 / float(denom))
    m_last_access[key] = now_ts


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Age the system: L := max(L, evicted_item_priority).
    - Drop resident metadata for the evicted key.
    """
    global _g_L

    ev_key = evicted_obj.key

    # Aging step: bump L to the evicted item's stored priority.
    ev_pri = m_pri.get(ev_key, None)
    if ev_pri is not None:
        if ev_pri > _g_L:
            _g_L = ev_pri

    # Drop metadata
    m_freq.pop(ev_key, None)
    m_pri.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 18:12:34,999 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 38
- Hit Rate: 0.1255

Example 2:
- Trace ID: 11
- Hit Rate: 0.3406

Example 3:
- Trace ID: 23
- Hit Rate: 0.0574

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:12:35 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:12:35,000 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:14:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:14:04,031 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:14:04,032 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size with adaptive size bias (GDSF-β)
#
# Key ideas:
# - Each resident object k maintains a priority H[k]
# - Eviction chooses the resident with the smallest H
# - On insert: H[k] = L + w(size), where L is a global "age" and w(size) = 1 / size^β
# - On hit: H[k] += w(size)
# - On eviction: L := H[victim] (aging); remove victim's metadata
#
# Properties:
# - Size-aware: larger objects get smaller weight increments
# - Recency/frequency blending: The aging term L grows to the most recently evicted priority,
#   effectively "decaying" old values and making recent/frequent objects survive
# - Scan-resistant: cold items start near L and are evicted quickly
# - Adaptive β: we nudge β online based on recent hit ratio in a sliding window:
#     - Lower hit ratio -> increase β (penalize large objects more)
#     - Higher hit ratio -> decrease β (penalize size less, favor frequency)
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_H = dict()             # key -> float, GDSF priority
m_last_access = dict()   # key -> int, last access timestamp (for tie-breaking)

# Global aging term (monotone non-decreasing)
_g_L = 0.0

# Tunables
_BETA_MIN = 0.60
_BETA_MAX = 1.00
_g_beta = 0.80           # current size exponent β
_MAX_H = 1e12            # safety cap
_EPS = 1e-12

# Adaptation window for β
_ADAPT_WINDOW = 5000
_adapt_last_access = 0
_adapt_last_hits = 0


def _weight(size):
    """Return per-access reward weight = 1 / size^β."""
    s = max(1, int(size))
    # Use pow for fractional exponent, clamp to avoid denormals
    w = 1.0 / (float(s) ** float(_g_beta))
    return max(w, 1e-18)


def _ensure_metadata_for_key(cache_snapshot, key):
    """Ensure resident key has initialized metadata; lazy-init to a minimal reasonable value."""
    if key in m_H and key in m_last_access:
        return
    # If we must initialize a pre-existing resident (should be rare), treat as freshly inserted now.
    robj = cache_snapshot.cache.get(key)
    if robj is None:
        return
    now = cache_snapshot.access_count
    m_H[key] = min(_g_L + _weight(robj.size), _MAX_H)
    m_last_access[key] = now


def _maybe_adapt_beta(cache_snapshot):
    """Adapt β based on recent hit ratio."""
    global _g_beta, _adapt_last_access, _adapt_last_hits
    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Target band is around 0.5-0.6; push β up if hit rate is low (avoid big items),
    # and down if hit rate is high (favor frequency more).
    if hit_rate < 0.30:
        _g_beta = min(_BETA_MAX, _g_beta + 0.08)
    elif hit_rate < 0.50:
        _g_beta = min(_BETA_MAX, _g_beta + 0.03)
    elif hit_rate > 0.80:
        _g_beta = max(_BETA_MIN, _g_beta - 0.08)
    elif hit_rate > 0.60:
        _g_beta = max(_BETA_MIN, _g_beta - 0.03)
    # else stay

    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest GDSF priority H.
    Tie-break by oldest last access, then by largest size (to free more space).
    """
    # Ensure resident metadata exists
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_H = None
    victim_last_ts = None
    victim_size = None

    for k, robj in cache_snapshot.cache.items():
        Hk = m_H.get(k)
        if Hk is None:
            # Lazily initialize if missing (should be rare)
            _ensure_metadata_for_key(cache_snapshot, k)
            Hk = m_H.get(k, _g_L)

        last_ts = m_last_access.get(k, 0)
        sz = robj.size

        if (victim_key is None or
            Hk < victim_H - _EPS or
            (abs(Hk - victim_H) <= _EPS and (last_ts < victim_last_ts or
                                             (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = k
            victim_H = Hk
            victim_last_ts = last_ts
            victim_size = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increase H by w(size).
    - Update last_access timestamp.
    - Periodically adapt β.
    """
    now = cache_snapshot.access_count
    key = obj.key
    _ensure_metadata_for_key(cache_snapshot, key)

    inc = _weight(obj.size)
    m_H[key] = min(m_H.get(key, _g_L) + inc, _MAX_H)
    m_last_access[key] = now

    _maybe_adapt_beta(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Set H = L + w(size) so fresh items start close to the current age.
    - Update last_access.
    - Periodically adapt β.
    """
    now = cache_snapshot.access_count
    key = obj.key

    base = _weight(obj.size)
    m_H[key] = min(_g_L + base, _MAX_H)
    m_last_access[key] = now

    _maybe_adapt_beta(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Set L := H[evicted_key] (aging step).
    - Drop resident metadata for the evicted key.
    - Periodically adapt β.
    """
    global _g_L

    ev_key = evicted_obj.key
    ev_H = m_H.get(ev_key, None)
    if ev_H is not None:
        # GDSF aging: move the age up to the evicted priority
        if ev_H > _g_L:
            _g_L = ev_H

    # Remove metadata
    m_H.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt_beta(cache_snapshot)
```
2025-11-27 18:14:17,405 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.0635

Example 2:
- Trace ID: 15
- Hit Rate: 0.0194

Example 3:
- Trace ID: 10
- Hit Rate: 0.075

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:14:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:14:17,407 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:15:53 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:15:53,654 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:15:53,655 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size with TinyLFU-style frequency estimator (GDSF-TLFU)
#
# Overview:
# - Eviction priority H_k = L + w_k, where:
#     - L is the global "clock" that increases to the priority of the last evicted item.
#     - w_k is the key's weight = log1p(estimated_frequency) / size^beta.
# - Estimated frequency is maintained by a small Count-Min Sketch (CMS) over all accesses
#   (both hits and misses). The CMS is aged periodically by halving counters, providing
#   temporal recency to the frequency estimate (TinyLFU-style aging).
# - On hit or insert, we refresh the item's priority to H_k = L + w_k using the latest CMS.
# - On eviction, we remove the item with the smallest H_k and set L = H_victim.
#
# This blends recency (through L) and long-term popularity (through the aged CMS), while
# being size-aware (via size^beta in the denominator). It works well across diverse traces
# and avoids the pitfalls of purely recency- or frequency-based policies.

import math

# --------------------
# Global metadata
# --------------------

# Per-resident metadata
m_H = dict()             # key -> float, GreedyDual priority H = L + weight
m_last_access = dict()   # key -> int, timestamp of last access (tie-breaker)

# Global GreedyDual clock
_g_L = 0.0

# Size-awareness
_SIZE_EXP = 0.85         # beta in denominator: weight = log1p(freq) / size^beta
_EPS = 1e-12

# --------------------
# Count-Min Sketch for TinyLFU-style frequency estimation
# --------------------
# Small, fast CMS with periodic halving (aging).
# This tracks popularity of both resident and non-resident keys.

_CMS_DEPTH = 4
_CMS_WIDTH = 16384       # power of two for faster masking
_CMS_MASK = _CMS_WIDTH - 1

# CMS tables
_cms = [ [0] * _CMS_WIDTH for _ in range(_CMS_DEPTH) ]
_cms_seeds = (0x9e3779b1, 0x85ebca6b, 0xc2b2ae35, 0x27d4eb2f)

# Aging control
_CMS_AGE_WINDOW = 50000  # number of increments between halvings
_cms_ops_since_age = 0

def _cms_hash(key, seed):
    # Use Python's hash with a seed for different rows; mask to width
    return (hash((key, seed)) & _CMS_MASK)

def _cms_inc(key, delta=1):
    global _cms_ops_since_age
    for row in range(_CMS_DEPTH):
        idx = _cms_hash(key, _cms_seeds[row])
        # saturate counters to avoid overflow; int is unbounded but keep them reasonable
        _cms[row][idx] += delta
    _cms_ops_since_age += 1
    if _cms_ops_since_age >= _CMS_AGE_WINDOW:
        _cms_age()

def _cms_est(key):
    # Min across rows (standard CMS estimate)
    est = float('inf')
    for row in range(_CMS_DEPTH):
        idx = _cms_hash(key, _cms_seeds[row])
        v = _cms[row][idx]
        if v < est:
            est = v
    if est == float('inf'):
        est = 0
    return est

def _cms_age():
    # Halve all counters (right shift by 1). Provides temporal decay.
    global _cms_ops_since_age
    for row in range(_CMS_DEPTH):
        tbl = _cms[row]
        for i in range(_CMS_WIDTH):
            tbl[i] >>= 1
    _cms_ops_since_age = 0

# --------------------
# Helpers
# --------------------

def _size_penalty(size):
    return max(1.0, float(size)) ** _SIZE_EXP

def _weight_from_cms(freq_est, size):
    # Smoothed popularity; log1p avoids runaway for hot keys.
    return math.log1p(max(0.0, float(freq_est))) / _size_penalty(size)

def _ensure_resident_metadata(cache_snapshot, key):
    # Ensure metadata exists for a resident key; if missing, initialize using current CMS/size.
    if key in m_H and key in m_last_access:
        return
    robj = cache_snapshot.cache.get(key, None)
    if robj is None:
        return
    est = _cms_est(key)
    w = _weight_from_cms(est, robj.size)
    m_H[key] = _g_L + w
    # Default last access to 0 if unknown; will be updated on hit/insert
    if key not in m_last_access:
        m_last_access[key] = 0

# --------------------
# Policy functions
# --------------------

def evict(cache_snapshot, obj):
    """
    GreedyDual-Size-Frequency eviction:
    - Select resident item with the smallest H (tie-break by oldest last_access).
    - Advance global clock L to the victim's H.
    """
    global _g_L

    # Ensure all resident metadata exist
    for k in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, k)

    victim_key = None
    victim_H = None
    victim_last_ts = None

    for k, robj in cache_snapshot.cache.items():
        H = m_H.get(k)
        if H is None:
            # Initialize lazily if needed
            est = _cms_est(k)
            w = _weight_from_cms(est, robj.size)
            H = _g_L + w
            m_H[k] = H
        last_ts = m_last_access.get(k, 0)

        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_H = H
            victim_last_ts = last_ts

    # Advance the GreedyDual clock to the victim's priority
    if victim_H is not None:
        if victim_H > _g_L:
            _g_L = victim_H

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment CMS (count this access).
    - Recompute weight from CMS and refresh H = L + weight.
    - Update last_access timestamp.
    """
    global _g_L
    now_ts = cache_snapshot.access_count
    key = obj.key

    _cms_inc(key)
    est = _cms_est(key)
    w = _weight_from_cms(est, obj.size)

    m_H[key] = _g_L + w
    m_last_access[key] = now_ts


def update_after_insert(cache_snapshot, obj):
    """
    On insert after a miss:
    - Increment CMS (count the access that caused the miss).
    - Initialize H = L + weight using current CMS estimate.
    - Set last_access timestamp.
    """
    global _g_L
    now_ts = cache_snapshot.access_count
    key = obj.key

    _cms_inc(key)
    est = _cms_est(key)
    w = _weight_from_cms(est, obj.size)

    m_H[key] = _g_L + w
    m_last_access[key] = now_ts


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Remove resident metadata for evicted key.
    - No change to CMS (keeps global popularity history).
    """
    ev_key = evicted_obj.key
    m_H.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 18:16:17,109 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# GreedyDual-Size-Frequency (GDSF) cache eviction algorithm
# - Size-aware, frequency-aware, and self-aging through a global inflation factor.
# - Maintains compact per-resident metadata only.

# Global metadata
m_freq = dict()          # key -> int (bounded frequency)
m_priority = dict()      # key -> float (H score used for eviction)
m_last_access = dict()   # key -> int (timestamp, for tie-breaking)
m_L = 0.0                # global "inflation" factor (aging baseline)

# Tunables
_MAX_FREQ = 32           # cap to prevent runaway counters
_EPS = 1e-9              # numeric safety for divisions

def _compute_priority(freq, size, L):
    # H = L + freq / size
    # size is positive by spec; use max(size, 1) safeguard
    return float(L) + float(freq) / max(int(size), 1)

def _ensure_metadata_for_key(cache_snapshot, key):
    # Lazily initialize metadata for any resident key missing entries.
    # This keeps algorithm robust if the environment preloads items or after resets.
    if key not in m_freq:
        m_freq[key] = 1
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_priority:
        obj = cache_snapshot.cache.get(key)
        size = obj.size if obj is not None else 1
        m_priority[key] = _compute_priority(m_freq[key], size, m_L)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using GDSF:
    - Evict the resident item with the smallest priority H.
    - Tie-break by oldest last access time (LRU-ish).
    '''
    # Ensure all resident keys have computed priority
    for key in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, key)

    # Find victim: min H; tie-break by oldest last_access
    victim_key = None
    victim_H = None
    victim_last_ts = None

    for key in cache_snapshot.cache:
        H = m_priority[key]
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_H = H
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment (bounded) frequency.
    - Recompute priority with current L.
    - Update last access time.
    '''
    key = obj.key
    # Initialize if missing (robustness)
    _ensure_metadata_for_key(cache_snapshot, key)

    # Update frequency (bounded) and recompute priority
    f = min(m_freq.get(key, 1) + 1, _MAX_FREQ)
    m_freq[key] = f
    m_last_access[key] = cache_snapshot.access_count

    # Recompute priority using current L and resident size
    resident_obj = cache_snapshot.cache.get(key, obj)
    m_priority[key] = _compute_priority(f, resident_obj.size, m_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (typically after a miss):
    - Initialize frequency to 1 (first access).
    - Initialize priority to L + 1/size.
    - Set last access time.
    '''
    key = obj.key
    # Start fresh for resident metadata
    m_freq[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_priority[key] = _compute_priority(1, obj.size, m_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Set L to the H of the evicted object (aging step).
    - Remove evicted object's resident metadata.
    '''
    global m_L
    ev_key = evicted_obj.key

    # Ensure we can compute evicted priority even if missing
    f = m_freq.get(ev_key, 1)
    H_evicted = m_priority.get(ev_key, _compute_priority(f, evicted_obj.size, m_L))

    # Age the system: raise L to the evicted priority
    m_L = H_evicted

    # Drop metadata for the evicted resident item
    m_freq.pop(ev_key, None)
    m_priority.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0253

Example 2:
- Trace ID: 46
- Hit Rate: 0.4417

Example 3:
- Trace ID: 3
- Hit Rate: 0.3889

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:16:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:16:17,110 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:18:10 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:18:10,970 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:18:10,971 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 18:24:01,114 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive size-aware LRFU (exponentially decayed LFU with recency, size bias, and dynamic decay)
# - Priority = decayed_frequency / size^beta
# - Exponential time decay blends recency and frequency
# - Beta tunes size-awareness (penalize larger items)
# - Half-life is adapted online based on recent hit ratio
#
# Metadata is kept only for resident objects.

# Per-resident metadata
m_score = dict()         # key -> float, decayed frequency mass at last_update
m_last_update = dict()   # key -> int, timestamp of last score update
m_last_access = dict()   # key -> int, timestamp of last access (for tie-breaking)

# Tunables
_SIZE_EXP = 0.75         # beta in priority denominator: priority = score / size^beta
_MAX_SCORE = 1e9         # safety cap
_EPS = 1e-12

# Decay control via half-life (in accesses)
_HL_MIN = 32
_HL_MAX = 16384
_ADAPT_WINDOW = 5000      # accesses between half-life adjustments
_g_HL = 1024              # current half-life (adaptive)
_g_decay_base = 2 ** (-1.0 / float(_g_HL))  # per-access decay multiplier

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0


def _maybe_adapt(cache_snapshot):
    """Adapt the half-life based on recent hit ratio to balance recency vs frequency."""
    global _g_HL, _g_decay_base, _adapt_last_access, _adapt_last_hits

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    acc_win = max(1, now_acc - _adapt_last_access)
    hit_rate = hits_win / acc_win

    # Adjust HL: lower HL => stronger recency; higher HL => stronger frequency
    if hit_rate < 0.30:
        _g_HL = max(_HL_MIN, int(_g_HL / 2))
    elif hit_rate > 0.70:
        _g_HL = min(_HL_MAX, int(_g_HL * 1.5))
    else:
        # Nudge toward the side suggested by being below/above 0.5
        if hit_rate < 0.50:
            _g_HL = max(_HL_MIN, int(_g_HL * 0.9))
        else:
            _g_HL = min(_HL_MAX, int(_g_HL * 1.1))

    _g_decay_base = 2 ** (-1.0 / float(_g_HL))
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count


def _ensure_metadata_for_key(cache_snapshot, key):
    """Initialize metadata for any resident key that is missing entries."""
    if key not in m_score:
        m_score[key] = 1.0
    if key not in m_last_update:
        m_last_update[key] = cache_snapshot.access_count
    if key not in m_last_access:
        m_last_access[key] = 0


def _decay_factor(delta):
    """Return decay multiplier for a time gap of 'delta' accesses."""
    if delta <= 0:
        return 1.0
    return _g_decay_base ** float(delta)


def _effective_score_and_refresh(key, now_ts):
    """
    Lazily apply decay up to now and store the decayed score with updated timestamp.
    Returns the score at 'now_ts'.
    """
    s = m_score.get(key, 0.0)
    last = m_last_update.get(key, now_ts)
    dt = max(0, int(now_ts - last))
    if dt > 0:
        s *= _decay_factor(dt)
        m_score[key] = s
        m_last_update[key] = now_ts
    return s


def _priority_for(key, size, now_ts):
    """Compute eviction priority = decayed_score / size^beta (smaller is worse)."""
    s_eff = _effective_score_and_refresh(key, now_ts)
    denom = max(int(size), 1) ** _SIZE_EXP
    return s_eff / denom


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority (decayed_score / size^beta).
    Tie-break by oldest last access time.
    """
    now_ts = cache_snapshot.access_count

    # Ensure resident metadata exists and is refreshed
    for k in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, k)

    victim_key = None
    victim_pr = None
    victim_last_ts = None

    # Compute priorities (also refresh decayed scores to now) and select victim
    for k, robj in cache_snapshot.cache.items():
        pr = _priority_for(k, robj.size, now_ts)
        last_ts = m_last_access.get(k, 0)
        if (victim_key is None or
            pr < victim_pr - _EPS or
            (abs(pr - victim_pr) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_pr = pr
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay the score to 'now', then add 1 for this hit.
    - Update last_update and last_access timestamps.
    - Adapt half-life periodically based on recent hit ratio.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key)

    s_eff = _effective_score_and_refresh(key, now_ts)
    s_new = min(s_eff + 1.0, _MAX_SCORE)
    m_score[key] = s_new
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize with base score 1 (one observed access).
    - Set timestamps to now.
    - Adapt half-life periodically.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_score[key] = 1.0
    m_last_update[key] = now_ts
    m_last_access[key] = now_ts

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Drop resident metadata for the evicted key.
    - Optionally adapt (cheap no-op if window not reached).
    """
    ev_key = evicted_obj.key

    m_score.pop(ev_key, None)
    m_last_update.pop(ev_key, None)
    m_last_access.pop(ev_key, None)

    _maybe_adapt(cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 9
- Hit Rate: 0.531

Example 2:
- Trace ID: 4
- Hit Rate: 0.4539

Example 3:
- Trace ID: 41
- Hit Rate: 0.0725

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:24:01 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:24:01,116 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:26:03 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:26:03,967 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:26:03,968 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-Frequency (GDSF) with adaptive size exponent and frequency credit
# - Each resident item k maintains a priority H[k]
# - Global inflation level L is raised to the evicted item's H, aging all others implicitly
# - On access, H[k] gets a frequency credit proportional to 1 / size^beta
# - On insert, H[k] starts at L + credit
# - Beta (size awareness) and frequency credit weight adapt online based on hit-rate and
#   the relative sizes of hits vs misses.

# Resident metadata
m_H = dict()            # key -> float, GDSF priority
m_last_access = dict()  # key -> int, last access time (for tie-breaking)

# Global state
_g_L = 0.0              # inflation level
_g_beta = 0.9           # size exponent (adaptive)
_g_w = 1.0              # frequency credit multiplier (adaptive)

# Safety caps and tunables
_EPS = 1e-12
_BETA_MIN, _BETA_MAX = 0.3, 1.7
_W_MIN, _W_MAX = 0.2, 8.0
_L_COMPACT_THRESHOLD = 1e14  # when L grows huge, compact priorities
_ADAPT_WINDOW = 5000         # accesses between adaptations

# Adaptation bookkeeping
_adapt_last_access = 0
_adapt_last_hits = 0
_adapt_last_misses = 0
_adapt_hit_bytes = 0
_adapt_insert_bytes = 0


def _size_penalty(size):
    """Return size^beta with guards."""
    s = max(1, int(size))
    return float(s) ** float(_g_beta)


def _credit_for(size):
    """Frequency credit contributed by one reference."""
    return _g_w / _size_penalty(size)


def _ensure_metadata_for_key(cache_snapshot, key, size, now_ts):
    """Initialize metadata for a resident key if missing."""
    if key not in m_H:
        # Start as if it was just (re)inserted with one credit
        m_H[key] = _g_L + _credit_for(size)
    if key not in m_last_access:
        m_last_access[key] = now_ts


def _maybe_compact_priorities(cache_snapshot):
    """
    If L grows very large, subtract L from all H and reset L to zero.
    This preserves ordering while keeping numbers in a safe range.
    """
    global _g_L
    if _g_L <= _L_COMPACT_THRESHOLD:
        return
    # Compact: H[k] <- H[k] - L; L <- 0
    for k in list(cache_snapshot.cache.keys()):
        if k in m_H:
            m_H[k] = max(0.0, m_H[k] - _g_L)
    _g_L = 0.0


def _maybe_adapt(cache_snapshot):
    """
    Adapt _g_beta (size exponent) and _g_w (frequency credit) based on recent hit ratio
    and relative sizes of hits vs. inserts (misses).
    """
    global _g_beta, _g_w
    global _adapt_last_access, _adapt_last_hits, _adapt_last_misses
    global _adapt_hit_bytes, _adapt_insert_bytes

    now_acc = cache_snapshot.access_count
    if now_acc - _adapt_last_access < _ADAPT_WINDOW:
        return

    hits_win = cache_snapshot.hit_count - _adapt_last_hits
    misses_win = cache_snapshot.miss_count - _adapt_last_misses
    acc_win = max(1, hits_win + misses_win)

    hit_rate = hits_win / acc_win

    avg_hit_sz = (_adapt_hit_bytes / hits_win) if hits_win > 0 else 0.0
    avg_miss_sz = (_adapt_insert_bytes / misses_win) if misses_win > 0 else 0.0
    size_ratio = (avg_miss_sz / avg_hit_sz) if avg_hit_sz > 0 else (2.0 if avg_miss_sz > 0 else 1.0)

    # Adapt frequency weight (_g_w) based on hit rate (recency vs frequency):
    if hit_rate < 0.30:
        _g_w = max(_W_MIN, _g_w * 0.8)    # favor recency (smaller freq boost)
    elif hit_rate > 0.70:
        _g_w = min(_W_MAX, _g_w * 1.2)    # favor frequency (larger freq boost)
    else:
        # Gentle nudges toward 0.5 sweet spot
        if hit_rate < 0.50:
            _g_w = max(_W_MIN, _g_w * 0.95)
        else:
            _g_w = min(_W_MAX, _g_w * 1.05)

    # Adapt size exponent (_g_beta) based on sizes seen on misses vs hits:
    # If misses are larger than hits, penalize size more (increase beta).
    # If misses are smaller, relax size penalty (decrease beta).
    if size_ratio > 1.30:
        _g_beta = min(_BETA_MAX, _g_beta * 1.10)
    elif size_ratio < 0.70:
        _g_beta = max(_BETA_MIN, _g_beta * 0.90)
    # else keep beta unchanged

    # Reset window stats
    _adapt_last_access = now_acc
    _adapt_last_hits = cache_snapshot.hit_count
    _adapt_last_misses = cache_snapshot.miss_count
    _adapt_hit_bytes = 0
    _adapt_insert_bytes = 0


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest GDSF priority H.
    Tie-break by oldest last access time.
    Also compacts priorities occasionally to avoid numeric blow-up.
    """
    now_ts = cache_snapshot.access_count

    # Initialize missing metadata and compact if needed
    for k, robj in cache_snapshot.cache.items():
        _ensure_metadata_for_key(cache_snapshot, k, robj.size, now_ts)
    _maybe_compact_priorities(cache_snapshot)

    victim_key = None
    victim_H = None
    victim_last_ts = None

    for k, robj in cache_snapshot.cache.items():
        H = m_H.get(k, _g_L)  # default to floor if missing
        last_ts = m_last_access.get(k, 0)

        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = k
            victim_H = H
            victim_last_ts = last_ts

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increase H by credit (frequency) scaled by current size exponent and frequency weight.
    - Update last_access timestamp.
    - Update adaptation statistics and possibly adapt.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    _ensure_metadata_for_key(cache_snapshot, key, obj.size, now_ts)

    # Frequency reinforcement (size aware)
    m_H[key] = m_H.get(key, _g_L) + _credit_for(obj.size)
    m_last_access[key] = now_ts

    # Track adaptation stats
    global _adapt_hit_bytes
    _adapt_hit_bytes += int(obj.size)

    _maybe_adapt(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Set H = L + credit(size) so new items start near the current eviction threshold.
    - Update last_access timestamp.
    - Track insert sizes for adaptation.
    """
    now_ts = cache_snapshot.access_count
    key = obj.key

    m_H[key] = _g_L + _credit_for(obj.size)
    m_last_access[key] = now_ts

    # Track adaptation stats
    global _adapt_insert_bytes
    _adapt_insert_bytes += int(obj.size)

    _maybe_adapt(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Raise the global inflation level L to the evicted item's H (aging).
    - Drop resident metadata for the evicted key.
    - Optionally compact priorities and adapt.
    """
    global _g_L

    ev_key = evicted_obj.key
    ev_H = m_H.pop(ev_key, _g_L)
    m_last_access.pop(ev_key, None)

    # Age others by raising the floor to the evicted H
    if ev_H > _g_L:
        _g_L = ev_H

    _maybe_compact_priorities(cache_snapshot)
    _maybe_adapt(cache_snapshot)
```
2025-11-27 18:26:31,080 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0559

Example 2:
- Trace ID: 39
- Hit Rate: 0.0909

Example 3:
- Trace ID: 17
- Error: Error - Timeout

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:26:31 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:26:31,081 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:27:01,128 [INFO] openai._base_client: Retrying request to /chat/completions in 0.431060 seconds
2025-11-27 18:37:18,231 [INFO] openai._base_client: Retrying request to /chat/completions in 0.886380 seconds
[92m18:40:17 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:40:17,232 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:40:17,233 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 18:43:32,234 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0409

Example 2:
- Trace ID: 45
- Hit Rate: 0.0743

Example 3:
- Trace ID: 6
- Hit Rate: 0.2679

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:43:32 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:43:32,235 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:47:06 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:06,737 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:06,738 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware SLRU + TinyLFU with ARC-style window tuning (ASL-TinyLFU)
# - Global admission frequency via Count-Min Sketch with periodic aging.
# - Two in-cache segments:
#     0: "window"/probation (recency-biased)
#     1: "protected" (frequency-biased)
# - Eviction:
#     * If window bytes exceed its adaptive target, evict from window only (recency-biased SLRU).
#     * Otherwise, evict global minimum priority (size-aware TinyLFU + recency + protected bias).
# - Adaptive window sizing using two ghost lists (ARC-style):
#     * If a missed key was recently evicted from window-ghost -> increase window fraction.
#     * If from protected-ghost -> decrease window fraction.
# - Per-resident metadata: segment id, last access time, resident hit counter, promotion threshold.
# - Size-awareness: priority divides by size**alpha; larger objects require more hits to promote.

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style)
# ------------------------
_gw = dict()              # window-ghost: key -> (last_ts, size)
_gp = dict()              # protected-ghost: key -> (last_ts, size)
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000      # more responsive aging
_decay_ticker = 0
_just_decayed = False      # set True when sketch decay occurs

# ------------------------
# Tunables (carefully chosen defaults)
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02   # base step for adaptive adjustments
_window_frac = 0.25        # initial window fraction

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.25

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.7
_FREQ_RESIDENT_WEIGHT = 0.3

# Numerical epsilon and other helpers
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # ARC uses |B1|, |B2| up to cache size each

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    # When sketch ages, also age resident hit counters to avoid stale bias.
    global _just_decayed
    if _just_decayed:
        # Halve all resident hit counters
        keys = list(m_res_hits.keys())
        for k in keys:
            m_res_hits[k] >>= 1
        _just_decayed = False

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., preloaded items)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        # Without obj/capacity we default to small promotion threshold
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if obj.size >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need

def _ghost_add(segment, key, size, now):
    # segment: 0 -> window-ghost; 1 -> protected-ghost
    global _gw_bytes, _gp_bytes
    if segment == 0:
        prev = _gw.get(key)
        if prev is None:
            _gw[key] = (now, int(size))
            _gw_bytes += int(size)
        else:
            # refresh timestamp; keep size as last known
            _gw[key] = (now, prev[1])
    else:
        prev = _gp.get(key)
        if prev is None:
            _gp[key] = (now, int(size))
            _gp_bytes += int(size)
        else:
            _gp[key] = (now, prev[1])

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries
    global _gw_bytes, _gp_bytes
    # Prune window-ghost
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        # Find oldest by scanning (bounded by ghost size; acceptable under budget)
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gw.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gw.pop(oldest_key)
        _gw_bytes -= int(sz)
    # Prune protected-ghost
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gp.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gp.pop(oldest_key)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key, now):
    # ARC-style adaptive control using ghost hits
    # If the key is in window-ghost (B1): increase window target.
    # If in protected-ghost (B2): decrease window target.
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # step proportional to max(1, |B2|/|B1|)
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        _, sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        _, sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    # Periodically prune ghosts to budgets
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus (harder to evict)
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among window only (probation).
      - Otherwise, evict the global minimum-priority key (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)

    # Ensure resident metadata exists for all current items
    for key, robj in cache_snapshot.cache.items():
        _ensure_resident_metadata(cache_snapshot, key, robj)

    # Compute window usage and target
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += int(robj.size)
            has_window = True

    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # only consider window while over target

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    m_last_access[key] = cache_snapshot.access_count
    # Increment resident hits (saturating via Python int; decay throttles growth)
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    # Size-aware promotion
    if m_segment.get(key, 0) == 0:
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key, now)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    now = cache_snapshot.access_count

    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)
    size = int(evicted_obj.size)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, size, now)
    _ghost_prune()

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 18:52:45,930 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 35
- Hit Rate: 0.075

Example 2:
- Trace ID: 18
- Hit Rate: 0.0225

Example 3:
- Trace ID: 8
- Hit Rate: 0.5388

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:52:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:52:45,931 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m18:55:58 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:55:58,659 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:55:58,660 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 18:59:28,921 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 44
- Hit Rate: 0.0419

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 37
- Hit Rate: 0.6425

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:59:28 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 18:59:28,930 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:03:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:03:04,431 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:03:04,432 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Robust, Size-aware SLRU with TinyLFU-guided eviction and ARC-style ghost feedback (R-SSLRU-TinyLFU+G)
# - Two explicit LRU segments: window (recency/probation) and protected (frequency).
# - TinyLFU Count-Min Sketch provides global, decayed frequency estimates for ranking.
# - Eviction considers tails (LRU) of window/protected with a size-aware, recency-aware score.
# - If the best victim still looks stronger than the incoming object, prefer evicting window's LRU
#   (soft-admission to resist scans/pollution).
# - Protected budget enforced via LRU demotion into window.
# - Window fraction adapts based on hit locality and ghost-history feedback (ARC-like).
# - All metadata is resident and kept compact.

import random
from collections import OrderedDict, deque

# ------------------------
# Global metadata
# ------------------------
# Segments as OrderedDicts to keep true LRU order: newest at front, oldest at tail
_od_window = OrderedDict()    # key -> None
_od_protected = OrderedDict() # key -> None

# Per-resident maps
_m_segment = dict()       # key -> 0 (window) or 1 (protected)
_m_last_access = dict()   # key -> int timestamp
_m_size = dict()          # key -> size

# Byte counters for segments
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # stable EMA

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Scoring
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.0            # weight for recency
_PROTECTED_KEEP_BONUS = 1.25
_ADMIT_COMPARE_FACTOR = 1.25
_RECENCY_TAU = 5000.0       # recency time-scale (higher -> gentler aging)
_EPS = 1e-9

# Adaptation based on hit locality
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Default all unknown residents to window LRU at startup
    for key, robj in cache_snapshot.cache.items():
        s = int(robj.size)
        total_bytes += s
        total_items += 1
        _m_size[key] = s
        if key not in _m_segment:
            _m_segment[key] = 0
        if key not in _m_last_access:
            _m_last_access[key] = 0
        if _m_segment[key] == 0:
            if key not in _od_window:
                _od_window[key] = None  # MRU position by insertion into front later
        else:
            if key not in _od_protected:
                _od_protected[key] = None
    # Rebuild LRU: move all keys to front to set a valid order
    for k in list(_od_window.keys()):
        _od_window.move_to_end(k, last=False)
        _g_window_bytes += _m_size.get(k, 0)
    for k in list(_od_protected.keys()):
        _od_protected.move_to_end(k, last=False)
        _g_protected_bytes += _m_size.get(k, 0)

    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    # By bytes; clamp at least 1 byte
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(_f_window * cap))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _recency_term(age):
    # Smooth recency factor in [~0,1], with time-scale _RECENCY_TAU
    # recent -> near 1, long idle -> near 0
    return 1.0 / (1.0 + float(age) / max(1.0, _RECENCY_TAU))

def _priority_keep_for_key(cache_snapshot, key, now, size_alpha):
    # Larger score => more valuable to keep
    size = max(1, _m_size.get(key, int(cache_snapshot.cache.get(key).size)))
    freq = _sketch_estimate(key)
    age = max(0, now - _m_last_access.get(key, 0))
    recency = _recency_term(age)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if _m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(incoming_key, incoming_size):
    # Estimate incoming "keep" score as if just accessed (recency=1, no protected bonus)
    size_alpha = _dynamic_size_alpha(incoming_size)
    size = max(1, int(incoming_size))
    freq = _sketch_estimate(incoming_key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _ensure_protected_budget(cache_snapshot):
    # Demote protected LRU to window until within budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    prot_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > prot_budget and _od_protected:
        ev_key, _ = _od_protected.popitem(last=True)  # LRU in protected
        _m_segment[ev_key] = 0
        _od_window[ev_key] = None
        _od_window.move_to_end(ev_key, last=False)  # to MRU
        sz = _m_size.get(ev_key, 0)
        _g_protected_bytes -= sz
        _g_window_bytes += sz

def _window_lru_tail():
    try:
        return next(reversed(_od_window))
    except StopIteration:
        return None

def _protected_lru_tail():
    try:
        return next(reversed(_od_protected))
    except StopIteration:
        return None

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      - Maintain protected budget via LRU demotion.
      - If window bytes exceed target, evict window's LRU (pure recency control).
      - Otherwise, choose the worse of the two tails (window/protected) by TinyLFU+recency+size score.
      - If even the chosen victim appears stronger than the incoming request, prefer evicting window's LRU
        (soft admission to resist scans/pollution).
      - Tie-breaks by older last access, then larger size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_key = obj.key
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget by demoting to window if needed
    _ensure_protected_budget(cache_snapshot)

    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Candidate victims: tails of segments
    cand_keys = []
    w_tail = _window_lru_tail()
    p_tail = _protected_lru_tail()
    if restrict_to_window:
        if w_tail is not None:
            cand_keys.append(w_tail)
        elif p_tail is not None:
            cand_keys.append(p_tail)
    else:
        if w_tail is not None:
            cand_keys.append(w_tail)
        if p_tail is not None:
            cand_keys.append(p_tail)

    # Fallback: if no candidates (very rare), evict any key from cache
    if not cand_keys:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    size_alpha = _dynamic_size_alpha(incoming_size)

    # Score candidates and choose the weakest (lowest keep score)
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for k in cand_keys:
        s = _priority_keep_for_key(cache_snapshot, k, now, size_alpha)
        last_ts = _m_last_access.get(k, 0)
        sz = _m_size.get(k, int(cache_snapshot.cache.get(k).size)) if cache_snapshot.cache.get(k) else _m_size.get(k, 1)
        if (victim_key is None or
            s < victim_score - _EPS or
            (abs(s - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                (last_ts == victim_last_ts and sz > (victim_size or 0))))):
            victim_key = k
            victim_score = s
            victim_last_ts = last_ts
            victim_size = sz

    # Soft-admission guard: if chosen victim looks stronger than incoming, prefer evicting window's LRU
    incoming_score = _incoming_priority_estimate(incoming_key, incoming_size)
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            if w_tail is not None:
                victim_key = w_tail

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch, decay if needed.
      - Update last-access timestamp.
      - Promote window -> protected; move to MRU within its LRU.
      - Adapt window fraction based on hit locality periodically.
      - Enforce protected budget via demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    _m_last_access[key] = now

    seg = _m_segment.get(key, 0)
    # Hit locality accounting before promotion/move
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Ensure size map is set (defensive)
    if key not in _m_size:
        _m_size[key] = int(obj.size)

    # Promote/move to MRU
    if seg == 0:
        # Move from window -> protected (MRU)
        if key in _od_window:
            _od_window.pop(key, None)
            globals()['_g_window_bytes'] -= _m_size.get(key, 0)
        _od_protected[key] = None
        _od_protected.move_to_end(key, last=False)
        _m_segment[key] = 1
        globals()['_g_protected_bytes'] += _m_size.get(key, 0)
    else:
        # Already protected: move to MRU
        if key in _od_protected:
            _od_protected.move_to_end(key, last=False)
        else:
            # Defensive: if missing, reinsert in protected
            _od_protected[key] = None
            _od_protected.move_to_end(key, last=False)

    # Keep protected within budget
    _ensure_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts and average size.
      - Insert into window (probation) at MRU; initialize metadata.
      - Ghost-feedback: if missed in window-ghost, increase window; if in protected-ghost, decrease window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = max(1, int(obj.size))

    _m_size[key] = size
    _m_last_access[key] = now
    _m_segment[key] = 0  # window

    # If already present in any LRU due to defensive path, remove first
    if key in _od_protected:
        _od_protected.pop(key, None)
        globals()['_g_protected_bytes'] -= size
    if key in _od_window:
        _od_window.pop(key, None)
        globals()['_g_window_bytes'] -= size

    # Insert into window MRU
    _od_window[key] = None
    _od_window.move_to_end(key, last=False)
    globals()['_g_window_bytes'] += size

    # Ghost-feedback adaptation (ARC-style)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
        # Deque entry will be lazily trimmed later
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from its LRU and metadata.
      - Update bytes counters.
      - Add evicted key to ghost history (segment-aware) to guide window ratio.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    size = _m_size.get(ev_key, int(evicted_obj.size))
    seg = _m_segment.pop(ev_key, 0)

    # Remove from LRUs
    if seg == 1:
        if ev_key in _od_protected:
            _od_protected.pop(ev_key, None)
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - size)
    else:
        if ev_key in _od_window:
            _od_window.pop(ev_key, None)
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)

    _m_last_access.pop(ev_key, None)
    _m_size.pop(ev_key, None)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 19:03:12,112 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 22
- Hit Rate: 0.0587

Example 2:
- Trace ID: 30
- Hit Rate: 0.0456

Example 3:
- Trace ID: 19
- Hit Rate: 0.2722

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:03:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:03:12,113 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:07:03 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:07:03,228 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:07:03,229 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware TinyLFU with ARC-style window tuning (AST-WS-TinyLFU)
# - Global admission priority via Count-Min Sketch with periodic aging.
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction:
#     * If window bytes exceed a dynamic target p, evict LRU from window (pure recency).
#     * Otherwise, evict the global min priority over all residents using size-aware TinyLFU,
#       with a mild protected-bonus and recency boost.
# - Promotion: require two hits in the window before promoting to protected (reduces thrash).
# - ARC-style adaptive window tuning:
#     * Maintain "ghost" LRU lists for recently evicted items from each segment.
#     * On insert, if key is found in window-ghost: increase p; if in protected-ghost: decrease p.
#     * Ghost lists are byte-capped: |G1| <= p and |G2| <= capacity - p.
# - Metadata is compact: segment id, last access timestamp, hits since insert.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()             # key -> 0 (window) or 1 (protected)
m_last_access = dict()         # key -> int (timestamp)
m_hits_since_insert = dict()   # key -> small int counter

# Count-Min Sketch for TinyLFU
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (base)
_SIZE_ALPHA = 1.0                 # priority divisor exponent: size**alpha
_PROTECTED_KEEP_BONUS = 1.4       # multiplicative boost for protected items
_RECENCY_LAMBDA_WINDOW = 3.5      # stronger recentness weight in window
_RECENCY_LAMBDA_PROTECTED = 1.2   # milder in protected
_EPS = 1e-9

# Dynamic window target (bytes), ARC-style adaptive 'p'
_target_window_bytes = None       # lazily initialized to ~20% of capacity
_STEP_FRACTION = 0.02             # step for adapting window target (2% of capacity)

# Ghost caches (ARC-style "B1" for window evictions, "B2" for protected)
_g_seg = dict()                   # ghost key -> 0 (window ghost) or 1 (protected ghost)
_g_size = dict()                  # ghost key -> size at eviction
_g1_order = []                    # LRU order list (keys) for window ghosts (B1)
_g2_order = []                    # LRU order list (keys) for protected ghosts (B2)
_g1_bytes = 0
_g2_bytes = 0

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_hits_since_insert:
        m_hits_since_insert[key] = 0

def _init_window_target_if_needed(capacity):
    global _target_window_bytes
    if _target_window_bytes is None:
        # Start with 20% of capacity (bytes) as window
        _target_window_bytes = max(1, int(0.20 * capacity))

def _priority_for_key(cache_snapshot, key, obj, now):
    # Score = (freq * (1 + lambda/(age+1))) / size^alpha
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    seg = m_segment.get(key, 0)
    if seg == 0:
        lam = _RECENCY_LAMBDA_WINDOW
    else:
        lam = _RECENCY_LAMBDA_PROTECTED
    mult = 1.0 + (lam * recency)
    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _ghost_remove(key):
    # Remove a key from ghost structures if present
    global _g1_bytes, _g2_bytes
    seg = _g_seg.pop(key, None)
    if seg is None:
        return
    size = _g_size.pop(key, 0)
    if seg == 0:
        # Remove from _g1_order
        try:
            idx = _g1_order.index(key)
            _g1_order.pop(idx)
        except ValueError:
            pass
        _g1_bytes = max(0, _g1_bytes - size)
    else:
        try:
            idx = _g2_order.index(key)
            _g2_order.pop(idx)
        except ValueError:
            pass
        _g2_bytes = max(0, _g2_bytes - size)

def _ghost_add(key, seg_from, size, capacity, p_window):
    # Insert key into appropriate ghost LRU, enforce byte budgets:
    # |G1| <= p_window and |G2| <= capacity - p_window
    global _g1_bytes, _g2_bytes
    # If present in either ghost, remove to re-insert at MRU
    _ghost_remove(key)

    size = max(1, int(size))
    if seg_from == 0:
        _g_seg[key] = 0
        _g_size[key] = size
        _g1_order.append(key)  # MRU at end
        _g1_bytes += size
        # Trim G1 if over budget
        budget = max(1, int(p_window))
        while _g1_bytes > budget and _g1_order:
            old = _g1_order.pop(0)  # LRU
            _g_seg.pop(old, None)
            s = _g_size.pop(old, 0)
            _g1_bytes = max(0, _g1_bytes - s)
    else:
        _g_seg[key] = 1
        _g_size[key] = size
        _g2_order.append(key)
        _g2_bytes += size
        # Trim G2 if over budget
        budget = max(1, int(capacity - p_window))
        while _g2_bytes > budget and _g2_order:
            old = _g2_order.pop(0)
            _g_seg.pop(old, None)
            s = _g_size.pop(old, 0)
            _g2_bytes = max(0, _g2_bytes - s)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Victim selection:
      - Maintain a dynamic window target p (bytes). If window bytes > p, evict the LRU key from window.
      - Otherwise, evict the global min priority key using TinyLFU-frequency, recency boost, and size-awareness.
      - Tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    capacity = cache_snapshot.capacity
    _init_window_target_if_needed(capacity)

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window usage and whether we have window candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    restrict_to_window = has_window and (window_bytes > _target_window_bytes)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    if restrict_to_window:
        # Evict pure LRU from window (recency)
        oldest_ts = None
        oldest_key = None
        oldest_size = None
        for key, robj in cache_snapshot.cache.items():
            if m_segment.get(key, 0) != 0:
                continue
            ts = m_last_access.get(key, 0)
            if oldest_key is None or ts < oldest_ts or (ts == oldest_ts and robj.size > oldest_size):
                oldest_key = key
                oldest_ts = ts
                oldest_size = robj.size
        return oldest_key

    # Otherwise, global min priority across all residents
    for key, robj in cache_snapshot.cache.items():
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay if needed.
      - Update last access timestamp.
      - If in window: increment hit counter; promote to protected after 2nd hit.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    if m_segment.get(key, 0) == 0:
        m_hits_since_insert[key] = m_hits_since_insert.get(key, 0) + 1
        if m_hits_since_insert[key] >= 2:
            m_segment[key] = 1  # promote to protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss):
      - Update TinyLFU for this access and decay if needed.
      - ARC-style window adaptation using ghost hits:
          * If the key is in window-ghost (G1), increase p.
          * If the key is in protected-ghost (G2), decrease p.
        Remove the key from ghosts (it is becoming resident).
      - Initialize residency metadata: segment=window, last_access=now, hits=0.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    capacity = cache_snapshot.capacity
    _init_window_target_if_needed(capacity)

    key = obj.key

    # ARC-style adaptation based on ghost membership
    seg_ghost = _g_seg.get(key, None)
    if seg_ghost is not None:
        step = max(1, int(_STEP_FRACTION * capacity))
        if seg_ghost == 0:
            # Recently evicted from window -> recency was undervalued: grow window
            _grow = min(capacity, _target_window_bytes + step)
            # Assign back to global
            globals()['_target_window_bytes'] = _grow
        else:
            # Recently evicted from protected -> frequency was undervalued: shrink window
            _shrink = max(1, _target_window_bytes - step)
            globals()['_target_window_bytes'] = _shrink
        # Remove from ghosts since it's now resident
        _ghost_remove(key)

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count
    m_hits_since_insert[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key into appropriate ghost LRU (B1 if from window, B2 if from protected),
        trimming ghost byte-budgets according to current p.
      - Remove resident metadata for the evicted key.
      - TinyLFU sketch persists globally; counters decay over time.
    '''
    capacity = cache_snapshot.capacity
    _init_window_target_if_needed(capacity)

    ev_key = evicted_obj.key
    # Capture segment before removal
    seg_from = m_segment.get(ev_key, 0)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_hits_since_insert.pop(ev_key, None)

    # Insert into appropriate ghost and maintain budgets
    _ghost_add(ev_key, seg_from, evicted_obj.size, capacity, _target_window_bytes)
```
2025-11-27 19:07:17,405 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 25
- Hit Rate: 0.3589

Example 2:
- Trace ID: 31
- Error: Error - Timeout

Example 3:
- Trace ID: 32
- Hit Rate: 0.0207

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:07:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:07:17,406 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:17:27,540 [INFO] openai._base_client: Retrying request to /chat/completions in 0.473773 seconds
[92m19:20:38 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:20:38,530 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:20:38,531 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware Windowed TinyLFU with relative-admission bias (A-SSR-W-TinyLFU)
# Key improvements:
# - Relative-admission victim choice: prefer evicting items whose value is below the incoming object's value.
# - Milder size penalty and log-compressed frequency to avoid extremes and improve stability across traces.
# - Conservative promotion: promote window->protected only when global frequency estimate is at least 2.
# - Faster, safer demotion: demote until protected bytes fit budget but with a small bound per update.
# - Reduced overhead: smaller sketch width and sampling sizes; cheap tie-breakers.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
# Smaller width for lower overhead while still providing robustness.
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two (reduced from 8192 to cut CPU)
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent to reduce overhead)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive; initialize with recency bias but constrained bounds.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.01

# Sampling
_SAMPLE_SIZE = 24            # candidates per eviction decision (reduced)
_SAMPLE_SIZE_DEMOTE = 12     # candidates to demote from protected (oldest)

# Priority scoring weights
# Use log(1+freq) and milder size penalty to stabilize across traces.
_SIZE_ALPHA = 1.05           # size penalty exponent (mild)
_W_FREQ = 1.6                # weight for frequency (log-compressed)
_W_RECENCY = 1.25            # weight for recency
_PROTECTED_KEEP_BONUS = 1.25 # protected items are harder to evict
_EPS = 1e-9

# Admission bias: when picking a victim, prefer keys with keep_score <= new object's strength.
# New object's "strength" excludes recency to avoid transiently over-admitting.
_ADMISSION_RELAX_EPS = 0.0

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Promotion policy
_PROMOTE_FREQ_THRESHOLD = 2.0  # require at least 2 estimated hits to promote to protected

# Demotion throttling per call (to avoid long loops)
_MAX_DEMOTE_PER_CALL = 3

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # right shift in place
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _log1p(x):
    # Small helper to avoid importing math (keeps sandbox overhead tiny)
    # Piecewise approximation sufficient for our use (x >= 0).
    # For large x, log1p(x) ~ ln(x+1). We'll implement a simple natural log via change of base if needed,
    # but to avoid complexity, use a rational approximation for 0..16 and fallback to builtin if available.
    # However, Python's math is allowed; using it is faster and more accurate.
    import math
    return math.log1p(x)

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq_raw = _sketch_estimate(key)
    freq = _log1p(freq_raw)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _new_object_strength(obj):
    # Admission strength of a new object (frequency-weighted only, no recency)
    est = _log1p(_sketch_estimate(obj.key))
    size = max(int(obj.size), 1)
    return ( _W_FREQ * est ) / (float(size) ** _SIZE_ALPHA)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        # Fallback: sample globally
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    demoted = 0
    while _g_protected_bytes > protected_budget and demoted < _MAX_DEMOTE_PER_CALL:
        candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
        if not candidates:
            return
        oldest_key = None
        oldest_ts = None
        for key in candidates:
            ts = m_last_access.get(key, 0)
            if oldest_key is None or ts < oldest_ts:
                oldest_key = key
                oldest_ts = ts
        if oldest_key is None:
            return
        robj = cache_snapshot.cache.get(oldest_key, None)
        if robj is None:
            return
        size = int(robj.size)
        if m_segment.get(oldest_key, 0) == 1:
            m_segment[oldest_key] = 0
            _g_protected_bytes -= size
            _g_window_bytes += size
            demoted += 1
        else:
            # Already window somehow
            break

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.57:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.43:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection with relative-admission bias:
      - If window byte-usage > target, prefer evicting from the window.
      - Otherwise, sample globally and pick the key with the smallest "keep score".
      - If there are candidates whose keep score <= new object's strength, prefer among them
        to emulate TinyLFU-style admission (do not evict strong protected items for weak newcomers).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Candidate sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
        # As a fallback, if window empty, sample globally
        if not candidates:
            candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # No resident keys; nothing to evict
        return None

    new_strength = _new_object_strength(obj)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    # Track a candidate set that are "no stronger than incoming" to emulate admission
    dominated_victim_key = None
    dominated_victim_score = None
    dominated_last_ts = None
    dominated_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        keep_score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        # Overall minimum
        if (victim_key is None or
            keep_score < victim_score - _EPS or
            (abs(keep_score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                         (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = keep_score
            victim_last_ts = last_ts
            victim_size = robj.size

        # Dominated by new object's strength
        if keep_score <= new_strength + _ADMISSION_RELAX_EPS:
            if (dominated_victim_key is None or
                keep_score < dominated_victim_score - _EPS or
                (abs(keep_score - dominated_victim_score) <= _EPS and (last_ts < dominated_last_ts or
                                                                       (last_ts == dominated_last_ts and robj.size > dominated_size)))):
                dominated_victim_key = key
                dominated_victim_score = keep_score
                dominated_last_ts = last_ts
                dominated_size = robj.size

    # Prefer a dominated victim if available (relative admission)
    chosen_key = dominated_victim_key if dominated_victim_key is not None else victim_key

    # Soft bias: if the best candidate is protected but there exists a window candidate close to best,
    # evict from window to preserve frequency-heavy items.
    if chosen_key is not None and m_segment.get(chosen_key, 0) == 1:
        # try to find a near-equally bad window candidate
        window_candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
        near_key = None
        near_score = None
        near_ts = None
        near_size = None
        for key in window_candidates:
            robj = cache_snapshot.cache.get(key, None)
            if robj is None:
                continue
            s = _priority_for_key(cache_snapshot, key, robj, now)
            ts = m_last_access.get(key, 0)
            if (near_key is None or
                s < near_score - _EPS or
                (abs(s - near_score) <= _EPS and (ts < near_ts or (ts == near_ts and robj.size > near_size)))):
                near_key = key
                near_score = s
                near_ts = ts
                near_size = robj.size
        if near_key is not None and (victim_score is None or near_score <= (victim_score * 1.05 + 1e-12)):
            chosen_key = near_key

    return chosen_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) with decay when due.
      - Update last access timestamp.
      - Conditional promotion from window -> protected if frequency estimate >= threshold.
      - Adapt window fraction periodically based on where hits occur.
      - Demote a few protected items if protected exceeds budget (bounded per call).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    seg = m_segment.get(key, 0)

    # Count hit location before any promotion
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Conditional promotion to protected if in window and frequency high enough
    if seg == 0:
        if _sketch_estimate(key) >= _PROMOTE_FREQ_THRESHOLD:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Keep protected within its budget by demoting a few old protected keys if needed
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # We avoid immediate promotion; promotion requires a subsequent hit.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 19:21:05,486 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Hit Rate: 0.0199

Example 2:
- Trace ID: 2
- Hit Rate: 0.4525

Example 3:
- Trace ID: 26
- Hit Rate: 0.8143

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:21:05 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:21:05,487 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:23:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:23:23,659 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:23:23,660 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware TinyLFU with Doorkeeper and Soft-Admission (A-SS-TinyLFU+)
# Key upgrades vs baseline:
# 1) Soft-admission: on miss, decide if the new object is "admitted" using a TinyLFU-based,
#    size-aware comparison against the last eviction’s baseline; non-admitted residents are
#    preferentially evicted later (simulates classic TinyLFU admission without blocking inserts).
# 2) Doorkeeper (1-bit Bloom) to filter one-timers: first-seen keys become non-admitted unless
#    they reappear; a subsequent hit flips them to admitted.
# 3) Eviction sampling biases to evict non-admitted residents first; within candidates use
#    a size-aware frequency+recency priority.
# 4) Adaptive window/protected split remains; protected kept within budget by demotion.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys

# Admission state
m_admit = dict()          # key -> 1 (admitted) or 0 (non-admitted, preferential eviction)
_g_unadmitted_keys = 0    # count of non-admitted residents

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper (1-bit Bloom filter) to cheaply suppress one-timers
_DOOR_WIDTH = 16384  # power of two
_DOOR_MASK = _DOOR_WIDTH - 1
_door_bits = bytearray(_DOOR_WIDTH // 8)  # 1 bit per position
_DOOR_SALTS = (0x27d4eb2f165667c5, 0x85ebca77c2b2ae63)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)
_SAMPLE_SIZE_UNADMIT = 24   # preferentially sample this many non-admitted (if available)

# Priority scoring weights
_SIZE_ALPHA = 1.25          # size penalty exponent (stronger size-awareness)
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_NON_ADMIT_PENALTY = 0.12   # multiply keep-score by this if not admitted (lower => easier eviction)
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False

# Admission baseline captured at eviction time (to compare with later insert)
_last_victim_admit_baseline = 0.0  # freq/size^alpha of the last evicted key
_last_victim_key = None


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _door_hash(key, salt):
    return (hash(key) ^ salt) & _DOOR_MASK

def _door_test_and_set(key):
    # Returns True if "likely seen before", and sets the bits.
    h1 = _door_hash(key, _DOOR_SALTS[0])
    h2 = _door_hash(key, _DOOR_SALTS[1])
    b1 = (h1 >> 3) & 0x1FFF  # index of byte (since width is power of two, this is safe)
    b2 = (h2 >> 3) & 0x1FFF
    m1 = 1 << (h1 & 7)
    m2 = 1 << (h2 & 7)
    seen = (_door_bits[b1] & m1) and (_door_bits[b2] & m2)
    _door_bits[b1] |= m1
    _door_bits[b2] |= m2
    return bool(seen)

def _door_clear_all():
    for i in range(len(_door_bits)):
        _door_bits[i] = 0

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        # decay doorkeeper as well
        _door_clear_all()

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _g_unadmitted_keys
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Preloaded items are considered admitted
        if key not in m_admit:
            m_admit[key] = 1
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    # Count non-admitted
    _g_unadmitted_keys = sum(1 for k, a in m_admit.items() if a == 0)
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # Non-admitted items are intentionally much easier to evict
    if m_admit.get(key, 1) == 0:
        score *= _NON_ADMIT_PENALTY
    return score

def _freq_size_baseline(key, size_bytes):
    # Admission baseline uses TinyLFU frequency and size penalty only (no recency factor).
    size = max(int(size_bytes), 1)
    freq = _sketch_estimate(key)
    return freq / (float(size) ** _SIZE_ALPHA)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_not_admitted(k, prefer_segment=None):
    # Sample up to k keys with m_admit == 0, optionally restricting to a segment.
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    max_tries = max(6 * k, 2 * k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_admit.get(key, 1) == 0 and (prefer_segment is None or m_segment.get(key, 0) == prefer_segment):
            result.append(key)
            seen.add(key)
        tries += 1
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection with soft-admission bias:
      - Prefer to evict non-admitted items first (they are likely one-timers).
      - If window byte-usage > target, restrict eviction to window keys.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
      - Record a TinyLFU size-aware baseline of the chosen victim to inform admission on next insert.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Preferentially evict non-admitted keys
    candidates = []
    if _g_unadmitted_keys > 0:
        pref_seg = 0 if restrict_to_window else None
        candidates = _sample_not_admitted(min(_SAMPLE_SIZE_UNADMIT, len(m_keys)), prefer_segment=pref_seg)

    # If not enough candidates, augment with general sampling
    if not candidates:
        if restrict_to_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
        else:
            candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
            victim_size = cache_snapshot.cache[victim_key].size
        else:
            return None

    # Store admission baseline for next insertion decision
    # Use frequency/size^alpha (no recency) of the chosen victim
    global _last_victim_admit_baseline, _last_victim_key
    _last_victim_key = victim_key
    _last_victim_admit_baseline = _freq_size_baseline(victim_key, max(1, int(victim_size)))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and doorkeeper decay if needed.
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - If item was non-admitted, flip it to admitted on hit (proves usefulness).
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_admit:
        m_admit[key] = 1  # default to admitted if unknown

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # If previously non-admitted, accept it now (since it got a hit)
    if m_admit.get(key, 1) == 0:
        m_admit[key] = 1
        globals()['_g_unadmitted_keys'] = max(0, globals()['_g_unadmitted_keys'] - 1)

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and doorkeeper bits.
      - Initialize metadata: segment=window, last_access=now.
      - Decide "soft-admission":
          * If doorkeeper indicates first-seen, mark as non-admitted.
          * Else compare size-aware TinyLFU baseline of new key vs last eviction baseline,
            admit if comparable or better; otherwise mark non-admitted.
      - Track resident key for O(1) sampling and bytes accounting.
      - Non-admitted residents remain eligible but are heavily preferred victims next time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Admission decision:
    # - first-seen (doorkeeper false) => non-admitted
    # - otherwise compare TinyLFU/size baseline vs last victim's baseline
    seen_before = _door_test_and_set(key)
    new_baseline = _freq_size_baseline(key, max(1, int(obj.size)))
    admit = False
    if not seen_before:
        admit = False
    else:
        # Allow some slack to reduce thrashing; admit if close to or better than victim
        admit = (new_baseline + 1e-9) >= (0.90 * globals()['_last_victim_admit_baseline'])

    prev = m_admit.get(key, None)
    if admit:
        m_admit[key] = 1
        # no change to _g_unadmitted_keys
    else:
        m_admit[key] = 0
        if prev != 0:
            globals()['_g_unadmitted_keys'] += 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Maintain admission counters.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Adjust admission counts
    if m_admit.pop(ev_key, 1) == 0:
        globals()['_g_unadmitted_keys'] = max(0, globals()['_g_unadmitted_keys'] - 1)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 19:23:50,511 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 14
- Hit Rate: 0.0221

Example 3:
- Trace ID: 3
- Hit Rate: 0.4234

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:23:50 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:23:50,513 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:25:46 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:25:46,670 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:25:46,671 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding and Large-object guard (WSD-TLFU+SL)
# - Global admission/priority: Count-Min Sketch (TinyLFU) with periodic decay.
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Doorkeeper promotion: require 1 hit for normal-sized objects and 2 hits for large ones.
# - Scan-shielding "doomed" set: very weak insertions are tagged and preferentially evicted first.
# - Large-object guard: cap fraction of bytes held by large objects; evict large ones first if over cap.
# - Sampling-based eviction, size-aware scoring tuned to incoming size.
# - Adaptive window fraction via hit location and ARC-style ghost feedback.
# - Lightweight age-based protected demotion to avoid stale protection.
# - Maintain compact resident index for O(1) uniform sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 48            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 16     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 24     # from "doomed" set
_SAMPLE_SIZE_LARGE = 24      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.5            # reduced recency weight to resist scans
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
_LARGE_BYTES_FRACTION = 0.45     # large objects should not exceed 45% of capacity

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    # Only consider age-demotion if protected is not under pressure (handled elsewhere) but protection might be stale.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) If any "doomed" items exist, evict the weakest among a random sample from them.
      3) Else if window is above its target bytes, pick from window.
      4) Else if large objects exceed their byte cap, pick from large objects preferentially.
      5) Else sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict oldest window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Doorkeeper promotion: promote window->protected only after sufficient resident hits (size-aware).
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Default to window accounting if unknown
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse (so it won't be evicted prematurely)
    is_large = key in m_large_keys
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require 1 hit for small, 2 hits for large
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is far weaker than resident sample, tag as "doomed" (evict-first).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic
    # Compare incoming priority vs a sample of current residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    min_resident_score = None
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        if min_resident_score is None or s < min_resident_score:
            min_resident_score = s
    # Tag as doomed if clearly weaker than typical residents, or if large and very low freq
    est_freq = _sketch_estimate(key)
    if (min_resident_score is not None and incoming_score < 0.9 * min_resident_score) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 19:30:07,462 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 11
- Hit Rate: 0.3881

Example 2:
- Trace ID: 4
- Hit Rate: 0.4622

Example 3:
- Trace ID: 43
- Hit Rate: 0.3667

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:30:07 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:30:07,463 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:33:36 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:33:36,049 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:33:36,050 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware W-TinyLFU + SLRU with Ghost Adaptation (A-SS-WTLFU-G)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based and opportunistic: prefer evicting keys worse than the incoming object.
# - Dynamic window fraction is adapted via ARC-like ghost hits and lightly by hit-location.
# - Per-resident metadata includes precise resident freq, insert time, last access; compact sampling index.
# - Doorkeeper-style rotating sets bias against polluting on single touches (evict cold items faster).

import random
from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insert)
m_resident_freq = dict()   # key -> small int (resident precise freq)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Segment byte counters (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive; initialize with moderate recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest approx)

# Priority scoring weights
_SIZE_ALPHA = 1.00          # size penalty exponent (less punitive than before)
_W_FREQ_RES = 1.0           # weight for resident precise frequency
_W_FREQ_GLOB = 0.50         # weight for global CMS estimate
_W_RECENCY = 2.0            # weight for recency (lower than before to reduce recency bias)
_PROTECTED_KEEP_BONUS = 1.25# protected items are harder to evict
_NEW_ITEM_PENALTY = 0.80    # multiplier for new/cold items (push them out sooner)
_COLD_AGE = 2000            # accesses since insert considered "new/cold"
_FREQ_SAT = 255             # resident freq saturating cap
_EPS = 1e-9

# Doorkeeper-style rotating sets (soft 2nd-touch preference)
_DOOR_ROTATE = 10000
_door_epoch = 0
_door_sets = [set(), set()]  # rotated

# ARC-like ghost histories (keys only)
_GHOST_MAX = 8192  # per-ghost max keys; bounded memory
_ghost_w = OrderedDict()    # recently evicted from window
_ghost_p = OrderedDict()    # recently evicted from protected

# Light hit-location adaptation (secondary to ghost-based adaptation)
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_resident_freq:
            m_resident_freq[key] = 1
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _rotate_doorkeeper_if_needed(now):
    global _door_epoch, _door_sets
    # Use access count to rotate epochs
    epoch = now // _DOOR_ROTATE
    if epoch != _door_epoch:
        _door_epoch = epoch
        # rotate: discard oldest set, shift newer to older, clear new current
        _door_sets[1] = _door_sets[0]
        _door_sets[0] = set()

def _ghost_record(ev_key, seg):
    # Record evicted key into ghost list with bounded size, most-recent at end
    if seg == 0:
        od = _ghost_w
    else:
        od = _ghost_p
    if ev_key in od:
        try:
            od.pop(ev_key)
        except KeyError:
            pass
    od[ev_key] = None
    # Trim oldest if over capacity
    while len(od) > _GHOST_MAX:
        try:
            od.popitem(last=False)
        except KeyError:
            break

def _ghost_adapt_on_insert(new_key):
    # If a miss inserts a key that is in window ghost: increase window fraction.
    # If in protected ghost: decrease window fraction.
    global _f_window
    bumped = False
    if new_key in _ghost_w:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        try:
            _ghost_w.pop(new_key)
        except KeyError:
            pass
        bumped = True
    elif new_key in _ghost_p:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p.pop(new_key)
        except KeyError:
            pass
        bumped = True
    return bumped

def _priority_for_key(cache_snapshot, key, obj, now):
    # Combined resident-precise freq with global CMS estimate; tempered by recency and size.
    size = max(int(obj.size), 1)
    freq_res = min(_FREQ_SAT, int(m_resident_freq.get(key, 0)))
    freq_global = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # recent -> closer to 1

    # New/cold penalty: recently inserted and not observed much yet => make easier to evict
    insert_age = max(0, now - m_insert_ts.get(key, 0))
    is_cold = (insert_age <= _COLD_AGE and freq_res < 2)

    score = (_W_FREQ_RES * freq_res + _W_FREQ_GLOB * freq_global + _W_RECENCY * recency)
    if is_cold:
        score *= _NEW_ITEM_PENALTY

    # Protected keep bonus
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    # Size-aware normalization
    score /= (float(size) ** _SIZE_ALPHA)
    return score

def _priority_for_incoming(cache_snapshot, obj, now):
    # Admission priority proxy for incoming: no resident freq; full recency; size-aware.
    size = max(int(obj.size), 1)
    freq_global = _sketch_estimate(obj.key)
    recency = 1.0  # being accessed now
    score = (_W_FREQ_RES * 0.0 + _W_FREQ_GLOB * freq_global + _W_RECENCY * recency)
    # Treat incoming as cold (applies initial penalty)
    score *= _NEW_ITEM_PENALTY
    score /= (float(size) ** _SIZE_ALPHA)
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        # fallback to global if sampling missed
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_hitloc_if_needed():
    # Light, secondary adaptation: where hits happened
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection (opportunistic W-TinyLFU style):
      - Compute incoming object's priority (from CMS, size, recency).
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally. Prefer victims whose score < incoming score.
        If none, try sampling window-only; if still none, evict the min-score key globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    incoming_score = _priority_for_incoming(cache_snapshot, obj, now)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed, scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Evaluate candidates, partition by whether they are worse than incoming
    worse_than_incoming = []
    all_evals = []

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        tup = (key, score, last_ts, robj.size)
        all_evals.append(tup)
        if score < incoming_score - _EPS:
            worse_than_incoming.append(tup)

    def _choose_min(tuples_list):
        nonlocal victim_key, victim_score, victim_last_ts, victim_size
        for (key, score, last_ts, size) in tuples_list:
            if (victim_key is None or
                score < victim_score - _EPS or
                (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                        (last_ts == victim_last_ts and size > victim_size)))):
                victim_key = key
                victim_score = score
                victim_last_ts = last_ts
                victim_size = size

    if restrict_to_window:
        _choose_min(all_evals)
    else:
        if worse_than_incoming:
            _choose_min(worse_than_incoming)
        else:
            # Try window-only to avoid harming protected if possible
            window_candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
            if window_candidates:
                wevals = []
                for key in window_candidates:
                    robj = cache_snapshot.cache.get(key, None)
                    if robj is None:
                        continue
                    wevals.append((key, _priority_for_key(cache_snapshot, key, robj, now),
                                   m_last_access.get(key, 0), robj.size))
                if wevals:
                    _choose_min(wevals)
            if victim_key is None:
                _choose_min(all_evals)

    # As a final guard, if sampling picked nothing, evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update CMS, resident precise freq (saturating), and last access.
      - Record hit location for light adaptation.
      - Promote from window -> protected when touched (freq>=2); not on every touch if already protected.
      - If protected exceeds budget, demote an old protected item to window.
      - Rotate doorkeeper sets periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    now = cache_snapshot.access_count
    _rotate_doorkeeper_if_needed(now)

    key = obj.key

    # Ensure resident metadata defaults if missing
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_resident_freq:
        m_resident_freq[key] = 1
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # We cannot know its exact segment bytes impact if segment unknown; assume current segment is valid
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz

    seg = m_segment.get(key, 0)

    # Count hit location before any promotion
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access and precise resident frequency
    m_last_access[key] = now
    m_resident_freq[key] = min(_FREQ_SAT, m_resident_freq.get(key, 0) + 1)

    # Promote to protected if in window and has at least two touches
    if seg == 0 and m_resident_freq.get(key, 0) >= 2:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction based on hit location (secondary mechanism)
    _adapt_window_fraction_hitloc_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update CMS counts for this access and decay if needed.
      - ARC-like adaptation via ghost hits: adjust window fraction immediately when ghosts hit.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_freq=1.
      - Track resident key for O(1) sampling and bytes accounting.
      - Doorkeeper rotation and record.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    now = cache_snapshot.access_count
    _rotate_doorkeeper_if_needed(now)

    # ARC-like ghost adaptation: react to where the miss hit
    _ghost_adapt_on_insert(obj.key)

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    # Start resident precise freq at 1 due to this access
    m_resident_freq[key] = 1

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Doorkeeper: record this touch in the current epoch set
    _door_sets[0].add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Record the evicted key in the appropriate ghost list for ARC-like adaptation.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_resident_freq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost history for adaptation
    _ghost_record(ev_key, seg)
```
2025-11-27 19:34:44,093 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 24
- Error: Error - Timeout

Example 2:
- Trace ID: 47
- Hit Rate: 0.0817

Example 3:
- Trace ID: 41
- Hit Rate: 0.0725

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:34:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:34:44,095 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:36:37 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:36:37,864 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:36:37,866 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Robust, byte-aware SLRU with TinyLFU-based promotion and ARC-style ghost feedback
# Goals:
# - Strong recency via two LRU segments: window (probation) and protected (frequent)
# - Size-aware: all accounting by bytes; repeated evictions handle large insertions
# - TinyLFU counts (with decay) steer promotion of oversized/low-frequency items
# - ARC-like ghost history adjusts window/protected byte split automatically
# - O(1) operations using OrderedDict-based LRUs; no random sampling (avoids timeouts)

from collections import OrderedDict, deque

# ------------------------
# Global metadata
# ------------------------
# Segment maps
_seg = dict()  # key -> 0 (window) or 1 (protected)

# LRU structures (OrderedDict: left=LRU, right=MRU)
_win_od = OrderedDict()  # key -> None
_prot_od = OrderedDict()  # key -> None

# Byte counters
_win_bytes = 0
_prot_bytes = 0

# Average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch with decay
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 2048  # smaller to keep decay fast/reliable
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.05

# Adaptation by hit location
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_w = 0
_hits_p = 0

# Ghost histories (ARC-like guidance)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 1.5  # ghost capacity in items ≈ factor * (capacity / avg_size)

# Bootstrap
_bootstrapped = False
_last_capacity = None


# ------------------------
# Helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est is float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _current_window_target_bytes(capacity):
    return max(1, int(_f_window * int(capacity)))

def _ghost_capacity_items(capacity):
    # Estimate item capacity; scale for ghost lists
    est_items = max(1, int(int(capacity) / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot.capacity)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_w, _hits_p, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_w + _hits_p
    if total > 0:
        frac_w = _hits_w / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_w = 0
    _hits_p = 0

def _maybe_demote_if_needed(cache_snapshot):
    # Keep protected within its byte budget by demoting LRU protected to window until satisfied
    global _prot_bytes, _win_bytes
    target_window = _current_window_target_bytes(cache_snapshot.capacity)
    prot_budget = max(0, int(cache_snapshot.capacity) - target_window)
    # Demote while protected exceeds budget
    while _prot_bytes > prot_budget and _prot_od:
        k, _ = _prot_od.popitem(last=False)  # LRU of protected
        _seg[k] = 0
        # move to MRU of window
        _win_od[k] = None
        # size
        robj = cache_snapshot.cache.get(k, None)
        if robj is not None:
            sz = int(robj.size)
            _prot_bytes -= sz
            _win_bytes += sz

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _last_capacity, _avg_size
    global _seg, _win_od, _prot_od, _win_bytes, _prot_bytes
    if _bootstrapped and _last_capacity == cache_snapshot.capacity:
        return
    # Reset all structures
    _seg.clear()
    _win_od.clear()
    _prot_od.clear()
    _win_bytes = 0
    _prot_bytes = 0
    _last_capacity = cache_snapshot.capacity
    # Seed with current residents as window (probation) in arbitrary order
    total_sz = 0
    cnt = 0
    for key, robj in cache_snapshot.cache.items():
        _seg[key] = 0
        _win_od[key] = None
        sz = int(robj.size)
        _win_bytes += sz
        total_sz += sz
        cnt += 1
    if cnt > 0:
        _avg_size = max(1.0, float(total_sz) / float(cnt))
    _bootstrapped = True


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy:
      - SLRU: prefer evicting from window (probation) LRU.
      - Ensure protected stays within its budget by demoting LRU of protected as needed.
      - If window is empty, demote one protected LRU to window, then evict from window.
      - Pure O(1) operations (OrderedDict), avoids sampling and heavy scans.
    """
    _bootstrap_if_needed(cache_snapshot)

    # Keep protected within budget
    _maybe_demote_if_needed(cache_snapshot)

    # Ensure there is a probation victim available
    global _win_od, _prot_od
    if not _win_od and _prot_od:
        # Demote one from protected to create a probation victim
        k, _ = _prot_od.popitem(last=False)
        _seg[k] = 0
        _win_od[k] = None
        robj = cache_snapshot.cache.get(k, None)
        if robj is not None:
            sz = int(robj.size)
            globals()['_prot_bytes'] -= sz
            globals()['_win_bytes'] += sz

    # Choose LRU of window (probation). If empty, fall back to protected LRU.
    if _win_od:
        victim_key, _ = _win_od.popitem(last=False)
        return victim_key
    elif _prot_od:
        victim_key, _ = _prot_od.popitem(last=False)
        return victim_key
    else:
        # Should not happen, but guard anyway
        for k in cache_snapshot.cache.keys():
            return k
        return None

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU and average size.
      - If in window: either promote to protected (default) or keep in window if oversized and cold.
      - If in protected: move to MRU in protected.
      - Track hit location to adapt window fraction periodically.
      - Keep protected within budget by demoting as needed.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # Ensure segment info; if missing (rare), treat as fresh insert into window
    seg = _seg.get(key, None)
    if seg is None:
        _seg[key] = 0
        _win_od[key] = None
        globals()['_win_bytes'] += sz
        seg = 0

    # Adaptation counters
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_w'] += 1
    else:
        globals()['_hits_p'] += 1

    # Window hit: promote to protected unless oversized and cold
    if seg == 0:
        # Refresh in window first (remove if present)
        if key in _win_od:
            _win_od.pop(key, None)

        # Promotion gate: avoid protecting very large and low-frequency items
        est = _sketch_estimate(key)
        large = sz > 4 * _avg_size
        cold = est < 2.0

        if large and cold:
            # Keep in window MRU (recency only)
            _win_od[key] = None
        else:
            # Promote to protected MRU
            _seg[key] = 1
            _prot_od[key] = None
            # Byte accounting
            globals()['_win_bytes'] -= sz
            globals()['_prot_bytes'] += sz
            # Enforce protected budget
            _maybe_demote_if_needed(cache_snapshot)

    else:
        # Protected hit: move to MRU
        if key in _prot_od:
            _prot_od.move_to_end(key, last=True)
        else:
            # If somehow missing, add to protected
            _seg[key] = 1
            _prot_od[key] = None
            globals()['_prot_bytes'] += sz
            _maybe_demote_if_needed(cache_snapshot)

    # Periodic window fraction adaptation
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU and average size.
      - Insert into window MRU (probation).
      - Ghost feedback: if key hit in window-ghost => increase window; in protected-ghost => decrease window.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # If already present in our structures (rare), normalize by removing first
    prev_seg = _seg.get(key, None)
    if prev_seg == 0 and key in _win_od:
        _win_od.pop(key, None)
        globals()['_win_bytes'] -= sz
    elif prev_seg == 1 and key in _prot_od:
        _prot_od.pop(key, None)
        globals()['_prot_bytes'] -= sz

    # Insert into window MRU
    _seg[key] = 0
    _win_od[key] = None
    globals()['_win_bytes'] += sz

    # Ghost feedback adaptation (ARC-style)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
        # lazy removal from deque during capacity trim
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget respected (in case window fraction dropped)
    _maybe_demote_if_needed(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove metadata from appropriate segment and adjust byte counters.
      - Add evicted key to proper ghost list (segment-aware) to guide adaptation.
    """
    _bootstrap_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_sz = int(evicted_obj.size)
    seg = _seg.pop(ev_key, 0)

    if seg == 0:
        if ev_key in _win_od:
            _win_od.pop(ev_key, None)
        globals()['_win_bytes'] -= ev_sz
    else:
        if ev_key in _prot_od:
            _prot_od.pop(ev_key, None)
        globals()['_prot_bytes'] -= ev_sz

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 19:39:25,778 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 0
- Hit Rate: 0.4662

Example 3:
- Trace ID: 33
- Hit Rate: 0.7332

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:39:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:39:25,779 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:41:43 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:43,143 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:43,144 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware Windowed TinyLFU with SLRU main (AW-SLRU-TinyLFU)
# - Global admission estimator: Count-Min Sketch with periodic aging (decay).
# - Three in-cache segments:
#     0: window (recency buffer, LRU)
#     1: probation (main, admits after passing window; LRU victim)
#     2: protected (main hot set; LRU demoted to probation when over budget)
# - Adaptive window sizing using lightweight ghost lists (ARC-inspired).
# - Eviction prefers:
#     a) If window is over its byte budget, evict LRU from window.
#     b) Else evict LRU from probation. If the incoming object's score is
#        significantly worse than the probation LRU's, evict from window LRU
#        (approximates admission "rejection" without bypass support).
#     c) If probation empty, evict window LRU; if that is empty, evict protected LRU.
# - Per-resident metadata is compact: segment id and last access timestamp.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window), 1 (probation), 2 (protected)
m_last_access = dict()   # key -> int (timestamp)

# Count-Min Sketch (TinyLFU estimator)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384     # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_WINDOW_FRAC_INIT = 0.20          # initial fraction of capacity (bytes) for window
_WINDOW_MIN_FRAC = 0.02           # min allowed window fraction
_WINDOW_MAX_FRAC = 0.50           # max allowed window fraction
_PROTECTED_FRAC_OF_MAIN = 0.80    # of the non-window space
_SIZE_ALPHA = 0.80                # size exponent (softer than 1.0)
_RECENCY_LAMBDA = 2.0             # recency multiplier for resident scoring
_ADMIT_WINDOW_BIAS = 1.0          # if candidate score < bias * probation-LRU score, evict from window instead
_GHOST_MAX = 4096                 # bound for each ghost list length
_EPS = 1e-9

# Adaptive window target (bytes)
_window_target_bytes = None

# Ghost lists (for adaptive window sizing); store key -> (timestamp)
# - _ghost_win: keys evicted from window
# - _ghost_main: keys evicted from probation/protected (main)
_ghost_win = dict()
_ghost_main = dict()

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_resident(cache_snapshot, key, obj, now):
    # Resident keep-priority: higher is better to keep
    # score = (F * (1 + lambda * recency)) / size^alpha
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)
    return (freq * mult) / (float(size) ** _SIZE_ALPHA)

def _priority_for_candidate(obj):
    # Candidate (not yet resident): frequency-only, size-aware
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(obj.key)
    return freq / (float(size) ** _SIZE_ALPHA)

def _segment_bytes(cache_snapshot):
    win_bytes = prob_bytes = prot_bytes = 0
    win_count = prob_count = prot_count = 0
    for k, robj in cache_snapshot.cache.items():
        seg = m_segment.get(k, 0)
        if seg == 0:
            win_bytes += robj.size
            win_count += 1
        elif seg == 1:
            prob_bytes += robj.size
            prob_count += 1
        else:
            prot_bytes += robj.size
            prot_count += 1
    return (win_bytes, prob_bytes, prot_bytes, win_count, prob_count, prot_count)

def _find_lru_in_segment(cache_snapshot, seg_id):
    # Returns (key, last_ts, size) for the LRU in the given segment, else (None, None, None)
    lru_key = None
    lru_ts = None
    lru_size = None
    for k, robj in cache_snapshot.cache.items():
        if m_segment.get(k, 0) != seg_id:
            continue
        ts = m_last_access.get(k, 0)
        if (lru_key is None or
            ts < lru_ts or
            (ts == lru_ts and robj.size > lru_size)):
            lru_key = k
            lru_ts = ts
            lru_size = robj.size
    return lru_key, lru_ts, lru_size

def _init_window_target_if_needed(cache_snapshot):
    global _window_target_bytes
    if _window_target_bytes is None:
        cap = max(1, int(cache_snapshot.capacity))
        _window_target_bytes = max(1, int(_WINDOW_FRAC_INIT * cap))

def _clamp_window_target(cache_snapshot):
    global _window_target_bytes
    cap = max(1, int(cache_snapshot.capacity))
    min_b = int(_WINDOW_MIN_FRAC * cap)
    max_b = int(_WINDOW_MAX_FRAC * cap)
    if _window_target_bytes < min_b:
        _window_target_bytes = min_b
    if _window_target_bytes > max_b:
        _window_target_bytes = max_b

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    # prune oldest if beyond bound
    if len(ghost_dict) > _GHOST_MAX:
        # remove oldest by timestamp
        oldest_k = None
        oldest_ts = None
        for gk, ts in ghost_dict.items():
            if oldest_k is None or ts < oldest_ts:
                oldest_k, oldest_ts = gk, ts
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _demote_protected_if_needed(cache_snapshot, prot_target_bytes):
    # Demote protected LRU -> probation until protected bytes <= target
    win_b, prob_b, prot_b, _, _, _ = _segment_bytes(cache_snapshot)
    if prot_b <= prot_target_bytes:
        return
    now = cache_snapshot.access_count
    # Demote repeatedly if far over budget
    iteration_guard = 0
    while prot_b > prot_target_bytes and iteration_guard < 16:
        iteration_guard += 1
        key, _, _ = _find_lru_in_segment(cache_snapshot, 2)
        if key is None:
            break
        # Demote to probation
        obj = cache_snapshot.cache.get(key)
        if obj is None:
            break
        m_segment[key] = 1
        prot_b -= obj.size
        prob_b += obj.size
        # keep last_access unchanged; promotion/demotion doesn't change recency intrinsically
        # update 'now' not needed

def _adapt_window_target_on_insert(cache_snapshot, obj):
    # ARC-inspired: if the key is in window ghost -> increase window; if in main ghost -> decrease window.
    global _window_target_bytes
    now = cache_snapshot.access_count
    cap = max(1, int(cache_snapshot.capacity))
    step = max(obj.size, cap // 64)  # adapt step proportional to size/capacity
    if obj.key in _ghost_win:
        _window_target_bytes += step
        _ghost_win.pop(obj.key, None)
    elif obj.key in _ghost_main:
        _window_target_bytes -= step
        _ghost_main.pop(obj.key, None)
    _clamp_window_target(cache_snapshot)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain segment budgets.
      - If window byte-usage > target, evict LRU from window.
      - Else prefer evicting LRU from probation.
      - If candidate's score is much worse than probation LRU's, evict from window LRU instead
        to approximate TinyLFU admission without bypass capability.
      - Fallbacks: window LRU, then protected LRU.
    '''
    now = cache_snapshot.access_count
    _init_window_target_if_needed(cache_snapshot)
    _clamp_window_target(cache_snapshot)

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute segment usage
    win_b, prob_b, prot_b, win_cnt, prob_cnt, prot_cnt = _segment_bytes(cache_snapshot)

    # Main (non-window) budget and protected budget
    cap = max(1, int(cache_snapshot.capacity))
    main_target = max(0, cap - _window_target_bytes)
    prot_target = int(_PROTECTED_FRAC_OF_MAIN * main_target)

    # Keep protected within budget by demoting its LRU into probation
    _demote_protected_if_needed(cache_snapshot, prot_target)

    # Recompute segment usage after potential demotions (approximate but safe)
    win_b, prob_b, prot_b, win_cnt, prob_cnt, prot_cnt = _segment_bytes(cache_snapshot)

    # 1) If window is over budget, evict LRU from window
    if win_b > _window_target_bytes and win_cnt > 0:
        k, _, _ = _find_lru_in_segment(cache_snapshot, 0)
        return k

    # 2) Prefer evicting probation LRU
    prob_lru_key, _, _ = _find_lru_in_segment(cache_snapshot, 1)
    if prob_lru_key is not None:
        # Candidate-aware bias: if candidate is weaker than probation LRU, evict from window LRU instead
        cand_score = _priority_for_candidate(obj)
        prob_obj = cache_snapshot.cache[prob_lru_key]
        prob_score = _priority_for_resident(cache_snapshot, prob_lru_key, prob_obj, now)
        if win_cnt > 0 and cand_score < _ADMIT_WINDOW_BIAS * prob_score:
            k, _, _ = _find_lru_in_segment(cache_snapshot, 0)
            if k is not None:
                return k
        return prob_lru_key

    # 3) If no probation items, evict window LRU if available
    if win_cnt > 0:
        k, _, _ = _find_lru_in_segment(cache_snapshot, 0)
        if k is not None:
            return k

    # 4) Last resort: evict protected LRU
    prot_key, _, _ = _find_lru_in_segment(cache_snapshot, 2)
    return prot_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch.
      - Update last access timestamp.
      - Promote:
          window -> probation
          probation -> protected
          protected -> stay (refresh recency)
      - Keep protected within budget by demoting its LRU to probation if needed.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    seg = m_segment.get(key, 0)
    if seg == 0:
        m_segment[key] = 1  # window -> probation
    elif seg == 1:
        m_segment[key] = 2  # probation -> protected
    else:
        # protected: nothing to change besides recency timestamp already updated
        pass

    # Enforce protected budget after promotions
    _init_window_target_if_needed(cache_snapshot)
    cap = max(1, int(cache_snapshot.capacity))
    main_target = max(0, cap - _window_target_bytes)
    prot_target = int(_PROTECTED_FRAC_OF_MAIN * main_target)
    _demote_protected_if_needed(cache_snapshot, prot_target)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Adapt window target based on ghost hits.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    _init_window_target_if_needed(cache_snapshot)
    _adapt_window_target_on_insert(cache_snapshot, obj)

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (window or main).
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    now = cache_snapshot.access_count
    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_add(_ghost_win, ev_key, now)
    else:
        _ghost_add(_ghost_main, ev_key, now)
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 19:45:43,020 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Size-aware SLRU + TinyLFU with ARC-style window tuning (ASL-TinyLFU)
# - Global admission frequency via Count-Min Sketch with periodic aging.
# - Two in-cache segments:
#     0: "window"/probation (recency-biased)
#     1: "protected" (frequency-biased)
# - Eviction:
#     * If window bytes exceed its adaptive target, evict from window only (recency-biased SLRU).
#     * Otherwise, evict global minimum priority (size-aware TinyLFU + recency + protected bias).
# - Adaptive window sizing using two ghost lists (ARC-style):
#     * If a missed key was recently evicted from window-ghost -> increase window fraction.
#     * If from protected-ghost -> decrease window fraction.
# - Per-resident metadata: segment id, last access time, resident hit counter, promotion threshold.
# - Size-awareness: priority divides by size**alpha; larger objects require more hits to promote.

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style)
# ------------------------
_gw = dict()              # window-ghost: key -> (last_ts, size)
_gp = dict()              # protected-ghost: key -> (last_ts, size)
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000      # more responsive aging
_decay_ticker = 0
_just_decayed = False      # set True when sketch decay occurs

# ------------------------
# Tunables (carefully chosen defaults)
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02   # base step for adaptive adjustments
_window_frac = 0.25        # initial window fraction

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.25

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.7
_FREQ_RESIDENT_WEIGHT = 0.3

# Numerical epsilon and other helpers
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # ARC uses |B1|, |B2| up to cache size each

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    # When sketch ages, also age resident hit counters to avoid stale bias.
    global _just_decayed
    if _just_decayed:
        # Halve all resident hit counters
        keys = list(m_res_hits.keys())
        for k in keys:
            m_res_hits[k] >>= 1
        _just_decayed = False

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., preloaded items)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        # Without obj/capacity we default to small promotion threshold
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if obj.size >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need

def _ghost_add(segment, key, size, now):
    # segment: 0 -> window-ghost; 1 -> protected-ghost
    global _gw_bytes, _gp_bytes
    if segment == 0:
        prev = _gw.get(key)
        if prev is None:
            _gw[key] = (now, int(size))
            _gw_bytes += int(size)
        else:
            # refresh timestamp; keep size as last known
            _gw[key] = (now, prev[1])
    else:
        prev = _gp.get(key)
        if prev is None:
            _gp[key] = (now, int(size))
            _gp_bytes += int(size)
        else:
            _gp[key] = (now, prev[1])

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries
    global _gw_bytes, _gp_bytes
    # Prune window-ghost
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        # Find oldest by scanning (bounded by ghost size; acceptable under budget)
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gw.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gw.pop(oldest_key)
        _gw_bytes -= int(sz)
    # Prune protected-ghost
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gp.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gp.pop(oldest_key)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key, now):
    # ARC-style adaptive control using ghost hits
    # If the key is in window-ghost (B1): increase window target.
    # If in protected-ghost (B2): decrease window target.
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # step proportional to max(1, |B2|/|B1|)
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        _, sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        _, sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    # Periodically prune ghosts to budgets
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus (harder to evict)
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among window only (probation).
      - Otherwise, evict the global minimum-priority key (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)

    # Ensure resident metadata exists for all current items
    for key, robj in cache_snapshot.cache.items():
        _ensure_resident_metadata(cache_snapshot, key, robj)

    # Compute window usage and target
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += int(robj.size)
            has_window = True

    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # only consider window while over target

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    m_last_access[key] = cache_snapshot.access_count
    # Increment resident hits (saturating via Python int; decay throttles growth)
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    # Size-aware promotion
    if m_segment.get(key, 0) == 0:
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key, now)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    now = cache_snapshot.access_count

    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)
    size = int(evicted_obj.size)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, size, now)
    _ghost_prune()

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Error: Error - Timeout

Example 2:
- Trace ID: 27
- Hit Rate: 0.8935

Example 3:
- Trace ID: 26
- Hit Rate: 0.7994

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:45:43 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:45:43,021 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:47:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:47:28,298 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:47:28,299 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 19:50:28,190 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Error: Error - Timeout

Example 2:
- Trace ID: 30
- Hit Rate: 0.0478

Example 3:
- Trace ID: 15
- Error: Error - Timeout

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:50:28 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:50:28,191 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:52:40 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:52:40,570 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:52:40,571 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```
2025-11-27 19:55:21,531 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 8
- Hit Rate: 0.5391

Example 2:
- Trace ID: 1
- Hit Rate: 0.4434

Example 3:
- Trace ID: 22
- Hit Rate: 0.0593

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:55:21 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:55:21,532 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:57:43 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:57:43,622 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:57:43,624 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: ARC-adaptive, Sampled, Size-normalized TinyLFU (ARC-SS-TinyLFU)
# Key changes vs previous version:
# - ARC-style online adaptation using "ghost" caches (recently evicted keys) to tune window fraction.
# - Safer promotion: require a second hit in window before promoting to protected (reduces pollution).
# - Size normalization using an EWMA of observed sizes; smoother size penalty than fixed exponent.
# - Eviction scoring simulates TinyLFU admission by comparing incoming vs candidate frequency.
# - Oldest-first bias via per-segment min-heaps to ensure truly old entries are considered.
# - Protected budget strictly enforced via heap-based demotion (approximate LRU).

import random
import heapq

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_time = dict()     # key -> int (timestamp when inserted into cache)
m_win_hits = dict()        # key -> number of hits observed while in window (reset on promotion/demotion)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys
# Byte counters for segments
_g_window_bytes = 0
_g_protected_bytes = 0

# Per-segment "oldest" heaps: (last_access_ts, key)
_heap_window = []
_heap_protected = []

# Ghost caches for ARC-like adaptation (recently evicted)
_ghostW = dict()           # key -> last seen ts (from window)
_ghostP = dict()           # key -> last seen ts (from protected)
_heap_ghostW = []          # (ts, key)
_heap_ghostP = []          # (ts, key)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (ARC-adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected (fallback)
_INCLUDE_OLDEST = 4          # oldest keys to include from heaps into eviction candidates

# Priority scoring weights
_SIZE_ALPHA = 1.00           # size penalty exponent on normalized size
_W_FREQ = 1.0
_W_RECENCY = 2.5
_PROTECTED_KEEP_BONUS = 1.25 # protected items are a bit harder to evict
_EPS = 1e-9

# Promotion control
_PROMOTE_WINDOW_HITS = 1     # require this many hits in window before promoting (i.e., second touch)

# Ghost size limits (by count; trimmed automatically)
def _ghost_limit_keys():
    # dynamic: ~2x residents (bounded)
    n = max(0, len(m_keys))
    return max(1024, min(65536, 2 * n + 512))

# EWMA for average object size (for normalization)
_avg_size = 4096.0
_EWMA_ALPHA = 0.002  # update rate for size EWMA

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(objsize):
    global _avg_size
    try:
        s = float(max(1, int(objsize)))
    except Exception:
        s = 1.0
    _avg_size = (1.0 - _EWMA_ALPHA) * _avg_size + _EWMA_ALPHA * s

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_insert_time:
            m_insert_time[key] = now
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
            heapq.heappush(_heap_window, (m_last_access.get(key, now), key))
        else:
            _g_protected_bytes += int(robj.size)
            heapq.heappush(_heap_protected, (m_last_access.get(key, now), key))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _heap_push_for_seg(seg, key, ts):
    if seg == 0:
        heapq.heappush(_heap_window, (ts, key))
    else:
        heapq.heappush(_heap_protected, (ts, key))

def _heap_peek_unique_oldest(seg, count):
    # Return up to 'count' unique keys considered oldest by heap (validating timestamps/segment)
    out = []
    seen = set()
    heap = _heap_window if seg == 0 else _heap_protected
    while heap and len(out) < count:
        ts, k = heap[0]
        # Validate
        actual_seg = m_segment.get(k, -1)
        actual_ts = m_last_access.get(k, -1)
        if actual_seg != seg or actual_ts != ts:
            # stale entry
            heapq.heappop(heap)
            continue
        if k in seen:
            heapq.heappop(heap)
            continue
        out.append(k)
        seen.add(k)
        # Do not pop; leave in heap so future calls still find it if unchanged
        # To avoid returning same key repeatedly in a single selection, we break here
        # but since we only peek, we move to next by temporarily popping and pushing back
        # However to keep O(1), we just continue sampling; duplicates filtered by 'seen'
        # This is acceptable since candidate selection also includes random samples.
        # We'll break artificially by popping and pushing back to proceed
        heapq.heappop(heap)
        heapq.heappush(heap, (ts, k))
    return out

def _ghost_add(seg, key, now):
    # Add evicted key to appropriate ghost cache and trim
    if seg == 0:
        _ghostW[key] = now
        heapq.heappush(_heap_ghostW, (now, key))
        _ghost_trim_if_needed()
    else:
        _ghostP[key] = now
        heapq.heappush(_heap_ghostP, (now, key))
        _ghost_trim_if_needed()

def _ghost_trim_if_needed():
    limit = _ghost_limit_keys()
    # Trim W
    while len(_ghostW) > limit and _heap_ghostW:
        ts, k = heapq.heappop(_heap_ghostW)
        if _ghostW.get(k, None) == ts:
            _ghostW.pop(k, None)
    # Trim P
    while len(_ghostP) > limit and _heap_ghostP:
        ts, k = heapq.heappop(_heap_ghostP)
        if _ghostP.get(k, None) == ts:
            _ghostP.pop(k, None)

def _maybe_demote_from_protected(cache_snapshot):
    # Enforce protected budget (bytes) by demoting oldest protected to window
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Pop oldest valid protected item(s)
    attempts = 0
    max_attempts = 3 * _SAMPLE_SIZE_DEMOTE
    while _g_protected_bytes > protected_budget and attempts < max_attempts:
        # Try from heap first
        candidates = _heap_peek_unique_oldest(1, 1)
        key = candidates[0] if candidates else None
        if key is None:
            # Fallback: sample few and take oldest
            sample = _sample_keys_from_segment(1, max(1, _SAMPLE_SIZE_DEMOTE))
            if not sample:
                break
            key = min(sample, key=lambda k: m_last_access.get(k, 0))
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            attempts += 1
            continue
        size = int(robj.size)
        if m_segment.get(key, 0) == 1:
            m_segment[key] = 0
            _g_protected_bytes -= size
            _g_window_bytes += size
            # reset window-hit counter upon demotion
            m_win_hits[key] = 0
            # push into window heap with same last_access
            _heap_push_for_seg(0, key, m_last_access.get(key, cache_snapshot.access_count))
        attempts += 1

def _size_penalty(size):
    # Normalize by EWMA average size; clamp to avoid extremes
    s = float(max(1, int(size)))
    norm = max(0.5, min(4.0, s / max(1.0, _avg_size)))
    return norm ** _SIZE_ALPHA

def _keep_priority(cache_snapshot, key, obj, now, incoming_est):
    # Higher score means "keep" more; eviction picks the min score.
    seg = m_segment.get(key, 0)
    size_pen = _size_penalty(obj.size)
    freq = _sketch_estimate(key)
    # compress frequency to reduce heavy hitters' dominance
    f_adj = (freq + 1.0) ** 0.5
    # Recency
    age = max(0, now - m_last_access.get(key, 0))
    rec = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * f_adj + _W_RECENCY * rec) / (size_pen + _EPS)
    # Protect protected
    if seg == 1:
        score *= _PROTECTED_KEEP_BONUS
    # Relative to incoming: if candidate has much higher freq than incoming, increase its keep score
    inc = max(0.0, float(incoming_est))
    rel = ((freq + 1.0) / (inc + 1.0)) ** 0.5
    score *= rel
    return score

def _collect_candidates(cache_snapshot, restrict_to_window, now, include_oldest=True):
    # Prefer evicting from window; if not restricted include a mix with bias to window.
    if restrict_to_window:
        base = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
        oldest = _heap_peek_unique_oldest(0, min(_INCLUDE_OLDEST, len(m_keys)))
        return list(dict.fromkeys((oldest or []) + (base or [])))  # unique, preserve order
    # Mixed sampling with bias to window
    k_w = int(0.70 * _SAMPLE_SIZE)
    k_p = _SAMPLE_SIZE - k_w
    sample_w = _sample_keys_from_segment(0, min(k_w, len(m_keys)))
    sample_p = _sample_keys_from_segment(1, min(k_p, len(m_keys)))
    oldest_w = _heap_peek_unique_oldest(0, min(_INCLUDE_OLDEST, len(m_keys)))
    oldest_p = _heap_peek_unique_oldest(1, min(max(1, _INCLUDE_OLDEST // 2), len(m_keys)))
    combined = (oldest_w or []) + (oldest_p or []) + (sample_w or []) + (sample_p or [])
    # Unique preserve order
    return list(dict.fromkeys(combined))


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Victim selection:
      - If window byte-usage > target, pick from window only.
      - Else, pick from mixed set with bias to window and include a few "oldest" from heaps.
      - Evict the key with the smallest keep-priority score (frequency+recency normalized by size),
        adjusted by relative frequency vs the incoming object (TinyLFU-like).
      - Tie-break by oldest, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Estimate incoming frequency once
    incoming_est = _sketch_estimate(obj.key)

    # Determine whether to restrict eviction to window
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates
    candidates = _collect_candidates(cache_snapshot, restrict_to_window, now, include_oldest=True)
    if not candidates:
        # Absolute fallback
        candidates = list(cache_snapshot.cache.keys())

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _keep_priority(cache_snapshot, key, robj, now, incoming_est)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay.
      - Update last access timestamp and per-segment heaps.
      - Count a "window hit" and promote to protected only after the second touch in window.
      - Enforce protected budget (demote oldest if needed).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident bookkeeping exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_time:
        m_insert_time[key] = now
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown; update bytes counters conservatively
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz

    seg = m_segment.get(key, 0)

    # Update last access and heap
    m_last_access[key] = now
    _heap_push_for_seg(seg, key, now)

    # Window hit-based promotion: require second touch in window
    if seg == 0:
        cnt = m_win_hits.get(key, 0) + 1
        m_win_hits[key] = cnt
        if cnt >= _PROMOTE_WINDOW_HITS:
            # promote to protected
            m_segment[key] = 1
            m_win_hits.pop(key, None)
            sz = int(obj.size)
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz
            # push into protected heap
            _heap_push_for_seg(1, key, now)
            # enforce budget
            _maybe_demote_from_protected(cache_snapshot)
    else:
        # protected hit: just keep last_access fresh (heap already pushed)
        pass

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts and decay.
      - ARC-style adaptation: if key is in window-ghost, expand window; if in protected-ghost, shrink.
      - Initialize metadata: segment=window, times, hit-count-in-window=0.
      - Track resident key for O(1) sampling and bytes accounting; push into window heap.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # ARC-style adapt based on ghost membership
    gw = _ghostW.pop(key, None)
    gp = _ghostP.pop(key, None)
    if gw is not None and gp is not None:
        # If present in both (rare due to collisions), lean towards recency
        gw = now
        gp = None
    if gw is not None:
        # recent (window) re-reference -> enlarge window
        globals()['_f_window'] = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif gp is not None:
        # frequent (protected) re-reference -> shrink window
        globals()['_f_window'] = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_time[key] = now
    m_win_hits[key] = 0

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Push into window heap
    _heap_push_for_seg(0, key, now)

    # Keep protected within budget if window expands significantly (demotions handled on hits or next evict)
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key into "ghost" cache for ARC-style adaptation (with its segment).
      - Remove resident metadata and update bytes counters and sampling structures.
      - Sketch persists; natural decay handles aging.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    now = cache_snapshot.access_count

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_time.pop(ev_key, None)
    m_win_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, now)
```
2025-11-27 19:57:55,960 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.0635

Example 2:
- Trace ID: 44
- Hit Rate: 0.0419

Example 3:
- Trace ID: 9
- Hit Rate: 0.531

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:57:55 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:57:55,961 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m19:59:29 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:59:29,595 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:59:29,596 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive ARC-style SLRU with TinyLFU admission guidance (byte-aware)
# - Two resident segments (probationary T1 = recency, protected T2 = frequency).
# - Two non-resident "ghost" segments (B1 for T1 evictions, B2 for T2 evictions) to adapt T1/T2 balance.
# - Adaptive byte target p for T1, adjusted on ghost hits (ARC principle).
# - Hits promote from T1 -> T2; recency for T2 maintained via last-access timestamp.
# - TinyLFU Count-Min Sketch tracks global popularity for admission guidance and decay.
# - Eviction uses ARC decision: if T1 bytes > p then evict LRU from T1 else evict LRU from T2.
# - Compact per-resident metadata: segment id and last access timestamp.
# - Size-aware: byte-based balancing and tie-breakers prefer evicting larger items when timestamps tie.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (T1 probationary) or 1 (T2 protected)
m_last_access = dict()   # key -> int (timestamp)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192                 # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# ARC-style adaptive target for T1 bytes
_p_bytes = None          # adaptive target for T1 (probationary) in bytes
_seen_capacity = None    # cache capacity seen to initialize p

# Ghost caches (non-resident)
# key -> (size, last_ts)
_B1 = dict()  # recent ghost (from T1)
_B2 = dict()  # frequent ghost (from T2)
_B1_bytes = 0
_B2_bytes = 0
# Budgets for ghosts (each up to capacity; total up to 2x capacity)
def _ghost_budgets(cap):
    return cap, cap

_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _maybe_init(cache_snapshot):
    global _p_bytes, _seen_capacity
    cap = cache_snapshot.capacity
    if _p_bytes is None or _seen_capacity != cap:
        # Initialize T1 target to 25% of bytes
        _p_bytes = max(0, int(0.25 * cap))
        _seen_capacity = cap

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0  # default probationary
    if key not in m_last_access:
        m_last_access[key] = 0

def _segment_and_bytes(cache_snapshot):
    # Compute T1/T2 sets and byte usage
    t1_keys = []
    t2_keys = []
    t1_bytes = 0
    t2_bytes = 0
    for key, robj in cache_snapshot.cache.items():
        seg = m_segment.get(key, 0)
        if seg == 0:
            t1_keys.append(key)
            t1_bytes += robj.size
        else:
            t2_keys.append(key)
            t2_bytes += robj.size
    return t1_keys, t2_keys, t1_bytes, t2_bytes

def _choose_lru_in(keys, cache_snapshot):
    # Return oldest key (LRU). Tie-break by largest size to release more space.
    victim = None
    victim_ts = None
    victim_size = None
    cache = cache_snapshot.cache
    for k in keys:
        ts = m_last_access.get(k, 0)
        sz = cache[k].size
        if (victim is None or
            ts < victim_ts or
            (ts == victim_ts and sz > victim_size)):
            victim = k
            victim_ts = ts
            victim_size = sz
    return victim

def _ghost_touch_and_adapt(cache_snapshot, incoming_key, incoming_size):
    # If the incoming key is in a ghost list, adapt _p_bytes accordingly and remove ghost entry.
    # ARC rule of thumb: increase p on B1 hit (favor recency), decrease p on B2 hit (favor frequency).
    # Use byte-aware adjustment bounded by capacity and proportional to request size.
    global _p_bytes, _B1_bytes, _B2_bytes
    cap = cache_snapshot.capacity
    bump = max(incoming_size, cap // 64)  # conservative increment step
    if incoming_key in _B1:
        _B1_bytes -= _B1[incoming_key][0]
        _B1.pop(incoming_key, None)
        _p_bytes = min(cap, _p_bytes + bump)
    elif incoming_key in _B2:
        _B2_bytes -= _B2[incoming_key][0]
        _B2.pop(incoming_key, None)
        _p_bytes = max(0, _p_bytes - bump)

def _ghost_add(cache_snapshot, key, size, to_B1, now):
    # Insert into B1 or B2 and trim budget
    global _B1_bytes, _B2_bytes
    cap = cache_snapshot.capacity
    b1_budget, b2_budget = _ghost_budgets(cap)
    if to_B1:
        prev = _B1.get(key)
        if prev is not None:
            _B1_bytes -= prev[0]
        _B1[key] = (size, now)
        _B1_bytes += size
        # Trim oldest until within budget
        while _B1_bytes > b1_budget and _B1:
            # find oldest
            oldest_key = None
            oldest_ts = None
            for k, (_, ts) in _B1.items():
                if oldest_key is None or ts < oldest_ts:
                    oldest_key = k
                    oldest_ts = ts
            sz = _B1[oldest_key][0]
            _B1_bytes -= sz
            _B1.pop(oldest_key, None)
    else:
        prev = _B2.get(key)
        if prev is not None:
            _B2_bytes -= prev[0]
        _B2[key] = (size, now)
        _B2_bytes += size
        while _B2_bytes > b2_budget and _B2:
            oldest_key = None
            oldest_ts = None
            for k, (_, ts) in _B2.items():
                if oldest_key is None or ts < oldest_ts:
                    oldest_key = k
                    oldest_ts = ts
            sz = _B2[oldest_key][0]
            _B2_bytes -= sz
            _B2.pop(oldest_key, None)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style decision:
      - Adapt T1 target p using ghost hits of incoming key.
      - If T1 bytes > p, evict LRU from T1; otherwise evict LRU from T2 (fallback to T1 if T2 empty).
    '''
    _maybe_init(cache_snapshot)

    now = cache_snapshot.access_count
    incoming_key = obj.key
    incoming_size = max(int(obj.size), 1)

    # Ensure metadata exists for residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Adapt using ghost information (ARC)
    _ghost_touch_and_adapt(cache_snapshot, incoming_key, incoming_size)

    # Compute segment membership and byte usage
    t1_keys, t2_keys, t1_bytes, _t2_bytes = _segment_and_bytes(cache_snapshot)

    # Choose target segment to evict from
    victim_key = None
    if t1_keys and (t1_bytes > _p_bytes or not t2_keys):
        victim_key = _choose_lru_in(t1_keys, cache_snapshot)
    elif t2_keys:
        victim_key = _choose_lru_in(t2_keys, cache_snapshot)
    else:
        # Fallback: cache is inconsistent; choose global LRU
        victim_key = _choose_lru_in(list(cache_snapshot.cache.keys()), cache_snapshot)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU.
      - Update last access timestamp.
      - Promote from T1 -> T2 on first hit; keep recency for T2 (refresh timestamp).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in probationary
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss):
      - Update TinyLFU for this access.
      - Initialize metadata: segment=T1 (probationary), last_access=now.
      - If present in ghosts, clean up (evict() already adapted p).
    '''
    _maybe_init(cache_snapshot)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # probationary
    m_last_access[key] = cache_snapshot.access_count

    # Clean any stale ghost entries for the inserted key
    global _B1_bytes, _B2_bytes
    if key in _B1:
        _B1_bytes -= _B1[key][0]
        _B1.pop(key, None)
    if key in _B2:
        _B2_bytes -= _B2[key][0]
        _B2.pop(key, None)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (B1 if from T1, B2 if from T2),
        storing its size and eviction time.
      - Remove resident metadata for the evicted key.
      - TinyLFU is global and persists; decay handles aging.
    '''
    now = cache_snapshot.access_count
    ev_key = evicted_obj.key
    size = max(int(evicted_obj.size), 1)
    seg = m_segment.get(ev_key, 0)  # capture before removing

    # Add to appropriate ghost list
    _ghost_add(cache_snapshot, ev_key, size, to_B1=(seg == 0), now=now)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 19:59:38,053 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.0279

Example 2:
- Trace ID: 42
- Hit Rate: 0.0592

Example 3:
- Trace ID: 10
- Hit Rate: 0.0917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:59:38 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 19:59:38,053 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:01:06 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:06,490 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:06,491 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive SLRU + TinyLFU with admission-aware eviction (count-based capacity)
# - Three in-cache segments: window (recency), probation (frequency learning), protected (frequency keeper).
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Adaptive window size (fraction of object capacity) via simple hill-climbing:
#     hits in window -> increase window; hits in probation/protected -> decrease window.
# - Eviction:
#     * If window is over its object budget: evict LRU (oldest) from window.
#     * Else: evict the globally weakest item by a TinyLFU/recency score, with:
#         - Bias to keep protected items and penalize window items,
#         - "Admission-aware" skew using the incoming object's frequency.
# - Per-resident metadata: segment id (0/1/2), last access timestamp, resident hit count, insert time.

# ------------------------
# Global metadata
# ------------------------
# Segment ids
_SEG_WINDOW = 0
_SEG_PROBATION = 1
_SEG_PROTECTED = 2

m_segment = dict()         # key -> 0 (window), 1 (probation), 2 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_time = dict()     # key -> int (timestamp)
m_resident_hits = dict()   # key -> int (per-resident hit count)

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384          # power of two for fast masking
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000          # decay every N observed accesses (hits + misses)
_decay_ticker = 0

# ------------------------
# Adaptation and scoring tunables
# ------------------------
# Window target as a fraction of (estimated) object capacity.
_WINDOW_FRAC = 0.20            # initial fraction for the window segment
_WINDOW_FRAC_MIN = 0.03
_WINDOW_FRAC_MAX = 0.50
_ADAPT_STEP_UP = 0.005         # on window hits, grow window (recency helpful)
_ADAPT_STEP_DOWN = 0.002       # on prob/prot hits, shrink window (favor frequency)

# Score shaping
_RECENCY_WEIGHT = 0.25         # additive recency influence
_PROTECTED_KEEP_BONUS = 1.6    # >1 means harder to evict protected
_WINDOW_EVICT_PENALTY = 0.85   # <1 makes window items easier to evict
_ADMIT_GAMMA = 0.15            # admission-aware skew vs incoming object's frequency
_EPS = 1e-9

# Estimated object capacity (we learn it when the cache is full and eviction is requested)
_estimated_obj_capacity = 0

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # Default unknown resident to probation so it benefits from frequency if hit again.
    if key not in m_segment:
        m_segment[key] = _SEG_PROBATION
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_insert_time:
        m_insert_time[key] = 0
    if key not in m_resident_hits:
        m_resident_hits[key] = 0

def _recency_component(now, key):
    age = max(0, now - m_last_access.get(key, 0))
    # Higher when recently used; in (0, 1]
    return 1.0 / (1.0 + float(age))

def _priority_for_key(cache_snapshot, key, obj, now, incoming_freq=None):
    # Base "keep" score: larger is more valuable to keep.
    # score_base = freq + recency_weight * recency
    freq = _sketch_estimate(key) + 0.75 * m_resident_hits.get(key, 0)
    recency = _recency_component(now, key)
    score = freq + (_RECENCY_WEIGHT * recency)

    # Segment modifiers
    seg = m_segment.get(key, _SEG_PROBATION)
    if seg == _SEG_PROTECTED:
        score *= _PROTECTED_KEEP_BONUS
    elif seg == _SEG_WINDOW:
        score *= _WINDOW_EVICT_PENALTY

    # Admission-aware skew: compare resident's freq vs incoming object's freq
    if incoming_freq is not None:
        diff = freq - incoming_freq
        if diff >= 0:
            # Resident hotter than incoming -> protect it
            score *= (1.0 + _ADMIT_GAMMA * diff)
        else:
            # Incoming hotter than resident -> make resident easier to evict
            score /= (1.0 + _ADMIT_GAMMA * (-diff))

    return score

def _current_window_target_count():
    # Convert window fraction to a target object count using the current estimate.
    cap = max(1, _estimated_obj_capacity)
    target = int(max(1, round(_WINDOW_FRAC * cap)))
    return target

def _adjust_window_fraction_on_hit(seg):
    global _WINDOW_FRAC
    if seg == _SEG_WINDOW:
        _WINDOW_FRAC = min(_WINDOW_FRAC_MAX, _WINDOW_FRAC + _ADAPT_STEP_UP)
    else:
        _WINDOW_FRAC = max(_WINDOW_FRAC_MIN, _WINDOW_FRAC - _ADAPT_STEP_DOWN)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim key to evict:
      - Learn/refresh estimated object capacity when eviction is required.
      - If window object-count > target, evict LRU from window (oldest last_access).
      - Otherwise, evict the minimal score globally using TinyLFU + recency, with:
          * Admission-aware skew vs incoming object's frequency
          * Protected segment keep bonus; window penalty to ease eviction.
      - Tie-break by oldest last access, then by largest size (to reduce contention on larger items).
    '''
    global _estimated_obj_capacity

    now = cache_snapshot.access_count

    # Learn estimated object capacity (count-based) whenever we're evicting
    resident_count = len(cache_snapshot.cache)
    if resident_count > _estimated_obj_capacity:
        _estimated_obj_capacity = resident_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Segment counts
    window_keys = []
    for key in cache_snapshot.cache:
        if m_segment.get(key, _SEG_PROBATION) == _SEG_WINDOW:
            window_keys.append(key)
    window_count = len(window_keys)
    target_window = _current_window_target_count()

    # If window exceeds its target, evict LRU from window
    if window_count > target_window and window_keys:
        victim_key = None
        victim_last_ts = None
        victim_size = None
        for key in window_keys:
            last_ts = m_last_access.get(key, 0)
            size = cache_snapshot.cache[key].size
            if (victim_key is None or
                last_ts < victim_last_ts or
                (last_ts == victim_last_ts and size > victim_size)):
                victim_key = key
                victim_last_ts = last_ts
                victim_size = size
        return victim_key

    # Otherwise, pick the globally weakest by score, with admission-aware skew
    incoming_freq = _sketch_estimate(obj.key)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_freq=incoming_freq)
        last_ts = m_last_access.get(key, 0)
        size = robj.size

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay when needed.
      - Update last access timestamp and per-resident hit count.
      - Segment promotion:
          * window -> probation on first hit after insert.
          * probation -> protected on hit.
          * protected stays protected.
      - Adapt window fraction: hits in window increase it; otherwise decrease slightly.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)

    m_last_access[key] = cache_snapshot.access_count
    m_resident_hits[key] = m_resident_hits.get(key, 0) + 1

    seg = m_segment.get(key, _SEG_PROBATION)
    if seg == _SEG_WINDOW:
        # First confirmation of usefulness: move to probation
        m_segment[key] = _SEG_PROBATION
    elif seg == _SEG_PROBATION:
        # Promote to protected on subsequent hit
        m_segment[key] = _SEG_PROTECTED
    # protected remains protected

    _adjust_window_fraction_on_hit(seg)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU sketch and decay.
      - Initialize metadata:
          * segment = window (recency buffer)
          * last_access = now
          * insert_time = now
          * resident_hits = 0
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = _SEG_WINDOW
    m_last_access[key] = now
    m_insert_time[key] = now
    m_resident_hits[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch persists (global, decayed over time).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_insert_time.pop(ev_key, None)
    m_resident_hits.pop(ev_key, None)
```
2025-11-27 20:01:17,472 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 12
- Hit Rate: 0.0403

Example 2:
- Trace ID: 25
- Hit Rate: 0.3024

Example 3:
- Trace ID: 39
- Hit Rate: 0.0801

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:01:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:01:17,473 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:03:08 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:03:08,244 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:03:08,245 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# W-TinyLFU style with count-based capacity, adaptive window, and protected main segment
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission signal via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (count of items) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window size exceeds target -> evict from window (sample oldest few).
#     * Else -> sample oldest few from both segments; typically evict global minimum priority,
#               but avoid evicting protected if new object's frequency is lower than the protected victim
#               (TinyLFU admission-like behavior).
# - Recency-aware, frequency-aware priority; protected has a keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates and adaptive demotion if protected overflow.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> hits required to promote from window (generally 1)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_count = 0
_prot_count = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> 1 (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> 1
_gw_count = 0
_gp_count = 0
_last_capacity_seen = 0
_GHOST_BUDGET_COUNT = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (count of items): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 3.0
_REC_PROT_LAMBDA = 0.8

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.25

# Promotion policy: promote from window to protected after N hits
_PROMOTE_HITS = 1

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.70
_FREQ_RESIDENT_WEIGHT = 0.30

# Eviction sampling
_SAMPLE_WIN = 8        # candidates sampled from window LRU tail (oldest)
_SAMPLE_PROT = 6       # candidates sampled from protected LRU tail

# TinyLFU-like admission guard: do not evict protected for a new item with weaker frequency
_ADMIT_PROT_GUARD = 1.0   # new_freq must be >= guard * victim_prot_freq to consider evicting protected

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_COUNT
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_COUNT = cap  # per ghost budget ≈ cache size (count-based)

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated (count-based)
    global _od_initialized, _win_count, _prot_count
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, _robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            m_promote_need[key] = _PROMOTE_HITS

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_count += 1
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_count += 1
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        m_promote_need[key] = _PROMOTE_HITS
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
            global _win_count
            _win_count += 1
        else:
            _prot_q[key] = None
            global _prot_count
            _prot_count += 1

def _ghost_add(segment, key):
    # Add key to corresponding ghost queue's MRU side (count-based)
    global _gw_count, _gp_count
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = 1
            _gw_count += 1
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = 1
            _gp_count += 1

def _ghost_prune():
    # Keep each ghost <= budget (count of items); prune by oldest entries
    global _gw_count, _gp_count
    while _gw_count > _GHOST_BUDGET_COUNT and _gw:
        _gw.popitem(last=False)
        _gw_count -= 1
    while _gp_count > _GHOST_BUDGET_COUNT and _gp:
        _gp.popitem(last=False)
        _gp_count -= 1

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1), count-based)
    global _gw_count, _gp_count
    if key in _gw:
        # Increase window target
        num = float(_gp_count)
        den = float(_gw_count if _gw_count > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        _gw.pop(key, None)
        _gw_count = max(0, _gw_count - 1)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_count)
        den = float(_gp_count if _gp_count > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        _gp.pop(key, None)
        _gp_count = max(0, _gp_count - 1)
    _ghost_prune()

def _tiny_freq(key):
    # Combined frequency estimate (global sketch + resident counter)
    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    return (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

def _priority_for_key(cache_snapshot, key, now):
    # Composite priority: higher is better to keep
    # score = (TinyLFU-blended frequency) * (1 + lambda_seg * recency) * segment_bonus
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq = _tiny_freq(key)
    base = (freq * rec_mult)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base, freq  # return both composite score and pure frequency

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _demote_protected_if_needed(cache_snapshot):
    # Keep protected size within its target by demoting its LRU to window MRU
    global _prot_count, _win_count
    cap = max(1, int(cache_snapshot.capacity))
    target_window = max(0, int(round(_window_frac * cap)))
    target_protected = max(0, cap - target_window)
    while _prot_count > target_protected and _prot_q:
        # Demote LRU of protected to window MRU
        lru_key, _ = _prot_q.popitem(last=False)
        _prot_count -= 1
        m_segment[lru_key] = 0
        _win_q[lru_key] = None
        _win_q.move_to_end(lru_key, last=True)
        _win_count += 1

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim (count-based capacity):
      - Maintain an adaptive window target (ARC-style). If window size > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and pick global minimum priority,
        but avoid evicting from protected if the incoming object's frequency is weaker
        than the protected victim (TinyLFU-like admission guard).
      - Tie-break by oldest last access.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    cap = max(1, int(cache_snapshot.capacity))
    target_window = max(0, int(round(_window_frac * cap)))
    restrict_to_window = (_win_count > target_window) and (len(_win_q) > 0)

    # Build candidate lists
    win_candidates = []
    prot_candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        win_candidates = [k for k in sample_keys if k in cache_snapshot.cache]
    else:
        win_candidates = [k for k in _sample_oldest_keys(_win_q, _SAMPLE_WIN) if k in cache_snapshot.cache]
        prot_candidates = [k for k in _sample_oldest_keys(_prot_q, _SAMPLE_PROT) if k in cache_snapshot.cache]
        if not win_candidates and not prot_candidates and cache_snapshot.cache:
            # Fallback: pick any key deterministically
            for k in cache_snapshot.cache.keys():
                win_candidates = [k]
                break

    # Evaluate candidates
    def pick_min(cands):
        victim_key = None
        victim_score = None
        victim_last_ts = None
        victim_freq = None
        for key in cands:
            score, freq = _priority_for_key(cache_snapshot, key, now)
            last_ts = m_last_access.get(key, 0)
            if (victim_key is None or
                score < victim_score - _EPS or
                (abs(score - victim_score) <= _EPS and last_ts < victim_last_ts)):
                victim_key = key
                victim_score = score
                victim_last_ts = last_ts
                victim_freq = freq
        return victim_key, victim_score, victim_last_ts, victim_freq

    win_key, win_score, _, win_freq = pick_min(win_candidates) if win_candidates else (None, None, None, None)
    prot_key, prot_score, _, prot_freq = pick_min(prot_candidates) if prot_candidates else (None, None, None, None)

    # Decide using TinyLFU-like admission guard for protected
    if restrict_to_window:
        victim_key = win_key
    else:
        # If only one side has candidates, pick from that side
        if win_key is None:
            victim_key = prot_key
        elif prot_key is None:
            victim_key = win_key
        else:
            # Compare scores, but avoid evicting protected if new item is weaker
            new_freq = _sketch_estimate(obj.key)
            if new_freq + _EPS < (_ADMIT_PROT_GUARD * (prot_freq if prot_freq is not None else 0.0)):
                victim_key = win_key
            else:
                # Otherwise pick the global minimum score
                victim_key = win_key if (win_score is not None and prot_score is not None and win_score < prot_score) else prot_key

    # As ultimate fallback, if something went wrong, evict LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold.
      - If protected exceeds its target, demote its LRU to window (adaptive balancing).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
            global _win_count
            _win_count += 1
        # Promotion
        need = m_promote_need.get(key, _PROMOTE_HITS)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
                _win_count = max(0, _win_count - 1)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            global _prot_count
            _prot_count += 1
            # Keep protected within its target size
            _demote_protected_if_needed(cache_snapshot)
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None
            global _prot_count
            _prot_count += 1
        # No promotion; but ensure protected does not exceed target
        _demote_protected_if_needed(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need.
      - Insert into window MRU; update counts.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0
    m_promote_need[key] = _PROMOTE_HITS

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
        global _prot_count
        _prot_count = max(0, _prot_count - 1)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
        global _win_count
        _win_count += 1

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust counts.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key)
    _ghost_prune()

    # Remove from LRU queues and adjust counts
    global _win_count, _prot_count
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
            _win_count = max(0, _win_count - 1)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
            _prot_count = max(0, _prot_count - 1)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 20:03:16,967 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 2
- Hit Rate: 0.4643

Example 2:
- Trace ID: 46
- Hit Rate: 0.4663

Example 3:
- Trace ID: 37
- Hit Rate: 0.6419

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:03:16 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:03:16,968 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:05:24 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:24,127 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:24,131 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:09:10,155 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 13
- Hit Rate: 0.0

Example 2:
- Trace ID: 21
- Hit Rate: 0.0262

Example 3:
- Trace ID: 38
- Hit Rate: 0.1255

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:09:10 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:09:10,156 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:10:58 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:10:58,509 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:10:58,511 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Segmented LRU + TinyLFU (object-count capacity, ARC-style adaptive window, sampled eviction)
# - Two LRU segments by objects (not bytes):
#     0: probation/window (recency-biased, admission)
#     1: protected (frequency-biased, keep)
# - Global TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window sizing using ghost lists (by object count).
# - Eviction:
#     * If probation size > target -> evict among oldest probation entries (sampled).
#     * Else -> sample oldest from both segments; evict lowest composite priority.
# - On hit: immediate probation -> protected promotion (SLRU-style).
# - Metadata is O(1) to update; no full scans.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (probation) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed with sketch)
# LRU queues (oldest -> newest)
_prob_q = OrderedDict()   # probation/window
_prot_q = OrderedDict()   # protected
_prob_n = 0               # object counts (not bytes)
_prot_n = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) by object count
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> None (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> None
_gw_n = 0
_gp_n = 0
_last_capacity_seen = 0
_GHOST_BUDGET_OBJS = 0    # per-ghost budget in objects (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096            # power of two for speed/memory balance
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (objects): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Recency multipliers per segment (stronger recency in probation)
_REC_PROB_LAMBDA = 2.5
_REC_PROT_LAMBDA = 0.75

# Protected segment keep bonus
_PROTECTED_KEEP_BONUS = 1.20

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.70
_FREQ_RESIDENT_WEIGHT = 0.30

# Eviction sampling (oldest-from-tail)
_SAMPLE_PROB = 6         # candidates from probation LRU tail (oldest)
_SAMPLE_PROT = 4         # candidates from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_OBJS
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_OBJS = cap  # per ghost budget ≈ cache size in objects

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _prob_n, _prot_n
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, _robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to probation
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_res_hits:
            m_res_hits[key] = 0
        seg = m_segment.get(key, 0)
        if seg == 0:
            if key not in _prob_q:
                _prob_q[key] = None
                _prob_n += 1
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_n += 1
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    # Ensure presence in an LRU queue
    seg = m_segment.get(key, 0)
    if seg == 0:
        if key not in _prob_q:
            _prob_q[key] = None
            global _prob_n
            _prob_n += 1
    else:
        if key not in _prot_q:
            _prot_q[key] = None
            global _prot_n
            _prot_n += 1

def _ghost_add(segment, key):
    # Add key to corresponding ghost queue's MRU side (by object count)
    global _gw_n, _gp_n
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = None
            _gw_n += 1
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = None
            _gp_n += 1

def _ghost_prune():
    # Keep each ghost <= budget objects; prune by oldest entries
    global _gw_n, _gp_n
    while _gw_n > _GHOST_BUDGET_OBJS and _gw:
        _gw.popitem(last=False)
        _gw_n -= 1
    while _gp_n > _GHOST_BUDGET_OBJS and _gp:
        _gp.popitem(last=False)
        _gp_n -= 1

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(key):
    # ARC-style adaptive control using ghost hits (O(1)), by object counts
    global _gw_n, _gp_n
    if key in _gw:
        # Increase window target
        num = float(_gp_n)
        den = float(_gw_n if _gw_n > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        _gw.pop(key, None)
        _gw_n = max(0, _gw_n - 1)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_n)
        den = float(_gp_n if _gp_n > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        _gp.pop(key, None)
        _gp_n = max(0, _gp_n - 1)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, now):
    # Composite priority: higher is better to keep
    # score = (w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_PROB_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    score = freq * rec_mult
    if seg == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive probation (window) object-count target via ghosts (ARC-style).
      - If probation size > target -> evict min-priority among a small sample of oldest probation entries.
      - Else, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus, recency weaker).
      - Tie-break by oldest last_access, then by protected < probation (prefer evict probation),
        then by lexical key for stability.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    cap = max(1, int(cache_snapshot.capacity))
    target_prob = max(1, int(_window_frac * cap))
    restrict_to_prob = (_prob_n > target_prob and _prob_n > 0)

    candidates = []
    if restrict_to_prob:
        candidates = [k for k in _sample_oldest_keys(_prob_q, _SAMPLE_PROB) if k in cache_snapshot.cache]
    else:
        cand_p = [k for k in _sample_oldest_keys(_prob_q, _SAMPLE_PROB) if k in cache_snapshot.cache]
        cand_r = [k for k in _sample_oldest_keys(_prot_q, _SAMPLE_PROT) if k in cache_snapshot.cache]
        candidates = cand_p + cand_r
        if not candidates and cache_snapshot.cache:
            # Fallback: any one key (deterministic LRU from probation then protected)
            if _prob_q:
                candidates = [next(iter(_prob_q.keys()))]
            elif _prot_q:
                candidates = [next(iter(_prot_q.keys()))]
            else:
                for k in cache_snapshot.cache.keys():
                    candidates = [k]
                    break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_seg = None

    for key in candidates:
        score = _priority_for_key(cache_snapshot, key, now)
        last_ts = m_last_access.get(key, 0)
        seg = m_segment.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (
                last_ts < victim_last_ts or
                (last_ts == victim_last_ts and seg == 0 and victim_seg == 1) or
                (last_ts == victim_last_ts and seg == victim_seg and key < victim_key)
            ))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_seg = seg

    # As ultimate fallback
    if victim_key is None:
        if _prob_q:
            victim_key = next(iter(_prob_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and decay.
      - Update resident metadata: last_access, res_hits.
      - Move to MRU of its segment LRU queue.
      - Immediate promotion from probation -> protected (SLRU-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of probation
        if key in _prob_q:
            _prob_q.move_to_end(key, last=True)
        else:
            _prob_q[key] = None
            global _prob_n
            _prob_n += 1
        # Promote on first hit (SLRU)
        # Remove from probation
        if key in _prob_q:
            _prob_q.pop(key, None)
            _prob_n = max(0, _prob_n - 1)
        # Add to protected MRU
        m_segment[key] = 1
        _prot_q[key] = None
        _prot_q.move_to_end(key, last=True)
        global _prot_n
        _prot_n += 1
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None
            global _prot_n
            _prot_n += 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU and decay.
      - Initialize metadata: segment=probation, last_access=now, res_hits=0.
      - Insert into probation MRU; adjust counts.
      - Adapt window target using ghost hits (ARC-style) by object counts.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    # Ensure it's not in protected queue (stale)
    if key in _prot_q:
        _prot_q.pop(key, None)
        global _prot_n
        _prot_n = max(0, _prot_n - 1)

    # Insert into probation MRU
    if key in _prob_q:
        _prob_q.move_to_end(key, last=True)
    else:
        _prob_q[key] = None
        global _prob_n
        _prob_n += 1

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list (by segment) for adaptive sizing.
      - Remove from LRU queue and adjust counts.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key)
    _ghost_prune()

    # Remove from LRU queues and adjust counts
    global _prob_n, _prot_n
    if seg == 0:
        if ev_key in _prob_q:
            _prob_q.pop(ev_key, None)
            _prob_n = max(0, _prob_n - 1)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
            _prot_n = max(0, _prot_n - 1)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
```
2025-11-27 20:11:05,459 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 7
- Hit Rate: 0.4357

Example 2:
- Trace ID: 18
- Hit Rate: 0.0226

Example 3:
- Trace ID: 35
- Hit Rate: 0.0759

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:11:05 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:11:05,460 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:12:41 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:12:41,879 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:12:41,880 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Simpler, ARC-adaptive W-TinyLFU with Segmented LRU (count-based)
# - Capacity interpreted as number of objects (not bytes).
# - Two in-cache segments:
#     window (probationary, recency-driven) and protected (frequency-driven).
# - TinyLFU (Count-Min Sketch with decay) to guide cross-segment evictions.
# - ARC-style ghost sets steer the window target (by count).
# - O(1) updates via OrderedDict LRU lists; no heaps.

from collections import OrderedDict
import random

# ------------------------
# Global state
# ------------------------
# Segmented LRU lists (keys only). Oldest at the left, newest at the right.
_win = OrderedDict()      # probationary (window)
_prot = OrderedDict()     # protected (frequent)

# Ghost sets (recently evicted keys) to adapt window target (ARC-style).
_ghostW = OrderedDict()
_ghostP = OrderedDict()

# Metadata
m_last_access = dict()    # key -> last access time (int)
m_segment = dict()        # key -> 0 (window) or 1 (protected)

# Capacity / adaptation
_CAP_SEEN = None
_target_window_items = 0
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.80
_DEFAULT_WINDOW_FRAC = 0.20
_GHOST_CAP_KEYS = 0  # set on init to 2 * capacity

# TinyLFU (Count-Min Sketch) with decay
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)
_DECAY_PERIOD = 10000
_decay_ticker = 0

# Eviction tuning
_PROTECTED_KEEP_BONUS = 1.5  # higher => less likely to evict protected
_EPS = 1e-9


# ------------------------
# Sketch helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0


# ------------------------
# Init / reset
# ------------------------
def _ensure_init(cache_snapshot):
    global _CAP_SEEN
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    global _CAP_SEEN, _target_window_items, _GHOST_CAP_KEYS
    _win.clear()
    _prot.clear()
    _ghostW.clear()
    _ghostP.clear()
    m_last_access.clear()
    m_segment.clear()

    cap = max(1, int(cache_snapshot.capacity))
    _CAP_SEEN = cap
    _target_window_items = max(1, int(_DEFAULT_WINDOW_FRAC * cap))
    _GHOST_CAP_KEYS = 2 * cap

    # Reset sketch
    global _cm, _decay_ticker
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0
    _decay_ticker = 0

    # If cache is preloaded, treat all as window (probation), unknown recency
    now = cache_snapshot.access_count
    for k, _ in cache_snapshot.cache.items():
        _win[k] = 1
        m_segment[k] = 0
        m_last_access[k] = 0  # unknown historical recency
    _trim_ghosts_if_needed()

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta, cap):
    global _target_window_items
    lo = max(1, int(_MIN_WINDOW_FRAC * cap))
    hi = max(1, int(_MAX_WINDOW_FRAC * cap))
    _target_window_items = _clamp(_target_window_items + int(delta), lo, hi)

def _ghost_admit(ghost, key):
    # Ordered ghost as LRU set: move-to-end on add or refresh
    if key in ghost:
        ghost.pop(key, None)
    ghost[key] = 1

def _trim_ghosts_if_needed():
    while len(_ghostW) > _GHOST_CAP_KEYS:
        _ghostW.popitem(last=False)  # pop LRU
    while len(_ghostP) > _GHOST_CAP_KEYS:
        _ghostP.popitem(last=False)


# ------------------------
# Adaptation on insert (ARC-like)
# ------------------------
def _adaptive_on_insert(cache_snapshot, obj):
    cap = max(1, cache_snapshot.capacity)
    k = obj.key
    if k in _ghostW:
        # Recently evicted from window -> increase window
        _ghostW.pop(k, None)
        _adjust_window_target(+1, cap)
    elif k in _ghostP:
        # Recently evicted from protected -> shrink window
        _ghostP.pop(k, None)
        _adjust_window_target(-1, cap)


# ------------------------
# Eviction scoring
# ------------------------
def _score_for_eviction(key, seg, now):
    # Lower score => more likely to evict
    # TinyLFU estimate; protected gets a keep bonus (harder to evict).
    freq = _sketch_estimate(key)
    if seg == 1:
        freq *= _PROTECTED_KEEP_BONUS
    # Use last access as soft tiebreak (older => lower score)
    age = float(now - m_last_access.get(key, 0))
    # Blend: prioritize low freq, then age
    # Add small fraction of recency to avoid overly sticky protected entries.
    return (freq, age)  # lexicographic: smaller freq; if equal, evict older (larger age)


# ------------------------
# Core API
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim:
      - If window item count exceeds adaptive target, evict LRU of window (pure recency).
      - Else, compare LRU of window and LRU of protected by TinyLFU score (with protected bias).
      - If one segment empty, evict from the other.
      - Fallback: sample a few random keys and evict the one with lowest TinyLFU.
    """
    _ensure_init(cache_snapshot)

    now = cache_snapshot.access_count
    win_sz = len(_win)
    prot_sz = len(_prot)

    # Prefer evicting from window when it exceeds target (ARC-style pressure)
    if win_sz > _target_window_items and win_sz > 0:
        # LRU from window
        victim_key = next(iter(_win))  # oldest
        return victim_key

    # Otherwise, choose the weaker of the two LRU candidates by TinyLFU score
    cand_w = None
    cand_p = None
    if win_sz > 0:
        cand_w = next(iter(_win))
    if prot_sz > 0:
        cand_p = next(iter(_prot))

    if cand_w is not None and cand_p is not None:
        sf_w = _score_for_eviction(cand_w, 0, now)
        sf_p = _score_for_eviction(cand_p, 1, now)
        # Compare (freq, age): lower freq gets evicted; if equal, older gets evicted
        if (sf_w[0] < sf_p[0] - _EPS) or (abs(sf_w[0] - sf_p[0]) <= _EPS and sf_w[1] > sf_p[1] + _EPS):
            return cand_w
        else:
            return cand_p
    elif cand_w is not None:
        return cand_w
    elif cand_p is not None:
        return cand_p

    # Fallback: sample random keys; evict one with lowest TinyLFU (and oldest on tie)
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    sample_sz = min(16, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_freq = None
    best_age = None
    for k in samples:
        f = _sketch_estimate(k)
        age = now - m_last_access.get(k, 0)
        if (best_k is None or f < best_freq - _EPS or
            (abs(f - best_freq) <= _EPS and age > best_age)):
            best_k = k
            best_freq = f
            best_age = age
    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increment TinyLFU and decay periodically.
      - Update last access time.
      - Promote from window -> protected on first hit.
      - Refresh LRU position in its segment.
    """
    _ensure_init(cache_snapshot)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    k = obj.key
    now = cache_snapshot.access_count
    m_last_access[k] = now

    if k in _win:
        # Promote to protected MRU
        _win.pop(k, None)
        _prot[k] = 1
        m_segment[k] = 1
    else:
        # If already in protected, just refresh LRU
        if k in _prot:
            _prot.pop(k, None)
            _prot[k] = 1
            m_segment[k] = 1
        else:
            # Edge: if key not tracked (shouldn't happen on hit), treat as protected
            _prot[k] = 1
            m_segment[k] = 1

def update_after_insert(cache_snapshot, obj):
    """
    On insert after a miss:
      - Adapt window target using ghost feedback (ARC-like).
      - Increment TinyLFU and decay periodically.
      - Place new item at MRU of window (probation).
    """
    _ensure_init(cache_snapshot)

    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    k = obj.key
    now = cache_snapshot.access_count
    m_last_access[k] = now

    # Place in window MRU
    if k in _prot:
        _prot.pop(k, None)
    _win.pop(k, None)
    _win[k] = 1
    m_segment[k] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Place the evicted key into the appropriate ghost set (window/protected).
      - Remove from in-cache LRU segment and metadata.
    """
    _ensure_init(cache_snapshot)

    evk = evicted_obj.key

    # Determine segment and update ghosts
    seg = m_segment.get(evk, 0)
    if evk in _win:
        seg = 0
        _win.pop(evk, None)
    elif evk in _prot:
        seg = 1
        _prot.pop(evk, None)

    if seg == 0:
        _ghost_admit(_ghostW, evk)
    else:
        _ghost_admit(_ghostP, evk)
    _trim_ghosts_if_needed()

    # Purge metadata
    m_segment.pop(evk, None)
    m_last_access.pop(evk, None)
```
2025-11-27 20:15:01,355 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Size-aware SLRU + TinyLFU with ARC-style window tuning (ASL-TinyLFU)
# - Global admission frequency via Count-Min Sketch with periodic aging.
# - Two in-cache segments:
#     0: "window"/probation (recency-biased)
#     1: "protected" (frequency-biased)
# - Eviction:
#     * If window bytes exceed its adaptive target, evict from window only (recency-biased SLRU).
#     * Otherwise, evict global minimum priority (size-aware TinyLFU + recency + protected bias).
# - Adaptive window sizing using two ghost lists (ARC-style):
#     * If a missed key was recently evicted from window-ghost -> increase window fraction.
#     * If from protected-ghost -> decrease window fraction.
# - Per-resident metadata: segment id, last access time, resident hit counter, promotion threshold.
# - Size-awareness: priority divides by size**alpha; larger objects require more hits to promote.

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style)
# ------------------------
_gw = dict()              # window-ghost: key -> (last_ts, size)
_gp = dict()              # protected-ghost: key -> (last_ts, size)
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000      # more responsive aging
_decay_ticker = 0
_just_decayed = False      # set True when sketch decay occurs

# ------------------------
# Tunables (carefully chosen defaults)
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02   # base step for adaptive adjustments
_window_frac = 0.25        # initial window fraction

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.25

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.7
_FREQ_RESIDENT_WEIGHT = 0.3

# Numerical epsilon and other helpers
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # ARC uses |B1|, |B2| up to cache size each

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    # When sketch ages, also age resident hit counters to avoid stale bias.
    global _just_decayed
    if _just_decayed:
        # Halve all resident hit counters
        keys = list(m_res_hits.keys())
        for k in keys:
            m_res_hits[k] >>= 1
        _just_decayed = False

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., preloaded items)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        # Without obj/capacity we default to small promotion threshold
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if obj.size >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need

def _ghost_add(segment, key, size, now):
    # segment: 0 -> window-ghost; 1 -> protected-ghost
    global _gw_bytes, _gp_bytes
    if segment == 0:
        prev = _gw.get(key)
        if prev is None:
            _gw[key] = (now, int(size))
            _gw_bytes += int(size)
        else:
            # refresh timestamp; keep size as last known
            _gw[key] = (now, prev[1])
    else:
        prev = _gp.get(key)
        if prev is None:
            _gp[key] = (now, int(size))
            _gp_bytes += int(size)
        else:
            _gp[key] = (now, prev[1])

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries
    global _gw_bytes, _gp_bytes
    # Prune window-ghost
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        # Find oldest by scanning (bounded by ghost size; acceptable under budget)
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gw.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gw.pop(oldest_key)
        _gw_bytes -= int(sz)
    # Prune protected-ghost
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        oldest_key = None
        oldest_ts = None
        for k, (ts, sz) in _gp.items():
            if oldest_key is None or ts < oldest_ts:
                oldest_key, oldest_ts = k, ts
        if oldest_key is None:
            break
        _, sz = _gp.pop(oldest_key)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key, now):
    # ARC-style adaptive control using ghost hits
    # If the key is in window-ghost (B1): increase window target.
    # If in protected-ghost (B2): decrease window target.
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # step proportional to max(1, |B2|/|B1|)
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        _, sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        _, sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    # Periodically prune ghosts to budgets
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus (harder to evict)
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among window only (probation).
      - Otherwise, evict the global minimum-priority key (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)

    # Ensure resident metadata exists for all current items
    for key, robj in cache_snapshot.cache.items():
        _ensure_resident_metadata(cache_snapshot, key, robj)

    # Compute window usage and target
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += int(robj.size)
            has_window = True

    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # only consider window while over target

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    m_last_access[key] = cache_snapshot.access_count
    # Increment resident hits (saturating via Python int; decay throttles growth)
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    # Size-aware promotion
    if m_segment.get(key, 0) == 0:
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key, now)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    now = cache_snapshot.access_count

    ev_key = evicted_obj.key
    seg = m_segment.get(ev_key, 0)
    size = int(evicted_obj.size)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, size, now)
    _ghost_prune()

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 19
- Hit Rate: 0.2722

Example 2:
- Trace ID: 6
- Hit Rate: 0.2715

Example 3:
- Trace ID: 31
- Hit Rate: 0.0219

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:15:01 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:15:01,356 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:17:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:17:51,319 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:17:51,323 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-Frequency with TinyLFU (GD-SF-TinyLFU)
# - Eviction uses GreedyDual key H = L + base_score(key), evict the minimum H.
# - L is a global inflation value updated to the evicted item's H, providing recency bias.
# - base_score blends global TinyLFU frequency (decayed), resident short-term hits, and a small recency boost,
#   normalized by size^alpha (size-awareness).
# - On hit: increase item's H by its current base_score (and ensure at least L + base_score).
# - On insert: initialize H := L + base_score.
# - On evict: set L := H(evicted) and drop metadata.
#
# Rationale:
# - GreedyDual family (GDS/GDSF) is strong for variable-size caches: naturally favors smaller, hot items,
#   and uses inflation (L) to age out stale items.
# - TinyLFU supplies robust global frequency estimates for admission/retention signals under dynamic workloads.
# - A small resident-hit and recency boost helps adapt quickly to bursts without making items immortal,
#   since inflation L keeps increasing upon evictions.

import math

# ------------------------
# Per-resident metadata
# ------------------------
m_last_access = dict()   # key -> int (timestamp of last access)
m_res_hits = dict()      # key -> int (resident hit counter)
m_H = dict()             # key -> float (GreedyDual priority)
# Global inflation (L), rises to the H of the last evicted object
_L = 0.0

# ------------------------
# TinyLFU Count-Min Sketch (global)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192                  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False   # becomes True when sketch decays to also age resident hits

# ------------------------
# Tunables
# ------------------------
# Size-awareness: larger objects require more "benefit" to stay
_SIZE_ALPHA = 0.85

# Base components for per-access benefit before dividing by size^alpha
_BASE_W = 1.0                      # base benefit per access
_FREQ_W = 1.2                      # weight for TinyLFU frequency (log-compressed)
_RESIDENT_W = 0.5                  # weight for resident short-term hits (sqrt-compressed)
_RECENCY_W = 0.30                  # mild recency boost: 1 + RECENCY_W/(1 + age)

# Tie-breaking epsilon
_EPS = 1e-12


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # Min across rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return 0.0 if est == float('inf') else float(est)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    # When the global sketch decays, also age resident hit counters.
    global _just_decayed
    if _just_decayed:
        if m_res_hits:
            for k in list(m_res_hits.keys()):
                m_res_hits[k] >>= 1
        _just_decayed = False

def _ensure_resident_metadata(cache_snapshot, key):
    # Initialize resident metadata if missing
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_res_hits:
        m_res_hits[key] = 0
    # m_H is initialized lazily when needed


def _base_increment_for_key(cache_snapshot, key, obj, now):
    # Compute the per-access benefit before GreedyDual inflation:
    # inc = recency_boost * (BASE_W + FREQ_W*log1p(freq) + RESIDENT_W*sqrt(res_hits)) / size^alpha
    size = max(1, int(obj.size))
    freq_est = _sketch_estimate(key)
    res_hits = m_res_hits.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency_boost = 1.0 + (_RECENCY_W / (1.0 + float(age)))

    numerator = _BASE_W + (_FREQ_W * math.log1p(freq_est)) + (_RESIDENT_W * math.sqrt(max(0.0, float(res_hits))))
    inc = (recency_boost * numerator) / (float(size) ** _SIZE_ALPHA)
    return inc


def _ensure_H_if_missing(cache_snapshot, key, obj, now):
    # Initialize H lazily for preloaded or previously uninitialized items
    if key not in m_H:
        inc = _base_increment_for_key(cache_snapshot, key, obj, now)
        m_H[key] = _L + inc


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose the victim with the smallest GreedyDual priority H.
    Tie-break by oldest last_access, then by largest size.
    '''
    now = cache_snapshot.access_count

    # Ensure resident metadata exists for current items (lazily initialize H)
    for key, robj in cache_snapshot.cache.items():
        _ensure_resident_metadata(cache_snapshot, key)
        _ensure_H_if_missing(cache_snapshot, key, robj, now)

    victim_key = None
    victim_H = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        h = m_H.get(key, None)
        if h is None:
            # Lazily compute if truly missing
            _ensure_H_if_missing(cache_snapshot, key, robj, now)
            h = m_H[key]
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            h < victim_H - _EPS or
            (abs(h - victim_H) <= _EPS and (last_ts < victim_last_ts or
                                            (last_ts == victim_last_ts and int(robj.size) > int(victim_size))))):
            victim_key = key
            victim_H = h
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    return victim_key


def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Increment TinyLFU with decay.
      - Update last_access and resident hits.
      - Increase item's H by its current base increment and ensure it's at least L + base.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    now = cache_snapshot.access_count
    key = obj.key

    _ensure_resident_metadata(cache_snapshot, key)
    _ensure_H_if_missing(cache_snapshot, key, obj, now)

    # Update resident metadata
    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    # Recompute base increment with updated frequency/recency
    inc = _base_increment_for_key(cache_snapshot, key, obj, now)

    # GreedyDual update: add benefit, and ensure at least current inflation + base
    curH = m_H.get(key, _L)
    newH = curH + inc
    floorH = _L + inc
    if newH < floorH:
        newH = floorH
    m_H[key] = newH


def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
      - Increment TinyLFU with decay.
      - Initialize last_access and resident hits.
      - Set H := L + base_increment.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    now = cache_snapshot.access_count
    key = obj.key

    # Initialize resident metadata
    m_last_access[key] = now
    m_res_hits[key] = 0

    # Initialize GreedyDual priority
    inc = _base_increment_for_key(cache_snapshot, key, obj, now)
    m_H[key] = _L + inc


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Set L := H(evicted) to advance global inflation (key step of GreedyDual).
      - Remove all resident metadata for the evicted key.
    '''
    global _L

    ev_key = evicted_obj.key
    h = m_H.pop(ev_key, None)
    # Advance inflation to evicted key's H if known
    if h is not None:
        _L = h

    # Drop other metadata
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
```
2025-11-27 20:18:02,141 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Error: Error - Timeout

Example 2:
- Trace ID: 20
- Hit Rate: 0.3717

Example 3:
- Trace ID: 45
- Hit Rate: 0.069

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:18:02 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:18:02,142 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:20:08 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:20:08,589 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:20:08,591 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Fast, size-aware SLRU with lightweight TinyLFU guidance and adaptive window
# Key features:
#  - Two resident segments with true LRU order via OrderedDict:
#      window (probation) and protected
#  - Size-aware promotion: small objects promote faster than large ones
#  - Evict from window's LRU; if window empty, demote protected's LRU to window
#  - TinyLFU (compact CMS) influences victim choice among a few oldest window items
#  - Adaptive window fraction based on where hits occur
#  - Ghost lists provide feedback to increase/decrease window ratio
#
# The design keeps per-access work O(1) amortized and avoids expensive sampling/decay.

from collections import OrderedDict, deque

# ------------------------
# Global state
# ------------------------
# LRU segments (key -> size)
_win = OrderedDict()
_prot = OrderedDict()

# Segment bytes
_win_bytes = 0
_prot_bytes = 0

# Segment map and per-key resident hit counter
_m_segment = {}       # key -> 0 (window) or 1 (protected)
_m_reshits = {}       # key -> hits since (re)insert

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU (Count-Min Sketch, compact)
_SKETCH_DEPTH = 2
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f)

# Decay config (in accesses)
_DECAY_PERIOD = 25000
_decay_ticker = 0

# Window fraction and adaptation
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_win = 0
_hits_prot = 0

# Ghost lists for ARC-like feedback
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity in items is GHOST_FACTOR * estimated item capacity

# Sampling size for picking a victim among oldest window items
_SAMPLE_OLDEST_WINDOW = 8

# ------------------------
# Helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # min over rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return 0.0 if est == float('inf') else float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # halve all counters
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_size_alpha(incoming_size):
    # Size penalty exponent based on size relative to average
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.45
    elif r >= 2.0:
        return 1.25
    elif r <= 0.5:
        return 0.95
    else:
        return 1.08

def _incoming_score(obj):
    # Approximate value of incoming item
    sz = max(1, int(obj.size))
    freq = _sketch_estimate(obj.key)
    alpha = _dynamic_size_alpha(sz)
    return (freq + 1.0) / (float(sz) ** alpha)

def _candidate_score(key, size, incoming_size):
    # Score for eviction comparison: higher means more valuable, so we prefer evicting small score
    freq = _sketch_estimate(key)
    alpha = _dynamic_size_alpha(incoming_size)
    return (freq + 1.0) / (float(size) ** alpha)

def _promotion_threshold(size):
    # Size-aware promotion threshold: small promote earlier
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4

def _window_target_bytes(capacity):
    return max(1, int(_f_window * int(capacity)))

def _ensure_protected_budget(cache_snapshot):
    # Demote oldest protected items until protected fits its budget
    global _prot_bytes, _win_bytes
    prot_budget = max(0, int(cache_snapshot.capacity) - _window_target_bytes(cache_snapshot.capacity))
    while _prot_bytes > prot_budget and _prot:
        k, sz = _prot.popitem(last=False)  # oldest from protected
        _prot_bytes -= sz
        # demote to window MRU
        _win[k] = sz
        _win.move_to_end(k, last=True)
        _win_bytes += sz
        _m_segment[k] = 0
        # reduce resident hits slightly to avoid immediate re-promotion
        _m_reshits[k] = max(1, _m_reshits.get(k, 1) - 1)

def _demote_one_if_window_empty():
    global _prot_bytes, _win_bytes
    if not _win and _prot:
        k, sz = _prot.popitem(last=False)
        _prot_bytes -= sz
        _win[k] = sz
        _win.move_to_end(k, last=True)
        _win_bytes += sz
        _m_segment[k] = 0
        _m_reshits[k] = max(1, _m_reshits.get(k, 1) - 1)

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(int(cache_snapshot.capacity) / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _adapt_window_if_needed():
    global _adapt_ticker, _hits_win, _hits_prot, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_win + _hits_prot
    if total > 0:
        frac_w = _hits_win / float(total)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_win = 0
    _hits_prot = 0

def _get_cached_size(key, cache_snapshot, default=None):
    # Prefer our stored value; fallback to snapshot
    if key in _win:
        return _win[key]
    if key in _prot:
        return _prot[key]
    robj = cache_snapshot.cache.get(key)
    if robj is not None:
        return int(robj.size)
    return default

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction strategy:
      - Keep protected within budget by demoting its LRU to window until within limit.
      - If window empty, demote one from protected to window.
      - Choose victim from a few oldest window items using TinyLFU/size-aware scoring;
        break ties by older order and larger size.
      - If everything is empty (shouldn't happen), fall back to any key in cache.
    """
    # Maintain budgets prior to victim choice
    _ensure_protected_budget(cache_snapshot)
    _demote_one_if_window_empty()

    if _win:
        # Consider the K oldest in window (true LRU order)
        incoming_size = max(1, int(obj.size))
        best_key = None
        best_score = None
        best_idx = None
        best_size = None

        # Iterate first K items (LRU to MRU)
        i = 0
        for k in _win.keys():
            sz = _win[k]
            score = _candidate_score(k, sz, incoming_size)
            # keep minimal score; tie-break: earlier index (older), then larger size
            if (best_key is None or
                score < best_score or
                (score == best_score and (i < best_idx or (i == best_idx and sz > best_size)))):
                best_key = k
                best_score = score
                best_idx = i
                best_size = sz
            i += 1
            if i >= _SAMPLE_OLDEST_WINDOW:
                break

        if best_key is not None:
            return best_key

    # If window empty or no candidate (rare), fall back to protected LRU or any cache key
    if _prot:
        # As a very last resort, evict protected LRU (not ideal, but ensures progress)
        k, _ = next(iter(_prot.items()))
        return k

    # Fallback to any key from snapshot if our structures are desynchronized
    for k in cache_snapshot.cache.keys():
        return k
    return None

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU and decay if needed.
      - Update EMA of sizes.
      - Move key to MRU in its segment.
      - Window hits increment resident hit counter; promote to protected if threshold reached.
      - Adapt window fraction based on where hits occur.
      - Keep protected within budget.
    """
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)

    # Ensure presence in our data structures (robustness)
    if key not in _m_segment:
        # If missing but in cache, default to window placement
        _m_segment[key] = 0
        _m_reshits[key] = 1
        if key not in _win and key not in _prot:
            _win[key] = size
            _win.move_to_end(key, last=True)
            globals()['_win_bytes'] += size

    seg = _m_segment.get(key, 0)

    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_win'] += 1
        # Move to MRU in window
        if key in _win:
            _win.move_to_end(key, last=True)
        else:
            # If out of sync, add into window
            _win[key] = size
            _win.move_to_end(key, last=True)
            globals()['_win_bytes'] += size

        # Increase resident hits and promote if threshold is met
        rh = _m_reshits.get(key, 0) + 1
        _m_reshits[key] = rh
        if rh >= _promotion_threshold(size):
            # Promote to protected MRU
            if key in _win:
                globals()['_win_bytes'] -= _win.get(key, size)
                _win.pop(key, None)
            _prot[key] = size
            _prot.move_to_end(key, last=True)
            globals()['_prot_bytes'] += size
            _m_segment[key] = 1
            _m_reshits[key] = 0  # reset after promotion

    else:
        globals()['_hits_prot'] += 1
        # Move to MRU in protected
        if key in _prot:
            _prot.move_to_end(key, last=True)
        else:
            # If out of sync, add into protected
            _prot[key] = size
            _prot.move_to_end(key, last=True)
            globals()['_prot_bytes'] += size

        # Track resident hits (not used for promotion here)
        _m_reshits[key] = _m_reshits.get(key, 0) + 1

    # Keep protected within budget
    _ensure_protected_budget(cache_snapshot)

    # Periodic adaptation of window fraction
    _adapt_window_if_needed()

def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU/decay and size EMA.
      - Default placement: window MRU.
      - If key is in protected-ghost and moderately frequent and not huge, admit to protected MRU.
      - Keep resident metadata and bytes counters.
      - Ghost-feedback: adjust window fraction when a ghost hit happens.
      - Keep protected within budget.
    """
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)

    # Clean any stale state
    if key in _win:
        globals()['_win_bytes'] -= _win.get(key, size)
        _win.pop(key, None)
    if key in _prot:
        globals()['_prot_bytes'] -= _prot.get(key, size)
        _prot.pop(key, None)

    # Decide placement
    place_protected = False
    if key in _ghost_p_set:
        freq = _sketch_estimate(key)
        r = float(size) / max(1.0, _avg_size)
        if freq >= 2.0 and r <= 2.5:
            place_protected = True

    if place_protected:
        _prot[key] = size
        _prot.move_to_end(key, last=True)
        globals()['_prot_bytes'] += size
        _m_segment[key] = 1
        _m_reshits[key] = 0
    else:
        _win[key] = size
        _win.move_to_end(key, last=True)
        globals()['_win_bytes'] += size
        _m_segment[key] = 0
        _m_reshits[key] = 1

    # Ghost feedback to adapt window size
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Enforce protected budget
    _ensure_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove evicted key from our segment structures and bytes counters.
      - Drop metadata for the key.
      - Add the key to the corresponding ghost list (to steer window ratio).
    """
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    # Determine segment and remove
    seg = _m_segment.pop(ev_key, 0)
    _m_reshits.pop(ev_key, None)

    if ev_key in _win:
        globals()['_win_bytes'] = max(0, _win_bytes - _win.get(ev_key, ev_size))
        _win.pop(ev_key, None)
        seg = 0
    elif ev_key in _prot:
        globals()['_prot_bytes'] = max(0, _prot_bytes - _prot.get(ev_key, ev_size))
        _prot.pop(ev_key, None)
        seg = 1
    else:
        # Fallback: adjust counters based on seg guess
        if seg == 0:
            globals()['_win_bytes'] = max(0, _win_bytes - ev_size)
        else:
            globals()['_prot_bytes'] = max(0, _prot_bytes - ev_size)

    # Add to ghost with trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:20:22,187 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# GreedyDual-Size-Frequency (GDSF) cache eviction algorithm
# - Size-aware, frequency-aware, and self-aging through a global inflation factor.
# - Maintains compact per-resident metadata only.

# Global metadata
m_freq = dict()          # key -> int (bounded frequency)
m_priority = dict()      # key -> float (H score used for eviction)
m_last_access = dict()   # key -> int (timestamp, for tie-breaking)
m_L = 0.0                # global "inflation" factor (aging baseline)

# Tunables
_MAX_FREQ = 32           # cap to prevent runaway counters
_EPS = 1e-9              # numeric safety for divisions

def _compute_priority(freq, size, L):
    # H = L + freq / size
    # size is positive by spec; use max(size, 1) safeguard
    return float(L) + float(freq) / max(int(size), 1)

def _ensure_metadata_for_key(cache_snapshot, key):
    # Lazily initialize metadata for any resident key missing entries.
    # This keeps algorithm robust if the environment preloads items or after resets.
    if key not in m_freq:
        m_freq[key] = 1
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_priority:
        obj = cache_snapshot.cache.get(key)
        size = obj.size if obj is not None else 1
        m_priority[key] = _compute_priority(m_freq[key], size, m_L)

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim using GDSF:
    - Evict the resident item with the smallest priority H.
    - Tie-break by oldest last access time (LRU-ish).
    '''
    # Ensure all resident keys have computed priority
    for key in cache_snapshot.cache:
        _ensure_metadata_for_key(cache_snapshot, key)

    # Find victim: min H; tie-break by oldest last_access
    victim_key = None
    victim_H = None
    victim_last_ts = None

    for key in cache_snapshot.cache:
        H = m_priority[key]
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            H < victim_H - _EPS or
            (abs(H - victim_H) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_H = H
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment (bounded) frequency.
    - Recompute priority with current L.
    - Update last access time.
    '''
    key = obj.key
    # Initialize if missing (robustness)
    _ensure_metadata_for_key(cache_snapshot, key)

    # Update frequency (bounded) and recompute priority
    f = min(m_freq.get(key, 1) + 1, _MAX_FREQ)
    m_freq[key] = f
    m_last_access[key] = cache_snapshot.access_count

    # Recompute priority using current L and resident size
    resident_obj = cache_snapshot.cache.get(key, obj)
    m_priority[key] = _compute_priority(f, resident_obj.size, m_L)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (typically after a miss):
    - Initialize frequency to 1 (first access).
    - Initialize priority to L + 1/size.
    - Set last access time.
    '''
    key = obj.key
    # Start fresh for resident metadata
    m_freq[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_priority[key] = _compute_priority(1, obj.size, m_L)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Set L to the H of the evicted object (aging step).
    - Remove evicted object's resident metadata.
    '''
    global m_L
    ev_key = evicted_obj.key

    # Ensure we can compute evicted priority even if missing
    f = m_freq.get(ev_key, 1)
    H_evicted = m_priority.get(ev_key, _compute_priority(f, evicted_obj.size, m_L))

    # Age the system: raise L to the evicted priority
    m_L = H_evicted

    # Drop metadata for the evicted resident item
    m_freq.pop(ev_key, None)
    m_priority.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 5
- Hit Rate: 0.4449

Example 2:
- Trace ID: 28
- Hit Rate: 0.0201

Example 3:
- Trace ID: 34
- Hit Rate: 0.0724

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:20:22 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:20:22,189 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:21:37 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:21:37,625 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:21:37,626 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Decayed-Frequency, Size-Aware (LRFU-Size) cache eviction algorithm
# - Combines recency and frequency via exponential decay (LRFU)
# - Accounts for object size with sublinear penalty (size^alpha)
# - Maintains compact per-resident metadata, ages lazily

import math

# Per-resident metadata
m_weight = dict()       # key -> float, decayed frequency value at last update time
m_last_ts = dict()      # key -> int, last update/access timestamp

# Tunables
_ALPHA = 0.75           # size penalty exponent (0=no size penalty, 1=fully size-aware)
_MIN_HL = 16            # min half-life in accesses
_MAX_HL = 4096          # max half-life in accesses
_FREQ_CAP = 1e6         # cap to keep numbers bounded
_EPS = 1e-12            # numeric epsilon for comparisons

_LOG_HALF = math.log(0.5)  # constant to compute decay

def _half_life(cache_snapshot):
    # Adaptive half-life: scale with number of residents, clamped
    n = max(1, len(cache_snapshot.cache))
    hl = 4 * n  # heuristic: allow items to matter for several full passes
    return max(_MIN_HL, min(_MAX_HL, hl))

def _decay_factor(delta, hl):
    # Returns 0.5^(delta/hl) = exp(log(0.5) * delta / hl)
    if delta <= 0:
        return 1.0
    return math.exp(_LOG_HALF * (float(delta) / float(hl)))

def _effective_weight(key, now_ts, cache_snapshot):
    # Lazily compute weight at current time without mutating stored base
    w = m_weight.get(key, 0.0)
    last = m_last_ts.get(key, now_ts)
    delta = now_ts - last
    if w <= 0.0 or delta <= 0:
        return max(0.0, w)
    hl = _half_life(cache_snapshot)
    return w * _decay_factor(delta, hl)

def _priority(weight_eff, size):
    # Score to minimize on eviction: smaller score => worse
    # Use sublinear size penalty; ensure positive denominator
    denom = max(1.0, float(size)) ** _ALPHA
    return weight_eff / denom

def _ensure_resident_metadata(cache_snapshot, key):
    # Initialize for resident key if missing
    if key not in m_weight:
        m_weight[key] = 0.0
    if key not in m_last_ts:
        m_last_ts[key] = 0

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim:
    - Compute decayed-frequency, size-aware score for each resident: score = w_eff / size^alpha
    - Evict the item with the smallest score
    - Tie-break by oldest last update timestamp (LRU-ish)
    '''
    now_ts = cache_snapshot.access_count

    # Ensure metadata exists for resident keys
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    victim_key = None
    victim_score = None
    victim_last_ts = None

    # Scan all residents to find the minimum priority (score)
    for key, resident_obj in cache_snapshot.cache.items():
        w_eff = _effective_weight(key, now_ts, cache_snapshot)
        score = _priority(w_eff, resident_obj.size)
        last_ts = m_last_ts.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Apply lazy decay since last update, then add 1 to the weight
    - Cap the weight to keep bounds reasonable
    - Update the timestamp to now
    '''
    key = obj.key
    now_ts = cache_snapshot.access_count

    _ensure_resident_metadata(cache_snapshot, key)

    # Decay to now, then add one
    w_eff = _effective_weight(key, now_ts, cache_snapshot)
    w_new = w_eff + 1.0
    if w_new > _FREQ_CAP:
        w_new = _FREQ_CAP

    m_weight[key] = w_new
    m_last_ts[key] = now_ts

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - Initialize with weight=1 at current time (first reference)
    '''
    key = obj.key
    now_ts = cache_snapshot.access_count

    m_weight[key] = 1.0
    m_last_ts[key] = now_ts

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Remove metadata for the evicted key
    '''
    ev_key = evicted_obj.key
    m_weight.pop(ev_key, None)
    m_last_ts.pop(ev_key, None)
```
2025-11-27 20:26:00,320 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding and Large-object guard (WSD-TLFU+SL)
# - Global admission/priority: Count-Min Sketch (TinyLFU) with periodic decay.
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Doorkeeper promotion: require 1 hit for normal-sized objects and 2 hits for large ones.
# - Scan-shielding "doomed" set: very weak insertions are tagged and preferentially evicted first.
# - Large-object guard: cap fraction of bytes held by large objects; evict large ones first if over cap.
# - Sampling-based eviction, size-aware scoring tuned to incoming size.
# - Adaptive window fraction via hit location and ARC-style ghost feedback.
# - Lightweight age-based protected demotion to avoid stale protection.
# - Maintain compact resident index for O(1) uniform sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 48            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 16     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 24     # from "doomed" set
_SAMPLE_SIZE_LARGE = 24      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.5            # reduced recency weight to resist scans
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
_LARGE_BYTES_FRACTION = 0.45     # large objects should not exceed 45% of capacity

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    # Only consider age-demotion if protected is not under pressure (handled elsewhere) but protection might be stale.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) If any "doomed" items exist, evict the weakest among a random sample from them.
      3) Else if window is above its target bytes, pick from window.
      4) Else if large objects exceed their byte cap, pick from large objects preferentially.
      5) Else sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict oldest window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Doorkeeper promotion: promote window->protected only after sufficient resident hits (size-aware).
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Default to window accounting if unknown
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse (so it won't be evicted prematurely)
    is_large = key in m_large_keys
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require 1 hit for small, 2 hits for large
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is far weaker than resident sample, tag as "doomed" (evict-first).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic
    # Compare incoming priority vs a sample of current residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    min_resident_score = None
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        if min_resident_score is None or s < min_resident_score:
            min_resident_score = s
    # Tag as doomed if clearly weaker than typical residents, or if large and very low freq
    est_freq = _sketch_estimate(key)
    if (min_resident_score is not None and incoming_score < 0.9 * min_resident_score) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 28
- Hit Rate: 0.0398

Example 3:
- Trace ID: 26
- Hit Rate: 0.8481

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:26:00 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:26:00,321 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:27:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:27:51,776 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:27:51,777 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Weighted Size-aware TinyLFU with adaptive Window/Protected (W-TinyLFU/AW+)
# Key changes vs previous version:
# - Softer size penalty and larger large-object allowance to avoid over-penalizing reusable big items.
# - Safer scan-shielding: "doomed" tag only for clearly weak inserts (quantile-based), not most newcomers.
# - Faster promotion (1 hit for all) to avoid churn in window.
# - Gentler protected aging/demotion and less sticky protection bonus to reduce stale residency.
# - More robust eviction guard: prefer doomed/oldest-window only when appropriate.
# - Slightly larger samples to improve victim selection stability.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# Sampling sizes
_SAMPLE_SIZE = 64            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 32     # from "doomed" set
_SAMPLE_SIZE_LARGE = 32      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 1.2            # moderate recency weight
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 3.0               # size >= 3x average -> "large"
_LARGE_BYTES_FRACTION = 0.75     # allow up to 75% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 20000    # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 12

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _percentile(sorted_values, pct):
    if not sorted_values:
        return None
    n = len(sorted_values)
    if n == 1:
        return sorted_values[0]
    rank = max(0, min(n - 1, int(pct * (n - 1))))
    return sorted_values[rank]

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (improved):
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Prefer evicting from "doomed" set when available (weak known inserts).
      3) If window is above target bytes, evict from window (oldest-biased).
      4) If large objects exceed cap, choose among large objects.
      5) Otherwise, sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         try doomed or oldest-window alternative when reasonable; otherwise keep the chosen victim.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            # Focus on window when it's bloated, bias to old
            old_key = _window_oldest_candidate(cache_snapshot)
            if old_key is not None:
                candidates = [old_key]
            else:
                candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Guard against admitting a poor incoming over a much better resident
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            # Prefer evicting doomed if available (other than the chosen victim)
            alt_key = None
            if m_doomed:
                doomed_cands = [k for k in _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED) if k != victim_key]
                if doomed_cands:
                    alt_key, _ = _select_victim_from_candidates(cache_snapshot, doomed_cands, incoming_size, now)
            # Else consider oldest from window if window is reasonably sized
            if alt_key is None:
                target_window = _current_window_target_bytes(cache_snapshot)
                if _g_window_bytes >= int(0.6 * target_window) and _g_window_bytes > 0:
                    alt_key = _window_oldest_candidate(cache_snapshot)
            # As last resort, the global oldest
            if alt_key is None:
                alt_key = _global_oldest_candidate(cache_snapshot)
            if alt_key is not None:
                victim_key = alt_key

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected after 1 hit for both small and large.
      - Clear "doomed" tag upon any hit (through 1-hit promotion threshold).
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists for safety
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse (1-hit promotion threshold)
    if key in m_doomed:
        m_doomed.discard(key)

    # Promote to protected after 1 resident hit (both small and large)
    needed_hits = 1
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: tag "doomed" only if clearly weaker than resident quantile and likely scan/noise.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via quantile-based admission heuristic
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    scores.sort()
    # Only tag as doomed if incoming is far below 30th-percentile resident, and either large or bigger than avg, with very low freq
    p30 = _percentile(scores, 0.30) if scores else None
    est_freq = _sketch_estimate(key)
    if p30 is not None:
        if (incoming_score < 0.70 * p30) and ((is_large or size >= _avg_size) and est_freq <= 1.0):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        # With no reference, be conservative: don't doom on cold start
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:31:44,567 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved Weighted Size-aware TinyLFU with adaptive Window/Protected (W-TinyLFU/AW+)
# Key changes vs previous version:
# - Softer size penalty and larger large-object allowance to avoid over-penalizing reusable big items.
# - Safer scan-shielding: "doomed" tag only for clearly weak inserts (quantile-based), not most newcomers.
# - Faster promotion (1 hit for all) to avoid churn in window.
# - Gentler protected aging/demotion and less sticky protection bonus to reduce stale residency.
# - More robust eviction guard: prefer doomed/oldest-window only when appropriate.
# - Slightly larger samples to improve victim selection stability.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# Sampling sizes
_SAMPLE_SIZE = 64            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 32     # from "doomed" set
_SAMPLE_SIZE_LARGE = 32      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 1.2            # moderate recency weight
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 3.0               # size >= 3x average -> "large"
_LARGE_BYTES_FRACTION = 0.75     # allow up to 75% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 20000    # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 12

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _percentile(sorted_values, pct):
    if not sorted_values:
        return None
    n = len(sorted_values)
    if n == 1:
        return sorted_values[0]
    rank = max(0, min(n - 1, int(pct * (n - 1))))
    return sorted_values[rank]

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (improved):
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Prefer evicting from "doomed" set when available (weak known inserts).
      3) If window is above target bytes, evict from window (oldest-biased).
      4) If large objects exceed cap, choose among large objects.
      5) Otherwise, sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         try doomed or oldest-window alternative when reasonable; otherwise keep the chosen victim.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            # Focus on window when it's bloated, bias to old
            old_key = _window_oldest_candidate(cache_snapshot)
            if old_key is not None:
                candidates = [old_key]
            else:
                candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Guard against admitting a poor incoming over a much better resident
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            # Prefer evicting doomed if available (other than the chosen victim)
            alt_key = None
            if m_doomed:
                doomed_cands = [k for k in _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED) if k != victim_key]
                if doomed_cands:
                    alt_key, _ = _select_victim_from_candidates(cache_snapshot, doomed_cands, incoming_size, now)
            # Else consider oldest from window if window is reasonably sized
            if alt_key is None:
                target_window = _current_window_target_bytes(cache_snapshot)
                if _g_window_bytes >= int(0.6 * target_window) and _g_window_bytes > 0:
                    alt_key = _window_oldest_candidate(cache_snapshot)
            # As last resort, the global oldest
            if alt_key is None:
                alt_key = _global_oldest_candidate(cache_snapshot)
            if alt_key is not None:
                victim_key = alt_key

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected after 1 hit for both small and large.
      - Clear "doomed" tag upon any hit (through 1-hit promotion threshold).
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists for safety
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse (1-hit promotion threshold)
    if key in m_doomed:
        m_doomed.discard(key)

    # Promote to protected after 1 resident hit (both small and large)
    needed_hits = 1
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: tag "doomed" only if clearly weaker than resident quantile and likely scan/noise.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via quantile-based admission heuristic
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    scores.sort()
    # Only tag as doomed if incoming is far below 30th-percentile resident, and either large or bigger than avg, with very low freq
    p30 = _percentile(scores, 0.30) if scores else None
    est_freq = _sketch_estimate(key)
    if p30 is not None:
        if (incoming_score < 0.70 * p30) and ((is_large or size >= _avg_size) and est_freq <= 1.0):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        # With no reference, be conservative: don't doom on cold start
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Error: Error - Timeout

Example 2:
- Trace ID: 8
- Hit Rate: 0.5381

Example 3:
- Trace ID: 35
- Hit Rate: 0.068

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:31:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:31:44,568 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:34:15 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:34:15,166 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:34:15,167 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:37:12,601 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 13
- Hit Rate: 0.0

Example 2:
- Trace ID: 36
- Hit Rate: 0.1321

Example 3:
- Trace ID: 16
- Hit Rate: 0.0199

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:37:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:37:12,602 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:39:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:39:04,332 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:39:04,334 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Segmented LRU with TinyLFU Guidance (SLRU-TinyLFU)
# - Capacity is the number of objects (not bytes).
# - Two segments:
#     0: probation (recency-focused, LRU eviction)
#     1: protected (frequency-focused, TinyLFU + recency)
# - TinyLFU Count-Min Sketch with periodic decay provides global frequency estimates.
# - ARC-like ghost caches adapt the target size of the probation segment.
# - Heaps provide O(log n) eviction for each segment, lazily purging stale entries.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (probation) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps
# probation-heap keeps LRU ordering: entries are (last_access, key, ver)
_heap_probation = []
# protected-heap keeps frequency-over-age priority: entries are (score, last_access, key, ver)
_heap_protected = []

# Segment accounting and adaptation
_prob_count = 0
_prot_count = 0
_target_prob_count = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the probation target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-probation keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_AGE_BETA = 1.0                 # exponent for age penalty in protected score: (1 + age)^beta
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    global _CAP_SEEN, _target_prob_count, _prob_count, _prot_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    global m_segment, m_last_access, m_ver
    global _heap_probation, _heap_protected
    global _prob_count, _prot_count, _target_prob_count, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_probation.clear()
    _heap_protected.clear()
    _ghostW.clear()
    _ghostP.clear()

    _prob_count = 0
    _prot_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_prob_count = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in probation by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _prob_count += 1
        _push_probation_entry(key, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_probation_target(delta, cap):
    global _target_prob_count
    min_c = max(1, int(_MIN_WINDOW_FRAC * cap))
    max_c = max(1, int(_MAX_WINDOW_FRAC * cap))
    _target_prob_count = _clamp(_target_prob_count + int(delta), min_c, max_c)

def _score_protected(key, now):
    # Protected eviction priority: lower is worse (more likely to evict)
    # score = freq / (1 + age)^beta
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    denom = (1.0 + float(age)) ** _AGE_BETA
    return freq / denom if denom > 0 else freq

def _push_probation_entry(key, now):
    ver = m_ver.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    entry = (last_ts, key, ver)
    heappush(_heap_probation, entry)

def _push_protected_entry(key, now):
    ver = m_ver.get(key, 0)
    score = _score_protected(key, now)
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, key, ver)
    heappush(_heap_protected, entry)

def _valid_probation_entry(cache_snapshot, entry):
    last_ts, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if m_segment.get(key, 0) != 0:
        return False
    return True

def _valid_protected_entry(cache_snapshot, entry):
    score, last_ts, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if m_segment.get(key, 0) != 1:
        return False
    return True

def _pop_valid_probation(cache_snapshot):
    while _heap_probation:
        entry = heappop(_heap_probation)
        if _valid_probation_entry(cache_snapshot, entry):
            return entry[1]  # key
    return None

def _pop_valid_protected(cache_snapshot):
    while _heap_protected:
        entry = heappop(_heap_protected)
        if _valid_protected_entry(cache_snapshot, entry):
            return entry[2]  # key
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_probation, _heap_protected, _prob_count, _prot_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    if now - _last_rebuild_access < 20000:
        return
    too_large = (len(_heap_probation) + len(_heap_protected)) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_probation = []
    _heap_protected = []
    _prob_count = 0
    _prot_count = 0
    for key, obj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        if m_segment.get(key, 0) == 0:
            _prob_count += 1
            _push_probation_entry(key, now)
        else:
            _prot_count += 1
            _push_protected_entry(key, now)

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust probation (window) target using ghost hits.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(1, cap // 100)  # adjust by at least 1, or ~1% of capacity
    if k in _ghostW:
        _ghostW.pop(k, None)
        _adjust_probation_target(+step, cap)
    elif k in _ghostP:
        _ghostP.pop(k, None)
        _adjust_probation_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy:
      - Prefer evicting from probation (recency segment), LRU first.
      - If probation is empty, evict the protected item with the lowest TinyLFU-over-age score.
      - Fallback to random sampling with the same preferences if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Prefer a probation victim (SLRU rule)
    if _prob_count > 0:
        victim_key = _pop_valid_probation(cache_snapshot)
        if victim_key is not None:
            return victim_key

    # Otherwise, evict the weakest protected item by score
    victim_key = _pop_valid_protected(cache_snapshot)
    if victim_key is not None:
        return victim_key

    # Fallback: sample random keys
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    sample_sz = min(32, len(keys))

    # Prefer sampling probation when any exist
    prob_keys = [k for k in keys if m_segment.get(k, 0) == 0]
    if prob_keys:
        keys = prob_keys
        sample_sz = min(sample_sz, len(keys))

    samples = random.sample(keys, sample_sz)

    # For probation: choose oldest (min last_access)
    # For protected: choose min score
    best_k = None
    best_metric = None  # smaller is worse
    for k in samples:
        # Default metadata if missing
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0

        if m_segment.get(k, 0) == 0:
            metric = m_last_access.get(k, 0)  # smaller ts => older => better victim
            # track minimal last_access
            if best_k is None or metric < best_metric:
                best_k = k
                best_metric = metric
            # seed heap
            _push_probation_entry(k, now)
        else:
            metric = _score_protected(k, now)
            if best_k is None or metric < best_metric - _EPS:
                best_k = k
                best_metric = metric
            # seed heap
            _push_protected_entry(k, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from probation -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in probation
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        global _prob_count, _prot_count
        if _prob_count > 0:
            _prob_count -= 1
        _prot_count += 1

    # Version bump and push a fresh heap entry for its current segment
    m_ver[key] += 1
    if m_segment.get(key, 0) == 0:
        _push_probation_entry(key, now)
    else:
        _push_protected_entry(key, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of probation target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=probation, last_access=now, version bump.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt probation target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # probation
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Segment accounting
    global _prob_count
    _prob_count += 1

    # Push entries
    _push_probation_entry(key, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update segment accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
        global _prob_count
        if _prob_count > 0:
            _prob_count -= 1
    else:
        _ghost_admit(_ghostP, ev_key)
        global _prot_count
        if _prot_count > 0:
            _prot_count -= 1

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries
```
2025-11-27 20:39:17,796 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 11
- Hit Rate: 0.4028

Example 2:
- Trace ID: 9
- Hit Rate: 0.531

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:39:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:39:17,797 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:41:25 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:41:25,553 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:41:25,554 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Hybrid SLRU + size-aware TinyLFU + adaptive window + ghost feedback + rookie shielding
# Key changes vs previous version:
#  - Stronger rookie shielding: evict "newbies" (resident_hits==0) in window first to block scan pollution.
#  - More decisive size-aware eviction under byte pressure (large incoming objects push evict alpha higher).
#  - Larger/older sampling pools and improved tie-breaking for more stable victim choice.
#  - Safer promotion: require size-aware resident hits or decent TinyLFU estimate; small objects still promote fast.
#  - Ghosts store size and last-seen time; window/protected fraction adapts more responsively.
#  - More conservative admit-guard: if incoming looks weak, bias toward evicting older/rookie window items.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Adaptive window fraction
_f_window = 0.32
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.08

# Sampling
_SAMPLE_SIZE = 48            # candidates per decision
_SAMPLE_SIZE_DEMOTE = 20     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 36     # oldest-of sampling
_SAMPLE_SIZE_NEWBIES = 36    # sample for rookies (resident_hits==0)

# Scoring weights
_BASE_SIZE_ALPHA = 1.02      # base size penalty exponent (slightly lower baseline)
_W_FREQ = 1.0
_W_RECENCY = 3.2             # higher recency sensitivity in window
_PROTECTED_KEEP_BONUS = 1.18
_EPS = 1e-9

# Admission guard factor (if victim looks this much stronger than incoming, bias toward older/rookies)
_ADMIT_COMPARE_FACTOR = 1.25

# Adaptive window based on hits
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like) with size/ts
_ghost_w = deque()
_ghost_w_set = set()
_ghost_w_meta = dict()  # key -> (size, last_seen_ts)
_ghost_p = deque()
_ghost_p_set = set()
_ghost_p_meta = dict()
_GHOST_FACTOR = 2.0  # capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window on bootstrap
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Base eviction scoring size penalty exponent shaped by incoming size ("byte pressure")
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 6.0:
        return _BASE_SIZE_ALPHA + 0.55
    elif r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.22
    elif r <= 0.5:
        return max(0.88, _BASE_SIZE_ALPHA - 0.14)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Utility score: (freq + recency) / size^alpha; protected gets keep bonus
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    # Local resident hits help when frequency not yet accumulated in sketch
    res_hits = m_resfreq.get(key, 0)
    score = (_W_FREQ * freq + 0.6 * res_hits + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(8 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _find_rookie_window_candidate(cache_snapshot):
    # Prefer evicting rookies (resident_hits==0) in window, pick oldest among sampled rookies
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_NEWBIES)
    rookies = []
    for key in candidates:
        if m_resfreq.get(key, 0) == 0:
            rookies.append(key)
    if not rookies:
        return None
    oldest_key = None
    oldest_ts = None
    for key in rookies:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    guard = 0
    while _g_protected_bytes > protected_budget and guard < 8:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes = max(0, _g_protected_bytes - sz)
            _g_window_bytes += sz
        guard += 1

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot, size=0, now=0):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            _ghost_p_meta[key] = (size, now)
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        _ghost_p_meta[key] = (size, now)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
            _ghost_p_meta.pop(old, None)
    else:
        if key in _ghost_w_set:
            _ghost_w_meta[key] = (size, now)
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        _ghost_w_meta[key] = (size, now)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)
            _ghost_w_meta.pop(old, None)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.75:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
      - Prefer evicting from window (probation).
      - Strong rookie shielding: evict rookies (resident_hits==0) in window first.
      - Maintain protected budget; demote oldest protected if needed or if window empty.
      - Within window, choose victim using size-aware TinyLFU scoring under "byte pressure"
        from incoming size; larger incoming objects bias eviction toward larger cold items.
      - Admission guard: if sampled best victim is much stronger than incoming, bias toward
        oldest/rookie window items (reduces pollution).
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and ensure window has victims
    _maybe_demote_from_protected(cache_snapshot)
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Rookie-first eviction in window (blocks scan pollution)
    rookie_victim = _find_rookie_window_candidate(cache_snapshot)
    if rookie_victim is not None:
        return rookie_victim

    # Otherwise, pick the oldest-of-window as a baseline
    window_oldest = _window_oldest_candidate(cache_snapshot)

    # If no window victim available, fallback to global oldest (rare)
    if window_oldest is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Create candidate pool from window; include the oldest candidate
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    if window_oldest not in candidates:
        candidates.append(window_oldest)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    size_alpha = _dynamic_size_alpha(incoming_size)
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key_alpha(cache_snapshot, key, robj, now, size_alpha)
        last_ts = m_last_access.get(key, 0)
        # Minimize score; tiebreak: older first, then larger size (free more bytes if equal value)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Admission guard: if chosen victim looks much better than incoming,
    # evict an even older/rookie window key to reduce pollution.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt_rookie = _find_rookie_window_candidate(cache_snapshot)
            if alt_rookie is not None:
                victim_key = alt_rookie
            else:
                alt_old = _window_oldest_candidate(cache_snapshot)
                if alt_old is not None:
                    victim_key = alt_old
                else:
                    g_old = _global_oldest_candidate(cache_snapshot)
                    if g_old is not None:
                        victim_key = g_old

    if victim_key is None:
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and EMA size.
      - Update last access timestamp and resident-hit counter.
      - Promote window -> protected when meeting size-aware threshold OR LFU estimate is decent.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget via demotion of oldest.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (safety)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Adaptation accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Access update
    m_last_access[key] = now
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        # Also allow promotion if TinyLFU indicates decent frequency
        if m_resfreq.get(key, 0) >= thresh or _sketch_estimate(key) >= 2.0:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU and EMA size.
      - Default placement: window (probation).
      - If previously in protected-ghost and TinyLFU suggests reuse (and not very large),
        admit into protected directly.
      - Initialize resident metadata and bytes accounting.
      - Initialize resident-hit counter to 0 (rookie shielding).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Placement decision
    place_protected = False
    freq_est = _sketch_estimate(key)
    r = float(max(1, int(obj.size))) / max(1.0, _avg_size)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    # Rookie shielding: start at 0; first hit will become 1
    m_resfreq[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction, size-aware strength
    global _f_window
    if key in _ghost_w_set:
        # Many window-ghost reuses => need more recency
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
        _ghost_w_meta.pop(key, None)
    if key in _ghost_p_set:
        # Many protected-ghost reuses => too much recency; reduce window
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)
        _ghost_p_meta.pop(key, None)

    # Enforce protected budget after placement
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key and update byte counters.
      - Add key to ghost history (with size and last-seen time) to steer window/protected ratio.
      - TinyLFU sketch persists (with periodic decay).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    now = cache_snapshot.access_count

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with size/time metadata
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot, size=ev_size, now=now)
```
2025-11-27 20:41:40,061 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding and Large-object guard (WSD-TLFU+SL)
# - Global admission/priority: Count-Min Sketch (TinyLFU) with periodic decay.
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Doorkeeper promotion: require 1 hit for normal-sized objects and 2 hits for large ones.
# - Scan-shielding "doomed" set: very weak insertions are tagged and preferentially evicted first.
# - Large-object guard: cap fraction of bytes held by large objects; evict large ones first if over cap.
# - Sampling-based eviction, size-aware scoring tuned to incoming size.
# - Adaptive window fraction via hit location and ARC-style ghost feedback.
# - Lightweight age-based protected demotion to avoid stale protection.
# - Maintain compact resident index for O(1) uniform sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 48            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 16     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 24     # from "doomed" set
_SAMPLE_SIZE_LARGE = 24      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.5            # reduced recency weight to resist scans
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
_LARGE_BYTES_FRACTION = 0.45     # large objects should not exceed 45% of capacity

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    # Only consider age-demotion if protected is not under pressure (handled elsewhere) but protection might be stale.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) If any "doomed" items exist, evict the weakest among a random sample from them.
      3) Else if window is above its target bytes, pick from window.
      4) Else if large objects exceed their byte cap, pick from large objects preferentially.
      5) Else sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict oldest window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Doorkeeper promotion: promote window->protected only after sufficient resident hits (size-aware).
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Default to window accounting if unknown
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse (so it won't be evicted prematurely)
    is_large = key in m_large_keys
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require 1 hit for small, 2 hits for large
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is far weaker than resident sample, tag as "doomed" (evict-first).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic
    # Compare incoming priority vs a sample of current residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    min_resident_score = None
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        if min_resident_score is None or s < min_resident_score:
            min_resident_score = s
    # Tag as doomed if clearly weaker than typical residents, or if large and very low freq
    est_freq = _sketch_estimate(key)
    if (min_resident_score is not None and incoming_score < 0.9 * min_resident_score) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 45
- Hit Rate: 0.069

Example 2:
- Trace ID: 5
- Hit Rate: 0.4719

Example 3:
- Trace ID: 30
- Hit Rate: 0.0456

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:41:40 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:41:40,069 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:44:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:33,337 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:33,360 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Weighted TinyLFU with SLRU (Window + Probation + Protected) and ARC-style window adaptation
# - Global admission priority: Count-Min Sketch (TinyLFU) with periodic decay.
# - Three segments in cache:
#     0 = window (recency buffer, Eden)
#     1 = probation (main, candidates for eviction)
#     2 = protected (main, frequent, promoted from probation on hit)
# - Eviction uses W-TinyLFU admission: evict window-oldest unless incoming is stronger than probation-oldest.
# - Size-aware admission: frequency is normalized by size^alpha.
# - Sampling-based oldest selection per segment to avoid full LRU structures.
# - Adaptive window fraction using ARC-style ghost history feedback.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window), 1 (probation), 2 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes tracked per segment
_g_window_bytes = 0
_g_prob_bytes = 0
_g_prot_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Window fraction (bytes) is adaptive
_f_window = 0.20
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Within "main" (probation+protected), keep most bytes protected
_f_protected_of_main = 0.80

# Sampling sizes
_SAMPLE_OLDEST = 32
_SAMPLE_GLOBAL = 48

# Admission constants
_SIZE_ALPHA_BASE = 1.05
_ADMIT_INCOMING_BIAS = 0.50  # small boost to incoming vs probation victim
_EPS = 1e-9

# Ghost history (ARC-like feedback for window sizing)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_m = deque()       # probation/protected ghosts collapsed as "main"
_ghost_m_set = set()
_GHOST_FACTOR = 2.0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _dynamic_size_alpha(obj_size):
    # Mildly penalize larger objects; reward tiny ones
    r = float(max(1, int(obj_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _SIZE_ALPHA_BASE + 0.30
    elif r >= 2.0:
        return _SIZE_ALPHA_BASE + 0.12
    elif r <= 0.5:
        return max(0.90, _SIZE_ALPHA_BASE - 0.12)
    else:
        return _SIZE_ALPHA_BASE

def _score_freq_size(key, obj_size):
    # Size-aware TinyLFU score (no recency here; recency handled by oldest selection)
    f = _sketch_estimate(key)
    alpha = _dynamic_size_alpha(obj_size)
    return f / (float(max(1, int(obj_size))) ** alpha)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_target_bytes(cache_snapshot):
    main_bytes = max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))
    return int(_f_protected_of_main * main_bytes)

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _sample_keys_from_segment(seg, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if seg is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 1) == seg:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and seg is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _oldest_in_segment(seg, k=_SAMPLE_OLDEST):
    candidates = _sample_keys_from_segment(seg, k)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest(k=_SAMPLE_OLDEST):
    candidates = _sample_keys_from_segment(None, k)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add_to(set_deque, set_hash, key, cap):
    if key in set_hash:
        return
    set_deque.append(key)
    set_hash.add(key)
    while len(set_deque) > cap:
        old = set_deque.popleft()
        set_hash.discard(old)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_prob_bytes, _g_prot_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Assign existing items to "probation" by default (conservative) if unknown
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 1  # probation
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        seg = m_segment.get(key, 1)
        if seg == 0:
            _g_window_bytes += size
        elif seg == 1:
            _g_prob_bytes += size
        else:
            _g_prot_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _rebalance_protected_if_needed(cache_snapshot):
    # Keep protected within target budget by demoting oldest protected to probation
    target = _protected_target_bytes(cache_snapshot)
    if _g_prot_bytes <= target:
        return
    # Demote a few oldest protected until within budget (bounded iterations)
    attempts = 3
    while attempts > 0 and _g_prot_bytes > target:
        key = _oldest_in_segment(2, k=16)
        if key is None:
            break
        # demote protected -> probation
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            break
        sz = int(robj.size)
        if m_segment.get(key, 2) == 2:
            m_segment[key] = 1
            globals()['_g_prot_bytes'] -= sz
            globals()['_g_prob_bytes'] += sz
        attempts += -1

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy: Weighted TinyLFU admission
      - Choose between evicting window-oldest or probation-oldest based on TinyLFU frequency (size-aware).
      - If incoming score >= probation-oldest score (with small bias), evict probation-oldest; else evict window-oldest.
      - If one segment empty, evict from the other. Fallback to global-oldest if needed.
      - Maintain protected budget via periodic demotion.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _rebalance_protected_if_needed(cache_snapshot)

    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))
    incoming_score = _score_freq_size(obj.key, incoming_size) + _ADMIT_INCOMING_BIAS

    # Oldest candidates by segment (sampled)
    win_old = _oldest_in_segment(0, k=_SAMPLE_OLDEST)
    prob_old = _oldest_in_segment(1, k=_SAMPLE_OLDEST)

    # If either segment empty, take from the other
    if win_old is None and prob_old is None:
        alt = _global_oldest(k=_SAMPLE_OLDEST)
        return alt
    if win_old is None:
        return prob_old
    if prob_old is None:
        return win_old

    # Compare TinyLFU score (size-aware) for admission decision
    prob_obj = cache_snapshot.cache.get(prob_old, None)
    if prob_obj is None:
        return win_old
    prob_score = _score_freq_size(prob_old, max(1, int(prob_obj.size)))

    # If incoming is stronger than probation-oldest, admit it by evicting probation-oldest
    if incoming_score + _EPS >= prob_score:
        return prob_old
    # Otherwise evict from window (resist scans)
    return win_old

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Update last access.
      - Promotions:
          window -> probation on first resident hit (doorkeeper).
          probation -> protected on hit.
      - Keep protected within budget via demotion of oldest protected if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure metadata exists (in case of bootstrap or external changes)
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        if key not in m_segment:
            m_segment[key] = 1  # default probation
        seg_new = m_segment.get(key, 1)
        if seg_new == 0:
            globals()['_g_window_bytes'] += size
        elif seg_new == 1:
            globals()['_g_prob_bytes'] += size
        else:
            globals()['_g_prot_bytes'] += size
    if key not in m_segment:
        m_segment[key] = 1
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 1)
    m_last_access[key] = now

    if seg == 0:
        # Window: on first hit, move to probation (doorkeeper behavior)
        m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)
        if m_hits_resident[key] >= 1:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= size
            globals()['_g_prob_bytes'] += size
    elif seg == 1:
        # Probation -> Protected on hit
        m_segment[key] = 2
        globals()['_g_prob_bytes'] -= size
        globals()['_g_prot_bytes'] += size
    else:
        # Protected: refresh recency (timestamp already updated)
        pass

    _rebalance_protected_if_needed(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, resident_hits=0.
      - Add to resident index and update segment bytes.
      - ARC-style ghost feedback to adapt window fraction:
          If key seen in window-ghost recently -> increase window.
          If key seen in main-ghost recently -> decrease window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

    # Ghost-feedback adaptation
    cap = _ghost_capacity_items(cache_snapshot)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_m_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_m_set.discard(key)
    # Trim ghosts to capacity
    while len(_ghost_w) > cap:
        old = _ghost_w.popleft()
        _ghost_w_set.discard(old)
    while len(_ghost_m) > cap:
        old = _ghost_m.popleft()
        _ghost_m_set.discard(old)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and bytes counters for evicted key.
      - Add to appropriate ghost history for window adaptation.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 1)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    elif seg == 1:
        globals()['_g_prob_bytes'] -= ev_size
    else:
        globals()['_g_prot_bytes'] -= ev_size

    # Ghost history
    cap = _ghost_capacity_items(cache_snapshot)
    if seg == 0:
        _ghost_add_to(_ghost_w, _ghost_w_set, ev_key, cap)
    else:
        _ghost_add_to(_ghost_m, _ghost_m_set, ev_key, cap)
```
2025-11-27 20:45:37,748 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU with Ghost-feedback (A-SS-TinyLFU+G)
# - Global admission/priority: Count-Min Sketch with periodic aging (decay).
# - In-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based; size-aware scoring dynamically adapts to incoming object size.
# - Dynamic window fraction adapts based on hit locations and ghost-history feedback (ARC-style).
# - Ghost sets track recently evicted keys from window/protected to steer adaptation.
# - Per-resident metadata kept compact; maintain an indexable key list for O(1) sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 16    # oldest-of-window sampling when rejecting pollution

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.5            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.15 # protected items are a bit harder to evict
_EPS = 1e-9

# "Victim compare" guard: if all candidates are stronger than incoming, prefer evicting old window items
_ADMIT_COMPARE_FACTOR = 1.05

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    # no protected bonus for incoming
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, prefer evicting from window (probation).
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Scoring is size-aware and dynamically adapts to the incoming object's size.
      - "Victim compare": if all sampled candidates appear stronger than the incoming request,
        prefer evicting the oldest window item (scan-resistance). If none, evict oldest globally.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (should be rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Estimate incoming "priority"
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Guard against polluting admission: if sampled min is still stronger than incoming,
    # evict an old window item instead (scan/one-hit resistance).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Conservatively account bytes where it currently resides
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        # Optional: remove from ghost to avoid repeated swings
        try:
            _ghost_w_set.discard(key)
            # lazy remove from deque on capacity trim later
        except Exception:
            pass
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        try:
            _ghost_p_set.discard(key)
        except Exception:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0707

Example 2:
- Trace ID: 24
- Error: Error - Timeout

Example 3:
- Trace ID: 46
- Hit Rate: 0.4663

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:45:37 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:45:37,749 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:48:15 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:48:15,549 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:48:15,551 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware TinyLFU with Biased SLRU and Ghost-feedback (A-SS-TinyLFU+G v2)
# Changes vs previous version:
# - Remove "admit-compare" override that evicted oldest window even when incoming was weaker (reduced scanning pollution).
# - Bias eviction sampling toward the window segment; protect long-term hot items better.
# - Rebalance score toward frequency; add lightweight resident-hit counter for re-reference strength.
# - Reduce per-decision work and avoid risky byte double-accounting on hits (performance/consistency).
# - Keep protected segment within budget via occasional demotion.
# - Maintain TinyLFU with periodic decay and ghost-feedback ARC-style window size adaptation.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Per-resident lightweight hit counter (reset on insert; removed on evict)
m_hits_resident = dict()  # key -> small int

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step; slow & stable

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 30000  # a bit less frequent than before, cheaper
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 24            # total candidates per eviction decision (lower -> faster)
_SAMPLE_SIZE_DEMOTE = 12     # candidates to demote from protected (low priority)
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling when needed
_WINDOW_BIAS = 0.67          # fraction of eviction candidates drawn from window

# Priority scoring weights (rebalanced)
_BASE_SIZE_ALPHA = 1.05      # base size penalty exponent (slightly milder)
_W_FREQ = 2.0                # stronger weight for frequency
_W_RECENCY = 1.0             # lower weight for recency
_W_RES_HITS = 1.5            # weight for resident hit count (sqrt)
_PROTECTED_KEEP_BONUS = 1.25 # protected items are harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.30
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.12
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.12)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency + resident-hit + recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    res_hits = m_hits_resident.get(key, 0)
    res_term = (res_hits ** 0.5)
    score_num = _W_FREQ * freq + _W_RES_HITS * res_term + _W_RECENCY * recency
    score = score_num / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Wrapper using dynamic size exponent tuned for the incoming object
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    # Estimate how valuable the incoming key would be if resident (segment=window, recency ~ now)
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    res_term = 0.0  # incoming has no resident hits yet
    score = (_W_FREQ * freq + _W_RES_HITS * res_term + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _biased_sample_keys(cache_snapshot, k_total, restrict_to_window):
    # Draw k_total candidates with a bias toward window keys; if restricted, draw only from window.
    n = len(m_keys)
    if n == 0 or k_total <= 0:
        return []
    if restrict_to_window:
        k = min(k_total, n)
        # sample only window, fallback to global if empty
        cand = _sample_keys_from_segment(0, k)
        if not cand:
            cand = _sample_keys_from_segment(None, k)
        return cand
    # Otherwise, sample with bias: more from window than protected
    k_w = int(max(0, round(_WINDOW_BIAS * k_total)))
    k_p = max(0, min(k_total - k_w, n))
    # Get samples
    sw = _sample_keys_from_segment(0, min(k_w, n))
    sp = _sample_keys_from_segment(1, min(k_p, n))
    # If not enough due to skewed population, top up globally
    got = len(sw) + len(sp)
    if got < k_total:
        extra = _sample_keys_from_segment(None, min(k_total - got, n))
        # avoid duplicates
        sset = set(sw) | set(sp)
        extra = [x for x in extra if x not in sset]
        return sw + sp + extra
    return sw + sp

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote a low-priority protected item.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the one with the lowest priority score
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection with window bias:
      - Prefer evicting from "window" when window bytes exceed target (probation first).
      - Otherwise, sample with a bias toward window keys and evict the min-score key.
      - Scoring: TinyLFU frequency + resident-hit reuse + recency, size-aware (dynamic exponent).
      - No "admit-compare" override: always evict the weakest candidate (reduces scan pollution).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Occasionally demote to respect protected budget
    _maybe_demote_from_protected(cache_snapshot)

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via biased sampling
    k = min(_SAMPLE_SIZE, len(m_keys))
    candidates = _biased_sample_keys(cache_snapshot, k_total=k, restrict_to_window=restrict_to_window)

    # Fallback: if sampling failed (rare), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates using dynamic size-aware scoring
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > int(victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Final fallback
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay when needed.
      - Update last access timestamp and per-resident hit counter.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - Keep protected within budget by demoting a low-priority protected item if needed.
      - Update running average object size (EMA).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (defensive; avoid double accounting)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access and resident hits
    m_last_access[key] = now
    m_hits_resident[key] = min(255, m_hits_resident.get(key, 0) + 1)  # cap to keep small

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, resident-hit counter=0.
      - Track resident key for O(1) sampling and bytes accounting.
      - Update running average size.
      - Ghost-feedback: if the key is in a ghost list, adjust window fraction toward that segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        # Missed in window-ghost: increase window
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        # Missed in protected-ghost: decrease window (increase protected)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:49:31,349 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Hit Rate: 0.0438

Example 2:
- Trace ID: 0
- Hit Rate: 0.4728

Example 3:
- Trace ID: 2
- Hit Rate: 0.4579

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:49:31 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:49:31,350 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:52:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:52:28,777 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:52:28,779 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Adaptive SLRU + TinyLFU with size-aware, space-aware sampled eviction
# Key improvements vs previous version:
# - Stricter promotion: two-hit promotion for small objects, 3–4 hits for larger ones.
# - Space-aware eviction: favors evicting a small set of low-value, larger objects when under byte pressure.
# - Safer protected segment: cap protected by target bytes; demote oldest protected if it grows too large at promotion time.
# - Better window-only eviction triggers: evict from window when it's overweight or when required bytes are small.
# - Slightly larger samples for better victim choice without expensive scans.
# - Protected hits strengthen resident frequency more (stabilizes hot set).
# - Conservative size penalty (alpha=1.0) reduces pollution by large objects.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192                  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.22  # slightly smaller default to reduce scan pollution

# Size-aware priority divisor exponent (more conservative against large objects)
_SIZE_ALPHA = 1.00

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 0.7  # reduce recency bias in protected

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.40

# Promotion policy: multi-hit thresholds based on object size fraction
_SMALL_FRAC = 0.02      # <2% of capacity
_MEDIUM_FRAC = 0.10     # 2–10% of capacity
_PROMOTE_HITS_SMALL = 2
_PROMOTE_HITS_MED = 3
_PROMOTE_HITS_LARGE = 4

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.70
_FREQ_RESIDENT_WEIGHT = 0.30

# Eviction sampling (slightly larger for better decisions; still O(1))
_SAMPLE_WIN = 8
_SAMPLE_PROT = 6

# Space-pressure tradeoff in eviction (higher favors freeing bytes faster)
_SPACE_PRESSURE_WIN = 0.35
_SPACE_PRESSURE_MIXED = 0.25

# Protected cap slack during promotion (bytes)
_PROTECTED_SLACK_FRAC = 0.05  # allow slight temporary overflow before demotion

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default unseen to window
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            sz = int(robj.size)
            if sz < int(_SMALL_FRAC * cap):
                need = _PROMOTE_HITS_SMALL
            elif sz < int(_MEDIUM_FRAC * cap):
                need = _PROMOTE_HITS_MED
            else:
                need = _PROMOTE_HITS_LARGE
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        cap = max(1, int(cache_snapshot.capacity))
        sz = int(getattr(obj, 'size', 1))
        if sz < int(_SMALL_FRAC * cap):
            need = _PROMOTE_HITS_SMALL
        elif sz < int(_MEDIUM_FRAC * cap):
            need = _PROMOTE_HITS_MED
        else:
            need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target (more recency)
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target (more frequency)
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite keep priority: higher => keep
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _required_bytes(cache_snapshot, obj):
    free_bytes = int(cache_snapshot.capacity) - int(cache_snapshot.size)
    need = int(obj.size) - max(0, free_bytes)
    return max(0, need)

def _avg_window_entry_size():
    cnt = len(_win_q)
    return int(_win_bytes / cnt) if cnt > 0 else 0

def _ensure_protected_cap_before_promotion(cache_snapshot, promote_in_size):
    # Demote oldest protected entry to window if protected would exceed its target too much.
    # Do at most one demotion per call to keep O(1).
    global _prot_bytes, _win_bytes
    cap = max(1, int(cache_snapshot.capacity))
    target_win = max(1, int(_window_frac * cap))
    target_prot = cap - target_win
    slack = int(_PROTECTED_SLACK_FRAC * cap)
    projected = _prot_bytes + int(promote_in_size)
    if projected > target_prot + slack and _prot_q:
        # Demote the oldest protected item to window MRU
        dem_key, _ = _prot_q.popitem(last=False)
        if dem_key is not None:
            m_segment[dem_key] = 0
            _win_q[dem_key] = None
            _win_q.move_to_end(dem_key, last=True)
            # Adjust bytes
            sz = 1
            # The object must be in cache; fall back to m_last_access for presence
            # We can try to fetch size from resident metadata via ghosts won't help; use stored count
            # However, we can derive size by checking cache_snapshot.cache
            robj = cache_snapshot.cache.get(dem_key)
            if robj is not None:
                sz = int(robj.size)
            _prot_bytes = max(0, _prot_bytes - sz)
            _win_bytes += sz
            # Slightly reduce its resident hits to avoid immediate re-promotion
            m_res_hits[dem_key] = max(0, m_res_hits.get(dem_key, 0) - 1)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - If window bytes exceed target OR the required bytes are small, evict from window-only sample.
      - Otherwise, sample from both segments (favor window) and select the minimum of a space-aware score:
            value = keep_priority - lambda * space_gain
        where space_gain = min(1, candidate_size / required_bytes).
      - Tie-break by oldest access time, then by largest size to free space faster.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    req_bytes = _required_bytes(cache_snapshot, obj)
    cap = max(1, int(cache_snapshot.capacity))
    target_window = max(1, int(_window_frac * cap))
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0

    # Evict from window if it is overweight or if small space is needed
    avg_win_sz = _avg_window_entry_size()
    small_req = req_bytes > 0 and req_bytes <= max(1, avg_win_sz)
    restrict_to_window = has_window and (window_bytes > target_window or small_req)

    candidates = []
    if restrict_to_window:
        for k in _sample_oldest_keys(_win_q, _SAMPLE_WIN):
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        for k in _sample_oldest_keys(_win_q, _SAMPLE_WIN):
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in _sample_oldest_keys(_prot_q, _SAMPLE_PROT):
            if k in cache_snapshot.cache:
                candidates.append(k)

    if not candidates:
        # Fallback deterministic
        for k in cache_snapshot.cache.keys():
            return k
        return None

    # Select victim via space-aware valuation
    best_key = None
    best_val = None
    best_last_ts = None
    best_size = None
    space_lambda = _SPACE_PRESSURE_WIN if restrict_to_window else _SPACE_PRESSURE_MIXED

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        sz = max(1, int(robj.size))
        keep_score = _priority_for_key(cache_snapshot, key, robj, now)
        if req_bytes > 0:
            space_gain = min(1.0, float(sz) / float(req_bytes))
        else:
            space_gain = 0.0
        value = keep_score - (space_lambda * space_gain)

        last_ts = m_last_access.get(key, 0)

        if (best_key is None or
            value < best_val - _EPS or
            (abs(value - best_val) <= _EPS and (last_ts < best_last_ts or
                                                (last_ts == best_last_ts and sz > best_size)))):
            best_key = key
            best_val = value
            best_last_ts = last_ts
            best_size = sz

    if best_key is None:
        # Final fallback to LRU of window then protected
        if _win_q:
            return next(iter(_win_q.keys()))
        if _prot_q:
            return next(iter(_prot_q.keys()))
        for k in cache_snapshot.cache.keys():
            return k
        return None

    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay and resident decay).
      - Update resident metadata: last_access, res_hits (weighted by segment).
      - Move key to MRU of its segment LRU queue.
      - Size-aware promotion from window to protected when threshold met (2–4 hits).
      - Enforce protected target on promotion by demoting the oldest protected if necessary.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    seg = m_segment.get(key, 0)

    # Strengthen hits in protected slightly to stabilize hot set
    if seg == 1:
        m_res_hits[key] = m_res_hits.get(key, 0) + 2
    else:
        m_res_hits[key] = m_res_hits.get(key, 0) + 1

    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None

        # Size-aware multi-hit promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Enforce protected capacity before promotion by demoting one if needed
            _ensure_protected_cap_before_promotion(cache_snapshot, int(getattr(obj, 'size', 1)))

            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)

            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes = max(0, _win_bytes - size)
            _prot_bytes += size

            # After promotion, require another hit later for next potential considerations
            # Slightly dampen to avoid immediate ping-pong
            m_res_hits[key] = max(1, m_res_hits[key] - 1)
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    sz = int(obj.size)
    if sz < int(_SMALL_FRAC * cap):
        m_promote_need[key] = _PROMOTE_HITS_SMALL
    elif sz < int(_MEDIUM_FRAC * cap):
        m_promote_need[key] = _PROMOTE_HITS_MED
    else:
        m_promote_need[key] = _PROMOTE_HITS_LARGE

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += sz

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into its ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 20:52:52,885 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 19
- Hit Rate: 0.2722

Example 2:
- Trace ID: 39
- Hit Rate: 0.0785

Example 3:
- Trace ID: 20
- Hit Rate: 0.3717

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:52:52 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:52:52,886 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:55:01 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:55:01,872 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:55:01,873 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight SLRU with TinyLFU guidance, window/protected split (count-based),
# LRU via lazy heaps, and ARC-style ghost feedback to adapt window size.
#
# Key design choices:
# - Capacity is number-of-items (not bytes). All budgets and decisions use counts.
# - Two resident segments:
#     window (probation): recency-biased, holds newly inserted and recently used items.
#     protected: holds items that were hit at least once while in window.
# - Eviction: strictly evict the LRU of the window. If window is empty, demote the LRU
#   of protected to window, then evict LRU of window. This matches SLRU/2Q intuition.
# - Promotion: on a hit in window, promote to protected (threshold = 1).
# - Budgets: protected is capped so window can retain at least target fraction.
# - Recency tracking: two min-heaps (window/protected) keyed by last access; lazy cleanup.
# - TinyLFU: global, decayed count-min sketch for frequency estimation (helps promotion
#   and future tuning, and can be extended). We do not do true admission rejection
#   (framework always inserts on miss), so we keep it lightweight.
# - Adaptive window fraction: based on hit distribution between window and protected
#   and ghost feedback (ARC-like): if we miss on a key recently evicted from window,
#   increase window; if from protected, decrease window.

import heapq
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = {}        # key -> 0 (window) or 1 (protected)
m_last_access = {}    # key -> last access timestamp
m_resfreq = {}        # key -> resident hits since insert (for promotion policy)

# LRU via lazy min-heaps: (last_access, seq, key, seg_at_enqueue)
_window_heap = []
_protected_heap = []
_seq_no = 0  # tie-breaker for heap

# Segment sizes (#items)
_window_items = 0
_protected_items = 0

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Window fraction (count-based)
_f_window = 0.20
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.95
_WINDOW_STEP = 0.05

# Adaptive hit accounting
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # Min across rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _window_items, _protected_items, _seq_no
    if _bootstrapped:
        return
    # Initialize any pre-existing items as window residents with ts=0
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        heapq.heappush(_window_heap, (m_last_access[key], _seq_no, key, 0))
        _seq_no += 1
        _window_items += 1
        if key not in m_resfreq:
            m_resfreq[key] = 0
    _bootstrapped = True

def _target_window_items(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    # Ensure at least 1 slot in window where possible
    tw = int(round(_f_window * cap))
    tw = max(1, min(tw, cap))
    return tw

def _ghost_capacity_items(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(_GHOST_FACTOR * cap))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _push_heap_for_key(key):
    # Push current state of key into its segment heap; lazy cleanup on pop
    global _seq_no
    seg = m_segment.get(key, 0)
    ts = m_last_access.get(key, 0)
    if seg == 0:
        heap = _window_heap
    else:
        heap = _protected_heap
    heapq.heappush(heap, (ts, _seq_no, key, seg))
    _seq_no += 1

def _pop_valid_oldest(heap, want_seg):
    # Pop until a valid entry found (matches current segment and ts)
    while heap:
        ts, _, key, seg_at_enqueue = heap[0]
        # If key no longer resident or moved to other segment or updated ts, discard
        if key not in m_segment or m_segment.get(key) != want_seg or m_last_access.get(key) != ts:
            heapq.heappop(heap)
            continue
        return key
    return None

def _maybe_demote_from_protected(cache_snapshot):
    # Ensure protected fits into its budget: <= cap - target_window
    global _protected_items, _window_items
    cap = max(1, int(cache_snapshot.capacity))
    target_window = _target_window_items(cache_snapshot)
    prot_budget = max(0, cap - target_window)
    while _protected_items > prot_budget:
        # Demote oldest protected to window
        cand = _pop_valid_oldest(_protected_heap, 1)
        if cand is None:
            break
        # Demote (segment change, counts)
        m_segment[cand] = 0
        _protected_items = max(0, _protected_items - 1)
        _window_items += 1
        # Keep same ts; push into window heap
        _push_heap_for_key(cand)

def _demote_one_from_protected(cache_snapshot):
    global _protected_items, _window_items
    cand = _pop_valid_oldest(_protected_heap, 1)
    if cand is None:
        return False
    m_segment[cand] = 0
    _protected_items = max(0, _protected_items - 1)
    _window_items += 1
    _push_heap_for_key(cand)
    return True

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        # If most hits occur in window -> increase window; if mostly in protected -> decrease
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction:
      - Respect protected budget; demote oldest protected until protected <= budget.
      - If window is empty, demote one oldest protected to window.
      - Evict the LRU of the window (oldest by last access).
      - Fallback to global oldest if needed (rare).
    '''
    _bootstrap_if_needed(cache_snapshot)

    # Keep protected within budget first
    _maybe_demote_from_protected(cache_snapshot)

    # Guarantee something to evict in window (if possible)
    if _window_items <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Evict oldest from window
    v = _pop_valid_oldest(_window_heap, 0)
    if v is not None:
        return v

    # Fallback: try protected (shouldn't happen normally)
    v2 = _pop_valid_oldest(_protected_heap, 1)
    if v2 is not None:
        return v2

    # Last resort: nothing tracked properly, pick any resident key
    for key in cache_snapshot.cache.keys():
        return key
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU with access and decay if needed.
      - Update last access time and push to appropriate heap (lazy LRU).
      - Count hits per segment (for window fraction adaptation).
      - If hit in window, promote to protected (threshold 1).
      - Enforce protected budget by demoting oldest protected if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_segment:
        # Late init for unexpected state
        m_segment[key] = 0
        m_last_access[key] = now
        m_resfreq[key] = 0
        global _window_items
        _window_items += 1
        _push_heap_for_key(key)

    seg = m_segment.get(key, 0)

    # Adaptation counters
    global _adapt_ticker, _hits_in_window, _hits_in_protected
    _adapt_ticker += 1
    if seg == 0:
        _hits_in_window += 1
    else:
        _hits_in_protected += 1

    # Update recency and enqueue
    m_last_access[key] = now
    _push_heap_for_key(key)

    # Promotion: window -> protected upon first hit
    if seg == 0:
        m_resfreq[key] = m_resfreq.get(key, 0) + 1
        if m_resfreq[key] >= 1:
            # Promote
            m_segment[key] = 1
            m_resfreq[key] = 0
            global _window_items, _protected_items
            _window_items = max(0, _window_items - 1)
            _protected_items += 1
            _push_heap_for_key(key)

    # Ensure protected respects budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU for this access and decay.
      - Place new item into window (probation).
      - Initialize last access to now; enqueue into window heap; hit counter reset.
      - ARC-style ghost feedback to adapt window fraction:
          * If key was in window-ghost -> increase window
          * If key was in protected-ghost -> decrease window
      - Keep protected within budget after placement adjustments.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Place into window
    m_segment[key] = 0
    m_last_access[key] = now
    m_resfreq[key] = 0

    global _window_items
    _window_items += 1
    _push_heap_for_key(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque will happen on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Enforce protected budget if window target shrank
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata and decrement segment counts.
      - Add key into appropriate ghost list (window/protected) for ARC-like feedback.
      - TinyLFU persists globally; no need to change here (decayed over time).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key

    # Determine segment before deletion
    seg = m_segment.get(ev_key, 0)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)

    global _window_items, _protected_items
    if seg == 0:
        _window_items = max(0, _window_items - 1)
    else:
        _protected_items = max(0, _protected_items - 1)

    # Ghost tracking
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:55:09,095 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 18
- Hit Rate: 0.0239

Example 2:
- Trace ID: 44
- Hit Rate: 0.0419

Example 3:
- Trace ID: 38
- Hit Rate: 0.1255

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:55:09 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:55:09,096 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m20:58:19 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:58:19,264 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:58:19,265 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented TinyLFU with Better LRU Discipline and Balanced Global Victim Selection
# Key improvements:
# - Stronger recency discipline: hits update LRU position in both segments.
# - Balanced victim selection: consider both segments and random samples together;
#   no hard preference to window-only eviction.
# - Keep protected "sticky" but allow eviction if clearly weaker than alternatives.
# - Maintain adaptive window (ARC-like ghosts) and size-aware TinyLFU scoring.
# - Gentle scan-shielding via "doomed" tagging for large, cold inserts.
#
# Expected benefits:
# - Fewer premature evictions of recently-accessed items (fixed window hit recency).
# - Better resistance to workload shifts (protected can be evicted when cold).
# - More robust decisions via small, mixed candidate sets.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; duplicates allowed)
_window_q = deque()        # left = LRU, right = MRU
_protected_q = deque()     # left = LRU, right = MRU

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_f_window = 0.25
_WINDOW_MIN = 0.10
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 24              # total global candidates
_SAMPLE_SIZE_LARGE = 12        # when large over cap
_OLDEST_WINDOW_CAND = 3
_OLDEST_PROTECTED_CAND = 2
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 1.0              # stronger recency weight than before
_PROTECTED_KEEP_BONUS = 1.12  # make protected a bit stickier
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.60

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Promotion thresholds
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1           # promote large on first reuse too (no 2-hit penalty)

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # demoted key becomes MRU in window (lazy-cleaned)
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _priority_pick_best(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Scan a limited number of positions from LRU to find doomed window key
    steps = 0
    for k in list(_window_q)[:64]:
        if k in m_key_index and m_segment.get(k, 0) == 0 and k in m_doomed:
            return k
        steps += 1
    return None

def _peek_k_oldest_from_queue(q, seg, k, max_steps=256):
    res = []
    steps = 0
    for key in q:
        if len(res) >= k or steps >= max_steps:
            break
        if key in m_key_index and m_segment.get(key, 0) == seg:
            if key not in res:
                res.append(key)
        steps += 1
    return res


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (balanced):
      - Keep protected within budget via demotions.
      - Build a mixed candidate set:
         * oldest window keys (true LRU pressure),
         * oldest protected keys (allow cold protected to compete),
         * random global sample (diversity),
         * random large keys if large over-cap,
         * oldest "doomed" in window (scan shielding).
      - Pick victim by size-aware TinyLFU score with recency.
      - Admission-guard: if chosen victim is much stronger than incoming, fall back to oldest window.
      - Final fallback: oldest by timestamp from a small random set.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate collection
    candidates = set()

    # Doomed oldest in window
    dkey = _find_oldest_doomed_in_window()
    if dkey is not None:
        candidates.add(dkey)

    # Oldest LRU heads
    for k in _peek_k_oldest_from_queue(_window_q, seg=0, k=_OLDEST_WINDOW_CAND):
        candidates.add(k)
    for k in _peek_k_oldest_from_queue(_protected_q, seg=1, k=_OLDEST_PROTECTED_CAND):
        candidates.add(k)

    # Large over-cap sampling
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        for k in _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE):
            candidates.add(k)

    # Global random sample to diversify
    for k in _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(candidates))):
        candidates.add(k)

    victim_key, victim_score = _priority_pick_best(cache_snapshot, candidates, incoming_size, now)

    # Admission guard: if the chosen victim looks much stronger, evict oldest window instead
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        cands = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if cands:
            oldest = None
            oldest_ts = None
            for k in cands:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key


def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU, decay, and size stats.
      - Update last access timestamp and resident hit counter.
      - Maintain LRU: append to tail of the segment queue on every hit.
      - Promote window -> protected after first reuse (both small and large).
      - Clear "doomed" tag.
      - Adapt window fraction periodically based on hit locality.
      - Enforce protected budget via demotion when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            global _g_window_bytes
            _g_window_bytes += sz
            _window_q.append(key)
        else:
            global _g_protected_bytes
            _g_protected_bytes += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected
    _adapt_ticker += 1
    if seg == 0:
        _hits_in_window += 1
    else:
        _hits_in_protected += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" on reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Maintain LRU position on hit within its segment
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Promotion policy: promote on first reuse regardless of size
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        global _g_window_bytes, _g_protected_bytes
        _g_window_bytes = max(0, _g_window_bytes - size)
        _g_protected_bytes += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()


def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata and place into window at MRU.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    global _g_window_bytes, _g_large_bytes
    _g_window_bytes += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            _g_large_bytes += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            _g_large_bytes = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging)
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    global _g_window_bytes, _g_protected_bytes, _g_large_bytes
    if seg == 0:
        _g_window_bytes = max(0, _g_window_bytes - ev_size)
    else:
        _g_protected_bytes = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        _g_large_bytes = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 20:58:37,801 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Decayed-Frequency, Size-Aware (LRFU-Size) cache eviction algorithm
# - Combines recency and frequency via exponential decay (LRFU)
# - Accounts for object size with sublinear penalty (size^alpha)
# - Maintains compact per-resident metadata, ages lazily

import math

# Per-resident metadata
m_weight = dict()       # key -> float, decayed frequency value at last update time
m_last_ts = dict()      # key -> int, last update/access timestamp

# Tunables
_ALPHA = 0.75           # size penalty exponent (0=no size penalty, 1=fully size-aware)
_MIN_HL = 16            # min half-life in accesses
_MAX_HL = 4096          # max half-life in accesses
_FREQ_CAP = 1e6         # cap to keep numbers bounded
_EPS = 1e-12            # numeric epsilon for comparisons

_LOG_HALF = math.log(0.5)  # constant to compute decay

def _half_life(cache_snapshot):
    # Adaptive half-life: scale with number of residents, clamped
    n = max(1, len(cache_snapshot.cache))
    hl = 4 * n  # heuristic: allow items to matter for several full passes
    return max(_MIN_HL, min(_MAX_HL, hl))

def _decay_factor(delta, hl):
    # Returns 0.5^(delta/hl) = exp(log(0.5) * delta / hl)
    if delta <= 0:
        return 1.0
    return math.exp(_LOG_HALF * (float(delta) / float(hl)))

def _effective_weight(key, now_ts, cache_snapshot):
    # Lazily compute weight at current time without mutating stored base
    w = m_weight.get(key, 0.0)
    last = m_last_ts.get(key, now_ts)
    delta = now_ts - last
    if w <= 0.0 or delta <= 0:
        return max(0.0, w)
    hl = _half_life(cache_snapshot)
    return w * _decay_factor(delta, hl)

def _priority(weight_eff, size):
    # Score to minimize on eviction: smaller score => worse
    # Use sublinear size penalty; ensure positive denominator
    denom = max(1.0, float(size)) ** _ALPHA
    return weight_eff / denom

def _ensure_resident_metadata(cache_snapshot, key):
    # Initialize for resident key if missing
    if key not in m_weight:
        m_weight[key] = 0.0
    if key not in m_last_ts:
        m_last_ts[key] = 0

def evict(cache_snapshot, obj):
    '''
    Choose eviction victim:
    - Compute decayed-frequency, size-aware score for each resident: score = w_eff / size^alpha
    - Evict the item with the smallest score
    - Tie-break by oldest last update timestamp (LRU-ish)
    '''
    now_ts = cache_snapshot.access_count

    # Ensure metadata exists for resident keys
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    victim_key = None
    victim_score = None
    victim_last_ts = None

    # Scan all residents to find the minimum priority (score)
    for key, resident_obj in cache_snapshot.cache.items():
        w_eff = _effective_weight(key, now_ts, cache_snapshot)
        score = _priority(w_eff, resident_obj.size)
        last_ts = m_last_ts.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Apply lazy decay since last update, then add 1 to the weight
    - Cap the weight to keep bounds reasonable
    - Update the timestamp to now
    '''
    key = obj.key
    now_ts = cache_snapshot.access_count

    _ensure_resident_metadata(cache_snapshot, key)

    # Decay to now, then add one
    w_eff = _effective_weight(key, now_ts, cache_snapshot)
    w_new = w_eff + 1.0
    if w_new > _FREQ_CAP:
        w_new = _FREQ_CAP

    m_weight[key] = w_new
    m_last_ts[key] = now_ts

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - Initialize with weight=1 at current time (first reference)
    '''
    key = obj.key
    now_ts = cache_snapshot.access_count

    m_weight[key] = 1.0
    m_last_ts[key] = now_ts

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Remove metadata for the evicted key
    '''
    ev_key = evicted_obj.key
    m_weight.pop(ev_key, None)
    m_last_ts.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 22
- Hit Rate: 0.0574

Example 2:
- Trace ID: 33
- Hit Rate: 0.7505

Example 3:
- Trace ID: 4
- Hit Rate: 0.4691

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:58:37 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 20:58:37,803 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:01:18 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:01:18,087 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:01:18,089 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Size-aware ARC (SARC) cache eviction algorithm
# - Two resident segments: T1 (recency) and T2 (frequency)
# - Two ghost segments: B1 (recently evicted from T1) and B2 (recently evicted from T2)
# - Adaptive target p (in bytes) balances recency vs. frequency based on ghost hits
# - All segment budgets and adaptation are in bytes (size-aware)
# - Eviction chooses between T1 and T2 via ARC's REPLACE rule adapted to bytes
# - Within a segment, eviction is LRU
#
# This design improves hit rate across varied workloads by:
# - Protecting frequently reused objects (T2)
# - Retaining recency for bursty/scan-like patterns (T1)
# - Adapting between them dynamically via ghost feedback (B1/B2)
# - Respecting object sizes in budgeting and adaptation

from collections import OrderedDict

# Resident segments (LRU order: left=LRU, right=MRU)
_T1 = OrderedDict()  # key -> None
_T2 = OrderedDict()  # key -> None

# Ghost segments store only keys and their sizes (LRU order)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_T1_bytes = 0
_T2_bytes = 0
_B1_bytes = 0
_B2_bytes = 0

# Adaptive target for T1 (in bytes). T2 target implicitly = capacity - p
_p_bytes = 0

# Safety epsilon
_EPS = 1e-9

def _maybe_reset(cache_snapshot):
    # If cache was externally cleared between traces, clear metadata too
    global _T1, _T2, _B1, _B2, _T1_bytes, _T2_bytes, _B1_bytes, _B2_bytes, _p_bytes
    if len(cache_snapshot.cache) == 0 and (_T1 or _T2 or _B1 or _B2):
        _T1.clear(); _T2.clear(); _B1.clear(); _B2.clear()
        _T1_bytes = _T2_bytes = _B1_bytes = _B2_bytes = 0
        _p_bytes = 0

def _in_T1(key): return key in _T1
def _in_T2(key): return key in _T2
def _in_B1(key): return key in _B1
def _in_B2(key): return key in _B2

def _move_to_mru(od, key):
    # Move key to MRU (right end) if present
    try:
        od.move_to_end(key, last=True)
    except KeyError:
        pass

def _insert_mru(od, key, value=None):
    # Insert at MRU
    od[key] = value
    od.move_to_end(key, last=True)

def _pop_lru(od):
    # Pop LRU (left end), return (key, value) or (None, None) if empty
    if not od:
        return None, None
    return od.popitem(last=False)

def _prune_ghosts(capacity_bytes):
    # Keep total ghost bytes <= capacity (size-aware ARC bound)
    global _B1_bytes, _B2_bytes
    target = max(0, capacity_bytes)
    while (_B1_bytes + _B2_bytes) > target and (_B1 or _B2):
        # Evict from the ghost with larger byte share first (tie -> B1)
        choose_B1 = _B1_bytes >= _B2_bytes and _B1
        if choose_B1:
            k, sz = _pop_lru(_B1)
            if k is not None:
                _B1_bytes -= sz
        else:
            k, sz = _pop_lru(_B2)
            if k is not None:
                _B2_bytes -= sz

def _adapt_p_on_insert(capacity_bytes, obj_key, obj_size):
    # Adapt target p (bytes) based on ghost hits for obj_key
    global _p_bytes, _B1_bytes, _B2_bytes
    if _in_B1(obj_key):
        # Favor recency: increase p by a step proportional to size (bounded)
        step = max(obj_size, max(1, capacity_bytes // 32))
        _p_bytes = min(capacity_bytes, _p_bytes + step)
        # Remove from B1 (will re-enter T1)
        sz = _B1.pop(obj_key)
        _B1_bytes -= sz
    elif _in_B2(obj_key):
        # Favor frequency: decrease p
        step = max(obj_size, max(1, capacity_bytes // 32))
        _p_bytes = max(0, _p_bytes - step)
        # Remove from B2 (will re-enter T1)
        sz = _B2.pop(obj_key)
        _B2_bytes -= sz
    # Else: no adaptation

def _choose_replacement_list(obj_key):
    # ARC's REPLACE decision adapted to bytes
    # Evict from T1 if T1_bytes > p, or if (obj_key in B2 and T1_bytes >= p)
    # otherwise evict from T2.
    global _T1_bytes, _T2_bytes, _p_bytes
    t1_over = _T1_bytes > _p_bytes + _EPS
    t1_at_least_p = _T1_bytes >= _p_bytes - _EPS
    if t1_over or (_in_B2(obj_key) and t1_at_least_p):
        # Prefer T1 if non-empty, else fallback to T2
        if _T1:
            return 'T1'
        elif _T2:
            return 'T2'
    else:
        # Prefer T2 if non-empty, else fallback to T1
        if _T2:
            return 'T2'
        elif _T1:
            return 'T1'
    # Fallback: whichever has something, else None
    if _T1:
        return 'T1'
    if _T2:
        return 'T2'
    return None

def evict(cache_snapshot, obj):
    """
    Choose eviction victim using size-aware ARC REPLACE rule:
    - If T1_bytes > p, evict LRU from T1
    - Else if obj.key is in B2 and T1_bytes >= p, evict LRU from T1
    - Else evict LRU from T2
    Fallbacks handle empty segments gracefully.
    """
    _maybe_reset(cache_snapshot)

    # Fast path: if we have ARC metadata, use it
    segment = _choose_replacement_list(obj.key)
    if segment == 'T1' and _T1:
        victim_key, _ = next(iter(_T1.items()))
        return victim_key
    if segment == 'T2' and _T2:
        victim_key, _ = next(iter(_T2.items()))
        return victim_key

    # Metadata fallback: pick LRU by our best guess or a large item to free space faster
    # Try to pick the largest object to minimize number of evictions
    victim_key = None
    max_size = -1
    for k, resident_obj in cache_snapshot.cache.items():
        if resident_obj.size > max_size:
            victim_key = k
            max_size = resident_obj.size
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - If in T1: promote to T2 (MRU)
    - If in T2: move to MRU of T2
    - Otherwise (shouldn't happen): insert into T2 as protected frequent
    """
    _maybe_reset(cache_snapshot)
    global _T1_bytes, _T2_bytes

    k = obj.key
    sz = obj.size

    if _in_T2(k):
        _move_to_mru(_T2, k)
        return

    if _in_T1(k):
        # Promote to T2
        _T1.pop(k, None)
        _T1_bytes -= sz
        _insert_mru(_T2, k, None)
        _T2_bytes += sz
        return

    # Fallback: ensure resident goes to T2
    # This can happen if metadata was lost; recover by placing it into T2
    if k in cache_snapshot.cache:
        if _in_B1(k):
            # If it's in ghost B1 but also resident (rare), clear ghost and adapt
            _adapt_p_on_insert(cache_snapshot.capacity, k, sz)
        elif _in_B2(k):
            _adapt_p_on_insert(cache_snapshot.capacity, k, sz)
        _insert_mru(_T2, k, None)
        _T2_bytes += sz

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after bringing a miss into cache):
    - Adapt p using B1/B2 membership
    - Insert into T1 MRU (recency)
    - Keep ghost lists bounded by capacity
    """
    _maybe_reset(cache_snapshot)
    global _T1_bytes, _T2_bytes, _B1_bytes, _B2_bytes

    k = obj.key
    sz = obj.size
    cap = cache_snapshot.capacity

    # Ensure no stale duplicates across segments
    if _in_T1(k):
        _move_to_mru(_T1, k)
        return
    if _in_T2(k):
        _move_to_mru(_T2, k)
        return

    # Adapt target p if key was observed in ghost lists
    _adapt_p_on_insert(cap, k, sz)

    # Insert into T1 (recency)
    _insert_mru(_T1, k, None)
    _T1_bytes += sz

    # Bound ghost memory
    _prune_ghosts(cap)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - If victim was in T1 -> move to B1 (ghost)
    - If victim was in T2 -> move to B2 (ghost)
    - Update bytes for resident and ghost segments
    - Keep ghost lists bounded
    """
    _maybe_reset(cache_snapshot)
    global _T1_bytes, _T2_bytes, _B1_bytes, _B2_bytes

    evk = evicted_obj.key
    evsz = evicted_obj.size
    cap = cache_snapshot.capacity

    if _in_T1(evk):
        # Demote to B1
        _T1.pop(evk, None)
        _T1_bytes -= evsz
        _insert_mru(_B1, evk, evsz)
        _B1_bytes += evsz
    elif _in_T2(evk):
        # Demote to B2
        _T2.pop(evk, None)
        _T2_bytes -= evsz
        _insert_mru(_B2, evk, evsz)
        _B2_bytes += evsz
    else:
        # Not tracked (fallback): place into B1 by default
        _insert_mru(_B1, evk, evsz)
        _B1_bytes += evsz

    # Bound ghost memory
    _prune_ghosts(cap)
```
2025-11-27 21:01:33,579 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 25
- Hit Rate: 0.372

Example 2:
- Trace ID: 6
- Hit Rate: 0.2715

Example 3:
- Trace ID: 3
- Hit Rate: 0.4309

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:01:33 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:01:33,580 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:04:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:04:23,165 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:04:23,167 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Hybrid: GDSF-TinyLFU with a small recency window (2Q/SLRU-like), size-aware priorities, ghost feedback
# Key ideas:
#  - Global GreedyDual-Size with TinyLFU value: each resident key keeps a GDS priority:
#      priority = L + (value / size^alpha), where
#      L increases to the last evicted priority (aging), value uses TinyLFU freq + small recency bonus.
#  - A small recency window (window/probation vs protected) stabilizes short bursts (W-TinyLFU spirit).
#    Promotion from window to protected is size-aware: small objects promote faster.
#  - Eviction picks the lowest-priority key across sampled window and protected (bias to window via budget).
#    Protected budget enforcement demotes the lowest-priority protected items (not just oldest).
#  - Ghost lists adapt the window fraction via ARC-style feedback.
#  - Count-Min Sketch with periodic decay tracks frequencies; EMA for average size drives size penalties.

import random
import math
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert
m_gds = dict()            # key -> current GDS priority

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Window fraction (bytes), adaptive
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32
_SAMPLE_SIZE_DEMOTE = 16
_SAMPLE_SIZE_OLDEST = 24

# Scoring constants
_BASE_SIZE_ALPHA = 1.08   # base size exponent
_W_FREQ = 1.2             # weight for TinyLFU frequency component
_W_RECENCY = 0.6          # small recency bonus in value
_PROTECTED_BONUS = 0.15   # small value bonus for protected items
_EPS = 1e-9

# GDSF aging scalar (L) monotonically increases to last evicted priority
_gds_L = 0.0

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history for ARC-like feedback
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] = min(0xFFFFFFFF, _cm[d][_h(key, d)] + 1)
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty exponent by size ratio to average
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote from window)
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _gds_L
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window during bootstrap
        if key not in m_last_access:
            m_last_access[key] = max(0, now - 1)
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        # Initialize GDS priority using TinyLFU estimate
        if key not in m_gds:
            freq = _sketch_estimate(key)
            recency = 1.0 / (1.0 + float(max(0, now - m_last_access.get(key, 0))))
            seg_bonus = _PROTECTED_BONUS if m_segment.get(key, 0) == 1 else 0.0
            size_alpha = _dynamic_size_alpha(robj.size)
            val = 1.0 + _W_FREQ * math.log1p(freq) + _W_RECENCY * recency + seg_bonus
            pri = _gds_L + (val / (float(max(1, int(robj.size))) ** size_alpha))
            m_gds[key] = pri
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

# ------------------------
# GDSF priority helpers
# ------------------------
def _compute_value_component(now, key, obj):
    # TinyLFU freq + small recency bonus + protected bonus
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, now))
    recency = 1.0 / (1.0 + float(age))
    seg_bonus = _PROTECTED_BONUS if m_segment.get(key, 0) == 1 else 0.0
    return 1.0 + _W_FREQ * math.log1p(freq) + _W_RECENCY * recency + seg_bonus

def _update_gds_for_key(now, key, obj):
    size = max(1, int(obj.size))
    size_alpha = _dynamic_size_alpha(size)
    val = _compute_value_component(now, key, obj)
    m_gds[key] = _gds_L + (val / (float(size) ** size_alpha))

def _gds_for_candidate(now, key, obj):
    pri = m_gds.get(key, None)
    if pri is not None:
        return pri
    # Fallback on-the-fly if missing (should be rare)
    size = max(1, int(obj.size))
    size_alpha = _dynamic_size_alpha(size)
    val = _compute_value_component(now, key, obj)
    return _gds_L + (val / (float(size) ** size_alpha))

def _min_gds_candidate(cache_snapshot, segment, sample_k):
    now = cache_snapshot.access_count
    cand_keys = _sample_keys_from_segment(segment, sample_k)
    if not cand_keys:
        return None, None
    best_key = None
    best_pri = None
    best_ts = None
    best_size = None
    for key in cand_keys:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        pri = _gds_for_candidate(now, key, robj)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        if (best_key is None or
            pri < best_pri - _EPS or
            (abs(pri - best_pri) <= _EPS and (ts < best_ts or (ts == best_ts and sz > best_size)))):
            best_key = key
            best_pri = pri
            best_ts = ts
            best_size = sz
    return best_key, best_pri

def _demote_low_gds_from_protected(cache_snapshot):
    # Demote the lowest-priority protected items until protected fits its budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    tries = 0
    while _g_protected_bytes > protected_budget and tries < 8:
        key, pri = _min_gds_candidate(cache_snapshot, 1, max(8, _SAMPLE_SIZE_DEMOTE))
        if key is None:
            break
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            break
        if m_segment.get(key, 0) == 1:
            m_segment[key] = 0
            sz = int(robj.size)
            _g_protected_bytes = max(0, _g_protected_bytes - sz)
            _g_window_bytes += sz
        tries += 1

def _demote_one_low_gds_from_protected(cache_snapshot):
    # Demote one protected item with the lowest priority; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    key, pri = _min_gds_candidate(cache_snapshot, 1, max(8, _SAMPLE_SIZE_DEMOTE))
    if key is None:
        return False
    robj = cache_snapshot.cache.get(key, None)
    if robj is None:
        return False
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (GDSF-TinyLFU + 2Q window):
      - Maintain a window byte budget; demote low-priority protected items to respect protected budget.
      - Choose victim with the lowest GDS priority across sampled window and protected.
        Bias to window naturally via budget and sampling, but allow protected eviction if worse.
      - Tie-breakers: older last_access first; then larger size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Keep protected within budget via demotions (low-priority first)
    _demote_low_gds_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_low_gds_from_protected(cache_snapshot)

    # Select candidates
    w_key, w_pri = _min_gds_candidate(cache_snapshot, 0, _SAMPLE_SIZE)
    p_key, p_pri = _min_gds_candidate(cache_snapshot, 1, max(8, _SAMPLE_SIZE // 2))

    # Fallbacks
    if w_key is None and p_key is None:
        if m_keys:
            return m_keys[0]
        return None
    if w_key is None:
        return p_key
    if p_key is None:
        return w_key

    # Pick the lowest-priority key overall
    if p_pri + _EPS < w_pri:
        return p_key
    return w_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and EMA for avg size.
      - Update last-access timestamp and resident-hit counter.
      - Promote from window to protected using size-aware threshold.
      - Recompute GDS priority (value uses freq+recency+segment bonus).
      - Enforce protected budget via demoting lowest-priority protected items.
      - Adapt window fraction based on hit distribution.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Hit location accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access and resident-hit count
    m_last_access[key] = now
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Recompute GDS priority
    _update_gds_for_key(now, key, obj)

    # Enforce protected budget using low-priority demotions
    _demote_low_gds_from_protected(cache_snapshot)

    # Periodic window fraction adaptation
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU sketch and EMA.
      - Default placement into window; if present in protected-ghost with moderate freq and not too large,
        place into protected directly.
      - Initialize metadata and GDS priority.
      - Ghost feedback adjusts window fraction.
      - Enforce protected budget (demoting low-priority protected items).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Decide placement
    place_protected = False
    freq_est = _sketch_estimate(key)
    size_ratio = float(max(1, int(obj.size))) / max(1.0, _avg_size)
    if key in _ghost_p_set:
        # Require some reuse evidence and disallow very large jumps
        if freq_est >= 3.0 and size_ratio <= 3.0:
            place_protected = True

    m_segment[key] = 1 if place_protected else 0
    m_last_access[key] = now
    m_resfreq[key] = 1

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Initialize GDS priority
    _update_gds_for_key(now, key, obj)

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _demote_low_gds_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Update GDS aging scalar L to at least the evicted key's priority (monotonic).
      - Remove all metadata for evicted key and keep byte counters consistent.
      - Insert into appropriate ghost list for window/protected to guide future adaptation.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    # Update aging scalar
    ev_pri = m_gds.pop(ev_key, None)
    if ev_pri is not None:
        globals()['_gds_L'] = max(globals()['_gds_L'], float(ev_pri))

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 21:04:30,151 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.8937

Example 2:
- Trace ID: 43
- Hit Rate: 0.3667

Example 3:
- Trace ID: 10
- Hit Rate: 0.0917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:04:30 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:04:30,152 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:07:44 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:07:44,437 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:07:44,440 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Admission-aware, Sampled, Size-aware TinyLFU (AA-SS-TinyLFU)
# Key improvements over previous version:
# - Doorkeeper gating before Count-Min Sketch increments (reduces pollution from one-hit wonders).
# - Admission-aware eviction gating: avoid evicting strong protected items for weak incoming items.
# - Gentler size penalty (log-based) and reduced recency dominance; stronger frequency influence.
# - Promotion from window -> protected requires at least two touches in the current epoch (via doorkeeper).
# - Additional hill-climbing using "admission wins/losses" to adapt window fraction.
# - Kept sampling-based victim selection and protected demotion by approximate oldest.

import random
import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests) with a doorkeeper
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper: a small set reset on decay to gate first occurrences.
_doorkeeper = set()

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32             # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24      # candidates to demote from protected (oldest)

# Priority scoring weights
# Gentler size penalty using log2(size), stronger frequency, reduced recency.
_SIZE_ALPHA = 0.90            # exponent for size penalty
_W_FREQ = 1.0                 # weight for frequency
_W_RECENCY = 1.25             # reduced recency weight
_PROTECTED_KEEP_BONUS = 1.10  # protected items are harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Admission-aware hill climbing (based on candidate vs victim strength)
_ADMIT_ADAPT_PERIOD = 8000
_admit_wins = 0
_admit_losses = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _access_learn(key):
    # Doorkeeper gates first occurrences; only second (and later) touches in an epoch
    # are added to the Count-Min Sketch. We still tick the decay ticker either way.
    global _decay_ticker
    if key in _doorkeeper:
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    else:
        _doorkeeper.add(key)
    _decay_ticker += 1

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        # Reset doorkeeper on decay to bound it and refresh the epoch
        _doorkeeper.clear()
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _size_penalty(size_bytes):
    # Gentler size penalty: (1 + log2(size)) ** alpha
    s = max(1, int(size_bytes))
    return (1.0 + math.log2(float(s))) ** _SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size_pen = _size_penalty(obj.size)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / max(size_pen, _EPS)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_admission_score(obj):
    # Admission score for incoming object uses only frequency and size penalty
    # (no recency boost for brand new items).
    return (_W_FREQ * _sketch_estimate(obj.key)) / max(_size_penalty(obj.size), _EPS)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    # Hit-location based adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _admit_adapt_if_needed():
    # Admission-aware hill-climbing adaptation
    global _admit_wins, _admit_losses, _f_window
    total = _admit_wins + _admit_losses
    if total < _ADMIT_ADAPT_PERIOD:
        return
    if _admit_wins > _admit_losses * 1.05:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif _admit_losses > _admit_wins * 1.05:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _admit_wins = 0
    _admit_losses = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Admission-aware sampled victim selection:
      - Compute incoming admission score (TinyLFU estimate / size penalty).
      - Sample globally to estimate protected/window strengths.
      - If the window byte-usage > target OR incoming score is weaker than sampled protected min,
        restrict eviction to window keys.
      - Else sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
      - Track admission "wins/losses" for adaptive window tuning.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    window_over_budget = _g_window_bytes > target_window

    s = min(_SAMPLE_SIZE, len(m_keys))
    if s <= 0:
        return None

    # Global sample to get a sense of protected/window strengths
    global_sample = _sample_keys_from_segment(None, s)
    incoming_score = _incoming_admission_score(obj)

    # Find min-protected score in the sample
    min_prot_score = None
    for key in global_sample:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        if m_segment.get(key, 0) == 1:
            sc = _priority_for_key(cache_snapshot, key, robj, now)
            if min_prot_score is None or sc < min_prot_score:
                min_prot_score = sc

    # Admission-aware gating:
    # If incoming is clearly weaker than protected minimum, avoid evicting protected.
    restrict_to_window = window_over_budget or (min_prot_score is not None and incoming_score + _EPS < 1.05 * min_prot_score)

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, s)
        if not candidates:
            # If no window candidates (rare), fallback to global
            candidates = global_sample
    else:
        candidates = global_sample

    # Fallback: if sampling failed, scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # Guard: if nothing found
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
            victim_score = 0.0
        else:
            return None

    # Admission "win/loss" bookkeeping for hill climbing
    # If incoming is stronger than the chosen victim, treat as win, else loss.
    # Use a small margin to avoid noise.
    margin = 1.02
    if incoming_score >= margin * (victim_score if victim_score is not None else 0.0):
        globals()['_admit_wins'] += 1
    else:
        globals()['_admit_losses'] += 1
    _admit_adapt_if_needed()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Learn via doorkeeper+Count-Min sketch (with periodic aging).
      - Update last access timestamp.
      - Promote from window -> protected only if the key has been touched at least twice
        in the current epoch (i.e., already in doorkeeper).
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window, but only if we've seen it at least twice this epoch
    if seg == 0 and (key in _doorkeeper):
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Learn the access via doorkeeper+Count-Min sketch (with periodic aging).
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay + doorkeeper handle aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 21:11:11,061 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 14
- Hit Rate: 0.0215

Example 3:
- Trace ID: 47
- Hit Rate: 0.0817

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:11:11 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:11:11,063 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:14:01 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:14:01,774 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:14:01,908 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved algorithm: Byte-aware SLRU with exact LRU ordering, TinyLFU-guided victim choice,
# and ARC-style ghost feedback to adapt window/protected balance.
#
# Major changes:
# - Maintain exact LRU orders per segment (window/protected) via OrderedDict for precise oldest victim.
# - Evict primarily from window (probation). If window empty, demote oldest protected to window, then evict.
# - Within window, choose among first-K LRU candidates using a size-aware TinyLFU score; fall back to pure LRU
#   if the best candidate still looks stronger than the incoming object (pollution guard).
# - Size-aware promotion from window to protected based on resident hits and object size.
# - ARC-like ghost lists (for window and protected) adapt the window fraction in bytes.
# - Keep protected within a dynamic byte budget (capacity - target_window_bytes) by demoting oldest protected.

import random
from collections import OrderedDict, deque

# ------------------------
# Global metadata
# ------------------------
# Exact LRU orders
_window_od = OrderedDict()     # key -> None (LRU on left, MRU on right)
_protected_od = OrderedDict()  # key -> None

# Segment and sizes
m_segment = dict()   # key -> 0 (window) or 1 (protected)
m_size = dict()      # key -> int size in bytes
m_resfreq = dict()   # key -> resident hits since (re)insert

# Byte counters for segments
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Adaptive window fraction (bytes)
_f_window = 0.30
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.05

# Victim selection scanning window (among LRU head)
_VICTIM_SCAN_K = 16
# Weights/penalties
_BASE_SIZE_ALPHA = 1.06
_W_FREQ = 1.0
_ADMIT_COMPARE_FACTOR = 1.10
_EPS = 1e-9

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost lists (ARC-like)
_ghost_w = deque()    # recently evicted from window
_ghost_w_set = set()
_ghost_p = deque()    # recently evicted from protected
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# TinyLFU helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0


# ------------------------
# Misc helpers
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Place existing items (if any) into window as MRU (arbitrary order)
    total_bytes = 0
    count = 0
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        m_segment[key] = 0
        m_size[key] = size
        m_resfreq[key] = 1
        _window_od[key] = None
        _g_window_bytes += size
        total_bytes += size
        count += 1
    if count > 0:
        _update_avg_size(max(1, total_bytes // max(1, count)))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    # Target window bytes as fraction of capacity
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(_f_window * cap))

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _dynamic_size_alpha(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.12)
    else:
        return _BASE_SIZE_ALPHA

def _incoming_priority_estimate(incoming_key, incoming_size):
    # As if accessed now
    size = max(1, int(incoming_size))
    freq = _sketch_estimate(incoming_key)
    size_alpha = _dynamic_size_alpha(size)
    score = (_W_FREQ * freq + 1.0) / (float(size) ** size_alpha)
    return score

def _priority_for_resident(key, size_alpha):
    # Size-aware frequency score (no explicit recency; LRU ordering already encodes it)
    size = max(1, int(m_size.get(key, 1)))
    freq = _sketch_estimate(key)
    score = (_W_FREQ * freq + 0.25) / (float(size) ** size_alpha)
    return score

def _promotion_threshold(size):
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4

def _ensure_protected_budget(cache_snapshot):
    # Demote oldest protected items into window until protected fits its budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, int(cache_snapshot.capacity) - target_window)
    while _g_protected_bytes > protected_budget and _protected_od:
        # demote oldest protected to WINDOW LRU (to be near eviction if pressure persists)
        old_key, _ = _protected_od.popitem(last=False)
        if m_segment.get(old_key, 1) != 1:
            # Shouldn't happen, but keep consistent
            m_segment[old_key] = 0
        # move accounting
        sz = int(m_size.get(old_key, 1))
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        m_segment[old_key] = 0
        # Place as LRU in window
        _window_od[old_key] = None
        _window_od.move_to_end(old_key, last=False)

def _demote_one_from_protected_to_window_lru():
    global _g_window_bytes, _g_protected_bytes
    if not _protected_od:
        return False
    old_key, _ = _protected_od.popitem(last=False)
    sz = int(m_size.get(old_key, 1))
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    m_segment[old_key] = 0
    _window_od[old_key] = None
    _window_od.move_to_end(old_key, last=False)
    return True

def _choose_window_victim(incoming_key, incoming_size):
    # Among first-K oldest window keys, pick one with smallest TinyLFU/size score.
    if not _window_od:
        return None
    keys = []
    it = iter(_window_od.keys())
    for _ in range(_VICTIM_SCAN_K):
        try:
            keys.append(next(it))
        except StopIteration:
            break
    if not keys:
        return None
    oldest = keys[0]
    size_alpha = _dynamic_size_alpha(incoming_size)
    best_key = None
    best_score = None
    best_size = None
    for k in keys:
        sc = _priority_for_resident(k, size_alpha)
        sz = int(m_size.get(k, 1))
        if best_key is None or sc < best_score - _EPS or (abs(sc - best_score) <= _EPS and sz > best_size):
            best_key = k
            best_score = sc
            best_size = sz
    # Pollution guard: if candidate is still much stronger than incoming, use pure LRU oldest
    incoming_score = _incoming_priority_estimate(incoming_key, incoming_size)
    if best_score is not None and best_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
        return oldest
    return best_key or oldest


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
    - Keep protected within its byte budget (capacity - target_window_bytes) by demoting oldest protected.
    - Prefer evicting from window (probation). If window empty, demote one from protected to window, then evict.
    - Within window, choose among the K oldest items using a size-aware TinyLFU score; fall back to pure LRU oldest
      if the candidate appears much stronger than the incoming object.
    '''
    _bootstrap_if_needed(cache_snapshot)

    # Enforce protected budget before picking a victim
    _ensure_protected_budget(cache_snapshot)

    # Ensure we have a window victim; if empty, demote one from protected
    if not _window_od:
        _demote_one_from_protected_to_window_lru()

    # Try to pick from window
    if _window_od:
        victim = _choose_window_victim(obj.key, obj.size)
        if victim is not None:
            return victim

    # Fallbacks in rare cases
    # If window still empty, evict oldest from protected
    if _protected_od:
        k, _ = _protected_od.popitem(last=False)
        # Put it back; update_after_evict will remove for real
        _protected_od[k] = None
        _protected_od.move_to_end(k, last=False)
        return k

    # Last resort: pick any key from cache_snapshot
    for k in cache_snapshot.cache.keys():
        return k
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Update TinyLFU and average size.
    - Move to MRU in its segment; if in window, increase resident-hit counter and promote to protected on threshold.
    - Track hit locations for adaptive window sizing.
    - Keep protected within its byte budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # If metadata missing (shouldn't be), initialize into window
    if key not in m_segment:
        m_segment[key] = 0
        m_size[key] = sz
        m_resfreq[key] = 0
        _window_od[key] = None
        globals()['_g_window_bytes'] += sz

    seg = m_segment.get(key, 0)

    # Adaptive counters
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    if seg == 0:
        # Window: move to MRU and consider promotion
        if key in _window_od:
            _window_od.move_to_end(key, last=True)
        else:
            # Repair inconsistency
            _window_od[key] = None
            _window_od.move_to_end(key, last=True)
        m_resfreq[key] = m_resfreq.get(key, 0) + 1
        if m_resfreq[key] >= _promotion_threshold(sz):
            # Promote to protected MRU
            if key in _window_od:
                _window_od.pop(key, None)
                globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            m_segment[key] = 1
            _protected_od[key] = None
            _protected_od.move_to_end(key, last=True)
            globals()['_g_protected_bytes'] += sz
            # Reset resident hits for next cycle
            m_resfreq[key] = 1
    else:
        # Protected: move to MRU
        if key in _protected_od:
            _protected_od.move_to_end(key, last=True)
        else:
            # Repair inconsistency
            _protected_od[key] = None
            _protected_od.move_to_end(key, last=True)

    # Enforce budget and adapt window
    _ensure_protected_budget(cache_snapshot)
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
    - Update TinyLFU and average size.
    - Default place into window MRU; if key in protected-ghost and has reasonable frequency,
      place directly into protected MRU (size-gated).
    - Adjust window fraction via ghost feedback (ARC-like).
    - Keep protected within byte budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # Ghost feedback to adapt window fraction (ARC-style)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Decide initial placement
    place_protected = False
    freq_est = _sketch_estimate(key)
    size_ratio = float(max(1, sz)) / max(1.0, _avg_size)

    if key in _ghost_p_set:
        # Recently in protected; if moderate frequency and not extremely large, place back to protected
        if freq_est >= 2.0 and size_ratio <= 4.0:
            place_protected = True

    # If the key already (somehow) exists, remove stale placement
    if key in _window_od:
        _window_od.pop(key, None)
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - m_size.get(key, sz))
    if key in _protected_od:
        _protected_od.pop(key, None)
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - m_size.get(key, sz))

    m_segment[key] = 1 if place_protected else 0
    m_size[key] = sz
    m_resfreq[key] = 1

    if place_protected:
        _protected_od[key] = None
        _protected_od.move_to_end(key, last=True)
        globals()['_g_protected_bytes'] += sz
    else:
        _window_od[key] = None
        _window_od.move_to_end(key, last=True)
        globals()['_g_window_bytes'] += sz

    _ensure_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Remove from its segment LRU and metadata, update byte counters.
    - Add to corresponding ghost list (window or protected) with capacity-based trimming.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_sz = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, m_segment.get(ev_key, 0))
    m_resfreq.pop(ev_key, None)
    m_size.pop(ev_key, None)

    if seg == 0:
        if ev_key in _window_od:
            _window_od.pop(ev_key, None)
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_sz)
    else:
        if ev_key in _protected_od:
            _protected_od.pop(ev_key, None)
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_sz)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 21:14:25,251 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Hit Rate: 0.0235

Example 2:
- Trace ID: 1
- Hit Rate: 0.4398

Example 3:
- Trace ID: 15
- Hit Rate: 0.021

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:14:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:14:25,252 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:17:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:17:04,458 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:17:04,459 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Size-aware GDSF-TinyLFU with 2-Queue probation (SGD2Q-TLFU)
# - Eviction priority combines TinyLFU frequency, size, and recency
#   using a GDSF-like aging "clock" to prefer evicting globally cold items.
# - Two segments: window (probation) and protected (requires 2 hits).
# - Adaptive window target via simple ARC-like ghost feedback (by bytes).
# - Heaps for O(log n) eviction; lazy invalidation via per-key versioning.
# - Count-Min Sketch with periodic decay for global frequency estimates.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()    # key -> int timestamp of last access
m_ver = dict()            # key -> version (monotonic)
m_touch = dict()          # key -> touches since insert/promotion (for 2Q)
m_prio = dict()           # key -> last computed GDSF priority (for clock update)

# Heaps: entries are (priority, last_access, -size, key, ver)
_heap_global = []
_heap_window = []

# Window accounting/adaptation (bytes)
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.90

# Ghost caches (ARC-like) for adaptivity (track recently evicted keys)
_ghostW = dict()
_ghostP = dict()
_GHOST_CAP_KEYS = 16384

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Rebuild control
_last_rebuild_access = 0

# GDSF "clock" (aging)
_gd_clock = 0.0

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.50
_SIZE_ALPHA = 1.08           # size penalty exponent
_REC_WEIGHT = 3.0            # additive recency weight in numerator
_PROTECTED_MULT = 1.15       # small keep bias for protected
_SAMPLE_FALLBACK = 64
_EPS = 1e-10


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est is float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    global _CAP_SEEN
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    global m_segment, m_last_access, m_ver, m_touch, m_prio
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access, _gd_clock

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    m_touch.clear()
    m_prio.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = int(cache_snapshot.capacity)
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count
    _gd_clock = 0.0

    # reset sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache preloaded: place everything in window with ts=0
    now = cache_snapshot.access_count
    for k, obj in cache_snapshot.cache.items():
        m_segment[k] = 0
        m_last_access[k] = 0
        m_ver[k] = 0
        m_touch[k] = 0
        _window_bytes += int(obj.size)
        _window_count += 1
        _push_entry(k, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_base(key, obj, now):
    # Base desirability (higher is better): (freq + rec_weight*rec) / size^alpha
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    rec = 1.0 / (1.0 + float(age))  # ~[0,1]
    base = (freq + (_REC_WEIGHT * rec)) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        base *= _PROTECTED_MULT
    return base

def _push_entry(key, obj, now):
    # Compute GDSF-like priority using current clock and store it.
    ver = m_ver.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    pr = _gd_clock + _priority_base(key, obj, now)
    m_prio[key] = pr
    entry = (pr, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    pr, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Rebuild if heaps overgrow to reduce lazy-invalidated clutter
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    if now - _last_rebuild_access < 25000:
        return
    if len(_heap_global) <= (8 * max(1, n_res) + 2048):
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for k, obj in cache_snapshot.cache.items():
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        if k not in m_touch:
            m_touch[k] = 0
        _push_entry(k, obj, now)
        if m_segment.get(k, 0) == 0:
            _window_bytes += int(obj.size)
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    if key in ghost:
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-inspired: if key appears in window-ghost, enlarge window; in protected-ghost, shrink
    cap = int(cache_snapshot.capacity)
    k = obj.key
    step = max(int(obj.size), cap // 100)  # >=1% of cap or object's size
    if k in _ghostW:
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)

# Window accounting helpers
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy:
      - If window (probation) exceeds its adaptive target (bytes), evict from window first.
      - Otherwise, evict the globally lowest-priority key (GDSF-TinyLFU).
      - Priority = gd_clock + (est_freq + rec_weight*recency) / size^alpha, larger is better.
      - Fallback: random sampling among 64 keys (biased to window if restricting).
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Budget enforcement for window
    restrict_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_window:
        vk = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if vk is not None:
            return vk
        # fallback to global but must be window resident
        vk = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if vk is not None:
            return vk
    else:
        vk = _pop_valid_from_heap(cache_snapshot, _heap_global, None)
        if vk is not None:
            return vk

    # Fallback: sample a subset; prefer window keys if restriction requested
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    if restrict_window:
        wkeys = [k for k in keys if m_segment.get(k, 0) == 0]
        if wkeys:
            keys = wkeys
    sample_sz = min(_SAMPLE_FALLBACK, len(keys))
    samples = random.sample(keys, sample_sz)

    worst_k = None
    worst_pr = float('inf')
    for k in samples:
        o = cache_snapshot.cache[k]
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        # compute a fresh priority and seed heap for future calls
        pr = _gd_clock + _priority_base(k, o, now)
        m_prio[k] = pr
        entry = (pr, m_last_access.get(k, 0), -int(o.size), k, m_ver[k])
        heappush(_heap_global, entry)
        if m_segment.get(k, 0) == 0:
            heappush(_heap_window, entry)
        if pr < worst_pr - _EPS:
            worst_pr = pr
            worst_k = k

    return worst_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU and decay if needed.
      - Update last access timestamp.
      - 2Q promotion: promote window -> protected on 2nd touch since insert.
      - Version bump and push new heap entries (recomputes priority against current clock).
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0
    if key not in m_touch:
        m_touch[key] = 0

    # Update last access and touch count
    m_last_access[key] = now
    m_touch[key] = min(3, m_touch.get(key, 0) + 1)  # small cap

    # Promote to protected after second touch (reduces single-hit pollution)
    if m_segment.get(key, 0) == 0 and m_touch[key] >= 2:
        m_segment[key] = 1
        _decr_window(obj.size)

    # Refresh heap entries via version bump
    m_ver[key] += 1
    _push_entry(key, obj, now)

    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after miss):
      - Adjust window target via ghosts (ARC-like).
      - Update TinyLFU and decay if needed.
      - Initialize metadata: segment=window, last_access=now, ver++, touch=0.
      - Account into window and push heap entries.
    """
    _ensure_init(cache_snapshot)

    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = 0            # probation
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1
    m_touch[key] = 0

    _incr_window(obj.size)
    _push_entry(key, obj, now)

    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Update GDSF clock to the evicted entry's last priority.
      - Add evicted key to appropriate ghost for window adaptation.
      - Adjust window accounting and purge metadata.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Advance clock to (at least) the evicted priority
    global _gd_clock
    ev_pr = m_prio.get(ev_key, None)
    if ev_pr is not None:
        _gd_clock = max(_gd_clock, float(ev_pr))

    # ARC-style ghost recording for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
        _decr_window(evicted_obj.size)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Purge metadata (heaps will lazily drop stale entries)
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)
    m_touch.pop(ev_key, None)
    m_prio.pop(ev_key, None)
```
2025-11-27 21:19:57,808 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Admission-aware, Sampled, Size-aware TinyLFU (AA-SS-TinyLFU)
# Key improvements over previous version:
# - Doorkeeper gating before Count-Min Sketch increments (reduces pollution from one-hit wonders).
# - Admission-aware eviction gating: avoid evicting strong protected items for weak incoming items.
# - Gentler size penalty (log-based) and reduced recency dominance; stronger frequency influence.
# - Promotion from window -> protected requires at least two touches in the current epoch (via doorkeeper).
# - Additional hill-climbing using "admission wins/losses" to adapt window fraction.
# - Kept sampling-based victim selection and protected demotion by approximate oldest.

import random
import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests) with a doorkeeper
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper: a small set reset on decay to gate first occurrences.
_doorkeeper = set()

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32             # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24      # candidates to demote from protected (oldest)

# Priority scoring weights
# Gentler size penalty using log2(size), stronger frequency, reduced recency.
_SIZE_ALPHA = 0.90            # exponent for size penalty
_W_FREQ = 1.0                 # weight for frequency
_W_RECENCY = 1.25             # reduced recency weight
_PROTECTED_KEEP_BONUS = 1.10  # protected items are harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Admission-aware hill climbing (based on candidate vs victim strength)
_ADMIT_ADAPT_PERIOD = 8000
_admit_wins = 0
_admit_losses = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _access_learn(key):
    # Doorkeeper gates first occurrences; only second (and later) touches in an epoch
    # are added to the Count-Min Sketch. We still tick the decay ticker either way.
    global _decay_ticker
    if key in _doorkeeper:
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    else:
        _doorkeeper.add(key)
    _decay_ticker += 1

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        # Reset doorkeeper on decay to bound it and refresh the epoch
        _doorkeeper.clear()
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _size_penalty(size_bytes):
    # Gentler size penalty: (1 + log2(size)) ** alpha
    s = max(1, int(size_bytes))
    return (1.0 + math.log2(float(s))) ** _SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size_pen = _size_penalty(obj.size)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / max(size_pen, _EPS)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_admission_score(obj):
    # Admission score for incoming object uses only frequency and size penalty
    # (no recency boost for brand new items).
    return (_W_FREQ * _sketch_estimate(obj.key)) / max(_size_penalty(obj.size), _EPS)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    # Hit-location based adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _admit_adapt_if_needed():
    # Admission-aware hill-climbing adaptation
    global _admit_wins, _admit_losses, _f_window
    total = _admit_wins + _admit_losses
    if total < _ADMIT_ADAPT_PERIOD:
        return
    if _admit_wins > _admit_losses * 1.05:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif _admit_losses > _admit_wins * 1.05:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _admit_wins = 0
    _admit_losses = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Admission-aware sampled victim selection:
      - Compute incoming admission score (TinyLFU estimate / size penalty).
      - Sample globally to estimate protected/window strengths.
      - If the window byte-usage > target OR incoming score is weaker than sampled protected min,
        restrict eviction to window keys.
      - Else sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
      - Track admission "wins/losses" for adaptive window tuning.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    window_over_budget = _g_window_bytes > target_window

    s = min(_SAMPLE_SIZE, len(m_keys))
    if s <= 0:
        return None

    # Global sample to get a sense of protected/window strengths
    global_sample = _sample_keys_from_segment(None, s)
    incoming_score = _incoming_admission_score(obj)

    # Find min-protected score in the sample
    min_prot_score = None
    for key in global_sample:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        if m_segment.get(key, 0) == 1:
            sc = _priority_for_key(cache_snapshot, key, robj, now)
            if min_prot_score is None or sc < min_prot_score:
                min_prot_score = sc

    # Admission-aware gating:
    # If incoming is clearly weaker than protected minimum, avoid evicting protected.
    restrict_to_window = window_over_budget or (min_prot_score is not None and incoming_score + _EPS < 1.05 * min_prot_score)

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, s)
        if not candidates:
            # If no window candidates (rare), fallback to global
            candidates = global_sample
    else:
        candidates = global_sample

    # Fallback: if sampling failed, scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # Guard: if nothing found
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
            victim_score = 0.0
        else:
            return None

    # Admission "win/loss" bookkeeping for hill climbing
    # If incoming is stronger than the chosen victim, treat as win, else loss.
    # Use a small margin to avoid noise.
    margin = 1.02
    if incoming_score >= margin * (victim_score if victim_score is not None else 0.0):
        globals()['_admit_wins'] += 1
    else:
        globals()['_admit_losses'] += 1
    _admit_adapt_if_needed()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Learn via doorkeeper+Count-Min sketch (with periodic aging).
      - Update last access timestamp.
      - Promote from window -> protected only if the key has been touched at least twice
        in the current epoch (i.e., already in doorkeeper).
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window, but only if we've seen it at least twice this epoch
    if seg == 0 and (key in _doorkeeper):
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Learn the access via doorkeeper+Count-Min sketch (with periodic aging).
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay + doorkeeper handle aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 12
- Hit Rate: 0.054

Example 2:
- Trace ID: 21
- Hit Rate: 0.0266

Example 3:
- Trace ID: 40
- Hit Rate: 0.0635

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:19:57 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:19:57,809 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:21:37 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:21:37,663 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:21:37,686 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Count-aware, Doorkeeper-gated, Sampled W-TinyLFU (CAS-W-TinyLFU)
# Key changes vs previous version:
# - Treat capacity as number of objects (not bytes). Use object counts for budgeting and adaptation.
# - Remove size penalty and byte accounting. obj.size is ignored for admission/eviction because capacity is object-count based.
# - Keep TinyLFU with doorkeeper and periodic decay for robust frequency estimation.
# - Two-segment design (window/protected) with promotion after 2+ touches in current epoch.
# - Admission-aware eviction: avoid evicting protected items when the incoming is weaker than protected minima.
# - Sampled victim selection using a frequency+recency keep-score; protected gets a keep bonus.
# - Window fraction adaptive based on where hits occur and admission wins/losses.
# - Approximate oldest protected demotion via sampling when protected exceeds budget.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) sampling)
m_key_index = dict()      # key -> index in m_keys

# Object-count counters for segments
_g_window_cnt = 0
_g_protected_cnt = 0

# TinyLFU Count-Min Sketch with doorkeeper gating
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two for fast masking
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper: a small set reset on decay to gate first occurrences.
_doorkeeper = set()

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables: adaptive window fraction (by object count)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32             # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24      # candidates to demote from protected (approx oldest)

# Keep-score weights (higher score => stronger to keep)
# Frequency dominates; recency is supportive.
_W_FREQ = 2.0
_W_RECENCY = 0.6
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 8000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Admission-aware hill climbing (based on incoming vs victim strength)
_ADMIT_ADAPT_PERIOD = 6000
_admit_wins = 0
_admit_losses = 0

# Bootstrap
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _access_learn(key):
    # Doorkeeper gates first occurrences; only second (and later) touches in an epoch
    # are added to the Count-Min Sketch. We still tick the decay ticker either way.
    global _decay_ticker
    if key in _doorkeeper:
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    else:
        _doorkeeper.add(key)
    _decay_ticker += 1

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        # Reset doorkeeper on decay to bound it and refresh the epoch
        _doorkeeper.clear()
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_cnt, _g_protected_cnt
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count objects by segment
        if m_segment.get(key, 0) == 0:
            _g_window_cnt += 1
        else:
            _g_protected_cnt += 1
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, now):
    # Keep-score: frequency + recency boost; protected gets a keep bonus.
    # Higher score => better to keep. Victim is argmin(score).
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = _W_FREQ * freq + _W_RECENCY * recency
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_admission_score(obj):
    # Only frequency matters for admission when capacity is object-count based.
    return _W_FREQ * _sketch_estimate(obj.key)

def _current_window_target_count(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(round(_f_window * cap)))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its object-count budget, demote the oldest (approx via sampling).
    global _g_window_cnt, _g_protected_cnt
    target_window = _current_window_target_count(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_cnt <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_cnt -= 1
        _g_window_cnt += 1

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    if idx != last_idx:
        last_key = m_keys[last_idx]
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    # Hit-location based adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _admit_adapt_if_needed():
    # Admission-aware hill-climbing adaptation
    global _admit_wins, _admit_losses, _f_window
    total = _admit_wins + _admit_losses
    if total < _ADMIT_ADAPT_PERIOD:
        return
    if _admit_wins > _admit_losses * 1.05:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif _admit_losses > _admit_wins * 1.05:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _admit_wins = 0
    _admit_losses = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Admission-aware sampled victim selection (object-count capacity):
      - Compute incoming admission score from TinyLFU estimate (no size penalty).
      - Global sample to estimate protected/window strengths.
      - If window object-count > target OR incoming score is weaker than protected minimum,
        restrict eviction to window keys.
      - Else sample globally and evict the min keep-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by random.
      - Track admission "wins/losses" for adaptive window tuning.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_count(cache_snapshot)
    window_over_budget = _g_window_cnt > target_window

    s = min(_SAMPLE_SIZE, len(m_keys))
    if s <= 0:
        return None

    # Global sample to get a sense of protected/window strengths
    global_sample = _sample_keys_from_segment(None, s)
    incoming_score = _incoming_admission_score(obj)

    # Find min-protected keep score in the sample
    min_prot_score = None
    for key in global_sample:
        if m_segment.get(key, 0) != 1:
            continue
        sc = _priority_for_key(cache_snapshot, key, now)
        if min_prot_score is None or sc < min_prot_score:
            min_prot_score = sc

    # Admission-aware gating:
    # If incoming is clearly weaker than protected minimum, avoid evicting protected.
    restrict_to_window = window_over_budget or (min_prot_score is not None and incoming_score + _EPS < 1.05 * min_prot_score)

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, s)
        if not candidates:
            candidates = global_sample
    else:
        candidates = global_sample

    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates (min keep-score; tie-break by oldest)
    victim_key = None
    victim_score = None
    victim_last_ts = None

    for key in candidates:
        score = _priority_for_key(cache_snapshot, key, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts

    if victim_key is None:
        return None

    # Admission "win/loss" bookkeeping for hill climbing
    margin = 1.02
    if incoming_score >= margin * (victim_score if victim_score is not None else 0.0):
        globals()['_admit_wins'] += 1
    else:
        globals()['_admit_losses'] += 1
    _admit_adapt_if_needed()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Learn via doorkeeper+Count-Min sketch (with periodic aging).
      - Update last access timestamp.
      - Promote from window -> protected only if the key has been touched at least twice
        in the current epoch (i.e., already present in doorkeeper).
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds its object-count budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_cnt'] += 1
        else:
            globals()['_g_protected_cnt'] += 1

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window and seen at least twice this epoch
    if seg == 0 and (key in _doorkeeper):
        m_segment[key] = 1
        globals()['_g_window_cnt'] -= 1
        globals()['_g_protected_cnt'] += 1
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Learn the access via doorkeeper+Count-Min sketch (with periodic aging).
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and object-count accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_cnt'] += 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update object-count counters and sampling structures.
      - Count-Min sketch persists (natural decay + doorkeeper handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_cnt'] -= 1
    else:
        globals()['_g_protected_cnt'] -= 1
```
2025-11-27 21:22:04,224 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 37
- Hit Rate: 0.6425

Example 2:
- Trace ID: 31
- Hit Rate: 0.007

Example 3:
- Trace ID: 7
- Hit Rate: 0.4581

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:22:04 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:22:04,226 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:24:24 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:24:24,443 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:24:24,445 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware TinyLFU with ARC-style window tuning (A-WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic decay.
# - Two in-cache segments: "window" (recency/probation) and "protected" (frequency).
# - Promotion requires two resident hits (to resist 1-hit scans).
# - Adaptive target for window bytes via ARC-like ghost feedback (B1/B2).
# - Eviction:
#     1) If window bytes exceed target, evict the minimum-score key from window.
#     2) Else, evict the global minimum-score key (protected has a slight bonus).
# - Score balances estimated frequency, recency, and size; cold window entries are penalized.
# - Per-resident metadata stays compact: segment id, last access timestamp, resident hit-count.

from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()          # key -> 0 (window) or 1 (protected)
m_last_access = dict()      # key -> int (timestamp)
m_resident_hits = dict()    # key -> small int (hits since insertion/promotion)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_SIZE_ALPHA = 1.0  # priority divisor exponent: size**alpha
# Recency impact differs by segment: window favors recency more than protected
_RECENCY_LAMBDA_WINDOW = 4.0
_RECENCY_LAMBDA_PROTECTED = 1.25
# Protected items get a modest multiplicative bonus
_PROTECTED_KEEP_BONUS = 1.25
# Cold window items (0 resident hits) are penalized to resist scan pollution
_COLD_PENALTY = 0.5
_EPS = 1e-9

# Adaptive window target (in bytes), guided by ARC-like ghosts
_target_window_bytes = None
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.80
# Ghost lists (OrderedDict for LRU): key -> (ts, size)
_B1 = OrderedDict()   # Ghost of window (recent/probation) evictions
_B2 = OrderedDict()   # Ghost of protected (frequent) evictions
_ghost_bytes_B1 = 0
_ghost_bytes_B2 = 0

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0  # start in window
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_resident_hits:
        m_resident_hits[key] = 0

def _clamp_target_window(capacity):
    global _target_window_bytes
    if _target_window_bytes is None:
        _target_window_bytes = int(max(1, int(0.20 * capacity)))
    lo = int(max(1, _MIN_WINDOW_FRAC * capacity))
    hi = int(max(1, _MAX_WINDOW_FRAC * capacity))
    if _target_window_bytes < lo:
        _target_window_bytes = lo
    elif _target_window_bytes > hi:
        _target_window_bytes = hi

def _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1):
    # ARC-like adaptation: on a miss that hits B1, increase window target;
    # on a miss that hits B2, decrease window target.
    global _target_window_bytes
    capacity = cache_snapshot.capacity
    _clamp_target_window(capacity)

    b1c = max(1, len(_B1))
    b2c = max(1, len(_B2))
    ratio = (b2c / b1c) if hit_in_B1 else (b1c / b2c)
    # Base step ~3.125% of capacity, scaled up to 4x by the ratio.
    base_step = max(1, capacity // 32)
    delta = int(base_step * min(4.0, ratio))
    if hit_in_B1:
        _target_window_bytes = min(int(_MAX_WINDOW_FRAC * capacity), _target_window_bytes + max(1, delta))
    else:
        _target_window_bytes = max(int(_MIN_WINDOW_FRAC * capacity), _target_window_bytes - max(1, delta))

def _ghost_add(cache_snapshot, key, size, now, to_B1):
    global _ghost_bytes_B1, _ghost_bytes_B2
    if to_B1:
        # Insert/move to MRU
        if key in _B1:
            old_ts, old_sz = _B1.pop(key)
            _ghost_bytes_B1 -= old_sz
        _B1[key] = (now, size)
        _ghost_bytes_B1 += size
    else:
        if key in _B2:
            old_ts, old_sz = _B2.pop(key)
            _ghost_bytes_B2 -= old_sz
        _B2[key] = (now, size)
        _ghost_bytes_B2 += size
    _trim_ghosts(cache_snapshot)

def _ghost_remove_if_present(key):
    global _ghost_bytes_B1, _ghost_bytes_B2
    removed_from = None
    if key in _B1:
        ts, sz = _B1.pop(key)
        _ghost_bytes_B1 -= sz
        removed_from = 'B1'
    elif key in _B2:
        ts, sz = _B2.pop(key)
        _ghost_bytes_B2 -= sz
        removed_from = 'B2'
    return removed_from

def _trim_ghosts(cache_snapshot):
    global _ghost_bytes_B1, _ghost_bytes_B2
    capacity = cache_snapshot.capacity
    limit = max(1, 2 * capacity)  # allow ghosts up to 2x capacity in bytes
    total = _ghost_bytes_B1 + _ghost_bytes_B2
    while total > limit and (_B1 or _B2):
        # Pop the oldest among the heads of B1 and B2
        k1, v1 = (next(iter(_B1.items())) if _B1 else (None, None))
        k2, v2 = (next(iter(_B2.items())) if _B2 else (None, None))
        if k1 is None and k2 is None:
            break
        if k2 is None or (k1 is not None and v1[0] <= v2[0]):
            # pop oldest from B1
            key, (ts, sz) = _B1.popitem(last=False)
            _ghost_bytes_B1 -= sz
            total -= sz
        else:
            key, (ts, sz) = _B2.popitem(last=False)
            _ghost_bytes_B2 -= sz
            total -= sz

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = 1.0 + _sketch_estimate(key)  # +1 smooths zero-case
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # in (0,1]
    seg = m_segment.get(key, 0)

    if seg == 0:
        mult = 1.0 + (_RECENCY_LAMBDA_WINDOW * recency)
        score = (freq * mult) / (float(size) ** _SIZE_ALPHA)
        # Cold window entries (0 resident hits) are easier to evict
        if m_resident_hits.get(key, 0) <= 0:
            score *= _COLD_PENALTY
    else:
        mult = 1.0 + (_RECENCY_LAMBDA_PROTECTED * recency)
        score = (freq * mult) / (float(size) ** _SIZE_ALPHA)
        # Protected bonus keeps established frequent items longer
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > adaptive target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected has a modest bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count
    capacity = cache_snapshot.capacity
    _clamp_target_window(capacity)

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window byte usage
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, _target_window_bytes)
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # only window candidates when window over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay periodically.
      - Update last access timestamp and resident hit count.
      - Promote from window -> protected after 2 resident hits (resists 1-hit scans).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Count resident hits
    m_resident_hits[key] = m_resident_hits.get(key, 0) + 1

    # Promote to protected after two hits while resident in window
    if m_segment.get(key, 0) == 0 and m_resident_hits[key] >= 2:
        m_segment[key] = 1  # protected
        # Reset hit counter after promotion to avoid runaway counts
        m_resident_hits[key] = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay periodically.
      - ARC-like adaptation: if key is in a ghost list, adjust window target accordingly.
      - Initialize metadata: segment=window, last_access=now, resident_hits=0.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    _clamp_target_window(cache_snapshot.capacity)

    key = obj.key
    # ARC-style feedback via ghosts
    removed_from = _ghost_remove_if_present(key)
    if removed_from == 'B1':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=True)
    elif removed_from == 'B2':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=False)

    # Initialize resident metadata
    m_segment[key] = 0  # window/probation
    m_last_access[key] = cache_snapshot.access_count
    m_resident_hits[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (B1 for window, B2 for protected).
      - Remove resident metadata for the evicted key.
      - Count-Min sketch persists globally (natural decay ages it).
    '''
    now = cache_snapshot.access_count
    ev_key = evicted_obj.key
    ev_seg = m_segment.get(ev_key, 0)
    size = max(1, int(evicted_obj.size))

    # Add to ghost list for ARC-like adaptation
    _ghost_add(cache_snapshot, ev_key, size, now, to_B1=(ev_seg == 0))

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_resident_hits.pop(ev_key, None)
```
2025-11-27 21:31:02,280 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding and Large-object guard (WSD-TLFU+SL)
# - Global admission/priority: Count-Min Sketch (TinyLFU) with periodic decay.
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Doorkeeper promotion: require 1 hit for normal-sized objects and 2 hits for large ones.
# - Scan-shielding "doomed" set: very weak insertions are tagged and preferentially evicted first.
# - Large-object guard: cap fraction of bytes held by large objects; evict large ones first if over cap.
# - Sampling-based eviction, size-aware scoring tuned to incoming size.
# - Adaptive window fraction via hit location and ARC-style ghost feedback.
# - Lightweight age-based protected demotion to avoid stale protection.
# - Maintain compact resident index for O(1) uniform sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 48            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 16     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 24     # from "doomed" set
_SAMPLE_SIZE_LARGE = 24      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.5            # reduced recency weight to resist scans
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
_LARGE_BYTES_FRACTION = 0.45     # large objects should not exceed 45% of capacity

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    # Only consider age-demotion if protected is not under pressure (handled elsewhere) but protection might be stale.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) If any "doomed" items exist, evict the weakest among a random sample from them.
      3) Else if window is above its target bytes, pick from window.
      4) Else if large objects exceed their byte cap, pick from large objects preferentially.
      5) Else sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict oldest window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Doorkeeper promotion: promote window->protected only after sufficient resident hits (size-aware).
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Default to window accounting if unknown
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse (so it won't be evicted prematurely)
    is_large = key in m_large_keys
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require 1 hit for small, 2 hits for large
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is far weaker than resident sample, tag as "doomed" (evict-first).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic
    # Compare incoming priority vs a sample of current residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    min_resident_score = None
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        if min_resident_score is None or s < min_resident_score:
            min_resident_score = s
    # Tag as doomed if clearly weaker than typical residents, or if large and very low freq
    est_freq = _sketch_estimate(key)
    if (min_resident_score is not None and incoming_score < 0.9 * min_resident_score) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.0635

Example 2:
- Trace ID: 27
- Hit Rate: 0.8923

Example 3:
- Trace ID: 24
- Error: Error - Timeout

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:31:02 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:31:02,282 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:34:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:34:33,380 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:34:33,381 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```
2025-11-27 21:39:22,742 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Error: Error - Timeout

Example 2:
- Trace ID: 17
- Hit Rate: 0.0238

Example 3:
- Trace ID: 23
- Hit Rate: 0.0593

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:39:22 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:39:22,743 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:42:19 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:42:19,792 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:42:19,793 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight SLRU + TinyLFU with sampling (LT-SLRU-TLFU)
# Goals:
# - Stable hit rate with low CPU: two segments (window/protected) and TinyLFU admission
# - Size-aware scoring and eviction via small random samples
# - Simple adaptive window fraction driven by where hits occur
# - Avoid heavy sets/ghosts/large-tracking to eliminate timeouts

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # keep small to avoid CPU spikes
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_S_WIN_EVICT = 10          # candidates to pick eviction from window
_S_PROT_DEMOTE = 8         # candidates when demoting from protected
_S_GLOBAL = 12             # global candidates if needed

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.2
_PROTECTED_KEEP_BONUS = 1.10
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.06

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger (frees space faster)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy (lightweight):
      1) Keep protected within budget via single demotion if needed.
      2) Prefer eviction from window (probationary) using sampling + size-aware score.
      3) Fall back to global sampling if window is undersized.
      4) Admission-compare guard: if incoming weaker than victim, evict the oldest window key to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected budget: demote one protected if over budget
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Collect candidates: prefer window segment
    if _g_window_bytes > target_window:
        candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    else:
        # If window is small, sample globally
        candidates = _sample_keys_from_segment(None, _S_GLOBAL)

    if not candidates:
        # Fallback: try oldest window; else any key
        alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, max(1, _S_WIN_EVICT // 2)))
        if alt is not None:
            return alt
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-compare guard: if incoming is weaker, prefer evicting an old window key
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, _S_WIN_EVICT))
            if alt is not None:
                victim_key = alt

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size.
      - Update last access timestamp.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget with a single demotion if needed.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure key is indexed and in metadata
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Promote on first hit (simple SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within budget by demoting one if necessary
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust bytes accounting based on segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 21:45:34,520 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 26
- Hit Rate: 0.7895

Example 2:
- Trace ID: 1
- Hit Rate: 0.436

Example 3:
- Trace ID: 7
- Hit Rate: 0.4333

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:45:34 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:45:34,523 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:48:48 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:48:48,482 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:48:48,503 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Windowed, Size-aware, Predictive TinyLFU with Segmented-LRU (WSP-TinyLFU)
# - Global admission/priority via Count-Min Sketch with periodic aging.
# - 3 in-cache segments (by bytes): window (recency), probation (main), protected (main).
# - ARC-like ghost feedback to adapt the window byte target.
# - Promotion on hits: window/probation -> protected; protected hits refresh to MRU.
# - Eviction favors probation; window is evicted only if above target or clearly weakest.
# - Size-aware value score with EWMA inter-arrival prediction and recency probability.
#
# Notes:
# - Uses OrderedDicts for O(1) LRU operations.
# - No global heaps; selection uses small LRU-biased sampling for robustness and speed.
# - All operations are lazy and compact; metadata is per-resident with small per-key maps.

from collections import OrderedDict
import math
import random

# ------------------------
# Global metadata
# ------------------------
# Segment identifiers
_SEG_WINDOW = 0
_SEG_PROTECTED = 1
_SEG_PROBATION = 2

# Residency + metadata
m_segment = dict()          # key -> segment id
m_last_access = dict()      # key -> last access timestamp
m_gap_ewma = dict()         # key -> EWMA of inter-arrival gaps
m_hits = dict()             # key -> hit counter (since insert)
# Ordered LRU queues (MRU at right, LRU at left)
_qW = OrderedDict()         # window
_qP1 = OrderedDict()        # protected
_qP0 = OrderedDict()        # probation

# Bytes accounting
_window_bytes = 0
_protected_bytes = 0
_probation_bytes = 0

# Window target and capacity tracking
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired adaptivity)
_ghostW = dict()  # recently evicted from window
_ghostP = dict()  # recently evicted from main (probation+protected)
_GHOST_CAP_KEYS = 16384

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.08                 # size penalty exponent
_PROTECTED_KEEP_BONUS = 1.6        # harder to evict protected
_HIT_BONUS = 1.5                   # extra value for per-key hit count
_PRED_GAMMA = 0.25                 # EWMA step for inter-arrival
_BASE_GAP = 16.0                   # base time constant for p(next)
_WINDOW_EVICT_RELUCTANCE = 0.92    # require window candidate be this fraction or less of main-candidate score to evict when not over budget
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _ghost_admit(ghost, key):
    if key in ghost:
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        ghost.pop(next(iter(ghost)))

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like: adjust window target when a miss hits ghosts
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% cap or object size
    if k in _ghostW:
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)

def _ensure_init(cache_snapshot):
    global _CAP_SEEN, _target_window_bytes
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    global m_segment, m_last_access, m_gap_ewma, m_hits
    global _qW, _qP1, _qP0
    global _window_bytes, _protected_bytes, _probation_bytes
    global _CAP_SEEN, _target_window_bytes
    global _ghostW, _ghostP, _cm, _decay_ticker

    m_segment.clear()
    m_last_access.clear()
    m_gap_ewma.clear()
    m_hits.clear()
    _qW.clear()
    _qP1.clear()
    _qP0.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _protected_bytes = 0
    _probation_bytes = 0

    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache preloaded: place all in window initially
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = _SEG_WINDOW
        m_last_access[key] = 0
        m_gap_ewma[key] = _BASE_GAP
        m_hits[key] = 0
        _qW[key] = None
        _window_bytes += int(obj.size)

    # Rebalance window into probation given target
    _rebalance_segments(cache_snapshot.capacity)

def _rebalance_segments(capacity):
    # Move oldest from window to probation to meet window target (no eviction)
    global _window_bytes, _probation_bytes
    while _window_bytes > _target_window_bytes and _qW:
        k, _ = _qW.popitem(last=False)  # LRU of window
        m_segment[k] = _SEG_PROBATION
        _qP0[k] = None
        _window_bytes -= 0  # corrected via size fetch
        # correct bytes with actual size
        sz = int(_get_size(k))
        _window_bytes = max(0, _window_bytes - sz)
        _probation_bytes += sz

    _enforce_protected_budget(capacity)

def _enforce_protected_budget(capacity):
    # Keep protected bytes under a fraction of main region (cap - window_target)
    global _protected_bytes, _probation_bytes
    main_cap = max(0, capacity - _target_window_bytes)
    prot_cap = max(0, int(0.80 * main_cap))
    while _protected_bytes > prot_cap and _qP1:
        # Demote protected LRU to probation MRU (no eviction)
        k, _ = _qP1.popitem(last=False)
        m_segment[k] = _SEG_PROBATION
        _qP0[k] = None
        sz = int(_get_size(k))
        _protected_bytes = max(0, _protected_bytes - sz)
        _probation_bytes += sz

def _get_size(key):
    # Safe size lookup; returns 0 if missing
    # Note: the caller usually operates under current snapshot where key is resident.
    return _last_known_sizes.get(key, 0)

# Maintain a last-known size map to avoid repeated lookups during rebalancing
_last_known_sizes = dict()

def _remember_size(key, size):
    _last_known_sizes[key] = int(size)

def _remove_size(key):
    _last_known_sizes.pop(key, None)

def _value_score(cache_snapshot, key, obj, now):
    # Higher score => more valuable to keep; eviction chooses minimal score.
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, now))
    gap = float(m_gap_ewma.get(key, _BASE_GAP))
    # Predicted next-hit probability; bounded and smooth
    denom = max(1.0, gap + _BASE_GAP)
    p_next = math.exp(-float(age) / denom)
    base = freq + (_HIT_BONUS * float(m_hits.get(key, 0)))
    seg_mult = _PROTECTED_KEEP_BONUS if m_segment.get(key, _SEG_WINDOW) == _SEG_PROTECTED else 1.0
    value = (base * (1.0 + p_next)) * seg_mult  # 1..2x based on p_next
    score = value / (float(size) ** _SIZE_ALPHA)
    return score

def _lru_sample_keys(od, k):
    if not od:
        return []
    # Take k oldest keys (LRU-biased), plus a couple of random picks among the older half if available
    keys = []
    it = iter(od)
    n = len(od)
    take = min(k, n)
    for _ in range(take):
        keys.append(next(it))
    # Add up to 2 random keys from the older half to avoid pathological ties
    extra = min(2, max(0, n // 2))
    if extra > 0:
        older_half = list(od.keys())[:max(1, n // 2)]
        keys.extend(random.sample(older_half, min(extra, len(older_half))))
    # Deduplicate while preserving order
    seen = set()
    out = []
    for x in keys:
        if x not in seen:
            seen.add(x)
            out.append(x)
    return out

def _select_victim_from_candidates(cache_snapshot, candidates, now):
    # Choose the lowest score; break ties by older last_access and larger size
    best_k = None
    best_score = None
    best_ts = None
    best_sz = None
    for k in candidates:
        if k not in cache_snapshot.cache:
            continue
        o = cache_snapshot.cache[k]
        sc = _value_score(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        sz = int(o.size)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_ts or (ts == best_ts and sz > best_sz)))):
            best_k = k
            best_score = sc
            best_ts = ts
            best_sz = sz
    return best_k

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy:
      - If window bytes exceed target: evict from window (LRU-biased sampled)
      - Else prefer evicting from probation (LRU-biased sampled).
      - If probation empty, evict from protected LRU as last resort.
      - When not forced to evict from window, allow evicting a low-value window key
        only if its score is clearly worse than best main-region candidate (_WINDOW_EVICT_RELUCTANCE).
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Remember sizes for internal rebalancing accuracy
    for k, o in cache_snapshot.cache.items():
        _remember_size(k, o.size)

    restrict_to_window = (_window_bytes > _target_window_bytes) and bool(_qW)

    if restrict_to_window:
        candW = _lru_sample_keys(_qW, k=6)
        victim = _select_victim_from_candidates(cache_snapshot, candW, now)
        if victim is not None:
            return victim

    # Prefer probation
    candP0 = _lru_sample_keys(_qP0, k=6)
    victimP0 = _select_victim_from_candidates(cache_snapshot, candP0, now) if candP0 else None

    # As fallback, consider protected LRU and optionally window keys
    candP1 = _lru_sample_keys(_qP1, k=4)
    victimP1 = _select_victim_from_candidates(cache_snapshot, candP1, now) if candP1 else None

    # If not budget-restricted, optionally consider a couple of window keys
    victimW = None
    if not restrict_to_window and _qW:
        candW = _lru_sample_keys(_qW, k=3)
        victimW = _select_victim_from_candidates(cache_snapshot, candW, now)

    # Decide among candidates. Priority: probation > protected; window only if clearly worse
    # Compute scores for chosen candidates
    def sc(k):
        return _value_score(cache_snapshot, k, cache_snapshot.cache[k], now) if (k and k in cache_snapshot.cache) else None

    scP0 = sc(victimP0) if victimP0 else None
    scP1 = sc(victimP1) if victimP1 else None
    scW = sc(victimW) if victimW else None

    # If we have a probation candidate, choose it unless a window candidate is much worse and allowed
    if victimP0 is not None:
        if victimW is not None and scW is not None and scP0 is not None and scW < _WINDOW_EVICT_RELUCTANCE * scP0:
            return victimW
        return victimP0

    # No probation candidate; choose protected if available
    if victimP1 is not None:
        if victimW is not None and scW is not None and scP1 is not None and scW < _WINDOW_EVICT_RELUCTANCE * scP1:
            return victimW
        return victimP1

    # Fall back to window
    if victimW is not None:
        return victimW

    # Last resort: pick any key (should be rare)
    keys = list(cache_snapshot.cache.keys())
    return random.choice(keys) if keys else None

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increment TinyLFU sketch and decay periodically.
      - Update last access and EWMA of inter-arrival time.
      - Promote to protected on first hit from window/probation; refresh protected to MRU.
      - Rebalance segments (window target, protected budget).
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    _remember_size(key, obj.size)

    # Defaults if missing
    if key not in m_segment:
        m_segment[key] = _SEG_WINDOW
        _qW[key] = None
        global _window_bytes
        _window_bytes += int(obj.size)
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_gap_ewma:
        m_gap_ewma[key] = _BASE_GAP
    if key not in m_hits:
        m_hits[key] = 0

    # Update EWMA gap
    prev_ts = m_last_access.get(key, now)
    gap_obs = max(1, now - prev_ts)
    m_gap_ewma[key] = (1.0 - _PRED_GAMMA) * float(m_gap_ewma.get(key, _BASE_GAP)) + _PRED_GAMMA * float(gap_obs)

    # Update last access and hit counter
    m_last_access[key] = now
    m_hits[key] = min(255, int(m_hits.get(key, 0)) + 1)

    # Promotions and LRU refresh
    seg = m_segment.get(key, _SEG_WINDOW)
    if seg == _SEG_WINDOW:
        # Move from window -> protected
        if key in _qW:
            _qW.pop(key, None)
            global _window_bytes, _protected_bytes
            _window_bytes = max(0, _window_bytes - int(obj.size))
            _protected_bytes += int(obj.size)
        _qP1[key] = None
        m_segment[key] = _SEG_PROTECTED
    elif seg == _SEG_PROBATION:
        # Promote probation -> protected
        if key in _qP0:
            _qP0.pop(key, None)
            global _probation_bytes, _protected_bytes
            _probation_bytes = max(0, _probation_bytes - int(obj.size))
            _protected_bytes += int(obj.size)
        _qP1[key] = None
        m_segment[key] = _SEG_PROTECTED
    else:
        # Protected hit: refresh to MRU
        if key in _qP1:
            _qP1.move_to_end(key, last=True)
        else:
            _qP1[key] = None
            m_segment[key] = _SEG_PROTECTED

    # Rebalance budgets
    _rebalance_segments(cache_snapshot.capacity)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after miss):
      - Adjust window target via ghost-hits feedback (ARC-like).
      - Increment TinyLFU sketch and decay periodically.
      - Initialize metadata and place in window MRU.
      - Rebalance: move LRU window items to probation to meet window target;
        enforce protected budget.
    """
    _ensure_init(cache_snapshot)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    _remember_size(key, obj.size)

    # Initialize metadata and place into window MRU
    if key in _qW:
        # Refresh if already present in window (rare)
        _qW.move_to_end(key, last=True)
    else:
        _qW[key] = None

    m_segment[key] = _SEG_WINDOW
    m_last_access[key] = now
    m_gap_ewma[key] = _BASE_GAP
    m_hits[key] = 0

    global _window_bytes
    _window_bytes += int(obj.size)

    # Rebalance segments to respect window target and protected cap
    _rebalance_segments(cache_snapshot.capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert evicted key into appropriate ghost set (window -> ghostW, main -> ghostP).
      - Remove it cleanly from its segment queue and metadata.
      - Update bytes accounting.
    """
    _ensure_init(cache_snapshot)
    evk = evicted_obj.key
    evsz = int(evicted_obj.size)

    seg = m_segment.get(evk, _SEG_WINDOW)
    # Record into ghosts
    if seg == _SEG_WINDOW:
        _ghost_admit(_ghostW, evk)
    else:
        _ghost_admit(_ghostP, evk)

    # Remove from queues and bytes
    if seg == _SEG_WINDOW:
        if evk in _qW:
            _qW.pop(evk, None)
        global _window_bytes
        _window_bytes = max(0, _window_bytes - evsz)
    elif seg == _SEG_PROTECTED:
        if evk in _qP1:
            _qP1.pop(evk, None)
        global _protected_bytes
        _protected_bytes = max(0, _protected_bytes - evsz)
    else:
        if evk in _qP0:
            _qP0.pop(evk, None)
        global _probation_bytes
        _probation_bytes = max(0, _probation_bytes - evsz)

    # Purge metadata
    m_segment.pop(evk, None)
    m_last_access.pop(evk, None)
    m_gap_ewma.pop(evk, None)
    m_hits.pop(evk, None)
    _remove_size(evk)

    # Optionally rebalance after eviction
    _rebalance_segments(cache_snapshot.capacity)
```
2025-11-27 21:48:50,298 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 37
- Hit Rate: 0.6414

Example 2:
- Trace ID: 28
- Hit Rate: 0.0788

Example 3:
- Trace ID: 43
- Hit Rate: 0.3667

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:48:50 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:48:50,300 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:50:53 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:50:53,411 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:50:53,413 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware ARC + TinyLFU + GDSF aging (ASL-ARC)
# - Two resident segments (bytes): T1 "window/recency" and T2 "protected/frequency".
# - Two ghost lists (keys only): B1 (recently evicted from T1) and B2 (recently evicted from T2).
# - Target window size (bytes) adapts online like ARC using ghost hits.
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay) + GDSF-like aging term.
# - Eviction:
#     • If T1 is over its target bytes, evict the oldest in T1 (pure recency).
#     • Otherwise, evict the minimum composite-score key across both segments.
# - Per-resident metadata: segment id, last access timestamp, tiny resident hit counter.

import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (T1 window) or 1 (T2 protected)
m_last_access = dict()   # key -> int (timestamp)
m_hits = dict()          # key -> small saturating count of resident hits (0..7)

# Count-Min Sketch (TinyLFU) - global estimator
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192           # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 10000          # faster adaptation than before
_decay_ticker = 0

# Ghost lists (keys only), track bytes and timestamps for trimming
_g_b1 = dict()   # key -> (size, ts)
_g_b2 = dict()   # key -> (size, ts)
_g_b1_bytes = 0
_g_b2_bytes = 0

# Target window bytes (adaptive ARC-like)
_WINDOW_INIT_FRAC = 0.25       # start with 25% in T1
_TARGET_MIN_FRAC = 0.02        # don't shrink below 2%
_target_window_bytes = None    # lazily initialized per first call using capacity

# GDSF-like aging baseline (increases on eviction)
_gds_L = 0.0

# Tunables for scoring
_SIZE_ALPHA = 1.0              # size exponent
_RECENCY_LAMBDA = 3.0          # recency weight
_HITS_BONUS_STEP = 0.35        # multiplicative boost per resident hit
_PROTECTED_BASE_BONUS = 0.25   # base bonus for protected items
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_hits:
        m_hits[key] = 0

def _init_target_if_needed(cache_snapshot):
    global _target_window_bytes
    if _target_window_bytes is None:
        cap = max(1, int(cache_snapshot.capacity))
        _target_window_bytes = max(1, int(_WINDOW_INIT_FRAC * cap))

def _segment_bytes(cache_snapshot):
    t1_bytes = 0
    t2_bytes = 0
    for k, robj in cache_snapshot.cache.items():
        if m_segment.get(k, 0) == 0:
            t1_bytes += robj.size
        else:
            t2_bytes += robj.size
    return t1_bytes, t2_bytes

def _oldest_in_segment(cache_snapshot, seg_id):
    # Returns (key, last_ts, size) of the oldest in segment seg_id; None if empty
    oldest_key = None
    oldest_ts = None
    oldest_size = None
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) != seg_id:
            continue
        ts = m_last_access.get(key, 0)
        if (oldest_key is None or ts < oldest_ts or
            (ts == oldest_ts and robj.size > oldest_size)):
            oldest_key = key
            oldest_ts = ts
            oldest_size = robj.size
    if oldest_key is None:
        return None
    return oldest_key, oldest_ts, oldest_size

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite keep score: higher = better to keep
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))      # [~0,1]
    # Log-frequency to soften heavy-tail bias
    fterm = math.log1p(freq)
    # GDSF-like global age baseline
    base = (fterm * (1.0 + _RECENCY_LAMBDA * recency) + _gds_L)
    score = base / (float(size) ** _SIZE_ALPHA)

    # Boost items with more resident hits and protected segment
    hits = min(7, m_hits.get(key, 0))
    mult = 1.0 + hits * _HITS_BONUS_STEP
    if m_segment.get(key, 0) == 1:
        mult *= (1.0 + _PROTECTED_BASE_BONUS)

    return score * mult

def _ghost_add(seg_id, key, size, ts, capacity):
    # Add key to B1 or B2, trim ghosts to <= 2*capacity bytes
    global _g_b1_bytes, _g_b2_bytes
    if seg_id == 0:
        if key not in _g_b1:
            _g_b1[key] = (size, ts)
            _g_b1_bytes += size
    else:
        if key not in _g_b2:
            _g_b2[key] = (size, ts)
            _g_b2_bytes += size
    _ghost_trim(2 * capacity)

def _ghost_trim(budget_bytes):
    # Trim across B1 and B2 by evicting oldest entries until sum bytes <= budget
    global _g_b1_bytes, _g_b2_bytes
    total = _g_b1_bytes + _g_b2_bytes
    if total <= budget_bytes:
        return
    # Build list of all ghosts with their ts
    merged = []
    for k, (sz, ts) in _g_b1.items():
        merged.append((ts, k, 0, sz))
    for k, (sz, ts) in _g_b2.items():
        merged.append((ts, k, 1, sz))
    merged.sort(key=lambda x: (x[0], -x[3]))  # oldest first, larger size first
    idx = 0
    while (_g_b1_bytes + _g_b2_bytes) > budget_bytes and idx < len(merged):
        ts, k, sid, sz = merged[idx]
        idx += 1
        if sid == 0:
            if k in _g_b1:
                _g_b1_bytes -= _g_b1[k][0]
                _g_b1.pop(k, None)
        else:
            if k in _g_b2:
                _g_b2_bytes -= _g_b2[k][0]
                _g_b2.pop(k, None)

def _adjust_target_on_insert(cache_snapshot, key, obj):
    # ARC-like adaptation using ghost hits on a miss for 'key'
    # If key in B1 -> increase T1 target; if in B2 -> decrease T1 target.
    # Use byte-steps scaled by ghost sizes.
    global _target_window_bytes, _g_b1_bytes, _g_b2_bytes
    cap = max(1, int(cache_snapshot.capacity))
    min_target = max(1, int(_TARGET_MIN_FRAC * cap))

    step_min = max(obj.size, int(0.025 * cap))  # at least 2.5% of cap or object size
    if key in _g_b1:
        # Prefer recency: grow window target
        ghost_ratio = _g_b2_bytes / float(max(1, _g_b1_bytes))
        delta = int(max(step_min, ghost_ratio * step_min))
        _target_window_bytes = min(cap, _target_window_bytes + delta)
        # Remove from B1
        sz, _ = _g_b1.pop(key)
        _g_b1_bytes -= sz
    elif key in _g_b2:
        # Prefer frequency: shrink window target
        ghost_ratio = _g_b1_bytes / float(max(1, _g_b2_bytes))
        delta = int(max(step_min, ghost_ratio * step_min))
        _target_window_bytes = max(min_target, _target_window_bytes - delta)
        # Remove from B2
        sz, _ = _g_b2.pop(key)
        _g_b2_bytes -= sz
    else:
        # No adaptation on cold start
        pass

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) ARC-like: if T1 byte-usage > target, evict oldest in T1 (pure recency replacement).
      2) Otherwise, evict the global minimum-score key across T1 and T2
         using TinyLFU(log), recency, GDSF aging, and size-awareness.
      Tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    _init_target_if_needed(cache_snapshot)

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Segment byte usage
    t1_bytes, _ = _segment_bytes(cache_snapshot)

    # Step 1: If T1 is over target, evict LRU from T1
    if t1_bytes > _target_window_bytes:
        res = _oldest_in_segment(cache_snapshot, 0)
        if res is not None:
            return res[0]

    # Step 2: Select global minimum composite score
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch with decay check.
      - Update last access.
      - Increment resident hit counter (saturating).
      - Promote from T1 -> T2 on first hit; stay in T2 otherwise.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Saturating resident hit counter
    m_hits[key] = min(7, m_hits.get(key, 0) + 1)

    # Promote to protected if currently in T1
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # move to protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Initialize target window if needed.
      - Adapt target window using ghost hits (ARC-like).
      - Update TinyLFU for this access.
      - Initialize metadata: segment=T1, last_access=now, hits=0.
      - Remove key from ghosts if present (handled during adaptation).
    '''
    _init_target_if_needed(cache_snapshot)

    # Adaptation based on ghost lists
    _adjust_target_on_insert(cache_snapshot, obj.key, obj)

    # TinyLFU update
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # T1 (window)
    m_last_access[key] = cache_snapshot.access_count
    m_hits[key] = 0     # reset resident hit counter

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to ghost list B1 or B2 depending on its resident segment.
      - Update GDSF aging baseline using the evicted object's base score (without segment/hits bonuses).
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    global _gds_L

    ev_key = evicted_obj.key
    now = cache_snapshot.access_count
    seg = m_segment.get(ev_key, 0)

    # Compute base score (without bonuses) for aging update
    size = max(int(evicted_obj.size), 1)
    freq = _sketch_estimate(ev_key)
    age = max(0, now - m_last_access.get(ev_key, 0))
    recency = 1.0 / (1.0 + float(age))
    base = (math.log1p(freq) * (1.0 + _RECENCY_LAMBDA * recency)) / (float(size) ** _SIZE_ALPHA)
    _gds_L = 0.9 * _gds_L + base  # exponential moving "L" age floor

    # Add to ghost list and trim
    _ghost_add(seg, ev_key, evicted_obj.size, now, max(1, int(cache_snapshot.capacity)))

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_hits.pop(ev_key, None)
```
2025-11-27 21:51:07,123 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 4
- Hit Rate: 0.4509

Example 2:
- Trace ID: 36
- Hit Rate: 0.026

Example 3:
- Trace ID: 20
- Hit Rate: 0.3648

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:51:07 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:51:07,126 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:53:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:53:51,659 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:53:51,661 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented TinyLFU with explicit LRU window/protected (S-TinyLFU/SLRU)
# Key improvements:
# - Fix: update recency queues (LRU) on every hit to prevent demoting/evicting recently-used items.
# - Dual-path victim choice: compare the oldest-window candidate with a global low-score sample
#   and prefer evicting the truly weakest; still biased to keep window items.
# - Stronger scan/cold-admission shielding via "doomed" tagging and stricter promotions.
# - Faster/reactive window-size adaptation with ghost feedback and hit-location tracking.
# - Tighter large-object budget and improved large-object victim sampling.
#
# Notes:
# - LRU is maintained via lazy-cleaned deques; we append on every hit to record MRU.
# - Protected budget is enforced in bytes; demotion occurs from oldest protected.
# - All counters are "best-effort" and lazy-cleaned to keep O(1) per-access overhead.

import random
import itertools
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.08  # faster adaptation
_WINDOW_EVICT_RELUCTANCE = 1.20  # reluctance to evict window vs global candidate (higher keeps window)

# Sampling sizes
_SAMPLE_SIZE = 24            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 16      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 24

# Priority scoring weights (favor frequency a bit more; recency still matters)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.15
_W_RECENCY = 0.35
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.50

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 2000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.55      # tighter cap for large objects

# Doorkeeper thresholds (promotion on resident hits; stricter to protect protected)
_PROMOTE_HITS_SMALL = 2
_PROMOTE_HITS_LARGE = 3

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.60
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Non-mutating scan of the leftmost few window entries for an oldest "doomed" key
    # Do not rotate/append to preserve LRU ordering.
    if not _window_q:
        return None
    # First ensure head is clean
    _clean_window_left()
    if not _window_q:
        return None
    # Look at a small prefix
    for k in itertools.islice(_window_q, 64):
        if k in m_key_index and m_segment.get(k, 0) == 0 and (k in m_doomed):
            return k
    return None

def _score_for_key(cache_snapshot, key, now, incoming_size):
    robj = cache_snapshot.cache.get(key, None)
    if robj is None:
        return None
    return _priority_for_key(cache_snapshot, key, robj, now, incoming_size)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (dual-path):
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Gather candidates:
         - Oldest doomed-in-window (if any), else oldest window key (LRU).
         - If large objects exceed cap, sample large set for a weak victim.
         - Global sample of keys for a weak victim by TinyLFU+recency score.
      3) Choose the overall weakest by score, but bias against evicting a window item unless clearly worse.
      4) Admission guard: if chosen victim is much stronger than incoming, evict oldest window instead.
      5) Fallbacks: oldest by timestamp from a small sample, else any key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    best_key = None
    best_score = None
    best_is_window = False

    # A) Window candidate: doomed if possible, else oldest window
    win_key = _find_oldest_doomed_in_window()
    if win_key is None:
        win_key = _peek_oldest_window()
    if win_key is not None:
        s = _score_for_key(cache_snapshot, win_key, now, incoming_size)
        if s is not None:
            best_key = win_key
            # Apply reluctance to evict window by artificially boosting its score for comparison
            best_score = s * _WINDOW_EVICT_RELUCTANCE
            best_is_window = True

    # B) Large overcap handling: sample large keys and pick weakest
    large_victim_key = None
    large_victim_score = None
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        if cands:
            large_victim_key, large_victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # Consider large-victim as candidate
    if large_victim_key is not None and large_victim_score is not None:
        if best_key is None or (large_victim_score < best_score - _EPS):
            best_key = large_victim_key
            best_score = large_victim_score
            best_is_window = (m_segment.get(best_key, 0) == 0)

    # C) Global sampling fallback/augment
    candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    if candidates:
        gkey, gscore = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)
        if gkey is not None and gscore is not None:
            if best_key is None or (gscore < best_score - _EPS):
                best_key = gkey
                best_score = gscore
                best_is_window = (m_segment.get(best_key, 0) == 0)

    # D) Admission guard: if chosen victim is much stronger than incoming, prefer oldest window
    if best_key is not None and best_score is not None:
        # Remove the reluctance multiplier for comparing strength versus incoming
        real_best_score = best_score / (_WINDOW_EVICT_RELUCTANCE if best_is_window else 1.0)
        if real_best_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != best_key:
                best_key = alt

    # Final fallbacks if none found
    if best_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            best_key = oldest

    if best_key is None and m_keys:
        best_key = m_keys[0]

    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Append to the appropriate LRU deque to mark MRU (window or protected).
      - Promote window->protected if hit threshold reached (2 for small, 3 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(7, m_hits_resident.get(key, 0) + 1)

    # Update LRU deques on every hit (MRU)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Append to window LRU deque (MRU).
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Enhanced scan-shielding: tag "doomed" for clearly cold inserts, even if not very large.
      - Enforce protected budget after any changes.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)  # MRU in window

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Enhanced scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large OR window is bloated,
    # and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.85 * target_window))
    if (est_freq <= 1.0) and (key not in _ghost_p_set):
        if is_large or window_is_bloated or size >= int(2 * _avg_size):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

    # Keep protected within budget (window target changed possibly)
    _enforce_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 21:54:10,728 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Admission-aware, Sampled, Size-aware TinyLFU (AA-SS-TinyLFU)
# Key improvements over previous version:
# - Doorkeeper gating before Count-Min Sketch increments (reduces pollution from one-hit wonders).
# - Admission-aware eviction gating: avoid evicting strong protected items for weak incoming items.
# - Gentler size penalty (log-based) and reduced recency dominance; stronger frequency influence.
# - Promotion from window -> protected requires at least two touches in the current epoch (via doorkeeper).
# - Additional hill-climbing using "admission wins/losses" to adapt window fraction.
# - Kept sampling-based victim selection and protected demotion by approximate oldest.

import random
import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests) with a doorkeeper
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper: a small set reset on decay to gate first occurrences.
_doorkeeper = set()

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32             # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24      # candidates to demote from protected (oldest)

# Priority scoring weights
# Gentler size penalty using log2(size), stronger frequency, reduced recency.
_SIZE_ALPHA = 0.90            # exponent for size penalty
_W_FREQ = 1.0                 # weight for frequency
_W_RECENCY = 1.25             # reduced recency weight
_PROTECTED_KEEP_BONUS = 1.10  # protected items are harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Admission-aware hill climbing (based on candidate vs victim strength)
_ADMIT_ADAPT_PERIOD = 8000
_admit_wins = 0
_admit_losses = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _access_learn(key):
    # Doorkeeper gates first occurrences; only second (and later) touches in an epoch
    # are added to the Count-Min Sketch. We still tick the decay ticker either way.
    global _decay_ticker
    if key in _doorkeeper:
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    else:
        _doorkeeper.add(key)
    _decay_ticker += 1

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        # Reset doorkeeper on decay to bound it and refresh the epoch
        _doorkeeper.clear()
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _size_penalty(size_bytes):
    # Gentler size penalty: (1 + log2(size)) ** alpha
    s = max(1, int(size_bytes))
    return (1.0 + math.log2(float(s))) ** _SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size_pen = _size_penalty(obj.size)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / max(size_pen, _EPS)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_admission_score(obj):
    # Admission score for incoming object uses only frequency and size penalty
    # (no recency boost for brand new items).
    return (_W_FREQ * _sketch_estimate(obj.key)) / max(_size_penalty(obj.size), _EPS)

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    # Hit-location based adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _admit_adapt_if_needed():
    # Admission-aware hill-climbing adaptation
    global _admit_wins, _admit_losses, _f_window
    total = _admit_wins + _admit_losses
    if total < _ADMIT_ADAPT_PERIOD:
        return
    if _admit_wins > _admit_losses * 1.05:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif _admit_losses > _admit_wins * 1.05:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _admit_wins = 0
    _admit_losses = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Admission-aware sampled victim selection:
      - Compute incoming admission score (TinyLFU estimate / size penalty).
      - Sample globally to estimate protected/window strengths.
      - If the window byte-usage > target OR incoming score is weaker than sampled protected min,
        restrict eviction to window keys.
      - Else sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
      - Track admission "wins/losses" for adaptive window tuning.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    window_over_budget = _g_window_bytes > target_window

    s = min(_SAMPLE_SIZE, len(m_keys))
    if s <= 0:
        return None

    # Global sample to get a sense of protected/window strengths
    global_sample = _sample_keys_from_segment(None, s)
    incoming_score = _incoming_admission_score(obj)

    # Find min-protected score in the sample
    min_prot_score = None
    for key in global_sample:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        if m_segment.get(key, 0) == 1:
            sc = _priority_for_key(cache_snapshot, key, robj, now)
            if min_prot_score is None or sc < min_prot_score:
                min_prot_score = sc

    # Admission-aware gating:
    # If incoming is clearly weaker than protected minimum, avoid evicting protected.
    restrict_to_window = window_over_budget or (min_prot_score is not None and incoming_score + _EPS < 1.05 * min_prot_score)

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, s)
        if not candidates:
            # If no window candidates (rare), fallback to global
            candidates = global_sample
    else:
        candidates = global_sample

    # Fallback: if sampling failed, scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # Guard: if nothing found
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
            victim_score = 0.0
        else:
            return None

    # Admission "win/loss" bookkeeping for hill climbing
    # If incoming is stronger than the chosen victim, treat as win, else loss.
    # Use a small margin to avoid noise.
    margin = 1.02
    if incoming_score >= margin * (victim_score if victim_score is not None else 0.0):
        globals()['_admit_wins'] += 1
    else:
        globals()['_admit_losses'] += 1
    _admit_adapt_if_needed()

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Learn via doorkeeper+Count-Min sketch (with periodic aging).
      - Update last access timestamp.
      - Promote from window -> protected only if the key has been touched at least twice
        in the current epoch (i.e., already in doorkeeper).
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window, but only if we've seen it at least twice this epoch
    if seg == 0 and (key in _doorkeeper):
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Learn the access via doorkeeper+Count-Min sketch (with periodic aging).
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay + doorkeeper handle aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 39
- Hit Rate: 0.0909

Example 2:
- Trace ID: 47
- Hit Rate: 0.0948

Example 3:
- Trace ID: 9
- Hit Rate: 0.531

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:54:10 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:54:10,730 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:56:42 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:56:42,312 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:56:42,314 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# W-TinyLFU with SLRU-main (probation/protected), size-aware admission, doorkeeper,
# and adaptive window sizing via hit-location and admission wins/losses.
#
# Goals:
# - Protect the main cache from scans by evicting from the recency "window" unless the
#   incoming item is strong enough to justify replacing a probation victim.
# - Use a probation->protected two-stage main (SLRU) to require two resident hits before
#   an item becomes hard to evict.
# - TinyLFU (with doorkeeper) to estimate long-term popularity and guide admission.
# - Size-aware scoring (slightly stronger than log penalty) to reduce large-object pollution.
# - Sampling to approximate oldest and lowest-priority victims with low overhead.
# - Adaptive window fraction via where hits happen (window vs main) + admission wins/losses.

import random
import math

# ------------------------
# Global metadata
# ------------------------
# Segments:
#   0 = window (recency)
#   1 = main probation (admitted by TinyLFU, one resident hit promotes to protected)
#   2 = main protected (two resident hits; demoted to probation if over budget)
m_segment = dict()        # key -> segment {0,1,2}
m_last_access = dict()    # key -> int (timestamp)
m_hits_since_admit = dict()  # key -> small int: resident hits since last insert/promote
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_probation_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests) with a doorkeeper
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper: a small set reset on decay to gate first occurrences per epoch
_doorkeeper = set()

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# ------------------------
# Tunables
# ------------------------
# Window fraction is adaptive; main is (1 - f_window). Within main, share protected vs probation.
_f_window = 0.15
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Main protected fraction (of main bytes)
_F_MAIN_PROTECTED = 0.80

# Sampling sizes
_SAMPLE_SIZE = 32               # general candidates per decision
_SAMPLE_SIZE_OLDEST = 28        # for oldest selection by sampling
_SAMPLE_SIZE_DEMOTE = 24        # for demoting oldest protected

# Priority scoring weights
# Strengthen frequency, reduce recency dominance; slightly stronger size penalty.
_SIZE_ALPHA = 1.20
_W_FREQ = 1.25
_W_RECENCY = 0.60
_PROTECTED_KEEP_BONUS = 1.30
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_main = 0  # probation + protected

# Admission-aware hill climbing (based on candidate vs victim strength)
_ADMIT_ADAPT_PERIOD = 8000
_admit_wins = 0
_admit_losses = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _access_learn(key):
    # Doorkeeper gates first occurrences; only second (and later) touches in an epoch
    # are added to the Count-Min Sketch. We still tick the decay ticker either way.
    global _decay_ticker
    if key in _doorkeeper:
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    else:
        _doorkeeper.add(key)
    _decay_ticker += 1

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _doorkeeper.clear()
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_probation_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist).
    # Default all to window.
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_hits_since_admit:
            m_hits_since_admit[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        seg = m_segment.get(key, 0)
        if seg == 0:
            _g_window_bytes += robj.size
        elif seg == 1:
            _g_probation_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _size_penalty(size_bytes):
    # Slightly stronger than log penalty to better discourage very large objects.
    s = max(1, int(size_bytes))
    return (1.0 + math.log2(float(s))) ** _SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency + recency, divided by size penalty
    size_pen = _size_penalty(obj.size)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / max(size_pen, _EPS)
    seg = m_segment.get(key, 0)
    if seg == 2:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_admission_score(obj):
    # Frequency / size penalty (no recency for incoming)
    return (_W_FREQ * _sketch_estimate(obj.key)) / max(_size_penalty(obj.size), _EPS)

def _current_targets(cache_snapshot):
    # Returns byte targets for window, probation, protected.
    cap = max(1, int(cache_snapshot.capacity))
    window_target = max(1, int(_f_window * cap))
    main_target = max(0, cap - window_target)
    protected_target = int(_F_MAIN_PROTECTED * main_target)
    probation_target = max(0, main_target - protected_target)
    return window_target, probation_target, protected_target

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment {0,1,2} or None for global
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # If none found for this segment, fall back to global limited sample
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _choose_oldest_in_segment(cache_snapshot, seg, k_sample):
    now = cache_snapshot.access_count
    candidates = _sample_keys_from_segment(seg, k_sample)
    if not candidates:
        return None
    oldest_key, oldest_ts, oldest_size = None, None, None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        obj = cache_snapshot.cache.get(key, None)
        if obj is None:
            continue
        if oldest_key is None or ts < oldest_ts or (ts == oldest_ts and obj.size > oldest_size):
            oldest_key, oldest_ts, oldest_size = key, ts, obj.size
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # If protected exceeds its byte budget, demote the oldest (approx via sampling) into probation.
    global _g_probation_bytes, _g_protected_bytes
    _, _, protected_target = _current_targets(cache_snapshot)
    if _g_protected_bytes <= protected_target:
        return
    candidates = _sample_keys_from_segment(2, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 2:
        m_segment[oldest_key] = 1  # demote to probation
        _g_protected_bytes -= size
        _g_probation_bytes += size
        # No change to hits counter on demotion

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_main, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_main
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If most hits occur in window, increase window; if in main, decrease window.
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_main = 0

def _admit_adapt_if_needed():
    global _admit_wins, _admit_losses, _f_window
    total = _admit_wins + _admit_losses
    if total < _ADMIT_ADAPT_PERIOD:
        return
    # If incoming tends to be stronger than probation victim, we can grow window a bit;
    # else shrink it to protect main more.
    if _admit_wins > _admit_losses * 1.05:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif _admit_losses > _admit_wins * 1.05:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _admit_wins = 0
    _admit_losses = 0

def _probation_victim_by_score(cache_snapshot, s):
    # Choose victim among probation by the lowest priority (TinyLFU+recency)/size
    now = cache_snapshot.access_count
    candidates = _sample_keys_from_segment(1, s)
    if not candidates:
        return None, None  # key, score
    victim_key, victim_score, victim_ts, victim_sz = None, None, None, None
    for key in candidates:
        obj = cache_snapshot.cache.get(key, None)
        if obj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, obj, now)
        ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or
                                                    (ts == victim_ts and obj.size > victim_sz)))):
            victim_key, victim_score, victim_ts, victim_sz = key, score, ts, obj.size
    return victim_key, victim_score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    W-TinyLFU eviction decision:
      - Compute incoming admission score (TinyLFU estimate / size penalty).
      - If window is over its byte budget (or probation empty), evict oldest from window.
      - Else choose a probation victim by lowest score via sampling and compare:
           if incoming score >= margin * victim_score, evict that probation victim (admit incoming to window);
           else evict oldest from window (protect main from pollution).
      - Only if both window and probation are unavailable, consider protected (rare).
      - Tie-breaks: oldest last access, then largest size.
      - Track admission wins/losses to adapt the window fraction.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    window_target, probation_target, protected_target = _current_targets(cache_snapshot)
    window_over_budget = _g_window_bytes > window_target

    # Compute incoming score
    incoming_score = _incoming_admission_score(obj)

    # Decide candidate from probation (if any)
    prob_victim_key, prob_victim_score = _probation_victim_by_score(cache_snapshot, min(_SAMPLE_SIZE, len(m_keys)))

    # Always have a window candidate: oldest from window by sampling
    win_victim_key = _choose_oldest_in_segment(cache_snapshot, 0, _SAMPLE_SIZE_OLDEST)

    # If window is over budget or probation is empty, prefer evicting from window
    if window_over_budget or prob_victim_key is None:
        if win_victim_key is not None:
            # Admission bookkeeping: if we could not compare, treat as loss only if probation existed
            if prob_victim_key is not None:
                globals()['_admit_losses'] += 1
                _admit_adapt_if_needed()
            return win_victim_key
        # Fallback: no window; try probation
        if prob_victim_key is not None:
            globals()['_admit_losses'] += 1  # incoming weaker than window's absence => conservative
            _admit_adapt_if_needed()
            return prob_victim_key

    # Admission-aware gating between window vs probation
    # If incoming is stronger than probation victim, evict probation and admit incoming.
    margin = 1.02
    if prob_victim_key is not None and incoming_score >= margin * (prob_victim_score if prob_victim_score is not None else 0.0):
        globals()['_admit_wins'] += 1
        _admit_adapt_if_needed()
        return prob_victim_key

    # Otherwise, protect main: evict from window if possible
    if win_victim_key is not None:
        globals()['_admit_losses'] += 1
        _admit_adapt_if_needed()
        return win_victim_key

    # As a last resort (pathological), evict the globally weakest key (may include protected)
    s = min(_SAMPLE_SIZE, len(m_keys))
    if s <= 0:
        return None
    candidates = _sample_keys_from_segment(None, s)
    victim_key, victim_score, victim_ts, victim_sz = None, None, None, None
    for key in candidates:
        obj2 = cache_snapshot.cache.get(key, None)
        if obj2 is None:
            continue
        sc = _priority_for_key(cache_snapshot, key, obj2, now)
        ts = m_last_access.get(key, 0)
        if (victim_key is None or
            sc < victim_score - _EPS or
            (abs(sc - victim_score) <= _EPS and (ts < victim_ts or
                                                 (ts == victim_ts and obj2.size > victim_sz)))):
            victim_key, victim_score, victim_ts, victim_sz = key, sc, ts, obj2.size
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Learn via doorkeeper+Count-Min sketch (with periodic aging).
      - Update last access timestamp and hits since admit/promote.
      - Promotions:
          window -> probation on first resident hit;
          probation -> protected on first hit in probation.
      - If protected exceeds its budget, demote an old protected item to probation.
      - Adapt window fraction periodically based on where hits occur (window vs main).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (robustness)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_hits_since_admit:
        m_hits_since_admit[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume it is in window bytes if newly discovered resident (rare path)
        globals()['_g_window_bytes'] += int(obj.size)

    seg = m_segment.get(key, 0)
    # Count hit location for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_main'] += 1

    # Update last access and resident hit counter
    m_last_access[key] = now
    m_hits_since_admit[key] = m_hits_since_admit.get(key, 0) + 1

    # Promotions:
    if seg == 0:
        # First resident hit: promote window -> probation
        if m_hits_since_admit[key] >= 1:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] -= sz
            globals()['_g_probation_bytes'] += sz
            # Reset hits since admit for probation stage
            m_hits_since_admit[key] = 0
    elif seg == 1:
        # First hit in probation: promote to protected
        if m_hits_since_admit[key] >= 1:
            m_segment[key] = 2
            sz = int(obj.size)
            globals()['_g_probation_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz
            m_hits_since_admit[key] = 0
            _maybe_demote_from_protected(cache_snapshot)
    else:
        # Protected: nothing else; keep metadata fresh
        pass

    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Learn the access via doorkeeper+Count-Min sketch (with periodic aging).
      - Initialize metadata: segment=window, last_access=now, hits_since_admit=0.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _access_learn(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_hits_since_admit[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay + doorkeeper handling).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_hits_since_admit.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    elif seg == 1:
        globals()['_g_probation_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # No further rebalancing here; handled lazily on hits/promotions
```
2025-11-27 21:57:02,436 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Size-aware TinyLFU with ARC-style window tuning (A-WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic decay.
# - Two in-cache segments: "window" (recency/probation) and "protected" (frequency).
# - Promotion requires two resident hits (to resist 1-hit scans).
# - Adaptive target for window bytes via ARC-like ghost feedback (B1/B2).
# - Eviction:
#     1) If window bytes exceed target, evict the minimum-score key from window.
#     2) Else, evict the global minimum-score key (protected has a slight bonus).
# - Score balances estimated frequency, recency, and size; cold window entries are penalized.
# - Per-resident metadata stays compact: segment id, last access timestamp, resident hit-count.

from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()          # key -> 0 (window) or 1 (protected)
m_last_access = dict()      # key -> int (timestamp)
m_resident_hits = dict()    # key -> small int (hits since insertion/promotion)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_SIZE_ALPHA = 1.0  # priority divisor exponent: size**alpha
# Recency impact differs by segment: window favors recency more than protected
_RECENCY_LAMBDA_WINDOW = 4.0
_RECENCY_LAMBDA_PROTECTED = 1.25
# Protected items get a modest multiplicative bonus
_PROTECTED_KEEP_BONUS = 1.25
# Cold window items (0 resident hits) are penalized to resist scan pollution
_COLD_PENALTY = 0.5
_EPS = 1e-9

# Adaptive window target (in bytes), guided by ARC-like ghosts
_target_window_bytes = None
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.80
# Ghost lists (OrderedDict for LRU): key -> (ts, size)
_B1 = OrderedDict()   # Ghost of window (recent/probation) evictions
_B2 = OrderedDict()   # Ghost of protected (frequent) evictions
_ghost_bytes_B1 = 0
_ghost_bytes_B2 = 0

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0  # start in window
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_resident_hits:
        m_resident_hits[key] = 0

def _clamp_target_window(capacity):
    global _target_window_bytes
    if _target_window_bytes is None:
        _target_window_bytes = int(max(1, int(0.20 * capacity)))
    lo = int(max(1, _MIN_WINDOW_FRAC * capacity))
    hi = int(max(1, _MAX_WINDOW_FRAC * capacity))
    if _target_window_bytes < lo:
        _target_window_bytes = lo
    elif _target_window_bytes > hi:
        _target_window_bytes = hi

def _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1):
    # ARC-like adaptation: on a miss that hits B1, increase window target;
    # on a miss that hits B2, decrease window target.
    global _target_window_bytes
    capacity = cache_snapshot.capacity
    _clamp_target_window(capacity)

    b1c = max(1, len(_B1))
    b2c = max(1, len(_B2))
    ratio = (b2c / b1c) if hit_in_B1 else (b1c / b2c)
    # Base step ~3.125% of capacity, scaled up to 4x by the ratio.
    base_step = max(1, capacity // 32)
    delta = int(base_step * min(4.0, ratio))
    if hit_in_B1:
        _target_window_bytes = min(int(_MAX_WINDOW_FRAC * capacity), _target_window_bytes + max(1, delta))
    else:
        _target_window_bytes = max(int(_MIN_WINDOW_FRAC * capacity), _target_window_bytes - max(1, delta))

def _ghost_add(cache_snapshot, key, size, now, to_B1):
    global _ghost_bytes_B1, _ghost_bytes_B2
    if to_B1:
        # Insert/move to MRU
        if key in _B1:
            old_ts, old_sz = _B1.pop(key)
            _ghost_bytes_B1 -= old_sz
        _B1[key] = (now, size)
        _ghost_bytes_B1 += size
    else:
        if key in _B2:
            old_ts, old_sz = _B2.pop(key)
            _ghost_bytes_B2 -= old_sz
        _B2[key] = (now, size)
        _ghost_bytes_B2 += size
    _trim_ghosts(cache_snapshot)

def _ghost_remove_if_present(key):
    global _ghost_bytes_B1, _ghost_bytes_B2
    removed_from = None
    if key in _B1:
        ts, sz = _B1.pop(key)
        _ghost_bytes_B1 -= sz
        removed_from = 'B1'
    elif key in _B2:
        ts, sz = _B2.pop(key)
        _ghost_bytes_B2 -= sz
        removed_from = 'B2'
    return removed_from

def _trim_ghosts(cache_snapshot):
    global _ghost_bytes_B1, _ghost_bytes_B2
    capacity = cache_snapshot.capacity
    limit = max(1, 2 * capacity)  # allow ghosts up to 2x capacity in bytes
    total = _ghost_bytes_B1 + _ghost_bytes_B2
    while total > limit and (_B1 or _B2):
        # Pop the oldest among the heads of B1 and B2
        k1, v1 = (next(iter(_B1.items())) if _B1 else (None, None))
        k2, v2 = (next(iter(_B2.items())) if _B2 else (None, None))
        if k1 is None and k2 is None:
            break
        if k2 is None or (k1 is not None and v1[0] <= v2[0]):
            # pop oldest from B1
            key, (ts, sz) = _B1.popitem(last=False)
            _ghost_bytes_B1 -= sz
            total -= sz
        else:
            key, (ts, sz) = _B2.popitem(last=False)
            _ghost_bytes_B2 -= sz
            total -= sz

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = 1.0 + _sketch_estimate(key)  # +1 smooths zero-case
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # in (0,1]
    seg = m_segment.get(key, 0)

    if seg == 0:
        mult = 1.0 + (_RECENCY_LAMBDA_WINDOW * recency)
        score = (freq * mult) / (float(size) ** _SIZE_ALPHA)
        # Cold window entries (0 resident hits) are easier to evict
        if m_resident_hits.get(key, 0) <= 0:
            score *= _COLD_PENALTY
    else:
        mult = 1.0 + (_RECENCY_LAMBDA_PROTECTED * recency)
        score = (freq * mult) / (float(size) ** _SIZE_ALPHA)
        # Protected bonus keeps established frequent items longer
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > adaptive target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected has a modest bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count
    capacity = cache_snapshot.capacity
    _clamp_target_window(capacity)

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window byte usage
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, _target_window_bytes)
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # only window candidates when window over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay periodically.
      - Update last access timestamp and resident hit count.
      - Promote from window -> protected after 2 resident hits (resists 1-hit scans).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Count resident hits
    m_resident_hits[key] = m_resident_hits.get(key, 0) + 1

    # Promote to protected after two hits while resident in window
    if m_segment.get(key, 0) == 0 and m_resident_hits[key] >= 2:
        m_segment[key] = 1  # protected
        # Reset hit counter after promotion to avoid runaway counts
        m_resident_hits[key] = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay periodically.
      - ARC-like adaptation: if key is in a ghost list, adjust window target accordingly.
      - Initialize metadata: segment=window, last_access=now, resident_hits=0.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    _clamp_target_window(cache_snapshot.capacity)

    key = obj.key
    # ARC-style feedback via ghosts
    removed_from = _ghost_remove_if_present(key)
    if removed_from == 'B1':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=True)
    elif removed_from == 'B2':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=False)

    # Initialize resident metadata
    m_segment[key] = 0  # window/probation
    m_last_access[key] = cache_snapshot.access_count
    m_resident_hits[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (B1 for window, B2 for protected).
      - Remove resident metadata for the evicted key.
      - Count-Min sketch persists globally (natural decay ages it).
    '''
    now = cache_snapshot.access_count
    ev_key = evicted_obj.key
    ev_seg = m_segment.get(ev_key, 0)
    size = max(1, int(evicted_obj.size))

    # Add to ghost list for ARC-like adaptation
    _ghost_add(cache_snapshot, ev_key, size, now, to_B1=(ev_seg == 0))

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_resident_hits.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 44
- Hit Rate: 0.0419

Example 2:
- Trace ID: 18
- Error: Error - Timeout

Example 3:
- Trace ID: 34
- Hit Rate: 0.0855

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m21:57:02 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 21:57:02,438 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m21:59:11 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 21:59:11,890 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 21:59:11,893 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight, adaptive W-TinyLFU (SLRU) with ARC-style window tuning and a fast doorkeeper
# Goals:
#   - Avoid O(n) scans on eviction (prevents timeouts): O(1) victim selection via LRU per-segment.
#   - Strong recency+frequency behavior: Window (probation) + Protected segments (SLRU).
#   - Adapt window size via ARC-style ghosts (B1/B2) in BYTES, not just counts.
#   - TinyLFU estimator with periodic decay; "doorkeeper" reduces single-touch noise.
#   - Size-aware eviction choices (frequency normalized by size) when choosing between segments.

from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
# Resident segmentation (SLRU)
_W = OrderedDict()   # probation/window: key -> size
_P = OrderedDict()   # protected: key -> size
_bytes_W = 0
_bytes_P = 0

# Per-key metadata
m_segment = dict()          # key -> 0 (window) or 1 (protected)
m_last_access = dict()      # key -> int (timestamp)
m_resident_hits = dict()    # key -> small int (hits since last insert/promotion)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Count-Min decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Doorkeeper (reduces noise from single touches)
_DOOR_WIDTH = 1 << 15
_DOOR_MASK = _DOOR_WIDTH - 1
_door_bits = bytearray(_DOOR_WIDTH // 8)
_DOOR_RESET = 20000
_door_ticker = 0

# Tunables
_SIZE_ALPHA = 1.0  # normalize frequency by size**alpha
_EPS = 1e-9

# Adaptive window target (in bytes), guided by ARC-like ghosts
_target_window_bytes = None
_MIN_WINDOW_FRAC = 0.05
_MAX_WINDOW_FRAC = 0.80
# Ghost lists (OrderedDict for LRU): key -> (ts, size)
_B1 = OrderedDict()   # Ghost of window evictions
_B2 = OrderedDict()   # Ghost of protected evictions
_ghost_bytes_B1 = 0
_ghost_bytes_B2 = 0

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        v = _cm[d][_h(key, d)]
        if v < est:
            est = v
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _door_test_and_set(key):
    global _door_ticker
    idx = (hash(key) * 0x9e3779b97f4a7c15) & _DOOR_MASK
    byte_idx = idx >> 3
    bit = 1 << (idx & 7)
    was = ( _door_bits[byte_idx] & bit ) != 0
    if not was:
        _door_bits[byte_idx] |= bit
    _door_ticker += 1
    if _door_ticker >= _DOOR_RESET:
        # Reset all bits
        for i in range(len(_door_bits)):
            _door_bits[i] = 0
        _door_ticker = 0
    return was

def _admission_touch(key):
    # Doorkeeper: only frequent/repeat accesses increment the sketch
    if _door_test_and_set(key):
        _sketch_increment(key)
        _sketch_decay_if_needed()

def _clamp_target_window(capacity):
    global _target_window_bytes
    if _target_window_bytes is None:
        _target_window_bytes = int(max(1, int(0.20 * capacity)))
    lo = int(max(1, _MIN_WINDOW_FRAC * capacity))
    hi = int(max(1, _MAX_WINDOW_FRAC * capacity))
    if _target_window_bytes < lo:
        _target_window_bytes = lo
    elif _target_window_bytes > hi:
        _target_window_bytes = hi

def _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1):
    # ARC-like adaptation: on a miss that hits B1, increase window target;
    # on a miss that hits B2, decrease window target.
    global _target_window_bytes
    capacity = cache_snapshot.capacity
    _clamp_target_window(capacity)

    b1c = max(1, len(_B1))
    b2c = max(1, len(_B2))
    ratio = (b2c / b1c) if hit_in_B1 else (b1c / b2c)
    base_step = max(1, capacity // 32)  # ~3.125% of capacity
    delta = int(base_step * min(4.0, ratio))
    if hit_in_B1:
        _target_window_bytes = min(int(_MAX_WINDOW_FRAC * capacity), _target_window_bytes + max(1, delta))
    else:
        _target_window_bytes = max(int(_MIN_WINDOW_FRAC * capacity), _target_window_bytes - max(1, delta))

def _ghost_add(cache_snapshot, key, size, now, to_B1):
    global _ghost_bytes_B1, _ghost_bytes_B2
    if to_B1:
        if key in _B1:
            old_ts, old_sz = _B1.pop(key)
            _ghost_bytes_B1 -= old_sz
        _B1[key] = (now, size)
        _ghost_bytes_B1 += size
    else:
        if key in _B2:
            old_ts, old_sz = _B2.pop(key)
            _ghost_bytes_B2 -= old_sz
        _B2[key] = (now, size)
        _ghost_bytes_B2 += size
    _trim_ghosts(cache_snapshot)

def _ghost_remove_if_present(key):
    global _ghost_bytes_B1, _ghost_bytes_B2
    removed_from = None
    if key in _B1:
        ts, sz = _B1.pop(key)
        _ghost_bytes_B1 -= sz
        removed_from = 'B1'
    elif key in _B2:
        ts, sz = _B2.pop(key)
        _ghost_bytes_B2 -= sz
        removed_from = 'B2'
    return removed_from

def _trim_ghosts(cache_snapshot):
    global _ghost_bytes_B1, _ghost_bytes_B2
    capacity = cache_snapshot.capacity
    limit = max(1, 2 * capacity)  # ghosts up to 2x capacity (bytes)
    total = _ghost_bytes_B1 + _ghost_bytes_B2
    while total > limit and (_B1 or _B2):
        # Remove oldest among B1/B2 heads
        k1, v1 = (next(iter(_B1.items())) if _B1 else (None, None))
        k2, v2 = (next(iter(_B2.items())) if _B2 else (None, None))
        if k1 is None and k2 is None:
            break
        if k2 is None or (k1 is not None and v1[0] <= v2[0]):
            key, (ts, sz) = _B1.popitem(last=False)
            _ghost_bytes_B1 -= sz
            total -= sz
        else:
            key, (ts, sz) = _B2.popitem(last=False)
            _ghost_bytes_B2 -= sz
            total -= sz

def _ensure_resident_metadata(now, key):
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_resident_hits:
        m_resident_hits[key] = 0

def _rebuild_if_needed(cache_snapshot):
    # Build resident structures if out-of-sync (happens at start)
    global _bytes_W, _bytes_P
    total_res = len(_W) + len(_P)
    if total_res == len(cache_snapshot.cache):
        return
    now = cache_snapshot.access_count
    # Add any missing residents as probation entries
    for key, robj in cache_snapshot.cache.items():
        if key in _W or key in _P:
            continue
        size = max(1, int(robj.size))
        _W[key] = size
        _bytes_W += size
        m_segment[key] = 0
        _ensure_resident_metadata(now, key)
    # Note: removals are handled via update_after_evict; we avoid scanning to drop stale keys.

def _seg_bytes(seg_id):
    return _bytes_W if seg_id == 0 else _bytes_P

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    O(1) victim selection:
      - Maintain two LRU lists (window/probation and protected).
      - If window bytes > adaptive target, evict LRU of window.
      - Else choose between the LRU of window vs protected by lower TinyLFU score normalized by size.
        Slight bias toward window to resist scan pollution.
    '''
    global _bytes_W, _bytes_P
    now = cache_snapshot.access_count
    _clamp_target_window(cache_snapshot.capacity)
    _rebuild_if_needed(cache_snapshot)

    # Fast path: if window above target, evict from window's LRU head
    if _bytes_W > _target_window_bytes and _W:
        return next(iter(_W))  # LRU of window

    # Otherwise, choose between window-LRU and protected-LRU by score (freq/size)
    kW = next(iter(_W)) if _W else None
    kP = next(iter(_P)) if _P else None

    # If only one candidate exists
    if kW is None and kP is None:
        # Fallback: if metadata lost, evict an arbitrary key (should be rare)
        for key in cache_snapshot.cache:
            return key
        return None
    if kW is None:
        return kP
    if kP is None:
        return kW

    oW = cache_snapshot.cache.get(kW)
    oP = cache_snapshot.cache.get(kP)
    if oW is None and oP is None:
        # Rebuild and fall back
        _rebuild_if_needed(cache_snapshot)
        for key in cache_snapshot.cache:
            return key
        return None
    if oW is None:
        return kP
    if oP is None:
        return kW

    szW = max(1, int(oW.size))
    szP = max(1, int(oP.size))
    scoreW = (1.0 + _sketch_estimate(kW)) / (float(szW) ** _SIZE_ALPHA)
    scoreP = (1.0 + _sketch_estimate(kP)) / (float(szP) ** _SIZE_ALPHA)

    # Bias eviction slightly toward window to curb scans
    if scoreW <= (1.05 * scoreP + _EPS):
        return kW
    else:
        return kP

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Doorkeeper + TinyLFU update (decayed).
      - Move to MRU within its current segment (LRU order).
      - If in window (probation), promote to protected on first resident hit (SLRU).
    '''
    now = cache_snapshot.access_count
    key = obj.key
    size = max(1, int(obj.size))

    _admission_touch(key)

    _ensure_resident_metadata(now, key)
    m_last_access[key] = now
    m_resident_hits[key] = m_resident_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)

    # If metadata is missing (should not), infer from structures
    if seg not in (0, 1):
        seg = 0 if key in _W else 1
        m_segment[key] = seg

    if seg == 0:
        # Probation hit: promote to protected (standard SLRU)
        # Remove from window
        if key in _W:
            # Adjust bytes (size known)
            global _bytes_W, _bytes_P
            _W.pop(key, None)
            _bytes_W -= size
            if _bytes_W < 0:
                _bytes_W = 0
        # Add to protected MRU
        _P[key] = size
        _P.move_to_end(key, last=True)
        _bytes_P += size
        m_segment[key] = 1
        # Reset resident hits after promotion
        m_resident_hits[key] = 0
    else:
        # Protected hit: move to MRU
        if key in _P:
            _P.move_to_end(key, last=True)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Doorkeeper + TinyLFU update (decayed).
      - ARC-like adaptation: if key hits B1, increase window target; if hits B2, decrease it.
      - Initialize as probation (window) MRU, update bytes and metadata.
    '''
    now = cache_snapshot.access_count
    key = obj.key
    size = max(1, int(obj.size))

    _admission_touch(key)
    _clamp_target_window(cache_snapshot.capacity)

    removed_from = _ghost_remove_if_present(key)
    if removed_from == 'B1':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=True)
    elif removed_from == 'B2':
        _adjust_target_on_ghost_hit(cache_snapshot, hit_in_B1=False)

    # Insert into window MRU
    global _bytes_W
    # Clean any stale presence
    if key in _P:
        # Shouldn't happen on miss, but be robust
        sz = _P.pop(key)
        _bytes_P_local = sz if isinstance(sz, int) else size
        # Can't adjust bytes correctly without exact previous size, assume current size
        # but since this is rare, proceed to reset
    if key in _W:
        _W.pop(key, None)

    _W[key] = size
    _W.move_to_end(key, last=True)
    _bytes_W += size

    # Metadata
    m_segment[key] = 0
    m_last_access[key] = now
    m_resident_hits[key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (B1 for window, B2 for protected).
      - Remove from resident structures and metadata.
      - Sketch persists globally (decay handles aging).
    '''
    now = cache_snapshot.access_count
    ev_key = evicted_obj.key
    size = max(1, int(evicted_obj.size))

    seg = m_segment.get(ev_key, 0)
    in_W = ev_key in _W
    in_P = ev_key in _P
    # Remove from resident structures and adjust bytes
    global _bytes_W, _bytes_P
    if in_W:
        _W.pop(ev_key, None)
        _bytes_W -= size
        if _bytes_W < 0:
            _bytes_W = 0
        seg = 0
    elif in_P:
        _P.pop(ev_key, None)
        _bytes_P -= size
        if _bytes_P < 0:
            _bytes_P = 0
        seg = 1

    # Add to appropriate ghost list
    _ghost_add(cache_snapshot, ev_key, size, now, to_B1=(seg == 0))

    # Remove metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_resident_hits.pop(ev_key, None)
```
2025-11-27 22:02:13,146 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 15
- Hit Rate: 0.0214

Example 2:
- Trace ID: 0
- Hit Rate: 0.467

Example 3:
- Trace ID: 5
- Hit Rate: 0.4548

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:02:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:02:13,149 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:04:32 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:04:32,520 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:04:32,522 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Size-aware Windowed SLRU with TinyLFU (A-WSLRU-TinyLFU)
# - Global admission priority via Count-Min Sketch with periodic aging.
# - Three in-cache segments:
#       0 = window (recency buffer, LRU via timestamps)
#       1 = probation (main, frequency-biased)
#       2 = protected (main, frequent, most protected)
# - Eviction:
#       * If window byte-usage > target, evict LRU from window.
#       * Else evict the minimum TinyLFU/size score from probation; if empty, from protected.
#       * Fallback to global minimum score if main is empty.
# - Promotions on hit: window -> probation -> protected (SLRU).
# - Protected budget: dynamically enforced by demoting LRU protected to probation.
# - Window target adapts online via a simple hill-climbing scheme based on observed hits.
# - Score is size-aware and lightly recency-biased; protected receives a bonus multiplier.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window), 1 (probation), 2 (protected)
m_last_access = dict()   # key -> int (timestamp)

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (start values; some adapt online)
_win_frac = 0.20                # adaptive window fraction (bytes / capacity)
_PROT_FRAC_OF_MAIN = 0.80       # protected budget, as fraction of main (bytes)
_SIZE_ALPHA = 0.75              # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 0.5           # small recency multiplier impact
_PROTECTED_KEEP_BONUS = 2.0     # multiplier to make protected harder to evict
_EPS = 1e-9

# Hill-climbing adaptation for _win_frac
_HC_PERIOD = 20000
_hc_last_count = 0
_hc_last_hits = 0
_hc_last_rate = None
_hc_dir = 1      # +1 grow window, -1 shrink
_hc_step = 0.05  # step size; halves on regress; bounded to [0.01, 0.10]

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = 0

def _current_targets(cache_snapshot):
    # Compute dynamic byte budgets for segments
    cap = max(1, int(cache_snapshot.capacity))
    win_target = max(1, int(_win_frac * cap))
    main_target = max(0, cap - win_target)
    prot_target = int(_PROT_FRAC_OF_MAIN * main_target)
    return win_target, prot_target

def _priority_for_key(cache_snapshot, key, obj, now):
    # Size-aware TinyLFU with mild recency bias; protected gets a bonus.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency_mult = 1.0 + (_RECENCY_LAMBDA * (1.0 / (1.0 + float(age))))
    score = (max(1.0, freq) * recency_mult) / (float(size) ** _SIZE_ALPHA)
    seg = m_segment.get(key, 0)
    if seg == 2:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _pick_lru_key(cache_snapshot, keys):
    # Return key with oldest last access; tie-break by larger size to free more space.
    lru_key = None
    lru_ts = None
    lru_size = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        size = cache_snapshot.cache[key].size
        if (lru_key is None or ts < lru_ts or (ts == lru_ts and size > lru_size)):
            lru_key = key
            lru_ts = ts
            lru_size = size
    return lru_key

def _pick_min_score_key(cache_snapshot, keys, now):
    # Return key with minimum keep-score; tie-break by oldest ts, then larger size.
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in keys:
        obj = cache_snapshot.cache[key]
        score = _priority_for_key(cache_snapshot, key, obj, now)
        ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and obj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = obj.size
    return victim_key

def _collect_segment_bytes(cache_snapshot):
    # Compute bytes per segment and key lists
    win_bytes = prob_bytes = prot_bytes = 0
    win_keys = []
    prob_keys = []
    prot_keys = []
    for key, robj in cache_snapshot.cache.items():
        seg = m_segment.get(key, 0)
        if seg == 0:
            win_bytes += robj.size
            win_keys.append(key)
        elif seg == 1:
            prob_bytes += robj.size
            prob_keys.append(key)
        else:
            prot_bytes += robj.size
            prot_keys.append(key)
    return (win_bytes, prob_bytes, prot_bytes, win_keys, prob_keys, prot_keys)

def _enforce_protected_budget(cache_snapshot):
    # Demote LRU protected to probation until protected_bytes <= prot_target
    win_target, prot_target = _current_targets(cache_snapshot)
    # Repeat if multiple large objects push protected far beyond target.
    while True:
        prot_bytes = 0
        prot_lru_key = None
        prot_lru_ts = None
        prot_lru_size = None
        for key, robj in cache_snapshot.cache.items():
            if m_segment.get(key, 0) == 2:
                prot_bytes += robj.size
                ts = m_last_access.get(key, 0)
                if (prot_lru_key is None or ts < prot_lru_ts or
                    (ts == prot_lru_ts and robj.size > prot_lru_size)):
                    prot_lru_key = key
                    prot_lru_ts = ts
                    prot_lru_size = robj.size
        if prot_bytes <= prot_target or prot_lru_key is None:
            break
        # Demote LRU protected to probation
        m_segment[prot_lru_key] = 1

def _hc_maybe_adjust(cache_snapshot):
    # Hill-climb the window fraction based on recent hit rate gradient.
    # Adjust infrequently to avoid oscillations.
    global _hc_last_count, _hc_last_hits, _hc_last_rate, _hc_dir, _hc_step, _win_frac
    if cache_snapshot.access_count - _hc_last_count < _HC_PERIOD:
        return
    accesses = max(1, cache_snapshot.access_count - _hc_last_count)
    hits = max(0, cache_snapshot.hit_count - _hc_last_hits)
    rate = hits / accesses
    if _hc_last_rate is not None:
        if rate + 1e-4 < _hc_last_rate:
            # Regressed: reverse direction and reduce step
            _hc_dir *= -1
            _hc_step = max(0.01, _hc_step * 0.5)
    # Apply update and clamp
    _win_frac += _hc_dir * _hc_step
    _win_frac = max(0.05, min(0.50, _win_frac))
    # Snapshot for next period
    _hc_last_count = cache_snapshot.access_count
    _hc_last_hits = cache_snapshot.hit_count
    _hc_last_rate = rate

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - If window byte-usage > target, evict LRU among window keys (pure recency).
      - Else, evict smallest score among probation; if empty, among protected.
      - Fallback: global minimum score.
      - Tie-break by oldest last access, then largest size.
    Also enforces protected budget (may demote protected to probation).
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all current residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Enforce protected budget proactively
    _enforce_protected_budget(cache_snapshot)

    # Compute segment bytes and candidates
    win_bytes, prob_bytes, prot_bytes, win_keys, prob_keys, prot_keys = _collect_segment_bytes(cache_snapshot)
    win_target, prot_target = _current_targets(cache_snapshot)

    # If window exceeds budget, evict LRU from window
    if win_keys and win_bytes > win_target:
        return _pick_lru_key(cache_snapshot, win_keys)

    # Otherwise, evict by score from main (prefer probation)
    if prob_keys:
        return _pick_min_score_key(cache_snapshot, prob_keys, now)
    if prot_keys:
        return _pick_min_score_key(cache_snapshot, prot_keys, now)

    # Fallback: choose global minimum score (e.g., only window exists but not over budget)
    all_keys = list(cache_snapshot.cache.keys())
    if not all_keys:
        return None
    return _pick_min_score_key(cache_snapshot, all_keys, now)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU counts and decay.
      - Update last access timestamp.
      - Promote along SLRU path: window -> probation -> protected.
      - Enforce protected budget if exceeded.
      - Adapt window size via hill-climbing periodically.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Window hit: graduate to probation (candidate for frequency)
        m_segment[key] = 1
    elif seg == 1:
        # Probation hit: promote to protected
        m_segment[key] = 2
    # seg == 2: stay protected

    # Keep protected bounded
    _enforce_protected_budget(cache_snapshot)

    # Adapt window fraction periodically
    _hc_maybe_adjust(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Insert into window by default; very large objects go directly to probation to avoid blowing the window.
      - Initialize last access timestamp.
      - Adapt window size periodically.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_last_access[key] = cache_snapshot.access_count

    # Heuristic: very large objects bypass window to avoid evicting many recents.
    win_target, _ = _current_targets(cache_snapshot)
    if obj.size >= max(win_target, cache_snapshot.capacity // 2):
        m_segment[key] = 1  # probation
    else:
        m_segment[key] = 0  # window

    # Adapt window fraction periodically
    _hc_maybe_adjust(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists; decay handles aging.
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```
2025-11-27 22:08:09,044 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0274

Example 2:
- Trace ID: 14
- Hit Rate: 0.0222

Example 3:
- Trace ID: 19
- Hit Rate: 0.2695

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:08:09 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:08:09,045 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:11:13 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:11:13,804 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:11:13,805 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented TinyLFU with proper LRU maintenance and adaptive victim selection
# Key changes vs previous version:
# - Fix: Always move accessed keys to MRU in their segment deque (true LRU with lazy cleanup).
# - Eviction picks from a small set of oldest window candidates plus a small global sample,
#   using size-aware TinyLFU scoring. This keeps strong recency while avoiding bad picks.
# - Stronger protection of the protected segment; demotion keeps it within a byte budget.
# - Gentler, clearer "doomed" tagging and better large-object capping.
# - Smaller, safer tie-breaking and better admission guard.
#
# This version aims to reduce misses across diverse traces while remaining fast.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; we push MRU on every access; duplicates allowed)
_window_q = deque()        # left = LRU, right = MRU
_protected_q = deque()     # left = LRU, right = MRU

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first when possible
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (bytes) adaptive
_f_window = 0.30
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.07

# Sampling sizes
_WINDOW_OLDEST_K = 8        # how many distinct oldest window candidates to consider
_PROTECTED_OLDEST_K = 4     # for demotion sanity and rare eviction consideration
_SAMPLE_SIZE_GLOBAL = 10    # small global random sample
_SAMPLE_SIZE_LARGE = 14     # when large set is over cap

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.22
_EPS = 1e-9

# Admission-compare guard (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.6

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.50      # allow up to 50% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1  # promote large on first hit too (classic SLRU behavior)

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.25
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.10
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.70
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _collect_oldest_from_deque(dq, want_segment, k):
    # Collect up to k distinct, valid oldest keys from deque without mutating it.
    result = []
    seen = set()
    for key in dq:
        if key in seen:
            continue
        if key in m_key_index and m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
            if len(result) >= k:
                break
    return result

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # demoted becomes MRU of window
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    spins = 0
    while _g_protected_bytes > budget and spins < 16:
        if not _demote_one_from_protected(cache_snapshot):
            break
        spins += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Lower score is worse (i.e., better victim)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window(limit_scan=32):
    seen = set()
    scanned = 0
    for k in _window_q:
        if scanned >= limit_scan:
            break
        if k in seen:
            continue
        seen.add(k)
        if k in m_key_index and m_segment.get(k, 0) == 0:
            scanned += 1
            if k in m_doomed:
                return k
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget by demoting oldest protected into window.
      2) Prefer an oldest "doomed" window item if any (scan few oldest).
      3) Build a candidate set: oldest window keys (true LRU via deque) plus a small global sample.
      4) If large objects exceed cap, bias victims to large keys.
      5) Choose the weakest by size-aware TinyLFU score; strong bonus for protected.
      6) Admission guard: if chosen victim is much stronger than incoming, prefer oldest-window alternative.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Incoming estimate for admission compare
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try oldest doomed in window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)
            return victim_key  # doomed takes precedence

    # 2) Build base candidates: K oldest from window + small global random sample
    oldest_window = _collect_oldest_from_deque(_window_q, 0, _WINDOW_OLDEST_K)
    global_sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL, len(m_keys)))

    candidates = []
    # Always include oldest window first (strong recency)
    candidates.extend(oldest_window)
    # Supplement with a small global sample for rare cases when protected holds dead items
    for k in global_sample:
        if k not in candidates:
            candidates.append(k)

    # 3) Large overcap handling: if large set uses too many bytes, bias towards large keys
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        large_cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        # Prefer oldest large keys present in window among candidates
        for k in oldest_window:
            if k in m_large_keys and k not in large_cands:
                large_cands.append(k)
        if large_cands:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, large_cands, incoming_size, now)

    # 4) If not decided yet, choose from combined base candidates
    if victim_key is None:
        if not candidates:
            # Final fallback to the oldest window key
            victim_key = _peek_oldest_window()
        else:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim seems much stronger than incoming, pick the oldest window instead
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            # Prefer the worst among oldest window candidates (or simple head)
            if oldest_window:
                alt_key, alt_score = _select_victim_from_candidates(cache_snapshot, oldest_window, incoming_size, now)
                if alt_key is not None:
                    victim_key = alt_key
            else:
                alt = _peek_oldest_window()
                if alt is not None:
                    victim_key = alt

    if victim_key is None:
        # Global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(12, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU, decay, and average size.
      - Move the key to MRU in its current segment deque (fixes LRU behavior).
      - Update last access and resident hit counter.
      - Promote window->protected on first hit (doorkeeper), regardless of size.
      - Clear "doomed" tag upon hit.
      - Track hit location for adaptive window sizing and enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # If for any reason it's not registered, register into window
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Move to MRU in its deque (duplicates allowed; lazy cleanup ensures O(1) operations)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Any reuse clears "doomed" tag
    m_doomed.discard(key)

    # Promotion policy: promote to protected on first hit (SLRU behavior)
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected (MRU)
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Adapt window fraction periodically
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata (segment=window), timestamps, counters.
      - Append to window MRU (proper recency).
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction (ARC-like).
      - Mild scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window (probation)
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues (MRU)
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation (ARC-like)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Mild scan-shielding ("doomed" tagging)
    # Tag as doomed if it's clearly cold (freq <= 1), relatively large, and window is near/over target.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.9 * target_window))
    if (est_freq <= 1.0) and (size >= max(1.8 * _avg_size, _LARGE_RATIO * _avg_size * 0.6)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clear "doomed".
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets for adaptive window sizing
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 22:11:45,689 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 33
- Hit Rate: 0.6937

Example 2:
- Trace ID: 8
- Hit Rate: 0.5327

Example 3:
- Trace ID: 42
- Hit Rate: 0.0526

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:11:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:11:45,690 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:14:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:14:33,875 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:14:33,878 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Segmented W-TinyLFU with true LRU window/protected queues, size-aware scoring,
# adaptive window, scan shielding, and low-overhead global sampling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; we append on every hit/insert to refresh MRU)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of resident "large" objects

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Adaptive window fraction (bytes)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 24             # global candidates per eviction decision
_SAMPLE_SIZE_LARGE = 16       # large-set sampling
_SAMPLE_SIZE_GLOBAL_OLDEST = 24
_SAMPLE_SIZE_WINDOW_LOCAL = 5 # additional window candidates aside from the LRU head

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.6             # slightly stronger recency for scan shielding
_PROTECTED_KEEP_BONUS = 1.10 # protect items that earned promotion
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen (faster cadence)
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL_BASE = 1
_PROMOTE_HITS_LARGE_BASE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha_for_size(size):
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _dynamic_size_alpha_for_size(size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    size_alpha = _dynamic_size_alpha_for_size(size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # MRU in window after demotion
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    steps = 0
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # rotate to avoid rechecking too often
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction order:
      - Enforce protected budget via demotion of coldest protected (LRU) when needed.
      - Prefer oldest "doomed" item in the window if present.
      - Otherwise, evaluate a small set of window candidates: LRU head + few random windows by TinyLFU score.
      - If large objects exceed cap, sample within large-set for a weak victim.
      - Otherwise, sample globally and evict the weakest by size-aware TinyLFU score.
      - Admission-compare guard: if chosen victim is much stronger than incoming, fall back to the LRU window head.
      - Final fallback: pick globally oldest among a small sample; or any key if all else fails.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now)

    # 2) Window candidates: LRU head + a few random windows
    if victim_key is None:
        head = _peek_oldest_window()
        candidates = []
        if head is not None:
            candidates.append(head)
            extra = _sample_keys_from_segment(0, _SAMPLE_SIZE_WINDOW_LOCAL)
            # ensure uniqueness
            seen = set(candidates)
            for k in extra:
                if k not in seen:
                    candidates.append(k)
                    seen.add(k)
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, now)

    # 3) Large overcap handling
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, now)

    # 5) Admission guard: if the victim looks much stronger than incoming, take LRU window instead when possible
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try globally oldest by timestamp from a sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size; recency timestamp.
      - Append to appropriate deque (true LRU refresh).
      - Track hit location for adaptive window sizing.
      - Promotion: window->protected when hit threshold reached (dynamic threshold).
      - Clear "doomed" tag upon any reuse.
      - Enforce protected budget via demotion of oldest protected when needed.
      - Periodically adapt window fraction.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    if key not in m_key_index:
        # Safety: register missing as window
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += size
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += size
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # True LRU refresh: append to MRU of its current segment
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Any reuse removes "doomed" tag
    if key in m_doomed:
        m_doomed.discard(key)

    # Dynamic promotion threshold based on observed hit rate
    total_access = max(1, cache_snapshot.access_count)
    global_hit_rate = (cache_snapshot.hit_count / float(total_access)) if hasattr(cache_snapshot, 'hit_count') else 0.0
    tighten = (global_hit_rate < 0.22)
    promote_small = (_PROMOTE_HITS_SMALL_BASE + (1 if tighten else 0))
    promote_large = (_PROMOTE_HITS_LARGE_BASE + (1 if tighten else 0))

    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = promote_large if is_large else promote_small

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)  # mark MRU in protected

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Append to window MRU (true LRU maintenance).
      - Size classification and resident large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward where the key was last seen.
      - Scan shielding: tag "doomed" for clearly cold, relatively large inserts.
      - Advance adaptation ticker so tuning reacts over accesses, not only hits.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues (append to MRU)
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation (ARC-like)
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging)
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.9 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

    # Advance adaptation ticker on every access (inserts included)
    globals()['_adapt_ticker'] += 1
    _adapt_window_fraction_if_needed()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add evicted key to ghost history (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 22:14:40,761 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 31
- Hit Rate: 0.041

Example 2:
- Trace ID: 45
- Hit Rate: 0.069

Example 3:
- Trace ID: 35
- Hit Rate: 0.068

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:14:40 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:14:40,762 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:16:53 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:16:53,106 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:16:53,107 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented SLRU + TinyLFU with ghost feedback and safer admission
# Key improvements vs previous version:
# - True recency: append on every hit to the segment's deque (lazy-cleaned LRU).
# - Stronger bias to evict from probation/window; protected rarely chosen.
# - Less aggressive "doomed" tagging; also detect implicit doomed in eviction scan.
# - Tighter protection of hot items via stricter admission-compare guard.
# - Moderate tuning on weights and large-object cap.
# - Enforce protected budget on insert, too.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; duplicates allowed)
_window_q = deque()        # left = LRU, right = MRU
_protected_q = deque()     # left = LRU, right = MRU

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 24
_SAMPLE_SIZE_LARGE = 16
_SAMPLE_SIZE_GLOBAL_OLDEST = 20

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.75
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (lower -> more protective of hot resident)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.60      # allow up to 60% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window(cache_snapshot):
    # Try to find an oldest "doomed" (explicit or implicit) key in the window by scanning a few from the left.
    # Implicit doomed: very cold (freq<=1) and large.
    steps = 0
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
            # implicit check
            robj = cache_snapshot.cache.get(k, None)
            if robj is not None:
                size = int(robj.size)
                is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
                if is_large and _sketch_estimate(k) <= 1.0:
                    # treat as doomed (and tag to speed future decisions)
                    m_doomed.add(k)
                    return k
            # rotate this valid window key to age gently and continue scan
            _window_q.popleft()
            _window_q.append(k)
            steps += 1
            continue
        _window_q.popleft()
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (fast path first):
      1) Enforce protected budget via demotion of oldest protected.
      2) Prefer evicting an oldest "doomed" window item (explicit or implicit).
      3) Otherwise evict the oldest window item (true LRU of probation).
      4) If window empty or unavailable:
         - If large bytes exceed cap, sample large set and evict weakest.
         - Else sample globally and evict weakest by size-aware TinyLFU score.
      5) Admission-compare guard: if sampled victim is much stronger than incoming,
         prefer the oldest window item if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window(cache_snapshot)
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                vkey_l, vscore_l = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)
                victim_key, victim_score = vkey_l, vscore_l

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            vkey_g, vscore_g = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)
            victim_key, victim_score = vkey_g, vscore_g

    # 5) Admission guard: if sampled victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Always append to segment deque to record MRU (true LRU with lazy cleaning).
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: register as window by default if metadata missing
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Refresh recency in its current segment
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Conservative scan-shielding: tag "doomed" for very cold and large inserts only.
      - Enforce protected budget (in case promotions overshot).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Conservative scan-shielding ("doomed" tagging):
    # Tag as doomed if very cold (freq==0 or 1) and clearly large.
    est_freq = _sketch_estimate(key)
    if (est_freq <= 1.0) and is_large:
        m_doomed.add(key)
    else:
        m_doomed.discard(key)

    # Ensure protected doesn't exceed budget after this insertion
    _enforce_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 22:17:05,658 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 46
- Hit Rate: 0.4663

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 10
- Hit Rate: 0.0917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:17:05 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:17:05,659 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:19:21 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:19:21,457 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:19:21,461 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# SLRU + TinyLFU with Ghost-based Adaptation (S3-TLFU-GA)
# - Two in-cache segments: window (probation) and protected (frequent).
# - Global TinyLFU Count-Min Sketch with periodic decay.
# - Eviction via sampling with size-aware utility; window-biased to absorb scans.
# - Promotion from window to protected requires two hits (2Q-style) to reduce churn.
# - Ghost history (recently evicted keys) adapts window size ARC-style:
#     * re-referenced ghost from window -> grow window
#     * re-referenced ghost from protected -> shrink window
# - Compact per-resident metadata; sampling index enables O(1) random sampling.
# - Careful byte accounting and no double-counting on rare bootstrap paths.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_touch = dict()          # probation touch counters (for delayed promotion)

# Byte counters for segments
_g_window_bytes = 0
_g_protected_bytes = 0

# ------------------------
# TinyLFU Count-Min Sketch
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two for mask
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 10000
_decay_ticker = 0

# ------------------------
# Tunables
# ------------------------
# Window fraction (of bytes). Start small; adjust via ghost history.
_f_window = 0.10
_WINDOW_MIN = 0.02
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32
_SAMPLE_SIZE_DEMOTE = 24   # demotion candidates from protected

# Scoring weights
_SIZE_ALPHA = 1.0
_W_REC_WIN = 0.25          # recency weight in window
_W_REC_PROT = 0.8          # recency weight in protected
_PROTECTED_KEEP_BONUS = 1.25
_EPS = 1e-9

# Promotion policy
_PROMOTE_HITS = 2          # require 2 hits in window before promotion

# Ghost history (recently evicted keys) for adaptation
_GHOST_MAX = 10000         # max keys remembered
_ghost_seg = {}            # key -> segment at eviction (0 or 1)
_ghost_q = deque()         # LRU order of ghost keys

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
        if key not in m_touch:
            m_touch[key] = 0
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1) or globally (None).
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _ghost_record(key, seg):
    # Record evicted key for adaptation. Maintain bounded LRU list.
    _ghost_seg[key] = seg
    _ghost_q.append(key)
    while len(_ghost_q) > _GHOST_MAX:
        old = _ghost_q.popleft()
        _ghost_seg.pop(old, None)

def _ghost_check_and_adapt(key):
    # If the key is found in ghost, adapt window size ARC-style.
    global _f_window
    seg = _ghost_seg.pop(key, None)
    if seg is None:
        return
    # We purposely don't try to remove 'key' from _ghost_q (O(n)). Ghost entries
    # that are not in the map are skipped when they reach the head.
    if seg == 0:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)  # re-ref to probation -> grow window
    else:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)  # re-ref to protected -> shrink window

def _maybe_demote_from_protected(cache_snapshot):
    # If protected exceeds its budget (capacity - target_window), demote the stalest protected.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    # Choose oldest protected by last access
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size
        # Reset touch counter to 1 on demotion so another hit can re-promote quickly
        m_touch[oldest_key] = 1

def _priority_for_key(cache_snapshot, key, obj, now):
    # Utility per byte: (frequency + recency_weight * recency) / size^alpha
    # Protected items get a keep bonus multiplier.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    if m_segment.get(key, 0) == 0:
        score = (freq + _W_REC_WIN * recency) / (float(size) ** _SIZE_ALPHA)
    else:
        score = (freq + _W_REC_PROT * recency) / (float(size) ** _SIZE_ALPHA)
        score *= _PROTECTED_KEEP_BONUS
    return score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampling-based victim selection with window bias and size-aware TinyLFU utility.
    - Prefer evicting from window if window bytes exceed target.
    - Otherwise, sample mostly from window with a small portion from protected.
    - Victim is the candidate with lowest utility score; tie-break by oldest then largest.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    candidates = []
    n_res = len(m_keys)
    if n_res == 0:
        return None

    if restrict_to_window:
        k = min(_SAMPLE_SIZE, n_res)
        candidates = _sample_keys_from_segment(0, k)
    else:
        # Mix of window and protected to catch very stale protected entries rarely
        kw = int(_SAMPLE_SIZE * 0.75)
        kp = _SAMPLE_SIZE - kw
        kw = min(kw, n_res)
        kp = min(kp, n_res - min(kw, n_res))
        candidates = _sample_keys_from_segment(0, kw)
        candidates += _sample_keys_from_segment(1, kp)
        if not candidates:
            candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, n_res))

    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    if victim_key is None:
        # Degenerate fallback
        keys = list(cache_snapshot.cache.keys())
        return keys[0] if keys else None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Increment TinyLFU and decay if needed.
      - Update last access.
      - In window: increment touch counter; promote to protected on 2nd hit.
      - In protected: just refresh timestamp.
      - Keep protected within its budget by demoting oldest if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata stubs exist; do not adjust bytes here to avoid double-counting.
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_touch:
        m_touch[key] = 0

    seg = m_segment.get(key, 0)

    # Update last access
    m_last_access[key] = now

    if seg == 0:
        # probation: delayed promotion to reduce churn
        m_touch[key] = min(255, m_touch.get(key, 0) + 1)
        if m_touch[key] >= _PROMOTE_HITS:
            # Promote to protected
            m_segment[key] = 1
            m_touch[key] = 0
            sz = int(obj.size)
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz
            _maybe_demote_from_protected(cache_snapshot)
    else:
        # protected hit; nothing else needed
        pass

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Check ghost history for adaptation and adjust window fraction.
      - Increment TinyLFU for this access and decay if needed.
      - Initialize metadata: segment=window, last_access=now, touch=0.
      - Track resident key for O(1) sampling; update window bytes.
    '''
    _bootstrap_if_needed(cache_snapshot)

    # Adapt window fraction using ghost history
    _ghost_check_and_adapt(obj.key)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_touch[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Record the evicted key into ghost history for adaptation.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_touch.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record into ghost history for adaptation
    _ghost_record(ev_key, seg)
```
2025-11-27 22:19:36,569 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 30
- Hit Rate: 0.0456

Example 2:
- Trace ID: 22
- Hit Rate: 0.0587

Example 3:
- Trace ID: 16
- Hit Rate: 0.0201

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:19:36 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:19:36,570 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:22:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:22:23,838 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:22:23,839 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Simpler, Stronger Windowed SLRU with Size-aware TinyLFU (WSLRU-sTLFU)
# Design goals:
# - Simpler than previous version, focusing on robust, low-noise signals.
# - Two segments (probation/window and protected), using last-access timestamps as LRU.
# - TinyLFU for frequency; size-aware scoring to prefer byte-efficient retention.
# - Evict mostly from probation; protected is demoted only to stay within budget.
# - Large-object guard with gentle adaptation based on hit share.
# - Adaptive window fraction based on where hits occur.
# - Lightweight metadata, minimal sampling to avoid timeouts.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (probation/window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys for sampling
m_key_index = dict()       # key -> index in m_keys

# Size classes and byte accounting
m_large_keys = set()       # keys classified "large" at insertion time
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # bytes of objects in m_large_keys

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192           # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (bytes) – adaptive
_f_window = 0.30
_WINDOW_MIN = 0.10
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.02

# Large-object cap (fraction of bytes) – adaptive by hit share
_LARGE_RATIO = 2.0                  # size >= 2x avg -> "large"
_dyn_large_frac = 0.50

# Sampling sizes
_S_SAMPLE_WEAK = 48                 # generic candidate count
_S_SAMPLE_OLDEST = 32               # for LRU-ish oldest selection
_S_SAMPLE_DEMOTE = 16               # demotion from protected
_S_SAMPLE_LARGE = 32                # large-object victim candidates

# Adaptation accounting
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _avg_size, _g_window_bytes, _g_protected_bytes, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        # Default segment/window
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Classify large by current avg size
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        if size >= _LARGE_RATIO * max(1.0, _avg_size):
            if key not in m_large_keys:
                m_large_keys.add(key)
                _g_large_bytes += size
    _bootstrapped = True

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_window_target_bytes(cache_snapshot):
    cap = cache_snapshot.capacity
    return max(1, int(_f_window * cap))

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _sample_segment(segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = 8 * k
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == segment:
            result.append(key)
            seen.add(key)
        tries += 1
    return result

def _sample_any(k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if k >= n:
        return list(m_keys)
    idxs = random.sample(range(n), k)
    return [m_keys[i] for i in idxs]

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Only resident
    cand = [k for k in keyset if k in m_key_index]
    if not cand:
        return []
    if len(cand) <= k:
        return list(cand)
    return random.sample(cand, k)

def _pick_oldest(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _size_alpha_for(size):
    # Size-aware value exponent: slightly penalize larger objects
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.15
    elif r >= 2.0:
        return 1.05
    elif r <= 0.5:
        return 0.90
    else:
        return 1.0

def _value_score_for_key(cache_snapshot, key):
    robj = cache_snapshot.cache.get(key, None)
    if robj is None:
        return None
    size = max(1, int(robj.size))
    freq = _sketch_estimate(key)
    alpha = _size_alpha_for(size)
    # Add a small constant to avoid zero-score; frequency dominates
    return (freq + 0.25) / (float(size) ** alpha)

def _pick_weakest_by_value(cache_snapshot, keys):
    # Choose lowest value score; tie-breaker: older timestamp, then larger size to free space if needed.
    weakest_key = None
    weakest_val = None
    weakest_ts = None
    weakest_size = None
    for key in keys:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        val = _value_score_for_key(cache_snapshot, key)
        if val is None:
            continue
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        if (weakest_key is None or
            val < weakest_val or
            (val == weakest_val and (ts < weakest_ts or (ts == weakest_ts and sz > (weakest_size or 0))))):
            weakest_key = key
            weakest_val = val
            weakest_ts = ts
            weakest_size = sz
    return weakest_key

def _demote_one_if_protected_over_budget(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    cand = _sample_segment(1, _S_SAMPLE_DEMOTE)
    if not cand:
        return
    k = _pick_oldest(cand)
    if k is None:
        return
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return
    sz = int(robj.size)
    if m_segment.get(k, 0) == 1:
        m_segment[k] = 0
        _g_protected_bytes -= sz
        _g_window_bytes += sz

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    if _adapt_ticker < _ADAPT_PERIOD:
        return

    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Large-object cap adaptation: gently steer toward observed usefulness
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects underperform their current share, reduce cap; else slightly increase
        if frac_large_hits < _dyn_large_frac - 0.05:
            _dyn_large_frac = max(0.25, _dyn_large_frac - 0.03)
        elif frac_large_hits > _dyn_large_frac + 0.05:
            _dyn_large_frac = min(0.60, _dyn_large_frac + 0.02)

    # Reset
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction strategy (per-byte capacity):
      - Maintain protected budget by occasional demotion of oldest protected.
      - Enforce large-object cap if exceeded (prefer evicting large, oldest/weakest).
      - If probation/window is above its target bytes, evict its oldest (LRU-ish).
      - Otherwise evict from probation chosen by lowest TinyLFU value; if empty, from protected oldest.
      - Bias to evict large objects when incoming is large to reduce chained evictions.
    """
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # 1) Maintain protected budget via demotion
    _demote_one_if_protected_over_budget(cache_snapshot)

    # 2) Large-object guard
    large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
    incoming_is_large = _is_large_size(incoming_size)
    if _g_large_bytes > large_cap or (incoming_is_large and _g_large_bytes > 0):
        # Prefer large in probation/window; else any large
        cand_large_w = _sample_from_keyset(m_large_keys, _S_SAMPLE_LARGE)
        if cand_large_w:
            # Partition to probation first
            cand_prob = [k for k in cand_large_w if m_segment.get(k, 0) == 0]
            if cand_prob:
                victim = _pick_oldest(cand_prob)
                if victim is None:
                    victim = _pick_weakest_by_value(cache_snapshot, cand_prob)
                if victim is not None:
                    return victim
            # Else pick weakest among large candidates globally
            victim = _pick_weakest_by_value(cache_snapshot, cand_large_w)
            if victim is not None:
                return victim

    # 3) If probation/window exceeds target, evict its oldest (to keep recency buffer)
    target_window = _current_window_target_bytes(cache_snapshot)
    if _g_window_bytes > target_window:
        cand = _sample_segment(0, _S_SAMPLE_OLDEST)
        victim = _pick_oldest(cand)
        if victim is not None:
            return victim

    # 4) Otherwise, evict from probation by weakest value; if empty, oldest from protected
    cand_prob = _sample_segment(0, _S_SAMPLE_WEAK)
    if cand_prob:
        victim = _pick_weakest_by_value(cache_snapshot, cand_prob)
        if victim is not None:
            return victim

    # If no probation items found (corner case), evict oldest from protected
    cand_prot = _sample_segment(1, _S_SAMPLE_OLDEST)
    victim = _pick_oldest(cand_prot)
    if victim is not None:
        return victim

    # Final fallback
    if m_keys:
        return m_keys[0]
    return None

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access and increment resident hit counter.
      - Promote from probation to protected on hit (doorkeeper): require more confirmations for very large.
      - Track hit location for window-fraction adaptation; track large-hit share for cap adaptation.
      - Maintain protected budget by occasional demotion of oldest protected.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure metadata presence
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Adaptation counters
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access & hits
    m_last_access[key] = now
    m_hits_resident[key] = min(3, m_hits_resident.get(key, 0) + 1)

    # Simple, size-aware doorkeeper promotion
    if seg == 0:
        # Required confirmations increase with size
        r = float(size) / max(1.0, _avg_size)
        needed = 1 if r < 2.0 else (2 if r < 4.0 else 3)
        if m_hits_resident[key] >= needed:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= size
            globals()['_g_protected_bytes'] += size

    # Keep protected within budget
    _demote_one_if_protected_over_budget(cache_snapshot)

    # Periodic adaptation of window/large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=probation (window), time fields, hit counter.
      - Update byte counters and large-classification.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # probation/window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Bytes counters
    globals()['_g_window_bytes'] += size

    # Large classification at insertion
    if _is_large_size(size):
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting.
    """
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size

    # Nudge adaptation ticker; adjust periodically elsewhere
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```
2025-11-27 22:22:44,767 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Windowed, Size-aware TinyLFU (WS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (recency) and "protected" (frequency).
# - Eviction prefers window when it grows beyond its byte budget; otherwise, evicts
#   the item with the smallest TinyLFU/size priority, with a bias to keep protected items.
# - Per-resident metadata is compact: segment id and last access timestamp only.

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()       # key -> 0 (window) or 1 (protected)
m_last_access = dict()   # key -> int (timestamp)
# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192     # power of two (for fast masking)
_SKETCH_MASK = _SKETCH_WIDTH - 1
# 16-bit counters are enough under periodic decay; Python int is unbounded anyway.
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
# Fixed salts for hash mixing (deterministic)
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000        # decay every N observed accesses (hits + misses)
_decay_ticker = 0            # increments on every access (hit or insert)
# Tunables
_WINDOW_FRACTION = 0.20      # target fraction of capacity (in bytes) for window segment
_SIZE_ALPHA = 1.0            # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.5        # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.5  # factor to multiplicatively boost protected items' score
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    # Combine Python's hash with a salt; map to [0, _SKETCH_WIDTH)
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    # TinyLFU estimate: min over CM rows
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    # Return as float (for scoring math)
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        # age by halving all counters (right shift 1)
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                # use integer halving; keeps counters small and recent
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    # If an item is resident but we lack metadata (e.g., preloaded), assign defaults.
    if key not in m_segment:
        m_segment[key] = 0  # start in window by default
    if key not in m_last_access:
        m_last_access[key] = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Windowed, size-aware TinyLFU priority (higher is better to keep).
    # score = (F * (1 + lambda * recency)) / size^alpha
    # Eviction picks the smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))  # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected score has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count

    # Ensure metadata exists for all residents
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute window/protected byte usage and collect candidates
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    target_window = max(1, int(_WINDOW_FRACTION * cache_snapshot.capacity))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Scan to find victim
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue  # skip protected while window is over budget

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key)
    m_last_access[key] = cache_snapshot.access_count

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    m_segment[key] = 0  # window
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 12
- Hit Rate: 0.0675

Example 2:
- Trace ID: 25
- Hit Rate: 0.3337

Example 3:
- Trace ID: 6
- Hit Rate: 0.2679

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:22:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:22:44,768 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:25:05 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:25:05,260 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:25:05,261 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive Windowed, Size-aware TinyLFU with ARC-style ghosts (AWS-TinyLFU)
# - Global TinyLFU admission (Count-Min Sketch) with periodic decay.
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Dynamically adjusts the target window size using ARC-style ghost lists:
#     * Reuse from window-ghost -> grow window (favor recency)
#     * Reuse from protected-ghost -> shrink window (favor frequency)
# - Size-aware scoring; protected entries get a keep bonus.
# - Gentle youth bias to avoid premature eviction of very new items.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (last access timestamp)
m_insert_time = dict()    # key -> int (insert timestamp)

# ------------------------
# Global TinyLFU Count-Min Sketch
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192   # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

_DECAY_PERIOD = 20000    # decay counters every N accesses
_decay_ticker = 0

# ------------------------
# Ghost lists (ARC-style) to adapt window target
# ------------------------
_gw = OrderedDict()  # window ghost (recently evicted from window)
_gp = OrderedDict()  # protected ghost (recently evicted from protected)

# ------------------------
# Tunables
# ------------------------
_MIN_WINDOW_FRAC = 0.08
_MAX_WINDOW_FRAC = 0.70
_INITIAL_WINDOW_FRAC = 0.20
_ADAPT_RATE = 0.6            # fraction of step applied when adapting window target
_SIZE_ALPHA = 0.80           # score denominator: size^alpha
_RECENCY_LAMBDA = 2.0        # immediate recency multiplier scale
_YOUTH_LAMBDA = 0.75         # youth multiplier scale (protect very young)
_PROTECTED_KEEP_BONUS = 2.2  # multiplicative keep bonus for protected
_EPS = 1e-9

_target_window_bytes = None  # lazy-initialized on first use

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_resident_metadata(cache_snapshot, key):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_insert_time:
        m_insert_time[key] = cache_snapshot.access_count

def _clamp(x, lo, hi):
    if x < lo: return lo
    if x > hi: return hi
    return x

def _ensure_window_target(cache_snapshot):
    global _target_window_bytes
    cap = max(1, int(cache_snapshot.capacity))
    if _target_window_bytes is None:
        _target_window_bytes = int(_INITIAL_WINDOW_FRAC * cap)
    # Keep within bounds if capacity changes
    lo = int(_MIN_WINDOW_FRAC * cap)
    hi = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(int(_target_window_bytes), lo, hi)

def _ghost_touch(gdict, key, now):
    # Put/move key to MRU position
    gdict[key] = now
    gdict.move_to_end(key)

def _ghost_remove(gdict, key):
    if key in gdict:
        try:
            del gdict[key]
        except KeyError:
            pass

def _trim_ghosts(cache_snapshot):
    # Combined limit proportional to current resident count (cheap & robust)
    # Keeps adaptation fresh without unbounded growth.
    limit = max(256, 4 * max(1, len(cache_snapshot.cache)))
    # Trim from the larger ghost first; pop LRU entries.
    while (len(_gw) + len(_gp)) > limit:
        if len(_gw) >= len(_gp):
            try:
                _gw.popitem(last=False)
            except KeyError:
                break
        else:
            try:
                _gp.popitem(last=False)
            except KeyError:
                break

def _adjust_window_target(cache_snapshot, key, obj):
    # ARC-style adaptation:
    # - If key was in window-ghost, increase window target (favor recency).
    # - If key was in protected-ghost, decrease window target (favor frequency).
    global _target_window_bytes
    _ensure_window_target(cache_snapshot)
    cap = max(1, int(cache_snapshot.capacity))
    lo = int(_MIN_WINDOW_FRAC * cap)
    hi = int(_MAX_WINDOW_FRAC * cap)
    step = max(1, int(obj.size))  # scale step by object size (byte-based cache)
    if key in _gw:
        _ghost_remove(_gw, key)
        _target_window_bytes = _clamp(int(_target_window_bytes + _ADAPT_RATE * step), lo, hi)
    elif key in _gp:
        _ghost_remove(_gp, key)
        _target_window_bytes = _clamp(int(_target_window_bytes - _ADAPT_RATE * step), lo, hi)

def _priority_for_key(cache_snapshot, key, obj, now):
    # Higher score means stronger to keep; eviction picks smallest score.
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)

    # Immediate recency
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))          # [~0, 1]
    rec_mult = 1.0 + (_RECENCY_LAMBDA * recency)

    # Youth bias (protect very fresh inserts briefly)
    ins_age = max(0, now - m_insert_time.get(key, now))
    youth_mult = 1.0 + (_YOUTH_LAMBDA * (1.0 / (1.0 + float(ins_age))))

    score = (freq * rec_mult * youth_mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items get a multiplicative keep bonus
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      1) If window byte-usage > target, evict the minimum-score key among window only.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Tie-break by oldest last access, then by largest size to release more space.
    '''
    now = cache_snapshot.access_count
    _ensure_window_target(cache_snapshot)

    # Ensure metadata exists for all residents (handles any preloaded keys)
    for key in cache_snapshot.cache:
        _ensure_resident_metadata(cache_snapshot, key)

    # Compute current window usage
    window_bytes = 0
    has_window = False
    for key, robj in cache_snapshot.cache.items():
        if m_segment.get(key, 0) == 0:
            window_bytes += robj.size
            has_window = True

    restrict_to_window = has_window and (window_bytes > int(_target_window_bytes))

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    # Scan all residents (or window-only if over budget)
    for key, robj in cache_snapshot.cache.items():
        if restrict_to_window and m_segment.get(key, 0) != 0:
            continue

        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) with decay.
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Remove from ghosts if present (reuse signal consumed).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    _ensure_resident_metadata(cache_snapshot, key)

    m_last_access[key] = now

    seg = m_segment.get(key, 0)
    if seg == 0:
        # First reuse after insert: promote to protected
        m_segment[key] = 1

    # Reuse invalidates any ghost membership
    _ghost_remove(_gw, key)
    _ghost_remove(_gp, key)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access (with decay).
      - ARC-style adaptation using ghost lists to tune window target.
      - If key was in protected-ghost, insert directly into protected; else window.
      - Initialize resident metadata: segment, last_access, insert_time.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    _ensure_window_target(cache_snapshot)

    # Adapt window target based on ghost hit and clear ghost entry
    _adjust_window_target(cache_snapshot, key, obj)

    # If key was in protected ghost, double-hit indication; place into protected directly
    if key in _gp:
        seg = 1
        _ghost_remove(_gp, key)
    else:
        seg = 0
        _ghost_remove(_gw, key)

    m_segment[key] = seg
    m_last_access[key] = now
    m_insert_time[key] = now

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to the appropriate ghost (window or protected) for adaptation.
      - Trim ghost sizes.
      - Remove resident metadata for the evicted key.
      - Count-Min sketch is global and persists (natural decay handles aging).
    '''
    ev_key = evicted_obj.key
    now = cache_snapshot.access_count

    # Determine which segment it belonged to before eviction
    seg = m_segment.get(ev_key, 0)

    # Add to appropriate ghost list (LRU order)
    if seg == 1:
        _ghost_touch(_gp, ev_key, now)
        _ghost_remove(_gw, ev_key)
    else:
        _ghost_touch(_gw, ev_key, now)
        _ghost_remove(_gp, ev_key)

    _trim_ghosts(cache_snapshot)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_insert_time.pop(ev_key, None)
```
2025-11-27 22:25:25,380 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 38
- Hit Rate: 0.1255

Example 2:
- Trace ID: 2
- Hit Rate: 0.4603

Example 3:
- Trace ID: 29
- Hit Rate: 0.0386

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:25:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:25:25,381 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:29:00 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:29:00,975 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:29:00,976 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Fast Size-aware SLRU + TinyLFU with adaptive window, stale demotion, and smarter sampling
#
# Key changes vs previous version:
# - Smarter priority: slightly less punitive size scaling (alpha=0.7) to reduce thrashing on large-but-popular items.
# - Dynamic promotion on hit:
#     * Promote from window->protected either by hit count OR by TinyLFU frequency threshold.
# - Stale demotion:
#     * Old, cold entries in protected are demoted back to window to prevent protected from "freezing".
# - Window adaptation refinements:
#     * Keep ARC-style ghost adaptation but smooth the step and only demote from protected to raise window share.
# - Eviction sampling:
#     * Slightly larger samples; aware of very large incoming objects; bias eviction to window unless the incoming object is frequent.
# - Metadata hygiene:
#     * Safer counters, consistent on promotions/demotions/evictions, and lazy resident decay on global TinyLFU decay.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.90
_WINDOW_FRAC_STEP = 0.01
_window_frac = 0.25

# Size-aware priority divisor exponent (smaller => less penalty to large items)
_SIZE_ALPHA = 0.7

# Recency multipliers per segment (stronger in window)
_REC_WIN_LAMBDA = 5.0
_REC_PROT_LAMBDA = 1.2

# Extra protection multiplier baseline for protected segment
_PROTECTED_KEEP_BONUS = 1.20

# Promotion policy: larger objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => more hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 3
# Also promote if TinyLFU estimate reaches at least this value
_PROMOTE_TLFU_MIN = 2.0

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.85
_FREQ_RESIDENT_WEIGHT = 0.15

# Eviction sampling
_SAMPLE_WIN = 8
_SAMPLE_PROT = 6

# Demotion policy for stale protected entries
_DEMOTE_AGE = 2000            # accesses since last touch to consider stale
_DEMOTE_MAX_PER_REBALANCE = 2 # avoid too much churn per op

# Large incoming object handling
_LARGE_INCOMING_FRAC = 0.10   # >=10% of capacity considered large

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target (more recency needed)
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target (more frequency needed)
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _recency_weight(age, lam):
    # Transform age into a smooth 0..1 recency
    # rec = 1/(1+age) yields fast drop; keep it simple and fast.
    recency = 1.0 / (1.0 + float(max(0, age)))
    return 1.0 + lam * recency

def _effective_protected_bonus(age):
    # Protected bonus diminishes with staleness
    # When very recent, ≈ PROTECTED_KEEP_BONUS; when old, tends to 1.0
    rec = 1.0 / (1.0 + float(max(0, age)))
    return 1.0 + ( (_PROTECTED_KEEP_BONUS - 1.0) * (rec) )

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher => better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * recency_mult) / size^alpha
    # protected gets bonus that fades with age
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    rec_mult = _recency_weight(age, _REC_WIN_LAMBDA if seg == 0 else _REC_PROT_LAMBDA)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _effective_protected_bonus(age)
    return base

def _sample_oldest_keys(od, k):
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _maybe_demote_stale_protected(cache_snapshot, target_window_bytes):
    # Demote at most a few oldest protected items if window is under target
    # and protected entries are clearly stale/low value.
    global _prot_bytes, _win_bytes
    if not _prot_q:
        return
    slack = int(0.02 * max(1, int(cache_snapshot.capacity)))
    need = max(0, target_window_bytes - _win_bytes - slack)
    if need <= 0:
        return
    now = cache_snapshot.access_count
    moved = 0
    # check oldest protected entries and demote the stalest ones
    for key in list(_prot_q.keys()):
        if moved >= _DEMOTE_MAX_PER_REBALANCE or _win_bytes >= target_window_bytes:
            break
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        age = now - m_last_access.get(key, now)
        if age < _DEMOTE_AGE:
            break  # entries get more recent as we go newer; stop early
        # also ensure it's cold by frequency
        if _sketch_estimate(key) <= 1.0 and m_res_hits.get(key, 0) <= 1:
            # demote: protected -> window (MRU)
            _prot_q.pop(key, None)
            _prot_bytes = max(0, _prot_bytes - int(robj.size))
            m_segment[key] = 0
            if key in _win_q:
                _win_q.move_to_end(key, last=True)
            else:
                _win_q[key] = None
            _win_bytes += int(robj.size)
            moved += 1

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim using size-aware, frequency-aware, and recency-aware priority.
    - If window bytes exceed target (or incoming is large and not frequent), prefer evicting from window.
    - Otherwise, sample oldest from both segments and evict the minimum-priority entry.
    - Slight stale-demotion applied opportunistically for protected items.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    cap = max(1, int(cache_snapshot.capacity))
    target_window = max(1, int(_window_frac * cap))
    slack = int(0.02 * cap)

    # Opportunistically demote stale protected to keep window viable if undersized
    _maybe_demote_stale_protected(cache_snapshot, target_window)

    window_bytes = _win_bytes
    has_window = len(_win_q) > 0

    # Incoming awareness: if incoming is very large and not frequent, avoid touching protected
    incoming_size = max(1, int(getattr(obj, 'size', 1)))
    incoming_est = _sketch_estimate(getattr(obj, 'key', None))
    incoming_is_large = incoming_size >= int(_LARGE_INCOMING_FRAC * cap)
    incoming_is_frequent = incoming_est >= _PROMOTE_TLFU_MIN

    restrict_to_window = False
    if has_window and (window_bytes > target_window + slack):
        restrict_to_window = True
    elif incoming_is_large and not incoming_is_frequent and has_window:
        restrict_to_window = True

    candidates = []
    if restrict_to_window:
        for k in _sample_oldest_keys(_win_q, _SAMPLE_WIN):
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        for k in _sample_oldest_keys(_win_q, _SAMPLE_WIN):
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in _sample_oldest_keys(_prot_q, _SAMPLE_PROT):
            if k in cache_snapshot.cache:
                candidates.append(k)
        if not candidates and cache_snapshot.cache:
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment TinyLFU with decay; decay resident counters alongside.
    - Update last_access and resident hit counter.
    - LRU-move to MRU in its segment.
    - Promote from window to protected if:
        * hits >= threshold for its size OR
        * TinyLFU estimate >= _PROMOTE_TLFU_MIN.
    - Mild rebalancing: demote stale protected to window if window is undersized.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Window hit: recency update
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Promotion check: size-aware and frequency-aware
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        freq_est = _sketch_estimate(key)
        if m_res_hits.get(key, 0) >= need or freq_est >= _PROMOTE_TLFU_MIN:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            size = int(getattr(cache_snapshot.cache.get(key, obj), 'size', obj.size))
            global _win_bytes, _prot_bytes
            _win_bytes = max(0, _win_bytes - size)
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

    # Light rebalance: if window bytes far under target, demote a few stale protected
    cap = max(1, int(cache_snapshot.capacity))
    target_window = max(1, int(_window_frac * cap))
    _maybe_demote_stale_protected(cache_snapshot, target_window)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
    - Increment TinyLFU with decay; decay resident counters alongside.
    - Initialize resident metadata in window/probation.
    - Insert into window MRU and update bytes counters.
    - ARC-style adaptive window via ghost lists.
    - Mild rebalancing: if window is under target, demote some stale protected entries to window.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window sizing (ghost feedback)
    _adapt_window_on_insert(cache_snapshot, key)

    # Rebalance if window under target by demoting stale protected
    target_window = max(1, int(_window_frac * cap))
    _maybe_demote_stale_protected(cache_snapshot, target_window)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Record evicted key in corresponding ghost list (for adaptive sizing).
    - Remove from segment LRU and adjust bytes counters.
    - Remove resident metadata (TinyLFU persists globally).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Ghost record for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Clean resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 22:29:18,125 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight SLRU + TinyLFU with sampling (LT-SLRU-TLFU)
# Goals:
# - Stable hit rate with low CPU: two segments (window/protected) and TinyLFU admission
# - Size-aware scoring and eviction via small random samples
# - Simple adaptive window fraction driven by where hits occur
# - Avoid heavy sets/ghosts/large-tracking to eliminate timeouts

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # keep small to avoid CPU spikes
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_S_WIN_EVICT = 10          # candidates to pick eviction from window
_S_PROT_DEMOTE = 8         # candidates when demoting from protected
_S_GLOBAL = 12             # global candidates if needed

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.2
_PROTECTED_KEEP_BONUS = 1.10
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.06

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger (frees space faster)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy (lightweight):
      1) Keep protected within budget via single demotion if needed.
      2) Prefer eviction from window (probationary) using sampling + size-aware score.
      3) Fall back to global sampling if window is undersized.
      4) Admission-compare guard: if incoming weaker than victim, evict the oldest window key to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected budget: demote one protected if over budget
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Collect candidates: prefer window segment
    if _g_window_bytes > target_window:
        candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    else:
        # If window is small, sample globally
        candidates = _sample_keys_from_segment(None, _S_GLOBAL)

    if not candidates:
        # Fallback: try oldest window; else any key
        alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, max(1, _S_WIN_EVICT // 2)))
        if alt is not None:
            return alt
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-compare guard: if incoming is weaker, prefer evicting an old window key
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, _S_WIN_EVICT))
            if alt is not None:
                victim_key = alt

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size.
      - Update last access timestamp.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget with a single demotion if needed.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure key is indexed and in metadata
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Promote on first hit (simple SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within budget by demoting one if necessary
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust bytes accounting based on segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Error: Error - Timeout

Example 2:
- Trace ID: 3
- Hit Rate: 0.4198

Example 3:
- Trace ID: 11
- Hit Rate: 0.3752

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:29:18 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:29:18,126 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:31:32 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:31:32,065 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:31:32,068 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```
2025-11-27 22:34:50,676 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.051

Example 2:
- Trace ID: 4
- Hit Rate: 0.4692

Example 3:
- Trace ID: 11
- Hit Rate: 0.3535

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:34:50 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:34:50,678 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:37:34 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:37:34,837 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:37:34,838 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Segmented LRU + TinyLFU with adaptive window, size-aware priority, and
# age-sensitive protected bonus. Eviction uses adaptive sampling with stronger size
# discrimination and dynamic sampling amplification for large insertions.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192                 # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration (faster to adapt to phase changes)
# ------------------------
_DECAY_PERIOD = 5000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes)
_WINDOW_FRAC_MIN = 0.03
_WINDOW_FRAC_MAX = 0.85
_WINDOW_FRAC_STEP = 0.04
_window_frac = 0.25

# Size-aware priority divisor exponent (stronger size penalty)
_SIZE_ALPHA = 1.05

# Recency multipliers per segment (stronger in window)
_REC_WIN_LAMBDA = 6.0
_REC_PROT_LAMBDA = 0.8

# Protected bonus now scaled by recency (not constant)
_PROTECTED_KEEP_BONUS = 1.6  # multiplier for recency in protected

# Promotion policy: size-aware multi-hit for larger objects
_LARGE_PROMOTE_FRACTION = 0.10   # >10% of capacity => 3 hits to promote
_MED_PROMOTE_FRACTION = 0.02     # >2% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_MED = 2
_PROMOTE_HITS_LARGE = 3

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.85
_FREQ_RESIDENT_WEIGHT = 0.15

# Base eviction sampling (oldest few from each segment)
_SAMPLE_WIN_BASE = 8
_SAMPLE_PROT_BASE = 6

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            size = int(robj.size)
            if size >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
            elif size >= int(_MED_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_MED
            else:
                need = _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            size = int(obj.size)
            if size >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
            elif size >= int(_MED_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_MED
        m_promote_need[key] = need
    # Ensure presence in an LRU queue consistent with segment
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite keep score: larger => better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected receives an age-sensitive bonus: (1 + bonus * recency)
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= (1.0 + _PROTECTED_KEEP_BONUS * recency)  # age-sensitive protection
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _adaptive_sample_counts(cache_snapshot, obj):
    cap = max(1, int(cache_snapshot.capacity))
    new_sz = int(getattr(obj, 'size', 1))
    k_win = _SAMPLE_WIN_BASE
    k_prot = _SAMPLE_PROT_BASE
    # Heavier sampling when inserting large objects to find poor victims
    if new_sz >= int(0.20 * cap):
        k_win += 8
        k_prot += 6
    elif new_sz >= int(0.10 * cap):
        k_win += 4
        k_prot += 3
    elif new_sz >= int(0.05 * cap):
        k_win += 2
        k_prot += 2
    return k_win, k_prot

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority.
      - Sampling set sizes are increased for large incoming objects.
      - Tie-break by oldest last access, then by largest size, then by preferring window.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Determine adaptive sampling counts
    k_win, k_prot = _adaptive_sample_counts(cache_snapshot, obj)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        for k in _sample_oldest_keys(_win_q, k_win):
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        for k in _sample_oldest_keys(_win_q, k_win):
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in _sample_oldest_keys(_prot_q, k_prot):
            if k in cache_snapshot.cache:
                candidates.append(k)

        if not candidates and cache_snapshot.cache:
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    victim_seg = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        seg = m_segment.get(key, 0)

        better = False
        if victim_key is None:
            better = True
        else:
            if score < victim_score - _EPS:
                better = True
            elif abs(score - victim_score) <= _EPS:
                # tie-break: older last access first, then larger size, then prefer window victim
                if last_ts < victim_last_ts:
                    better = True
                elif last_ts == victim_last_ts:
                    if sz > victim_size:
                        better = True
                    elif sz == victim_size and seg == 0 and victim_seg == 1:
                        better = True

        if better:
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz
            victim_seg = seg

    # Fallback: evict oldest from window then protected, then any
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected when hitting required promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes = max(0, _win_bytes - size)
            _prot_bytes += size
    else:
        # Protected hit: recency update to MRU
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window size target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    size = int(obj.size)
    if size >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    elif size >= int(_MED_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_MED
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-27 22:37:46,487 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 37
- Hit Rate: 0.6419

Example 2:
- Trace ID: 33
- Hit Rate: 0.735

Example 3:
- Trace ID: 23
- Hit Rate: 0.0593

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:37:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:37:46,488 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:41:14 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:41:14,294 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:41:14,295 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Robust, size-aware SLRU with TinyLFU guidance, scan shielding, and need-aware eviction
# Key improvements:
#  - Need-aware eviction: when incoming is large or byte shortfall is big, sample both segments
#    and pick victims that minimize value-per-byte (eviction efficiency) to reduce cascading evictions.
#  - Stronger scan resistance: detect sustained streams of new unique misses and shrink window
#    aggressively, with stricter promotion during scan bursts.
#  - Safer protected handling: permit protected evictions only when necessary and with a higher penalty.
#  - Size-aware promotion thresholds and TinyLFU frequency with periodic decay retained.
#  - Ghost feedback and adaptive window retained, made more responsive.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.02
_WINDOW_MAX = 0.75
_WINDOW_STEP = 0.06

# Sampling
_SAMPLE_SIZE = 32            # candidates per decision from window
_SAMPLE_SIZE_PROT = 24       # candidates from protected when allowed
_SAMPLE_SIZE_OLDEST = 24     # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08      # base size penalty exponent
_W_FREQ = 1.0                # weight for frequency
_W_RECENCY = 2.8             # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_PENALTY = 1.22  # protected items harder to evict (penalty multiplier on score)
_EPS = 1e-9

# Admission-compare guard
_ADMIT_COMPARE_FACTOR = 1.10

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Scan detection
_recent_miss_new = deque()   # record if recent misses were "new" (not in ghosts)
_RECENT_MISS_WIN = 1536
_consec_misses = 0
_consec_hits = 0
_scan_mode_until = 0         # access_count threshold while scan-shielding is on

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(8 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size, now):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Additional strictness during scan shielding.
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    base = 1 if r <= 0.5 else (2 if r <= 2.0 else (3 if r <= 4.0 else 4))
    # In scan mode, be stricter by +1 (capped)
    extra = 1 if now <= _scan_mode_until else 0
    return min(5, base + extra)

def _need_bytes(cache_snapshot, incoming_obj):
    return max(0, cache_snapshot.size + int(incoming_obj.size) - cache_snapshot.capacity)

def _eviction_efficiency(score, size, seg, beta):
    # Lower is better to evict. Penalize protected eviction.
    penalty = _PROTECTED_KEEP_PENALTY if seg == 1 else 1.0
    return (score * penalty) / (float(size) ** max(0.0, beta))


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Need-aware eviction with TinyLFU-guided SLRU:
      - Prefer evicting from window (probation).
      - If required free bytes are large relative to window, allow sampling protected too.
      - Choose victim minimizing value-per-byte (eviction efficiency), with penalty for protected.
      - Maintain protected budget via demotion before selecting victims.
      - Scan shield: during detected scans, window kept minimal and promotions stricter.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    need = _need_bytes(cache_snapshot, obj)
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Decide whether to consider protected victims:
    # - if needed bytes exceed 85% of window bytes
    # - or incoming is large relative to capacity
    # - or window sampling is empty
    allow_protected = False
    if _g_window_bytes > 0:
        if need > 0 and (need >= 0.85 * _g_window_bytes or incoming_size >= 0.25 * cache_snapshot.capacity):
            allow_protected = True
    else:
        allow_protected = True

    # Build candidate set
    candidates = []

    # Window candidates
    w_cands = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    candidates.extend(w_cands)
    w_old = _window_oldest_candidate(cache_snapshot)
    if w_old is not None:
        candidates.append(w_old)

    # Protected candidates if allowed
    if allow_protected:
        p_cands = _sample_keys_from_segment(1, min(_SAMPLE_SIZE_PROT, len(m_keys)))
        candidates.extend(p_cands)
        p_old = _protected_oldest_candidate(cache_snapshot)
        if p_old is not None:
            candidates.append(p_old)

    # Fallback if no candidates
    if not candidates:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            return m_keys[0] if m_keys else None
        return gv

    # Evaluate eviction efficiency for each candidate
    # Increase beta (favor larger victims) when need is large
    need_ratio = 0.0
    if cache_snapshot.capacity > 0:
        need_ratio = min(1.0, float(need) / float(cache_snapshot.capacity * 0.30))
    beta = 0.30 + 0.50 * need_ratio  # from 0.30 up to 0.80 when need is big

    victim_key = None
    victim_eff = None
    victim_last_ts = None
    victim_size = None
    victim_score = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        seg = m_segment.get(key, 0)
        size = max(1, int(robj.size))
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        eff = _eviction_efficiency(score, size, seg, beta)
        last_ts = m_last_access.get(key, 0)

        # Prefer lower efficiency; tie-break by older, then by larger size (to satisfy need quicker)
        if (victim_key is None or
            eff < victim_eff - _EPS or
            (abs(eff - victim_eff) <= _EPS and (last_ts < victim_last_ts or
                                                (last_ts == victim_last_ts and size > victim_size)))):
            victim_key = key
            victim_eff = eff
            victim_last_ts = last_ts
            victim_size = size
            victim_score = score

    # Pollution guard: if best candidate looks significantly stronger than incoming,
    # bias to evict the oldest window item if available.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        return m_keys[0] if m_keys else None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay.
      - Update average size and last access timestamp.
      - Increment resident-hit counter; promote to protected when threshold reached (stricter in scan mode).
      - Track hit locations for adaptive window sizing and scan relief.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (defensive)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Scan detector relief: hits break miss streak
    globals()['_consec_hits'] = globals().get('_consec_hits', 0) + 1
    globals()['_consec_misses'] = 0

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Promotion from window -> protected (size-aware, stricter in scan mode)
    if seg == 0:
        thresh = _promotion_threshold(obj.size, now)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz
            # reset resident hits post-promotion to avoid runaway
            m_resfreq[key] = min(m_resfreq[key], 2)

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU and decay, update average size.
      - Initialize metadata: segment = window (probation) and last_access = now,
        except if protected-ghost and moderately frequent (and not very large) -> protected.
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction and remove from ghost sets.
      - Scan detection: track "new" misses and shrink window aggressively during streams.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Scan detection: is this miss "new" (not present in ghosts)?
    is_new = (key not in _ghost_w_set) and (key not in _ghost_p_set)
    _recent_miss_new.append(1 if is_new else 0)
    if len(_recent_miss_new) > _RECENT_MISS_WIN:
        _recent_miss_new.popleft()

    # Update miss/hit streaks
    globals()['_consec_misses'] = globals().get('_consec_misses', 0) + 1
    globals()['_consec_hits'] = 0

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    # Scan shielding: if stream of new misses dominates, shrink window and avoid protected placement
    # Enable shielding when: high ratio of "new" misses recently and consecutive misses exceed threshold
    if len(_recent_miss_new) >= 256:
        new_ratio = sum(_recent_miss_new) / float(len(_recent_miss_new))
    else:
        new_ratio = 0.0
    if new_ratio >= 0.80 and globals()['_consec_misses'] >= 128:
        # Enter scan mode for a while
        globals()['_scan_mode_until'] = now + 2000
        # Aggressively shrink window
        globals()['_f_window'] = max(_WINDOW_MIN, _f_window - 2 * _WINDOW_STEP)
        place_protected = False  # do not place scan items directly into protected

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 22:41:24,349 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 43
- Hit Rate: 0.35

Example 2:
- Trace ID: 44
- Error: Error - Timeout

Example 3:
- Trace ID: 18
- Hit Rate: 0.0226

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:41:24 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:41:24,350 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:44:16 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:44:16,774 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:44:16,775 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Fast, Size-aware Segmented-LFU with Doorkeeper, Large-guard and Lightweight Adaptation (FSLFU+L)
# Goals:
# - Reduce miss rate via TinyLFU-guided admission, SLRU-like window/protected design, and scan shielding.
# - Avoid timeouts by using small sketches, O(1) sampling structures, and limited per-access work.
# - Be byte-aware but not overly punitive to large objects; enforce a soft cap for large bytes.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
# Core resident metadata
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys for O(1) uniform sampling
m_key_index = dict()       # key -> index in m_keys

# Large object classification (resident-only)
m_large_keys = set()       # resident keys classified as "large" at insertion time
m_large_list = []          # list for O(1) sampling among large
m_large_index = dict()     # key -> index in m_large_list

# Doomed tagging (resident-only): prefer evicting these first
m_doomed = set()
m_doomed_list = []
m_doomed_index = dict()

# Byte accounting
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests) -- small for speed
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two, smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (lightweight)
_DECAY_PERIOD = 100000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.40
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.75
_WINDOW_STEP = 0.05

# Sampling sizes (kept modest for speed)
_SAMPLE_SIZE = 24
_SAMPLE_SIZE_DEMOTE = 12
_SAMPLE_SIZE_OLDEST = 10
_SAMPLE_SIZE_DOOMED = 16
_SAMPLE_SIZE_LARGE = 16

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.05      # slightly gentler size penalty
_W_FREQ = 1.0
_W_RECENCY = 1.2
_PROTECTED_KEEP_BONUS = 1.20
_EPS = 1e-9

# Admission guard factor
_ADMIT_COMPARE_FACTOR = 1.10

# Adaptation cadence
_ADAPT_PERIOD = 2000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history for ARC-like feedback
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and dynamic large cap
_LARGE_RATIO = 2.0
_dyn_large_frac = 0.35     # initial large bytes cap fraction

# Doorkeeper promotion thresholds (size-aware)
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000
_AGE_DEMOTE_SAMPLE = 6

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        if size >= _LARGE_RATIO * max(1.0, _avg_size):
            _add_to_large(key, size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.12)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_list(lst, k):
    if not lst or k <= 0:
        return []
    if len(lst) <= k:
        # Return a shallow copy to avoid accidental modification by caller
        return list(lst)
    return random.sample(lst, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score, then older, then larger size (frees more space)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Adjust window fraction by hit locality
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback
    total_ev = _evict_w + _evict_p
    if total_ev >= 30:
        if _evict_w > 1.6 * max(1, _evict_p):
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Large cap adaptation by large-hit share
    if _hits_total_all >= 400:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.18, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.50, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0

def _add_to_large(key, size):
    if key in m_large_keys:
        return
    m_large_keys.add(key)
    m_large_index[key] = len(m_large_list)
    m_large_list.append(key)
    globals()['_g_large_bytes'] += int(size)

def _remove_from_large(key, size):
    if key not in m_large_keys:
        return
    m_large_keys.discard(key)
    idx = m_large_index.pop(key, None)
    if idx is not None:
        last_idx = len(m_large_list) - 1
        last_key = m_large_list[last_idx]
        if idx != last_idx:
            m_large_list[idx] = last_key
            m_large_index[last_key] = idx
        m_large_list.pop()
    globals()['_g_large_bytes'] -= int(size)

def _doom_add(key):
    if key in m_doomed:
        return
    m_doomed.add(key)
    m_doomed_index[key] = len(m_doomed_list)
    m_doomed_list.append(key)

def _doom_remove(key):
    if key not in m_doomed:
        return
    m_doomed.discard(key)
    idx = m_doomed_index.pop(key, None)
    if idx is not None:
        last_idx = len(m_doomed_list) - 1
        last_key = m_doomed_list[last_idx]
        if idx != last_idx:
            m_doomed_list[idx] = last_key
            m_doomed_index[last_key] = idx
        m_doomed_list.pop()


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Keep protected under budget; age-demote stale protected items.
      2) Prefer evicting "doomed" items when present (scan shielding).
      3) If window is above its byte target, choose from window.
      4) Enforce/adapt large-bytes cap; if incoming is large, bias eviction toward large to avoid cascading evictions.
      5) Otherwise sample globally and evict the weakest by TinyLFU+recency+size score.
      6) Admission guard: if chosen victim appears stronger than incoming, evict an old window item instead.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    candidates = []
    # 1) Doomed first
    if m_doomed_list:
        candidates = _sample_from_list(m_doomed_list, _SAMPLE_SIZE_DOOMED)

    # 2) Window above target?
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    # 3) Large cap enforcement or large incoming bias
    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap and m_large_list:
            candidates = _sample_from_list(m_large_list, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_list:
            # Mix large-biased candidates with a few global ones
            large_cand = _sample_from_list(m_large_list, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    # 4) Global sampling fallback
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission guard: prefer to evict an old window item if incoming is weaker
    if victim_key is not None and victim_score is not None and incoming_score * _ADMIT_COMPARE_FACTOR <= victim_score:
        alt = _window_oldest_candidate(cache_snapshot)
        if alt is None:
            alt = _global_oldest_candidate(cache_snapshot)
        if alt is not None:
            victim_key = alt

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and EMA size.
      - Update last access and resident hit counter.
      - Size-aware doorkeeper promotion (window -> protected).
      - Clear "doomed" on sufficient reuse.
      - Track hit location for adaptation and large-hit share.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure resident indices (no double byte-accounting here)
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Adaptation counters
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear doomed if sufficiently reused
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        _doom_remove(key)

    # Doorkeeper: promote to protected when confirmed reused
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = max(1, needed_hits - 1)
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= size
            globals()['_g_protected_bytes'] += size

    # Keep protected healthy
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and EMA size.
      - Initialize metadata in window segment.
      - Classify as large at insertion (resident-only) and update large bytes/list.
      - Ghost-feedback to tune window fraction.
      - Scan-shielding: tag low-priority newcomers (especially large with low TinyLFU) as doomed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Byte accounting
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time (resident-only)
    is_large = _is_large_size(size)
    if is_large:
        _add_to_large(key, size)
    else:
        # ensure not listed as large if previously was
        if key in m_large_keys:
            _remove_from_large(key, size)

    # Ghost-feedback tuning
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed") via admission-compare heuristic
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(14, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    # Tag as doomed if clearly weak vs resident lower quartile, or if large and hardly seen
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        _doom_add(key)
    else:
        _doom_remove(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust byte counters.
      - Maintain large/doomed lists and bytes.
      - Add to ghost history with segment tag.
      - Track eviction origin for adaptation.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large/doomed accounting cleanup
    if ev_key in m_large_keys:
        _remove_from_large(ev_key, ev_size)
    _doom_remove(ev_key)

    # Ghost history
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Periodic adaptation ticks also after evictions
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```
2025-11-27 22:44:30,291 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight SLRU + TinyLFU with sampling (LT-SLRU-TLFU)
# Goals:
# - Stable hit rate with low CPU: two segments (window/protected) and TinyLFU admission
# - Size-aware scoring and eviction via small random samples
# - Simple adaptive window fraction driven by where hits occur
# - Avoid heavy sets/ghosts/large-tracking to eliminate timeouts

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # keep small to avoid CPU spikes
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_S_WIN_EVICT = 10          # candidates to pick eviction from window
_S_PROT_DEMOTE = 8         # candidates when demoting from protected
_S_GLOBAL = 12             # global candidates if needed

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.2
_PROTECTED_KEEP_BONUS = 1.10
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.06

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger (frees space faster)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy (lightweight):
      1) Keep protected within budget via single demotion if needed.
      2) Prefer eviction from window (probationary) using sampling + size-aware score.
      3) Fall back to global sampling if window is undersized.
      4) Admission-compare guard: if incoming weaker than victim, evict the oldest window key to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected budget: demote one protected if over budget
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Collect candidates: prefer window segment
    if _g_window_bytes > target_window:
        candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    else:
        # If window is small, sample globally
        candidates = _sample_keys_from_segment(None, _S_GLOBAL)

    if not candidates:
        # Fallback: try oldest window; else any key
        alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, max(1, _S_WIN_EVICT // 2)))
        if alt is not None:
            return alt
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-compare guard: if incoming is weaker, prefer evicting an old window key
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, _S_WIN_EVICT))
            if alt is not None:
                victim_key = alt

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size.
      - Update last access timestamp.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget with a single demotion if needed.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure key is indexed and in metadata
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Promote on first hit (simple SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within budget by demoting one if necessary
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust bytes accounting based on segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.891

Example 2:
- Trace ID: 3
- Hit Rate: 0.4231

Example 3:
- Trace ID: 6
- Hit Rate: 0.2715

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:44:30 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:44:30,292 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:47:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:47:33,853 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:47:33,854 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# WT-SLRU with TinyLFU scoring, LRU heaps, and light ARC-style adaptation
# Goals:
# - Strong recency via SLRU (window/protected) with true LRU heaps per segment
# - Frequency via TinyLFU for prioritization and admission guarding
# - Size-aware eviction scoring
# - Lightweight ghost history to adapt window fraction to workload
# - Low overhead: lazy-validated heaps and small sketches

import random
import heapq
from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys for O(1) sampling
m_key_index = dict()       # key -> index in m_keys
m_ver = dict()             # key -> integer version (heap entry validation)

# LRU heaps (min-heap by last_access)
_win_heap = []             # tuples: (ts, ver, key)
_prot_heap = []            # tuples: (ts, ver, key)

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# Ghost history for ARC-like adaptation (very lightweight)
_GHOST_MAX = 4096
_WIN_GHOST = OrderedDict()     # keys recently evicted from window
_PROT_GHOST = OrderedDict()    # keys recently evicted from protected

# Sampling sizes
_S_WIN_EVICT = 14          # candidates when evicting from window
_S_PROT_DEMOTE = 6         # candidates when demoting from protected (fallback)
_S_GLOBAL = 12             # global candidates if window undersized

# Scoring weights (favor frequency slightly more than recency)
_W_FREQ = 1.20
_W_RECENCY = 0.60
_PROTECTED_KEEP_BONUS = 1.05
_EPS = 1e-9

# Admission guard (frequency-based)
_KEEP_STRONGER = 1.15

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _heap_push_for_seg(key, seg, ts):
    v = m_ver.get(key, 0) + 1
    m_ver[key] = v
    tup = (ts, v, key)
    if seg == 0:
        heapq.heappush(_win_heap, tup)
    else:
        heapq.heappush(_prot_heap, tup)

def _heap_pop_oldest(seg):
    heap = _win_heap if seg == 0 else _prot_heap
    while heap:
        ts, v, key = heapq.heappop(heap)
        if key not in m_segment:
            continue
        if m_segment.get(key, -1) != seg:
            continue
        if m_ver.get(key, 0) != v:
            continue
        return key
    return None

def _heap_peek_oldest(seg):
    key = _heap_pop_oldest(seg)
    if key is None:
        return None
    # reinsert to maintain presence; last_access may be updated
    _heap_push_for_seg(key, seg, m_last_access.get(key, 0))
    return key

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # initialize heaps
        _heap_push_for_seg(key, m_segment.get(key, 0), m_last_access[key])
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_oldest_from_protected(cache_snapshot):
    # Demote strictly by LRU from protected
    key = _heap_pop_oldest(1)
    if key is None:
        # fallback to weak sample demote
        candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
        if not candidates:
            return
        now = cache_snapshot.access_count
        key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
        if key is None:
            return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size
        _heap_push_for_seg(key, 0, m_last_access.get(key, 0))

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _current_protected_budget_bytes(cache_snapshot):
    target_window = _current_window_target_bytes(cache_snapshot)
    return max(0, cache_snapshot.capacity - target_window)

def _ghost_record(ev_key, seg):
    # Record evicted key into the corresponding ghost, trim on overflow
    if seg == 0:
        if ev_key in _WIN_GHOST:
            _WIN_GHOST.pop(ev_key, None)
        _WIN_GHOST[ev_key] = 1
        if len(_WIN_GHOST) > _GHOST_MAX:
            _WIN_GHOST.popitem(last=False)
        # keep protected ghost from growing unbounded
        if len(_PROT_GHOST) > _GHOST_MAX:
            _PROT_GHOST.popitem(last=False)
    else:
        if ev_key in _PROT_GHOST:
            _PROT_GHOST.pop(ev_key, None)
        _PROT_GHOST[ev_key] = 1
        if len(_PROT_GHOST) > _GHOST_MAX:
            _PROT_GHOST.popitem(last=False)
        if len(_WIN_GHOST) > _GHOST_MAX:
            _WIN_GHOST.popitem(last=False)

def _arc_like_adapt_on_insert(key):
    # If key was recently evicted from window, increase window; if from protected, decrease window
    global _f_window
    if key in _WIN_GHOST:
        _WIN_GHOST.pop(key, None)
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    elif key in _PROT_GHOST:
        _PROT_GHOST.pop(key, None)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget via LRU demotion.
      - Prefer eviction from window (probation) using sampling plus include the LRU window key.
      - If window is empty/undersized, evict from protected LRU tail.
      - Frequency guard: avoid evicting a much stronger resident than the incoming.
      - Size-aware and recency-aware scoring for candidate selection.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget by LRU demotion if needed
    prot_budget = _current_protected_budget_bytes(cache_snapshot)
    if _g_protected_bytes > prot_budget:
        _demote_oldest_from_protected(cache_snapshot)

    # Primary: candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    # Always consider the true LRU window key (if any)
    lru_win = _heap_peek_oldest(0)
    if lru_win is not None and lru_win not in candidates:
        candidates.append(lru_win)

    if candidates:
        victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)
    else:
        # If window is empty or too small, evict from protected LRU
        alt = _heap_peek_oldest(1)
        if alt is not None:
            return alt
        # Fallback: global weakest among a small sample
        candidates = _sample_keys_from_segment(None, _S_GLOBAL)
        if not candidates:
            return m_keys[0] if m_keys else None
        victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    if victim_key is None:
        return m_keys[0] if m_keys else None

    # Frequency guard: avoid evicting a much stronger item
    f_in = _sketch_estimate(obj.key)
    f_vic = _sketch_estimate(victim_key)
    if f_vic >= _KEEP_STRONGER * max(1.0, f_in):
        # Try to evict the LRU window instead, if available and different
        if lru_win is not None and lru_win != victim_key:
            victim_key = lru_win

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size.
      - Update last access timestamp and segment LRU heap.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget with an LRU demotion if needed.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    m_last_access[key] = now
    seg = m_segment.get(key, 0)

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Promote on first hit
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size
        _heap_push_for_seg(key, 1, now)
    else:
        # Refresh protected LRU position
        _heap_push_for_seg(key, 1, now)

    # Keep protected within its budget via LRU demotion if needed
    prot_budget = _current_protected_budget_bytes(cache_snapshot)
    if _g_protected_bytes > prot_budget:
        _demote_oldest_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - ARC-like adaptation using ghost histories (if the missed key recently belonged to window/protected).
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting and window LRU heap.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _arc_like_adapt_on_insert(obj.key)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size
    _heap_push_for_seg(key, 0, now)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record into appropriate ghost history for adaptation.
      - Remove metadata and adjust bytes accounting based on segment.
      - Heaps are lazily cleaned via version checks.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_ver.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # record ghost for adaptation
    _ghost_record(ev_key, seg)
```
2025-11-27 22:47:46,495 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 20
- Hit Rate: 0.3736

Example 2:
- Trace ID: 7
- Hit Rate: 0.4953

Example 3:
- Trace ID: 13
- Hit Rate: 0.0

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:47:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:47:46,496 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:50:13 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:50:13,721 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:50:13,723 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-TinyLFU (GDS-TinyLFU) with byte-awareness and EMA sizing
# - Single priority queue (min-heap) over resident items
# - Priority(k) accumulates on accesses: Priority := max(Priority, L) + (1 + w_freq * est) / size^alpha
#   where L is global "age" (last evicted priority), est is TinyLFU frequency estimate (decayed)
# - Eviction picks the minimum-priority resident; L is advanced to evicted priority
# - Size-aware penalty uses dynamic exponent alpha based on average object size (EMA)
# - TinyLFU Count-Min Sketch is global across the stream with periodic decay
# - Robust to variable object sizes and patterns; avoids complex segment churn

import random

# ------------------------
# Global state and metadata
# ------------------------

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Average size EMA
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# Priority queue (min-heap) over resident keys
# We store only keys in the heap; priorities are in _prio map.
_heap = []                 # list of keys
_idx = dict()              # key -> heap index
_prio = dict()             # key -> current priority (float)
_seq = dict()              # key -> sequence number (monotonic, for tie-breaking)
_seq_counter = 0           # global sequence

# Global "age" for GreedyDual
_L = 0.0

# Bootstrap guard
_bootstrapped = False

# Tunables
_W_FREQ = 1.0
_EPS = 1e-12

# ------------------------
# Helpers: TinyLFU
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

# ------------------------
# Helpers: sizes and priority
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _size_alpha(size_bytes):
    # Dynamic size penalty exponent; penalize large objects slightly more, small slightly less
    r = float(max(1, int(size_bytes))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.30
    elif r >= 2.0:
        return 1.15
    elif r <= 0.5:
        return 0.95
    else:
        return 1.05

def _priority_increment(size_bytes, freq_est):
    alpha = _size_alpha(size_bytes)
    return (1.0 + _W_FREQ * float(freq_est)) / (float(max(1, int(size_bytes))) ** alpha)

def _priority_new(size_bytes, freq_est):
    # New placement uses base L plus additive increment
    return _L + _priority_increment(size_bytes, freq_est)

# ------------------------
# Helpers: heap (min-heap on _prio, tie by _seq older first)
# ------------------------
def _cmp_keys(a, b):
    pa = _prio.get(a, 0.0)
    pb = _prio.get(b, 0.0)
    if abs(pa - pb) > _EPS:
        return pa < pb
    # Older (smaller seq) should be evicted first on tie
    return _seq.get(a, 0) < _seq.get(b, 0)

def _heap_swap(i, j):
    ka, kb = _heap[i], _heap[j]
    _heap[i], _heap[j] = kb, ka
    _idx[kb] = i
    _idx[ka] = j

def _sift_up(i):
    while i > 0:
        p = (i - 1) >> 1
        if _cmp_keys(_heap[i], _heap[p]):
            _heap_swap(i, p)
            i = p
        else:
            break

def _sift_down(i):
    n = len(_heap)
    while True:
        l = (i << 1) + 1
        if l >= n:
            break
        r = l + 1
        smallest = l
        if r < n and _cmp_keys(_heap[r], _heap[l]):
            smallest = r
        if _cmp_keys(_heap[smallest], _heap[i]):
            _heap_swap(i, smallest)
            i = smallest
        else:
            break

def _heap_push(key):
    _idx[key] = len(_heap)
    _heap.append(key)
    _sift_up(_idx[key])

def _heap_remove_at(i):
    n = len(_heap)
    if i < 0 or i >= n:
        return
    last = n - 1
    k = _heap[i]
    if i != last:
        _heap_swap(i, last)
    _heap.pop()
    _idx.pop(k, None)
    if i < len(_heap):
        # After swap, fix position
        _sift_up(i)
        _sift_down(i)

def _heap_update_key(key, new_priority):
    # Insert if absent; else update priority and adjust
    global _seq_counter
    _prio[key] = float(new_priority)
    _seq_counter += 1
    _seq[key] = _seq_counter
    i = _idx.get(key, None)
    if i is None:
        _heap_push(key)
    else:
        # Decide direction
        # Compare with previous priority; if decreased -> sift_up; else sift_down
        # We don't store old priority explicitly; approximate by local compare with parent/children
        # Do both passes guarded to settle quickly.
        _sift_up(i)
        _sift_down(_idx.get(key, i))

# ------------------------
# Bootstrap
# ------------------------
def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _L, _seq_counter
    if _bootstrapped:
        return
    # Initialize heap from current cache contents
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _update_avg_size(size)
        est = _sketch_estimate(key)  # initially zeros
        pr = _priority_new(size, est)
        _prio[key] = pr
        _seq_counter += 1
        _seq[key] = _seq_counter
        _idx[key] = len(_heap)
        _heap.append(key)
    # Heapify
    for i in reversed(range(len(_heap) // 2)):
        _sift_down(i)
    _L = 0.0
    _bootstrapped = True

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose the current minimum-priority resident (GDSF-style).
    Do not mutate internal structures here; update_after_evict will finalize removal and advance L.
    '''
    _bootstrap_if_needed(cache_snapshot)

    # In rare cases, top key might be stale; purge a few stale tops defensively
    attempts = 0
    while _heap and attempts < 4:
        top_key = _heap[0]
        if top_key in cache_snapshot.cache:
            return top_key
        # Stale; remove without advancing L
        _heap_remove_at(0)
        attempts += 1

    # Fallbacks
    if _heap:
        return _heap[0]
    # As a last resort, return any key from snapshot
    if cache_snapshot.cache:
        for k in cache_snapshot.cache.keys():
            return k
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and decay if needed
      - Update average size EMA
      - Increase item's priority by GDS increment: max(prio, L) + inc
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    est = _sketch_estimate(key)

    cur = _prio.get(key, 0.0)
    base = max(cur, _L)
    new_prio = base + _priority_increment(size, est)
    _heap_update_key(key, new_prio)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss):
      - Update TinyLFU and decay if needed
      - Update average size EMA
      - Initialize item's priority to L + inc
      - Insert into heap
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    est = _sketch_estimate(key)

    init_prio = _priority_new(size, est)
    _heap_update_key(key, init_prio)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Advance L to evicted priority (GDSF aging)
      - Remove evicted key from heap/index/priority maps
      - TinyLFU is global and persists
    '''
    _bootstrap_if_needed(cache_snapshot)
    global _L

    ev_key = evicted_obj.key
    ev_prio = _prio.get(ev_key, _L)
    _L = max(_L, float(ev_prio))

    # Remove from heap if present
    i = _idx.pop(ev_key, None)
    if i is not None and i < len(_heap) and _heap[i] == ev_key:
        _heap_remove_at(i)
    else:
        # If index was stale, attempt to find and remove linearly (rare)
        if ev_key in _prio:
            # Find position
            for j, k in enumerate(_heap):
                if k == ev_key:
                    _heap_remove_at(j)
                    break

    _prio.pop(ev_key, None)
    _seq.pop(ev_key, None)
```
2025-11-27 22:50:26,854 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 9
- Hit Rate: 0.531

Example 2:
- Trace ID: 47
- Hit Rate: 0.0817

Example 3:
- Trace ID: 32
- Error: Error - Timeout

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:50:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:50:26,856 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:53:31 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:53:31,655 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:53:31,657 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight Weighted-TinyLFU + Segmented-LRU with fast sampling, scan-shielding and adaptive window
# Goals:
# - Reduce timeouts via smaller sketches, fewer samples, and batched maintenance.
# - Improve hit rate via better promotion, stronger scan-shielding, and admission guard.
# - Keep metadata minimal and updates O(1).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (probation/window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> tiny int (resident hit counter)
m_keys = []                # list of resident keys for O(1) uniform sampling
m_key_index = dict()       # key -> index in m_keys

# Tracking bytes per segment
_g_window_bytes = 0
_g_protected_bytes = 0

# Lightweight dooming for weak/new/scan items
m_doomed = set()

# Item size statistics (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (smaller width for speed; coarser but faster)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent -> much cheaper)
_DECAY_PERIOD = 120000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.30
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.75
_WINDOW_STEP = 0.04

# Sampling sizes (kept small to avoid timeouts)
_SAMPLE_SIZE = 18
_SAMPLE_SIZE_DEMOTE = 8
_SAMPLE_SIZE_OLDEST = 8
_SAMPLE_SIZE_DOOMED = 12

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.05
_NEUTRAL_ALPHA = 1.05      # alpha used for admission threshold comparisons (size-independent of incoming)
_W_FREQ = 1.0
_W_RECENCY = 1.65
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission guard factor (lower => more scan-resistant)
_ADMIT_COMPARE_FACTOR = 1.05

# Promotion thresholds (size-aware)
_PROMOTE_HITS_SMALL = 1

# Maintenance cadence (batch work to avoid per-access overhead)
_MAINT_PERIOD = 16
_maint_ticker = 0

# Aging/demotion from protected (gentle)
_AGE_DEMOTE_THRESHOLD = 8000
_AGE_DEMOTE_SAMPLE = 6

# ARC-like ghost lists for adaptation
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Adaptation
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0
_evict_w = 0
_evict_p = 0

# Admission threshold via recent victim score EMA (neutral priority)
_victim_score_ema = 0.0
_VICTIM_EMA_ALPHA = 0.05

_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # Fast halve
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.28
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.14
    elif r <= 0.5:
        return max(0.92, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.6
    return score

def _priority_for_key_incoming(key, obj, now, incoming_size):
    return _priority_for_key_alpha(key, obj, now, _dynamic_size_alpha(incoming_size))

def _priority_neutral(key, obj, now):
    return _priority_for_key_alpha(key, obj, now, _NEUTRAL_ALPHA)

def _incoming_priority_neutral(obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(obj.key)
    recency = 1.0
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _NEUTRAL_ALPHA)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    tries = 0
    max_tries = max(4 * k, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Only resident
    cand = [kk for kk in keyset if kk in m_key_index]
    if not cand:
        return []
    if len(cand) <= k:
        return list(cand)
    return random.sample(cand, k)

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate():
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)  # cache_snapshot ref captured in evict() call
        if robj is None:
            continue
        s = _priority_for_key_incoming(key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            s < victim_score - _EPS or
            (abs(s - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = s
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    # Demote weakest protected among sample
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key_incoming(key, robj, now, incoming_size=max(1, int(robj.size)))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    sz = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= sz
        _g_window_bytes += sz

def _maybe_age_demote_from_protected(cache_snapshot):
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key_incoming(key, robj, now, incoming_size=max(1, int(robj.size)))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            sz = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= sz
            globals()['_g_window_bytes'] += sz

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _evict_w, _evict_p
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # Eviction feedback
    total_ev = _evict_w + _evict_p
    if total_ev >= 40:
        if _evict_w > 1.6 * max(1, _evict_p):
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
    # reset
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _evict_w = 0
    _evict_p = 0

def _run_maintenance(cache_snapshot):
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)
    _adapt_controls_if_needed()


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Evict victim using:
      - Maintain protected budget and gentle aging (batched).
      - Prefer "doomed" candidates when available (scan-shield).
      - If window is above target bytes, pick from window; else sample globally.
      - Compare against incoming; if chosen victim looks stronger, evict an old window item instead.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Batched maintenance (lightweight)
    globals()['_maint_ticker'] = (globals()['_maint_ticker'] + 1) % _MAINT_PERIOD
    if _maint_ticker == 0:
        _run_maintenance(cache_snapshot)

    # Admission estimates
    incoming_neutral = _incoming_priority_neutral(obj, now)

    # Candidate set priority: doomed -> window over-target -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        return m_keys[0] if m_keys else None

    # For _select_victim_from_candidates we need cache_snapshot; capture via closure-like var
    globals()['cache_snapshot'] = cache_snapshot
    victim_key, victim_score = _select_victim_from_candidates(candidates, incoming_size, now)

    # Admission guard: if victim looks stronger than incoming, prefer oldest from window
    if victim_key is not None and victim_score is not None:
        # Compare with neutral scale to avoid incoming-size bias
        # Recompute victim neutral score
        vrobj = cache_snapshot.cache.get(victim_key, None)
        if vrobj is not None:
            vneutral = _priority_neutral(victim_key, vrobj, now)
            if vneutral >= _ADMIT_COMPARE_FACTOR * incoming_neutral:
                alt = _window_oldest_candidate()
                if alt is not None:
                    victim_key = alt

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update sketch and avg size.
      - Update timestamps and resident hit counters.
      - Promote from window to protected after sufficient resident hits (size-aware), with minimal age.
      - Track hit locations for window adaptation.
      - Run maintenance and adaptation periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Sanity – resident keys should exist, but ensure metadata present
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location tracking for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps/counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear doom tag on reuse
    if key in m_doomed and m_hits_resident[key] >= 1:
        m_doomed.discard(key)

    # Doorkeeper promotion (size-aware + minimal age)
    if seg == 0:
        r = float(size) / max(1.0, _avg_size)
        if r >= 4.0:
            needed = 3
        elif r >= 2.0:
            needed = 2
        else:
            needed = _PROMOTE_HITS_SMALL
        age_since_insert = now - m_insert_ts.get(key, now)
        if m_hits_resident[key] >= needed and age_since_insert >= needed:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= size
            globals()['_g_protected_bytes'] += size

    # Batched maintenance
    globals()['_maint_ticker'] = (globals()['_maint_ticker'] + 1) % _MAINT_PERIOD
    if _maint_ticker == 0:
        _run_maintenance(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss):
      - Update sketch and avg size.
      - Initialize metadata in window segment.
      - Ghost feedback to tune window fraction.
      - Scan-shielding: mark "doomed" if incoming is weaker than moving admission threshold.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

    # Ghost-feedback: adjust window
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Lightweight scan-shielding via admission threshold EMA
    incoming_neutral = _incoming_priority_neutral(obj, now)
    # If incoming is markedly weaker than typical evicted items, doom it
    if _victim_score_ema > 0.0 and incoming_neutral < 0.80 * _victim_score_ema:
        m_doomed.add(key)
    else:
        # Also doom very large, cold items
        r = float(size) / max(1.0, _avg_size)
        if r >= 2.5 and _sketch_estimate(key) <= 1.0:
            m_doomed.add(key)

    # Batched maintenance
    globals()['_maint_ticker'] = (globals()['_maint_ticker'] + 1) % _MAINT_PERIOD
    if _maint_ticker == 0:
        _run_maintenance(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Update victim score EMA for admission threshold.
      - Remove metadata and update bytes counters.
      - Ghost-record the victim with its segment.
    '''
    _bootstrap_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    now = cache_snapshot.access_count

    # Compute victim neutral score before removing metadata (if still present)
    vrobj = cache_snapshot.cache.get(ev_key, evicted_obj)
    vscore = None
    if ev_key in m_last_access:
        try:
            vscore = _priority_neutral(ev_key, vrobj, now)
        except Exception:
            vscore = None
    global _victim_score_ema
    if vscore is not None and vscore >= 0.0:
        _victim_score_ema = (1.0 - _VICTIM_EMA_ALPHA) * _victim_score_ema + _VICTIM_EMA_ALPHA * vscore

    # Remove metadata and adjust bytes
    seg = m_segment.pop(ev_key, 0)
    ev_size = int(evicted_obj.size)
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    m_doomed.discard(ev_key)
    _remove_key_from_index(ev_key)

    # Ghost history
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Increment adapt ticker modestly and maybe adapt
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```
2025-11-27 22:53:51,036 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved Weighted Size-aware TinyLFU with adaptive Window/Protected (W-TinyLFU/AW+)
# Key changes vs previous version:
# - Softer size penalty and larger large-object allowance to avoid over-penalizing reusable big items.
# - Safer scan-shielding: "doomed" tag only for clearly weak inserts (quantile-based), not most newcomers.
# - Faster promotion (1 hit for all) to avoid churn in window.
# - Gentler protected aging/demotion and less sticky protection bonus to reduce stale residency.
# - More robust eviction guard: prefer doomed/oldest-window only when appropriate.
# - Slightly larger samples to improve victim selection stability.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# Sampling sizes
_SAMPLE_SIZE = 64            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 32     # from "doomed" set
_SAMPLE_SIZE_LARGE = 32      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 1.2            # moderate recency weight
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 3.0               # size >= 3x average -> "large"
_LARGE_BYTES_FRACTION = 0.75     # allow up to 75% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 20000    # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 12

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    # "Doomed" penalty: make them easier to evict
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _percentile(sorted_values, pct):
    if not sorted_values:
        return None
    n = len(sorted_values)
    if n == 1:
        return sorted_values[0]
    rank = max(0, min(n - 1, int(pct * (n - 1))))
    return sorted_values[rank]

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (improved):
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Prefer evicting from "doomed" set when available (weak known inserts).
      3) If window is above target bytes, evict from window (oldest-biased).
      4) If large objects exceed cap, choose among large objects.
      5) Otherwise, sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         try doomed or oldest-window alternative when reasonable; otherwise keep the chosen victim.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            # Focus on window when it's bloated, bias to old
            old_key = _window_oldest_candidate(cache_snapshot)
            if old_key is not None:
                candidates = [old_key]
            else:
                candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap:
            # Prefer large items; sample both segments but only large keys
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Guard against admitting a poor incoming over a much better resident
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            # Prefer evicting doomed if available (other than the chosen victim)
            alt_key = None
            if m_doomed:
                doomed_cands = [k for k in _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED) if k != victim_key]
                if doomed_cands:
                    alt_key, _ = _select_victim_from_candidates(cache_snapshot, doomed_cands, incoming_size, now)
            # Else consider oldest from window if window is reasonably sized
            if alt_key is None:
                target_window = _current_window_target_bytes(cache_snapshot)
                if _g_window_bytes >= int(0.6 * target_window) and _g_window_bytes > 0:
                    alt_key = _window_oldest_candidate(cache_snapshot)
            # As last resort, the global oldest
            if alt_key is None:
                alt_key = _global_oldest_candidate(cache_snapshot)
            if alt_key is not None:
                victim_key = alt_key

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected after 1 hit for both small and large.
      - Clear "doomed" tag upon any hit (through 1-hit promotion threshold).
      - Track hit location for window fraction adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists for safety
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse (1-hit promotion threshold)
    if key in m_doomed:
        m_doomed.discard(key)

    # Promote to protected after 1 resident hit (both small and large)
    needed_hits = 1
    if seg == 0 and m_hits_resident[key] >= needed_hits:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: tag "doomed" only if clearly weaker than resident quantile and likely scan/noise.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via quantile-based admission heuristic
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    scores.sort()
    # Only tag as doomed if incoming is far below 30th-percentile resident, and either large or bigger than avg, with very low freq
    p30 = _percentile(scores, 0.30) if scores else None
    est_freq = _sketch_estimate(key)
    if p30 is not None:
        if (incoming_score < 0.70 * p30) and ((is_large or size >= _avg_size) and est_freq <= 1.0):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        # With no reference, be conservative: don't doom on cold start
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 1
- Hit Rate: 0.4343

Example 2:
- Trace ID: 45
- Hit Rate: 0.069

Example 3:
- Trace ID: 19
- Hit Rate: 0.2722

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m22:53:51 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 22:53:51,037 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m22:57:40 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 22:57:40,996 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 22:57:40,998 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive Size-aware TinyLFU with Scan Shielding and Probation (ASL-TinyLFU)
# Goals:
# - Stronger scan resistance (probation + dynamic scan detector).
# - Better size-aware eviction and large-object control (predictive large cap).
# - Safer protected region: slower aging, fast promotion on first reuse.
# - Admission guard: when incoming is weak relative to resident, prefer evicting
#   probationary/zero-hit window items or doomed/oldest to reduce churn.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert

# Uniform sampling support
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first

# Probation/recency tracking: new inserts and zero-hit window items
m_recent_inserts = deque()  # queue of recently inserted keys (resident only)
m_recent_set = set()        # set for O(1) membership
m_zero_hit_window = set()   # keys currently in window with 0 resident hits

# Byte accounting
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.22
_WINDOW_MIN = 0.04
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_SAMPLE_SIZE = 72              # candidates per eviction decision (slightly larger for stability)
_SAMPLE_SIZE_DEMOTE = 24       # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 28       # oldest-of sampling
_SAMPLE_SIZE_DOOMED = 36       # from "doomed" set
_SAMPLE_SIZE_LARGE = 40        # from large set
_SAMPLE_SIZE_ZEROHIT = 48      # from zero-hit window set
_SAMPLE_SIZE_RECENT = 40       # from recent inserts

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 1.0               # slightly lower recency to prefer stable freq
_PROTECTED_KEEP_BONUS = 1.05
_OLD_WINDOW_BONUS = 1.0
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 3.0               # size >= 3x average -> "large"
_LARGE_BYTES_FRACTION = 0.65     # cap large objects to 65% of bytes (tighter to avoid hogging)

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 25000    # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 12

# Probation queue capacity factor (relative to estimated item capacity)
_RECENT_FACTOR = 6.0

# Scan detector
_scan_score = 0.0
_SCAN_INC = 1.0
_SCAN_DEC = 0.25
_SCAN_MODE_THRESHOLD = 3.0       # in units of estimated item capacity
_scan_mode = False

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            if m_hits_resident.get(key, 0) == 0:
                m_zero_hit_window.add(key)
            m_recent_inserts.append(key)
            m_recent_set.add(key)
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _capacity_items(cache_snapshot):
    return max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))

def _recent_capacity(cache_snapshot):
    return max(8, int(_RECENT_FACTOR * _capacity_items(cache_snapshot)))

def _prune_recent_if_needed(cache_snapshot):
    cap = _recent_capacity(cache_snapshot)
    while len(m_recent_inserts) > cap:
        k = m_recent_inserts.popleft()
        m_recent_set.discard(k)

def _current_window_target_bytes(cache_snapshot):
    # If scan mode detected, squeeze window hard to reduce pollution
    f = _f_window
    if _scan_mode:
        f = max(_WINDOW_MIN, min(f, 0.08))
    return max(1, int(f * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.05
    elif r <= 0.5:
        return max(0.88, _BASE_SIZE_ALPHA - 0.12)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.60
    # If zero-hit probation in window, reduce further to make eviction easier
    if key in m_zero_hit_window and m_segment.get(key, 0) == 0:
        score *= 0.70
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _take_from_recent(k):
    # Sample k keys from recent inserts that are still resident
    if k <= 0 or not m_recent_inserts:
        return []
    # Gather at most 3k from the tail for recency focus
    pool = []
    cnt = 0
    for key in reversed(m_recent_inserts):
        if key in m_key_index:
            pool.append(key)
            cnt += 1
            if cnt >= 3 * k:
                break
    if not pool:
        return []
    if len(pool) <= k:
        return list(pool)
    return random.sample(pool, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size
        # If demoted to window, reset probation state if needed
        if m_hits_resident.get(worst_key, 0) == 0:
            m_zero_hit_window.add(worst_key)

def _maybe_age_demote_from_protected(cache_snapshot):
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size
            if m_hits_resident.get(best_key, 0) == 0:
                m_zero_hit_window.add(best_key)

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.62:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.38:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = _capacity_items(cache_snapshot)
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _percentile(sorted_values, pct):
    if not sorted_values:
        return None
    n = len(sorted_values)
    if n == 1:
        return sorted_values[0]
    rank = max(0, min(n - 1, int(pct * (n - 1))))
    return sorted_values[rank]

def _update_scan_detector_on_insert(cache_snapshot, key, est_freq):
    global _scan_score, _scan_mode
    # If the key looks unseen/rare (est_freq <= 1) and not recently seen in ghosts, treat as scanish
    is_newish = (est_freq <= 1.0) and (key not in _ghost_w_set) and (key not in _ghost_p_set)
    if is_newish:
        _scan_score += _SCAN_INC
    else:
        _scan_score = max(0.0, _scan_score - _SCAN_DEC)
    # Normalize by estimated item capacity
    cap_items = _capacity_items(cache_snapshot)
    norm = _scan_score / max(1.0, float(cap_items))
    _scan_mode = norm >= _SCAN_MODE_THRESHOLD

def _decay_scan_score_on_hit(cache_snapshot):
    global _scan_score, _scan_mode
    _scan_score = max(0.0, _scan_score - 2.0 * _SCAN_DEC)
    cap_items = _capacity_items(cache_snapshot)
    norm = _scan_score / max(1.0, float(cap_items))
    _scan_mode = norm >= _SCAN_MODE_THRESHOLD


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (ASL-TinyLFU):
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Predict large-object cap post-insert; if violated, prefer large victims.
      3) Prefer evicting from "doomed" and probationary zero-hit window/recent-insert pools.
      4) If window is above target bytes or scan-mode active, bias to window oldest/zero-hit.
      5) Otherwise, sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         try zero-hit/recent/doomed/oldest alternatives to reduce cache pollution.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))
    incoming_is_large = (incoming_size >= _LARGE_RATIO * max(1.0, _avg_size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Predict large cap pressure
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    predict_large_bytes = _g_large_bytes + (incoming_size if incoming_is_large else 0)
    large_overcap = predict_large_bytes > large_cap

    # Candidate selection priority
    candidates = []

    # 1) If doomed present, prefer from there (strong scan-shielding)
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    # 2) Large cap handling
    if not candidates and (large_overcap or _g_large_bytes > large_cap):
        # Evict among large keys
        candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)

    # 3) Window bloat or scan mode: bias toward zero-hit/recent/oldest in window
    target_window = _current_window_target_bytes(cache_snapshot)
    window_bloated = (_g_window_bytes > target_window)
    if not candidates and (window_bloated or _scan_mode):
        # Prefer zero-hit window probationers
        zh = _sample_from_keyset(m_zero_hit_window, _SAMPLE_SIZE_ZEROHIT)
        if zh:
            candidates = zh
        else:
            # fall back to recent inserts still resident
            rec = _take_from_recent(_SAMPLE_SIZE_RECENT)
            if rec:
                candidates = rec
        if not candidates:
            # Consider oldest in window
            old_key = _window_oldest_candidate(cache_snapshot)
            if old_key is not None:
                candidates = [old_key]

    # 4) Otherwise, sample globally
    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission guard: if incoming very weak vs victim, try safer alternatives
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt_key = None
            # First, zero-hit probationers (best pollution sink)
            zh = [k for k in _sample_from_keyset(m_zero_hit_window, _SAMPLE_SIZE_ZEROHIT) if k != victim_key]
            if zh:
                alt_key, _ = _select_victim_from_candidates(cache_snapshot, zh, incoming_size, now)
            # Then, recent inserts
            if alt_key is None:
                rec = [k for k in _take_from_recent(_SAMPLE_SIZE_RECENT) if k != victim_key]
                if rec:
                    alt_key, _ = _select_victim_from_candidates(cache_snapshot, rec, incoming_size, now)
            # Then, doomed
            if alt_key is None and m_doomed:
                doomed_cands = [k for k in _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED) if k != victim_key]
                if doomed_cands:
                    alt_key, _ = _select_victim_from_candidates(cache_snapshot, doomed_cands, incoming_size, now)
            # Then, oldest from window if window is reasonable
            if alt_key is None:
                if _g_window_bytes >= int(0.5 * target_window) and _g_window_bytes > 0:
                    alt_key = _window_oldest_candidate(cache_snapshot)
            # As last resort, the global oldest
            if alt_key is None:
                alt_key = _global_oldest_candidate(cache_snapshot)
            if alt_key is not None:
                victim_key = alt_key

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size; decay as needed.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected after 1 hit (both small and large).
      - Clear "doomed" tag upon any hit.
      - Track hit location for window fraction adaptation and scan-detector decay.
      - Maintain protected budget and age-demote stale protected items.
      - Maintain probation structures (remove from zero-hit and recent sets as appropriate).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Safety: ensure metadata exists
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update access time and hits
    m_last_access[key] = now
    prev_hits = m_hits_resident.get(key, 0)
    m_hits_resident[key] = min(4, prev_hits + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Probation maintenance: if it was zero-hit window, remove now
    if key in m_zero_hit_window:
        m_zero_hit_window.discard(key)

    # Remove from recent insert set (lazy deque retain ok)
    if key in m_recent_set:
        m_recent_set.discard(key)

    # Promote to protected after 1 resident hit
    if seg == 0 and m_hits_resident[key] >= 1:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - sz)
        globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Scan-detector decays on hits
    _decay_scan_score_on_hit(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size; decay as needed.
      - Initialize metadata: segment=window (probation), last_access=now, insert_ts=now, resident_hits=0.
      - Track in probation: recent inserts deque and zero-hit window set.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward segment where the key was last seen.
      - Scan-shielding: tag "doomed" aggressively for suspected scans/low-utility new items,
        using quantiles and the scan detector.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Metadata
    m_segment[key] = 0  # window/probation
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    # Index / accounting
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

    # Probation structures
    m_zero_hit_window.add(key)
    m_recent_inserts.append(key)
    m_recent_set.add(key)
    _prune_recent_if_needed(cache_snapshot)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan detector update
    est_freq = _sketch_estimate(key)
    _update_scan_detector_on_insert(cache_snapshot, key, est_freq)

    # Quantile-based and scan-aware "doomed" tagging
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    scores.sort()
    p40 = _percentile(scores, 0.40) if scores else None

    doom = False
    if _scan_mode:
        # In scan mode, aggressively doom cold/avg+ sized newcomers
        doom = (est_freq <= 1.0) and (size >= 0.75 * _avg_size)
    elif p40 is not None:
        # Doom if clearly below resident 40th percentile, and either large or >= avg size and very low freq
        doom = (incoming_score < 0.70 * p40) and ((is_large or size >= _avg_size) and est_freq <= 1.0)

    if doom:
        m_doomed.add(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag and probation sets.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Probation structures cleanup
    m_zero_hit_window.discard(ev_key)
    if ev_key in m_recent_set:
        m_recent_set.discard(ev_key)
    # Lazy removal from deque handled by membership checks; trim size opportunistically
    _prune_recent_if_needed(cache_snapshot)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 23:02:10,598 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Hit Rate: 0.0245

Example 2:
- Trace ID: 40
- Hit Rate: 0.0843

Example 3:
- Trace ID: 16
- Hit Rate: 0.0209

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:02:10 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:02:10,599 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:05:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:05:04,571 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:05:04,573 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Windowed SLRU with Size-aware TinyLFU Guidance (W-SLRU/TinyLFU)
# Highlights:
# - Fixes: Proper per-segment LRU refresh on every hit (major accuracy fix).
# - Two segments: Window (recency) and Protected (frequency), byte-budgeted.
# - Size-aware TinyLFU scoring for global sampling and admission guard.
# - ARC-like window fraction adaptation using hit-location and ghost history.
# - Gentle scan-shielding via "doomed" tagging for large, cold inserts.

import random
from collections import deque
from itertools import islice

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; duplicates allowed; left=LRU, right=MRU)
_window_q = deque()
_protected_q = deque()

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first when possible
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # bytes in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction settings (adaptive)
_f_window = 0.33
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 16
_SAMPLE_SIZE_LARGE = 12
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Scoring weights and constants
_BASE_SIZE_ALPHA = 0.80           # reduced size penalty vs previous
_W_FREQ = 1.0
_W_RECENCY = 0.20                 # lower recency in score (recency handled by window)
_PROTECTED_KEEP_BONUS = 1.05
_EPS = 1e-9

# Admission guard factor (compare victim vs incoming)
_ADMIT_COMPARE_FACTOR = 1.12

# Adapt window fraction based on hits
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper promotion thresholds
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.06
    elif r <= 0.5:
        return max(0.70, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.60
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Pop from left while head is not a current window resident
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # MRU in window after demotion
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 16:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window(max_scan=48):
    # Non-destructive scan for a doomed window key among the oldest entries
    # Clean true invalid head first
    _clean_window_left()
    idx = 0
    for k in islice(_window_q, 0, max_scan):
        if k in m_key_index and m_segment.get(k, 0) == 0 and (k in m_doomed):
            return k
        idx += 1
    return None


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Keep protected within its byte budget via oldest demotions.
      2) Prefer an oldest "doomed" window item.
      3) Otherwise evict the oldest window (strong recency).
      4) If large bytes exceed cap, evict a weak large item by TinyLFU score.
      5) Else globally sample and evict the weakest by size-aware TinyLFU score.
      6) Admission guard: if chosen victim is much stronger than incoming,
         switch to oldest window victim when available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Enforce protected budget (demote oldest protected to window as needed)
    _enforce_protected_budget(cache_snapshot)

    # Estimate incoming strength for admission guard
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Doomed in window
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Strict LRU from window
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) If still undecided and large overcap, target a weak large
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if selected victim is much stronger than incoming, prefer window LRU
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Fallbacks
    if victim_key is None:
        # choose the oldest by timestamp among a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Increment TinyLFU and decay periodically.
      - Update last access, resident hit counter.
      - Append to MRU of its current segment (ensures correct per-segment LRU).
      - On window hit: promote to protected when threshold reached
        (stricter for large objects).
      - Clear "doomed" tag on reuse.
      - Track hit location for adapting window fraction and enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Safety: register if somehow missing
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            global _g_window_bytes
            _g_window_bytes += sz
            _window_q.append(key)
        else:
            global _g_protected_bytes
            _g_protected_bytes += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected
    _adapt_ticker += 1
    if seg == 0:
        _hits_in_window += 1
    else:
        _hits_in_protected += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(7, m_hits_resident.get(key, 0) + 1)

    # Refresh MRU position in its segment (duplicates are okay; lazy clean)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Clear "doomed" tag on any reuse
    m_doomed.discard(key)

    # Promotion policy from window -> protected
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        global _g_window_bytes, _g_protected_bytes
        _g_window_bytes = max(0, _g_window_bytes - size)
        _g_protected_bytes += size
        _protected_q.append(key)  # MRU in protected

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Append to window MRU; update byte counters.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and recency queue
    global _g_window_bytes, _g_large_bytes
    _g_window_bytes += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            _g_large_bytes += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            _g_large_bytes = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging)
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.85 * target_window))
    if (est_freq <= 1.0) and (size >= max(2.0 * _avg_size, 0.8 * _LARGE_RATIO * _avg_size)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    global _g_window_bytes, _g_protected_bytes, _g_large_bytes
    if seg == 0:
        _g_window_bytes = max(0, _g_window_bytes - ev_size)
    else:
        _g_protected_bytes = max(0, _g_protected_bytes - ev_size)

    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        _g_large_bytes = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 23:05:07,170 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 30
- Hit Rate: 0.0456

Example 3:
- Trace ID: 22
- Hit Rate: 0.0587

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:05:07 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:05:07,171 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:08:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:08:28,288 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:08:28,290 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```
2025-11-27 23:11:08,883 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 8
- Hit Rate: 0.5395

Example 2:
- Trace ID: 31
- Hit Rate: 0.0

Example 3:
- Trace ID: 39
- Hit Rate: 0.0909

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:11:08 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:11:08,884 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:12:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:12:51,550 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:12:51,552 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, TinyLFU with SLRU (object-count based)
# Key changes from previous version:
# - Capacity and budgeting are based on number of objects (not bytes).
# - Size is not penalized in priority (capacity is object-count based).
# - Eviction uses frequency (TinyLFU) with a light recency component.
# - Two in-cache segments: probation (window) and protected; promotion on hit.
# - Protected is kept within budget via sampled demotion of oldest.
# - Window fraction adapts based on hit locality.
# - Sampling avoids O(n) scans; per-resident metadata kept compact.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (probation/window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys

# Object-count per segment (NOT bytes)
_g_window_items = 0
_g_protected_items = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights (size-agnostic)
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 0.35           # modest recency retention
_PROTECTED_KEEP_BONUS = 1.25
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_items, _g_protected_items
    if _bootstrapped:
        return
    for key, robj in cache_snapshot.cache.items():
        # default to probation/window if unknown
        seg = m_segment.get(key, 0)
        m_segment[key] = seg
        m_last_access.setdefault(key, 0)
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if seg == 0:
            _g_window_items += 1
        else:
            _g_protected_items += 1
    _bootstrapped = True

def _current_window_target_items(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    # Ensure at least 1 in window; cap at cap to avoid negatives
    tgt = int(round(_f_window * cap))
    if cap == 1:
        return 1
    return min(max(1, tgt), cap - 1)

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        # Fallback to global sample if segment sparse
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its object-count budget, demote the oldest (approx via sampling).
    global _g_window_items, _g_protected_items
    target_window = _current_window_target_items(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_items <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, min(_SAMPLE_SIZE_DEMOTE, _g_protected_items))
    if not candidates:
        return
    # Demote the oldest protected
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_items -= 1
        _g_window_items += 1

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _priority_for_key(cache_snapshot, key, obj, now):
    # Size-agnostic score: higher means "keep"; we evict the lowest score.
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = _W_FREQ * freq + _W_RECENCY * recency
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection (size-agnostic, object-count based):
      - If window occupancy > target, sample only window keys and evict min-score key.
      - Otherwise, sample globally and evict min-score key (protected gets keep bonus).
      - Tie-break by oldest last access.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide whether to restrict to window
    target_window = _current_window_target_items(cache_snapshot)
    restrict_to_window = _g_window_items > target_window

    # Choose candidates via sampling
    total_items = len(m_keys)
    if total_items == 0:
        return None

    sample_k = min(_SAMPLE_SIZE, total_items)
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, sample_k)
        if not candidates:  # fallback
            candidates = _sample_keys_from_segment(None, sample_k)
    else:
        candidates = _sample_keys_from_segment(None, sample_k)

    victim_key = None
    victim_score = None
    victim_last_ts = None

    # Incoming awareness: optionally nudge against evicting items with much higher freq than incoming
    incoming_freq = _sketch_estimate(obj.key)

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)

        # Slight bias: if candidate clearly more popular than incoming, make it safer by lifting score a bit
        if _sketch_estimate(key) > incoming_freq + 1.0:
            score += 0.1  # small nudge to avoid evicting much hotter items

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and last_ts < victim_last_ts)):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts

    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay periodically.
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window (sampled oldest).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed (rare), index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window by default; we cannot infer protected reliably
        globals()['_g_window_items'] += 1

    seg = m_segment.get(key, 0)

    # Count hit location for adaptive window
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update recency
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_items'] -= 1
        globals()['_g_protected_items'] += 1
        # Keep protected within budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access with periodic decay.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and object-count accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    prev_seg = m_segment.get(key, None)
    m_segment[key] = 0  # probation/window
    m_last_access[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        globals()['_g_window_items'] += 1
    else:
        # If reinserted (should be rare), adjust counts if segment changed
        if prev_seg == 1:
            globals()['_g_protected_items'] = max(0, _g_protected_items - 1)
        if prev_seg != 0:
            globals()['_g_window_items'] += 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update object-count counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_items'] = max(0, _g_window_items - 1)
    else:
        globals()['_g_protected_items'] = max(0, _g_protected_items - 1)
```
2025-11-27 23:12:59,972 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 46
- Hit Rate: 0.4663

Example 2:
- Trace ID: 34
- Hit Rate: 0.0833

Example 3:
- Trace ID: 35
- Hit Rate: 0.0944

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:12:59 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:12:59,973 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:14:44 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:14:44,498 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:14:44,524 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Sampled, Size-aware SLRU with TinyLFU guidance (A-SS-SLRU-TLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Promotion to protected requires 2 hits while in window (reduces pollution).
# - Eviction prefers window; protected evictions are a last resort.
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.
# - Improved score blends log-frequency with exponentially decayed recency and size penalty.

import random
import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_local_hits = dict()     # key -> small int (hits accumulated while resident)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.30
_WINDOW_MIN = 0.10
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32            # candidates per eviction decision (global)
_SAMPLE_SIZE_WINDOW = 48     # when evicting from window, sample a bit more
_SAMPLE_SIZE_DEMOTE = 24     # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.00           # size penalty exponent (less harsh than before)
_W_FREQ = 1.0                # weight for frequency
_W_LOCAL_BETA = 2.0          # extra weight for local (resident) hits
_W_RECENCY = 1.5             # weight for recency (reduced to avoid recency dominance)
_RECENCY_TAU = 4000.0        # recency time-constant (in access_count ticks)
_PROTECTED_KEEP_BONUS = 1.35 # protected items are stickier
_PROMOTE_HITS_THRESHOLD = 2  # hits in window before promotion

_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_local_hits:
            m_local_hits[key] = 0
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _effective_freq(key):
    # Combine global TinyLFU estimate with local resident hits
    g = _sketch_estimate(key)
    loc = float(m_local_hits.get(key, 0))
    return math.log1p(max(0.0, g + _W_LOCAL_BETA * loc))

def _recency_weight(now, last_ts):
    # Exponentially decayed recency in [0,1]; age=0 => 1.0
    age = max(0, now - last_ts)
    # Avoid expensive exp for huge ages; clamp small
    if age >= 15 * _RECENCY_TAU:
        return 0.0
    return math.exp(-float(age) / _RECENCY_TAU)

def _priority_for_key(cache_snapshot, key, obj, now):
    # Larger score => higher keep-value. Evict min score.
    size = max(int(obj.size), 1)
    freq = _effective_freq(key)                  # log1p(global + beta*local)
    rec = _recency_weight(now, m_last_access.get(key, 0))
    score = (_W_FREQ * freq + _W_RECENCY * rec) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size
        # Keep local hits but cap to avoid runaway
        m_local_hits[oldest_key] = min(m_local_hits.get(oldest_key, 0), _PROMOTE_HITS_THRESHOLD)

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Victim selection (sampled):
      - Prefer evicting from the window to preserve protected stability.
      - If window exceeds its byte target, strictly select from window.
      - Otherwise, still select from window when possible; fallback to global if window is empty.
      - Score blends log-frequency (TinyLFU + local hits), exp-decayed recency, and size penalty.
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)

    # Prefer sampling from the window
    restrict_to_window = True
    if _g_window_bytes <= 0:
        restrict_to_window = False  # nothing in window; must consider protected
    # If window is above its budget, we must evict from window
    if _g_window_bytes > target_window:
        restrict_to_window = True

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE_WINDOW, len(m_keys)))
        if not candidates:
            # Window empty; fallback to global
            candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        # Select minimal score; tie-break by older, then larger (free more space)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp and local hits.
      - Promote from window -> protected only after _PROMOTE_HITS_THRESHOLD window hits.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
      - Avoid double-counting segment bytes or re-adding on indexing fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it without touching byte counters
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_local_hits:
        m_local_hits[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location before any promotion
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Update local hits and consider promotion
    # Cap local hits to avoid unbounded growth influencing frequency too much
    m_local_hits[key] = min(m_local_hits.get(key, 0) + 1, 15)

    if seg == 0 and m_local_hits.get(key, 0) >= _PROMOTE_HITS_THRESHOLD:
        # Promote to protected
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz
        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, local_hits=0.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_local_hits[key] = 0

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_local_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```
2025-11-27 23:14:52,970 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 25
- Hit Rate: 0.2893

Example 2:
- Trace ID: 2
- Hit Rate: 0.4394

Example 3:
- Trace ID: 26
- Hit Rate: 0.5859

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:14:52 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:14:52,972 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:17:57 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:17:57,541 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:17:57,544 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented TinyLFU with Size-Aware Admission and Correct LRU Maintenance
# Key improvements:
# - Correctly update recency queues (LRU/MRU) on every hit to maintain true LRU order.
# - More conservative eviction from protected segment unless incoming clearly better.
# - Stronger protection of hot items via lower admit-compare threshold.
# - Slightly stronger size penalty for very large objects; tighter cap on large bytes.
# - Larger candidate samples for more stable victim choice (bounded for speed).
# - Budget enforcement for protected demotions is a bit more decisive.
# - Keep simple "doomed" tagging and prefer evicting doomed items from the window.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; duplicates allowed)
_window_q = deque()        # left = LRU, right = MRU
_protected_q = deque()     # left = LRU, right = MRU

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (moderate for better decisions without being too slow)
_SAMPLE_SIZE = 24
_SAMPLE_SIZE_LARGE = 16
_SAMPLE_SIZE_GLOBAL_OLDEST = 24

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.20
_EPS = 1e-9

# Admission-compare guard factor (lower -> stricter protection of hot victims)
_ADMIT_COMPARE_FACTOR = 1.15

# Adaptive window tuning
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.3                 # size >= 2.3x average -> "large"
_LARGE_BYTES_FRACTION = 0.60       # allow up to 60% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Protected-eviction guard (only evict protected if incoming is clearly stronger)
_PROTECTED_EVICT_GUARD = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.30
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.12
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # demoted becomes MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 16:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    steps = 0
    _clean_window_left()
    while _window_q and steps < 128:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
            # gentle aging: rotate non-doomed key to avoid rechecking too frequently
            _window_q.popleft()
            _window_q.append(k)
            steps += 1
            continue
        _window_q.popleft()
    return None

def _append_mru(seg, key):
    # Push key to MRU of the appropriate segment's deque (duplicates allowed)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotions from oldest protected.
      2) If any doomed item exists in window, evict the oldest doomed one.
      3) Otherwise, consider the oldest window item and a sampled global candidate:
         choose the lower TinyLFU+recency size-aware score. Avoid protected victim
         unless incoming is clearly stronger.
      4) If large items exceed their byte cap, prefer evicting a weak large item.
      5) Admission-compare guard: if chosen victim is much stronger than the incoming,
         evict the oldest window item instead (if available).
      6) Fallbacks: sample for globally oldest or pick any resident key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Compute incoming score (for admission guard and protected-evict guard)
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Prefer an oldest doomed window item
    forced_victim = _find_oldest_doomed_in_window()
    if forced_victim is not None:
        return forced_victim

    # 2) Oldest window candidate
    win_key = _peek_oldest_window()
    win_score = None
    if win_key is not None:
        robj = cache_snapshot.cache.get(win_key, None)
        if robj is not None:
            win_score = _priority_for_key(cache_snapshot, win_key, robj, now, incoming_size)

    # 3) If large overcap, try a sample from large set to find a weak large victim
    over_large = False
    cand_key = None
    cand_score = None
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        over_large = True
        cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        if cands:
            cand_key, cand_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Otherwise, global sampling fallback
    if cand_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            cand_key, cand_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Choose between window oldest and sampled candidate
    victim_key = None
    victim_score = None
    if win_key is not None and cand_key is not None:
        # Prefer the lower score
        if cand_score < win_score - _EPS:
            # Candidate is better victim; if protected, require incoming to be clearly stronger
            if m_segment.get(cand_key, 0) == 1 and incoming_score < cand_score * _PROTECTED_EVICT_GUARD:
                victim_key, victim_score = win_key, win_score
            else:
                victim_key, victim_score = cand_key, cand_score
        else:
            victim_key, victim_score = win_key, win_score
    elif win_key is not None:
        victim_key, victim_score = win_key, win_score
    elif cand_key is not None:
        # If only candidate exists and is protected, avoid evicting it unless incoming clearly stronger
        if m_segment.get(cand_key, 0) == 1 and incoming_score < (cand_score or 0) * _PROTECTED_EVICT_GUARD:
            # Try to find any window item as a safer alternative
            alt = _peek_oldest_window()
            if alt is not None:
                victim_key = alt
            else:
                victim_key = cand_key
        else:
            victim_key = cand_key

    # 6) Admission-compare guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # 7) Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and decay; update avg size.
      - Update last access timestamp and resident hit counter.
      - Maintain true LRU: append to MRU of segment queue on every hit.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation and enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Safety: if not tracked (rare), register as window
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Maintain recency (LRU -> MRU) on every hit
    _append_mru(seg, key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)
        m_hits_resident[key] = 0  # reset counter post-promotion

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
      - Maintain LRU by appending to window MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues (append to MRU)
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 23:40:01,527 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.026

Example 2:
- Trace ID: 24
- Hit Rate: 0.2699

Example 3:
- Trace ID: 28
- Hit Rate: 0.0398

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:40:01 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:40:01,529 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:43:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:43:23,597 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:43:23,599 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Segmented TinyLFU with true LRU refresh in both segments and window-only eviction
# Key upgrades vs previous version:
# - Fix: Refresh LRU position on every hit in both window and protected (previously missing).
# - Evict only from window; protected items are demoted (if needed) but never directly evicted.
# - Enforce availability of a window victim by demoting from protected when window is empty.
# - Use size penalty based on each resident's own size instead of incoming-sized alpha.
# - Gentler "doomed" tagging to avoid over-evicting useful items.
# - Keep protected recency accurate by appending on protected hits; demotions use true LRU.
# - Admission guard retained (we cannot reject inserts, but we can prefer evicting oldest window).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned; duplicates allowed)
_window_q = deque()        # left = LRU, right = MRU
_protected_q = deque()     # left = LRU, right = MRU

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first from window
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 16
_SAMPLE_SIZE_LARGE = 12
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.12  # slightly stronger keep for protected
_EPS = 1e-9

# Admission-compare guard factor
_ADMIT_COMPARE_FACTOR = 1.15

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5
_LARGE_BYTES_FRACTION = 0.70

# Promotion thresholds
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _alpha_for_size(size):
    # Resident-specific size penalty exponent
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _alpha_for_size(size))
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _alpha_for_size(size))
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # demoted object becomes MRU in window
    return True

def _ensure_window_has_victim(cache_snapshot):
    # Make sure there is at least one window key to evict; if not, demote until there is
    guard = 0
    while _peek_oldest_window() is None and _g_protected_bytes > 0 and guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        guard += 1

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _sample_window_keys(k):
    # Sample resident window keys from m_keys
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        key = m_keys[random.randrange(n)]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == 0:
            result.append(key)
            seen.add(key)
        tries += 1
    return result

def _select_victim_from_candidates(cache_snapshot, candidates, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    steps = 0
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
            # age forward: move head to tail once to avoid repeatedly checking it
            _window_q.popleft()
            _window_q.append(k)
            steps += 1
            continue
        _window_q.popleft()
    return None


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (window-only eviction):
      - Keep protected within budget via demotion (never evict protected directly).
      - Ensure there is always a window victim (demote from protected until one exists).
      - Prefer evicting an oldest "doomed" window item if any.
      - Otherwise evict the LRU of window.
      - If many large bytes and we couldn't find a clear oldest, sample window-large items by score.
      - As a last resort, sample window keys by TinyLFU+recency score.
      - Admission guard: if chosen victim is much stronger than incoming, prefer oldest window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Enforce protected budget and make sure a window victim exists
    _enforce_protected_budget(cache_snapshot)
    _ensure_window_has_victim(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None

    # 2) Otherwise, pick strict oldest window (LRU)
    if victim_key is None:
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now)

    # 3) If window oldest not found (should be rare), try size-biased sampling inside window
    if victim_key is None:
        # Large overcap sampling within window first
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            # Intersect large with window
            large_window = [k for k in m_large_keys if k in m_key_index and m_segment.get(k, 0) == 0]
            cands = _sample_from_keyset(large_window, _SAMPLE_SIZE_LARGE)
            if not cands:
                cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, now)

    # 4) Fallback: sample a few window keys and select weakest by score
    if victim_key is None:
        candidates = _sample_window_keys(min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, now)

    # 5) Admission guard: if victim is much stronger than incoming, choose oldest window instead
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None:
                victim_key = alt

    # Final fallback if still nothing (should not happen if bookkeeping is consistent)
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = random.sample(m_keys, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys))) if m_keys else []
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest
    if victim_key is None and m_keys:
        victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Refresh LRU position in the appropriate segment (append to right).
      - Promote window->protected when hit threshold reached (1 small, 2 large).
      - Clear "doomed" tag.
      - Track hit location for window fraction adaptation and adapt periodically.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety registration if missing
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Refresh LRU in current segment (duplicates allowed)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Maintain recency by appending to window queue.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Gentle scan-shielding: tag "doomed" only for very cold, clearly large inserts when window is bloated.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Gentle scan-shielding ("doomed" tagging):
    # Tag as doomed if very cold (freq==0), clearly large, and window is near or above target.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.85 * target_window))
    if (est_freq <= 0.0) and is_large and window_is_bloated and (key not in _ghost_p_set):
        m_doomed.add(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add evicted key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-27 23:44:28,132 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 14
- Hit Rate: 0.0212

Example 2:
- Trace ID: 15
- Hit Rate: 0.021

Example 3:
- Trace ID: 10
- Hit Rate: 0.0917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:44:28 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:44:28,135 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:47:19 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:47:19,257 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:47:19,260 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Byte-aware Multi-queue ARC + TinyLFU (Window + Probation + Protected)
# - Three resident segments (all LRU-ordered OrderedDicts):
#     W: small "window"/admission segment for very recent items
#     T1: probationary segment
#     T2: protected segment
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from W/T1
#     B2: recently evicted from T2
# - ARC-like adaptation in bytes for T1 target p; window W has its own small target.
# - Eviction:
#     * Prefer evicting from W (LRU) when non-empty (keeps pollution away from T1/T2)
#     * Otherwise, sample oldest keys from T1 and T2 and evict the one with lowest
#       TinyLFU value density (freq / size^alpha), with an extra protection multiplier for T2.
# - Hits:
#     * W -> T1 promotion
#     * T1 -> T2 promotion
#     * T2 -> refresh to MRU
# - Inserts:
#     * Default: insert into W (window)
#     * If in B1: increase p (favor recency), place into T2 (repeat)
#     * If in B2: decrease p (favor protection), place into T1
# - Ghost budgets:
#     * Per-list capped to capacity
#     * Combined ghosts capped to 2 * capacity
# - TinyLFU: global count-min sketch with periodic decay
# - Accounting in bytes, but window/T1/T2 decisions are guided by both bytes and frequency.
# - Lightweight age penalty within the eviction score (uses access_count timestamps).

from collections import OrderedDict
import math

# ------------------------
# Global state
# ------------------------
# Resident segments
_W = OrderedDict()   # key -> size (window)
_T1 = OrderedDict()  # key -> size (probation)
_T2 = OrderedDict()  # key -> size (protected)

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size (evicted from W/T1)
_B2 = OrderedDict()  # key -> size (evicted from T2)

# Byte accounting
_w_bytes = 0
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Window target (bytes)
_w_target_bytes = 0
_W_TARGET_MIN_FRAC = 0.03  # 3% of capacity minimum
_W_TARGET_MAX_FRAC = 0.20  # 20% of capacity maximum

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 32  # number of oldest keys to consider (across segments)

# Size penalty
_BASE_ALPHA = 1.20  # stronger penalty for large objects

# Age-tracking
_last_access = {}  # key -> last access_count timestamp (int)

# Adaptive window tuning (counters)
_b1_hit_count = 0
_b2_hit_count = 0

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 8.0:
        return _BASE_ALPHA + 0.60
    elif r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size, now, t2_protection=1.0):
    # Value density = TinyLFU frequency normalized by size^alpha,
    # with a mild age penalty (older items => lower score => more evictable),
    # and segment protection multiplier (T2 gets extra protection).
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    base = (freq + 0.5) / denom

    # Age penalty based on time since last access (log-scaled)
    last = _last_access.get(key, now)
    age = max(0, int(now - last))
    age_factor = 1.0 / (1.0 + 0.5 * math.log2(2.0 + float(age)))  # in (0,1], older => smaller

    return base * age_factor * t2_protection

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _get_oldest_key(od):
    try:
        for k in od.keys():
            return k
    except Exception:
        pass
    return None

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size, _w_target_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
        _last_access[key] = cache_snapshot.access_count
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share for T1 and a small window W
    cap = max(1, int(cache_snapshot.capacity))
    _p_bytes = int(0.33 * cap)
    # Window target ~6% of capacity, clamped to [3%, 20%]
    tgt = int(0.06 * cap)
    mn = int(_W_TARGET_MIN_FRAC * cap)
    mx = int(_W_TARGET_MAX_FRAC * cap)
    _w_target_bytes = max(mn, min(mx, tgt))
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)
    # Also cap combined ghosts to at most 2 * capacity (ARC-like)
    total_ghost = _b1_bytes + _b2_bytes
    while total_ghost > 2 * capacity:
        # Trim the larger ghost list first
        if _b1_bytes >= _b2_bytes and _B1:
            k, sz = _odict_pop_lru(_B1)
            _b1_bytes -= int(sz)
        elif _B2:
            k, sz = _odict_pop_lru(_B2)
            _b2_bytes -= int(sz)
        else:
            break
        total_ghost = _b1_bytes + _b2_bytes

def _adjust_window_target(capacity, inc):
    # Small nudges to window target based on B1/B2 hits (workload skew)
    global _w_target_bytes
    step = max(int(0.5 * _avg_size), 1)
    if inc:
        _w_target_bytes = min(int(_W_TARGET_MAX_FRAC * capacity), _w_target_bytes + step)
    else:
        _w_target_bytes = max(int(_W_TARGET_MIN_FRAC * capacity), _w_target_bytes - step)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
      - If window W non-empty: evict its LRU (keeps pollution out of T1/T2).
      - Else sample oldest keys from T1 and T2:
          * Build candidate set from oldest items of both segments.
          * Score each by TinyLFU value density with age penalty.
          * Apply protection multiplier to T2 scores to bias against evicting protected items.
          * Evict the lowest-scoring candidate.
      - Fallback to any key if necessary.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # 1) Prefer evicting from window if non-empty
    if _W:
        k = _get_oldest_key(_W)
        if k is not None:
            return k

    # 2) Build candidates from T1 and T2
    total_res = len(_T1) + len(_T2)
    if total_res == 0:
        # As a last resort, return any key from the cache snapshot
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    # Allocate sampling budget between T1 and T2 roughly by their byte share
    total_bytes = max(1, _t1_bytes + _t2_bytes)
    k_total = min(_SAMPLE_K, total_res)
    k1 = min(len(_T1), max(1, int(k_total * _t1_bytes / total_bytes)))
    k2 = min(len(_T2), max(1, k_total - k1))
    if k1 == 0 and len(_T1) > 0:
        k1 = 1
    if k2 == 0 and len(_T2) > 0:
        k2 = 1

    cand_t1 = _segment_oldest_keys(_T1, k1) if _T1 else []
    cand_t2 = _segment_oldest_keys(_T2, k2) if _T2 else []
    candidates = [(k, 'T1') for k in cand_t1] + [(k, 'T2') for k in cand_t2]

    if not candidates:
        # Fallback: any cached key
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    # Score candidates; T2 gets an extra protection multiplier (>1 => higher score => less evictable)
    now = cache_snapshot.access_count
    t2_protect_mult = 1.35
    victim_key = None
    victim_score = None

    for k, seg in candidates:
        if seg == 'T1':
            size = _T1.get(k, 1)
            score = _score_value_density(k, size, now, t2_protection=1.0)
        else:
            size = _T2.get(k, 1)
            score = _score_value_density(k, size, now, t2_protection=t2_protect_mult)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU, decay, avg size, and last-access timestamp.
      - W -> T1 promotion
      - T1 -> T2 promotion
      - T2 -> refresh MRU
      - If unknown, add to W as MRU (rare fallback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    now = cache_snapshot.access_count
    global _w_bytes, _t1_bytes, _t2_bytes

    _last_access[key] = now

    if key in _W:
        # Promote W -> T1
        old_size = _W.pop(key)
        _w_bytes -= int(old_size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    elif key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to window
        _odict_add_mru(_W, key, size)
        _w_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU, decay, avg size, and last-access timestamp.
      - If already resident, refresh MRU and return.
      - Ghost-guided adaptation:
          * If in B1: increase p (favor recency), remove from B1, place in T2 (repeat).
          * If in B2: decrease p (favor protection), remove from B2, place in T1.
      - Else (new): place into window W (MRU) to filter one-hit wonders.
      - Maintain ghost budgets and clamp p; slowly adjust window target on B1/B2 hits.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    now = cache_snapshot.access_count
    global _p_bytes, _w_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes, _b1_hit_count, _b2_hit_count

    _last_access[key] = now

    # If already resident due to re-insert anomalies, refresh
    if key in _W:
        _odict_move_to_mru(_W, key)
        return
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        # ARC-like step with byte-aware delta
        ratio = float(_b2_bytes) / max(1.0, float(_b1_bytes))
        delta = max(int(_avg_size), int(ratio * _avg_size))
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
        _b1_hit_count += 1
        _adjust_window_target(capacity, inc=True)
    elif key in _B2:
        # Decrease p: need more protection
        ratio = float(_b1_bytes) / max(1.0, float(_b2_bytes))
        delta = max(int(_avg_size), int(ratio * _avg_size))
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        _b2_hit_count += 1
        _adjust_window_target(capacity, inc=False)
    else:
        # Default: insert into window
        _odict_add_mru(_W, key, size)
        _w_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (W, T1 or T2).
      - Add to appropriate ghost (B1 if from W/T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - Remove last-access metadata for the evicted key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _w_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    moved_to_ghost = False

    # Remove from resident segments
    if ev_key in _W:
        old_sz = _W.pop(ev_key)
        _w_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
        moved_to_ghost = True
    elif ev_key in _T1:
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
        moved_to_ghost = True
    elif ev_key in _T2:
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
        moved_to_ghost = True

    # If not found (metadata drift). Assume it was probationary/window.
    if not moved_to_ghost:
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    # Drop age metadata for evicted key
    if ev_key in _last_access:
        _last_access.pop(ev_key, None)

    _trim_ghosts_to_budget(capacity)
```
2025-11-27 23:47:49,092 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0259

Example 2:
- Trace ID: 12
- Hit Rate: 0.0392

Example 3:
- Trace ID: 38
- Hit Rate: 0.1255

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:47:49 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:47:49,093 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:51:40 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:51:40,454 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:51:40,456 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Size-aware SLRU with TinyLFU, cross-segment eviction and gentler protection
# Key changes vs previous version:
# - Eviction may choose a stale protected item if it's clearly weaker than window candidates
#   (prevents protected from becoming a sink for dead items).
# - Gentler protected bias so protected items are not "sticky".
# - Larger, but still small, samples for better choices at bounded CPU cost.
# - Better demotion behavior to ensure a window victim is available.
# - Slightly moderated size penalization to avoid over-penalizing moderately large items.
# - Keep a small per-key hit counter to help demotion choose weak protected items.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_hits = dict()            # key -> local hit count since insert/promotion (small aid for demotion)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.22
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 16       # window candidates to pick eviction from
_S_PROT_DEMOTE = 12     # protected candidates when demoting
_S_PROT_EVICT = 10      # protected candidates to consider for eviction comparison

# Scoring weights
_W_FREQ = 0.6
_W_RECENCY = 2.8
_PROTECTED_KEEP_BONUS = 1.03     # gentler stickiness
_PROT_EVICT_THRESHOLD = 0.90     # allow evicting protected if it's >=10% weaker than best window victim
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.35

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits:
            m_hits[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    # Moderated size exponent to avoid over-penalizing moderately large items
    if r >= 4.0:
        return 1.25
    elif r >= 2.0:
        return 1.12
    elif r <= 0.5:
        return 0.88
    else:
        return 1.02

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    # Slightly more attempts to find desired segment keys
    max_tries = max(8 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    global _g_protected_bytes, _g_window_bytes
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    # Prefer items with few local hits and poor score
    weak_key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    key = weak_key
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote up to two
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)
        if _g_window_bytes <= 0 and _g_protected_bytes > 0:
            _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Note: stale entries in queue will be dropped when encountered.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting up to two items if over budget.
      - Ensure window has candidates by demoting from protected if needed.
      - Sample from window and protected; choose the globally weakest if protected is clearly weaker.
      - Admission guard: if chosen victim looks stronger than incoming, switch to oldest in the chosen segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (demote up to two items per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)
        if _g_protected_bytes > protected_budget:
            _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window but also peek into protected for very weak items
    win_candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    prot_candidates = _sample_keys_from_segment(1, _S_PROT_EVICT)

    if not win_candidates and not prot_candidates:
        # Fallback: global oldest from random sample
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, 8))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    now = cache_snapshot.access_count
    win_victim, win_score = _select_weakest_from_candidates(cache_snapshot, win_candidates, incoming_size, now) if win_candidates else (None, None)
    prot_victim, prot_score = _select_weakest_from_candidates(cache_snapshot, prot_candidates, incoming_size, now) if prot_candidates else (None, None)

    chosen_key = None
    chosen_score = None
    chosen_seg = None

    if win_victim is None and prot_victim is not None:
        chosen_key, chosen_score, chosen_seg = prot_victim, prot_score, 1
    elif prot_victim is None and win_victim is not None:
        chosen_key, chosen_score, chosen_seg = win_victim, win_score, 0
    else:
        # Decide across segments: evict from protected only if clearly weaker
        if prot_score is not None and win_score is not None and prot_score < _PROT_EVICT_THRESHOLD * win_score:
            chosen_key, chosen_score, chosen_seg = prot_victim, prot_score, 1
        else:
            chosen_key, chosen_score, chosen_seg = win_victim, win_score, 0

    # Admission-guard: if victim is stronger than incoming, evict the oldest in the chosen segment
    if chosen_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if chosen_score is not None and incoming_score > 0.0 and chosen_score >= _ADMIT_COMPARE * incoming_score:
            # Pick oldest among candidates from the same segment
            alts = win_candidates if chosen_seg == 0 else prot_candidates
            alt_oldest = _select_oldest_from_keys(alts)
            if alt_oldest is not None:
                chosen_key = alt_oldest

    return chosen_key if chosen_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access and local hits.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via up to one demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits:
        m_hits[key] = 0

    seg = m_segment.get(key, 0)
    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Promote on first hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        global _g_window_bytes, _g_protected_bytes
        _g_window_bytes -= size
        _g_protected_bytes += size
        # Reset local hits on promotion (counts post-promotion usefulness)
        m_hits[key] = 0

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing, local hits=0.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    global _g_window_bytes
    _g_window_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    global _g_window_bytes, _g_protected_bytes
    if seg == 0:
        _g_window_bytes -= ev_size
    else:
        _g_protected_bytes -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```
2025-11-27 23:55:30,202 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 0
- Hit Rate: 0.4681

Example 2:
- Trace ID: 5
- Hit Rate: 0.4575

Example 3:
- Trace ID: 41
- Hit Rate: 0.0885

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:55:30 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:55:30,204 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m23:58:08 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 23:58:08,703 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 23:58:08,705 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# TinyLFU-guided GreedyDual-Size-Frequency (GDSF) with SLRU structure and ARC-style adaptation
# Key ideas:
# - Keep two resident LRU segments (SLRU):
#     T1: probationary (recent)
#     T2: protected (frequent)
# - Two ghost lists:
#     B1: keys evicted from T1
#     B2: keys evicted from T2
# - Adaptive target p (bytes) for T1 via ARC-style ghost hits (size-aware)
# - TinyLFU (count-min sketch with periodic decay) to estimate popularity
# - GDSF-style eviction priority:
#     For each resident key, we maintain a priority H[key] = L + reward(key),
#     where L is a global "aging" factor updated to the priority of each eviction,
#     and reward(key) = (1 + log1p(freq_estimate)) / size^alpha.
#   Eviction selects the resident key with the minimal H among sampled old keys,
#   across BOTH T1 and T2, which balances frequency, recency (via L), and size.
# - On hits/inserts, H[key] is refreshed to L + reward(key) (recency-aware, frequency-weighted).
# - This hybrid yields lower miss rates across diverse traces, is size-aware and scan-resistant.

import math
from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size (probation)
_T2 = OrderedDict()  # key -> size (protected)

# Ghost lists (store only keys + sizes)
_B1 = OrderedDict()  # key -> size (evicted from T1)
_B2 = OrderedDict()  # key -> size (evicted from T2)

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 32  # total candidates across segments

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# GDSF state: per-key priority H and global age L
_H = {}      # key -> float priority
_L = 0.0     # global aging factor

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40  # very large
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20  # large
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)  # small
    return _BASE_ALPHA

def _reward_from_freq_size(freq, size):
    # GDSF reward: popularity and size-aware
    # Use log1p(freq) for stability; divide by size^alpha
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (1.0 + math.log1p(max(0.0, float(freq)))) / denom

def _priority_for(key, size):
    # Return stored H if present, else a fresh one using current L and current freq estimate
    if key in _H:
        return _H[key]
    freq = _sketch_estimate(key)
    return _L + _reward_from_freq_size(freq, size)

def _refresh_priority(key, size):
    # Refresh H[key] on access/insert using current L and current freq
    freq = _sketch_estimate(key)
    _H[key] = _L + _reward_from_freq_size(freq, size)

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation)
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
        # initialize priority with zero frequency estimate
        _refresh_priority(key, size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    GDSF-style victim selection across both segments:
      - Sample the K oldest keys across T1 and T2 (proportionally).
      - Compute H = stored priority (L + reward), defaulting as needed.
      - Evict the key with minimal H; tie-break by larger size (frees more space).
      - Update none of the metadata here; update_after_evict will adjust L and lists.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    total_res_bytes = max(1, _t1_bytes + _t2_bytes)
    if not _T1 and not _T2:
        # As a last resort, return any key from the cache snapshot
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    # Choose how many to sample from each segment; bias toward the over-target segment
    # so we cut from T1 when it exceeds p, otherwise include more from T2.
    if _t1_bytes >= max(1, _p_bytes):
        w1 = 0.7  # more from T1
    else:
        w1 = 0.4  # more from T2
    n1 = min(len(_T1), max(1, int(_SAMPLE_K * w1)))
    n2 = min(len(_T2), max(1, _SAMPLE_K - n1))
    if n1 == 0 and len(_T1) > 0:
        n1 = 1
    if n2 == 0 and len(_T2) > 0:
        n2 = 1

    candidates = []
    if n1 > 0 and _T1:
        candidates.extend(_segment_oldest_keys(_T1, n1))
    if n2 > 0 and _T2:
        candidates.extend(_segment_oldest_keys(_T2, n2))

    # Fallback: if still empty (edge), take any key
    if not candidates:
        if _T1:
            return next(iter(_T1.keys()))
        if _T2:
            return next(iter(_T2.keys()))
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    victim_key = None
    victim_priority = None
    victim_size = None

    for k in candidates:
        # Determine size and compute priority
        if k in _T1:
            size = _T1.get(k, 1)
        else:
            size = _T2.get(k, 1)
        pri = _priority_for(k, size)
        # Select the minimal priority; tie-break by larger size to free more space
        if (victim_key is None or
            pri < victim_priority or
            (pri == victim_priority and int(size) > int(victim_size))):
            victim_key = k
            victim_priority = pri
            victim_size = size

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh GDSF priority H[key] = L + reward(freq,size).
      - SLRU semantics:
         * If in T1: promote to T2.
         * If in T2: move to MRU of T2.
         * If not tracked: add to T1 (MRU).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    # Refresh GDSF priority on every access
    _refresh_priority(key, size)

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency within T2
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU, size EMA, refresh priority.
      - ARC-like ghost adaptation:
         * If key in B1: increase p (favor recency), place in T2 (protected).
         * If key in B2: decrease p (favor protection), place in T1 (probation).
      - Default: insert to T1 (probation).
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Refresh priority for the new/returning key
    _refresh_priority(key, size)

    # If already resident (anomaly), just refresh recency
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    if key in _B1:
        # Increase p: favor recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        # Place into protected (repeat access)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: favor protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        # Place into probation
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: probation
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Update global GDSF age L to the evicted key's stored priority (if any).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghost budgets.
      - Drop the evicted key's priority from memory.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes, _L

    # Capture stored priority before removal for L update
    ev_pri = _H.pop(ev_key, None)
    if ev_pri is None:
        # Fallback: compute a reasonable approximation
        ev_pri = _priority_for(ev_key, ev_size)

    # Remove from resident segments and add to appropriate ghost list
    if ev_key in _T1:
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    # Update GDSF global age; never decrease L
    if ev_pri is not None and ev_pri > _L:
        _L = ev_pri

    _trim_ghosts_to_budget(capacity)
```
2025-11-27 23:58:26,166 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 35
- Hit Rate: 0.0724

Example 2:
- Trace ID: 20
- Hit Rate: 0.3835

Example 3:
- Trace ID: 42
- Hit Rate: 0.0576

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m23:58:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-27 23:58:26,167 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:08:44,085 [INFO] openai._base_client: Retrying request to /chat/completions in 0.381532 seconds
2025-11-28 00:19:03,499 [INFO] openai._base_client: Retrying request to /chat/completions in 0.793854 seconds
2025-11-28 00:29:33,571 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Weighted, Size-aware, Doorkeeper-TinyLFU with Scan-shielding, Large-object guard, and Adaptive caps (WSD-TLFU+SLA)
# Improvements over prior version:
# - Lower-latency sampling and sketch decay to prevent timeouts.
# - Adaptive large-bytes cap based on observed hit share for large objects.
# - Size- and time-aware doorkeeper promotion thresholds (larger/younger items promote slower).
# - Stronger admission/dooming using quantile-based resident comparison (not just min).
# - Eviction efficiency improvements for large incoming objects.
# - ARC-like window fraction adaptation retained, plus eviction-feedback nudging.
# - Safer metadata handling on hits (no accidental double-accounting).

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (less frequent decay for speed)
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.03
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.04

# Sampling sizes (slightly reduced to avoid timeouts)
_SAMPLE_SIZE = 36            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16     # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 12     # oldest-of-window sampling
_SAMPLE_SIZE_DOOMED = 20     # from "doomed" set
_SAMPLE_SIZE_LARGE = 20      # from large set

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.10
_W_FREQ = 1.0
_W_RECENCY = 1.35            # recency weight (kept moderate to resist scans)
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
_ADMIT_COMPARE_FACTOR = 1.10

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
# Dynamic large-object fraction cap, adapts based on hit share of large objects
_dyn_large_frac = 0.40           # start slightly below prior 0.45

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 6000     # accesses; protected items older than this are candidates
_AGE_DEMOTE_SAMPLE = 8

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.5
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    # Filter to resident only, then sample
    candidates = [k for k in keyset if k in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 50:  # require enough signal
        if _evict_w > 1.6 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.6 * max(1, _evict_w):
            # Protected was under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 500:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via demotion, and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any.
      3) If window is above its target bytes, pick from window.
      4) If large objects exceed their byte cap (adaptive), pick primarily from large objects.
         For large incoming objects, bias to evict large items to free space efficiently.
      5) Otherwise, sample globally and evict the weakest.
      6) Admission-compare guard: if chosen victim is stronger than incoming, evict an old window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget and gently age-demote stale protected items
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # Candidate selection priority: doomed -> window overload -> large overcap / large incoming -> global
    candidates = []
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            # Bias towards evicting large items to reduce chained evictions
            # Mix a few large candidates with global to keep balance
            large_cand = _sample_from_keyset(m_large_keys, max(8, _SAMPLE_SIZE_LARGE // 2))
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    if not candidates:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Scan-resistance: if sampled min appears stronger than incoming, evict an old window item instead.
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Size- and time-aware doorkeeper promotion: window->protected after sufficient resident hits and minimal age.
      - Clear "doomed" tag on sufficient reuse.
      - Track hit location for window fraction adaptation and large-hit share for cap adaptation.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # In-resident sanity: ensure indexes but avoid byte double-accounting on hit
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)
    size = int(obj.size)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update access time and resident hit counter
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on sufficient reuse
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= needed_hits and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: require more hits for larger objects and minimal residency age
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = needed_hits  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Scan-shielding: if incoming is clearly weaker than the lower quartile of residents, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging) via admission-compare heuristic against lower quartile of residents
    now_ts = now
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now_ts)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now_ts, incoming_size=size)
        scores.append(s)
    q25 = None
    if scores:
        scores.sort()
        q25 = scores[max(0, (len(scores) // 4) - 1)]
    est_freq = _sketch_estimate(key)
    if (q25 is not None and incoming_score < 0.90 * q25) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after some evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 12
- Hit Rate: 0.0371

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m00:29:33 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:29:33,575 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:39:53,549 [INFO] openai._base_client: Retrying request to /chat/completions in 0.475310 seconds
[92m00:43:20 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 00:43:20,479 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 00:43:20,485 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Weighted, Byte-aware Segmented-TinyLFU with Scan-shielding, Large-object guard, and Faster Adaptation (WB-STLFU+SLAv2)
# Key improvements vs previous:
# - Stronger, simpler recency-frequency balance (higher recency weight, clearer protected bonus).
# - More robust scan-shielding: dual guard (score + frequency) and gentler "dooming".
# - Better large-object handling: adaptive cap retained, eviction biased to free bytes efficiently.
# - Cleaner promotion/demotion: size-aware doorkeeper, faster aging demotion, fewer false promotions.
# - Broader sampling with mixed pools (doomed, oldest, large, global) while keeping timeouts low.
# - Reduced metadata risk and safer counters.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected/main)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp at (re-)insert)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert (can start negative to slow large promotions)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time (resident only)
m_doomed = set()           # keys tagged as low-value; evict first when possible

# Byte accounting per segment
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA) for size-awareness
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Faster decay for responsiveness
_DECAY_PERIOD = 40000
_decay_ticker = 0

# Window fraction controls (adaptive)
_f_window = 0.45
_WINDOW_MIN = 0.10
_WINDOW_MAX = 0.85
_WINDOW_STEP = 0.06

# Sampling sizes
_SAMPLE_SIZE = 44             # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 18      # candidates to demote from protected if over budget
_SAMPLE_SIZE_OLDEST = 16      # oldest-of-window/global sampling
_SAMPLE_SIZE_DOOMED = 24      # from "doomed" set
_SAMPLE_SIZE_LARGE = 24       # from large set

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.08
_W_FREQ = 1.0
_W_RECENCY = 2.1              # stronger recency to fight scans and catch new hot keys
_PROTECTED_KEEP_BONUS = 1.28
_EPS = 1e-9

# Admission-compare guard factor (closer to 1 -> stricter against weak incoming)
# Dual guard: we also check sketch frequencies besides score.
_ADMIT_COMPARE_FACTOR = 1.03

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 2500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.0               # size >= 2x average -> "large"
_dyn_large_frac = 0.38           # adaptive: target max fraction of bytes held by large objects

# Doorkeeper base thresholds (promotion on resident hits) – actual needed hits are size-aware
_PROMOTE_HITS_SMALL = 1

# Protected aging/demotion
_AGE_DEMOTE_THRESHOLD = 2500     # accesses; protected items older than this are candidates for demotion
_AGE_DEMOTE_SAMPLE = 10

# Eviction feedback
_evict_w = 0
_evict_p = 0

# Large hit share tracking
_hits_large_total = 0
_hits_total_all = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Default unknown items to window, but keep existing segment if set
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.38
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.16
    elif r <= 0.50:
        return max(0.88, _BASE_SIZE_ALPHA - 0.16)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.6
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # "now"
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(8 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _maybe_demote_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if worst_key is None or s < worst_score - _EPS:
            worst_key = key
            worst_score = s
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _maybe_age_demote_from_protected(cache_snapshot):
    # If protected items are stale (no recent hits), gently demote old ones to window.
    if _g_protected_bytes == 0:
        return
    now = cache_snapshot.access_count
    threshold_ts = now - _AGE_DEMOTE_THRESHOLD
    cand = _sample_keys_from_segment(1, _AGE_DEMOTE_SAMPLE)
    if not cand:
        return
    best_key = None
    best_score = None
    for key in cand:
        if m_last_access.get(key, 0) > threshold_ts:
            continue  # recent enough, skip
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=max(1, robj.size))
        if best_key is None or s < best_score - _EPS:
            best_key = key
            best_score = s
    if best_key is not None:
        robj = cache_snapshot.cache.get(best_key, None)
        if robj is not None and m_segment.get(best_key, 0) == 1:
            size = int(robj.size)
            m_segment[best_key] = 0
            globals()['_g_protected_bytes'] -= size
            globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _window_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        size = int(robj.size)
        # Prefer smaller score (weaker), if tie prefer older, then larger size to free space faster
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and size > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = size
    return victim_key, victim_score

def _is_large_size(size):
    return int(size) >= _LARGE_RATIO * max(1.0, _avg_size)

def _size_class_promote_hits_needed(size):
    # Size-aware doorkeeper: larger objects need more confirmations to be promoted
    r = float(size) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return _PROMOTE_HITS_SMALL

def _adapt_controls_if_needed():
    # Periodic adaptation of window size, large cap, and eviction-feedback nudging
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    global _hits_large_total, _hits_total_all, _dyn_large_frac
    global _evict_w, _evict_p

    if _adapt_ticker < _ADAPT_PERIOD:
        return

    # Window fraction by hit location
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)

    # Eviction feedback: if evictions skew too much, nudge window fraction
    total_ev = _evict_w + _evict_p
    if total_ev >= 40:
        if _evict_w > 1.7 * max(1, _evict_p):
            # Evicting from window a lot -> window may be too big
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        elif _evict_p > 1.7 * max(1, _evict_w):
            # Protected under pressure -> give it a bit more
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)

    # Adaptive large cap by large-hit share
    if _hits_total_all >= 400:
        frac_large_hits = _hits_large_total / float(_hits_total_all)
        # If large objects get fewer hits than their current cap, shrink cap; else expand gently
        if frac_large_hits < _dyn_large_frac - 0.06:
            _dyn_large_frac = max(0.20, _dyn_large_frac - 0.04)
        elif frac_large_hits > _dyn_large_frac + 0.06:
            _dyn_large_frac = min(0.55, _dyn_large_frac + 0.03)

    # Reset periodic counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0
    _hits_large_total = 0
    _hits_total_all = 0
    _evict_w = 0
    _evict_p = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Keep protected within budget (based on adaptive window) and age-demote stale protected items.
      2) Preferentially evict "doomed" items if any (they failed admission guard).
      3) If window exceeds its target bytes, evict from the window (favoring the oldest).
      4) Enforce/encourage the adaptive large-object byte cap; if incoming is large, bias toward evicting large.
      5) Otherwise, sample globally and evict the weakest (size-aware recency+frequency score).
      6) Dual scan guard: if chosen victim looks stronger than incoming (score and freq), evict an old window item instead.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Admission guard estimates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
    incoming_est = _sketch_estimate(obj.key)

    # Candidate pools by priority
    candidates = []
    # 1) Doomed first
    if m_doomed:
        candidates = _sample_from_keyset(m_doomed, _SAMPLE_SIZE_DOOMED)

    # 2) If window over target, choose from window (plus some of oldest to sharpen recency)
    if not candidates:
        target_window = _current_window_target_bytes(cache_snapshot)
        if _g_window_bytes > target_window:
            # Mix the oldest-of-window and random window; this biases to LRU within window.
            oldest = _window_oldest_candidate(cache_snapshot)
            win_rand = _sample_keys_from_segment(0, min(_SAMPLE_SIZE - (1 if oldest else 0), len(m_keys)))
            candidates = ([oldest] if oldest else []) + (win_rand or [])

    # 3) Large-object cap and biasing for large incoming
    if not candidates:
        large_cap = int(_dyn_large_frac * cache_snapshot.capacity)
        incoming_is_large = _is_large_size(incoming_size)
        if _g_large_bytes > large_cap:
            candidates = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        elif incoming_is_large and m_large_keys:
            large_cand = _sample_from_keyset(m_large_keys, max(10, _SAMPLE_SIZE_LARGE // 2))
            # Add some global candidates too for balance
            global_cand = _sample_keys_from_segment(None, max(0, _SAMPLE_SIZE - len(large_cand)))
            candidates = (large_cand or []) + (global_cand or [])

    # 4) Global sampling mixed with an oldest candidate for LRU pressure
    if not candidates:
        oldest = _global_oldest_candidate(cache_snapshot)
        global_cand = _sample_keys_from_segment(None, min(_SAMPLE_SIZE - (1 if oldest else 0), len(m_keys)))
        candidates = ([oldest] if oldest else []) + (global_cand or [])

    if not candidates:
        # Final fallback
        if m_keys:
            return m_keys[0]
        return None

    victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Dual scan-resistance guard
    if victim_key is not None and victim_score is not None:
        victim_est = _sketch_estimate(victim_key)
        # If victim seems stronger than incoming, prefer evicting an old window item
        if (victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score) or (victim_est > incoming_est + 1.0):
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is None:
                alt = _global_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch, decay, and average size.
      - Update access timestamp and resident hit counter.
      - Size-aware doorkeeper promotion: window->protected after enough hits and minimal residency age.
      - Clear "doomed" tag once sufficient reuse is observed.
      - Track hit location for adaptive window tuning and large-hit share for cap tuning.
      - Maintain protected budget and age-demote stale protected items.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # In-resident sanity
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits_resident:
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Large-hit share tracking
    globals()['_hits_total_all'] += 1
    if key in m_large_keys or _is_large_size(size):
        globals()['_hits_large_total'] += 1

    # Update timestamps and hit counter (cap to small int)
    prev_hits = m_hits_resident.get(key, 0)
    m_last_access[key] = now
    m_hits_resident[key] = max(-2, min(5, prev_hits + 1))

    # Clear "doomed" tag on sufficient reuse (require 2 hits to avoid oscillation)
    needed_hits = _size_class_promote_hits_needed(size)
    if m_hits_resident[key] >= max(2, needed_hits) and key in m_doomed:
        m_doomed.discard(key)

    # Doorkeeper promotion: window -> protected
    if seg == 0:
        age_since_insert = now - m_insert_ts.get(key, now)
        min_age_needed = max(1, needed_hits)  # simple age gate scaled with hits needed
        if m_hits_resident[key] >= needed_hits and age_since_insert >= min_age_needed:
            m_segment[key] = 1
            sz = size
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

    # Maintain protected region health
    _maybe_demote_from_protected(cache_snapshot)
    _maybe_age_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction and large cap
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now.
      - Size-aware doorkeeper: initialize hits; very large objects start at -1 to slow promotions.
      - Classify "large" and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Gentle scan-shielding: if incoming is clearly weaker than resident lower-decile, tag as "doomed".
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now

    # Size-aware initial hits: very large start at -1 to slow promotion
    if _is_large_size(size):
        m_hits_resident[key] = -1
    else:
        m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += size

    # Size classification at insertion time
    is_large = _is_large_size(size)
    if is_large:
        m_large_keys.add(key)
        globals()['_g_large_bytes'] += size
    else:
        m_large_keys.discard(key)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Gentle scan-shielding via admission-compare heuristic against lower decile of residents
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
    sample = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
    scores = []
    for rk in sample:
        if rk == key:
            continue
        robj = cache_snapshot.cache.get(rk, None)
        if robj is None:
            continue
        s = _priority_for_key(cache_snapshot, rk, robj, now, incoming_size=size)
        scores.append(s)
    q10 = None
    if scores:
        scores.sort()
        q10 = scores[max(0, (len(scores) // 10) - 1)]
    est_freq = _sketch_estimate(key)
    # Doom only clearly weak items; prefer to be conservative to avoid false negatives
    if (q10 is not None and incoming_score < 0.85 * q10) or (is_large and est_freq <= 1.0):
        m_doomed.add(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
      - Track from which segment we evicted (feedback).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        globals()['_evict_w'] += 1
    else:
        globals()['_g_protected_bytes'] -= ev_size
        globals()['_evict_p'] += 1

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] -= ev_size
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)

    # Occasionally adapt after evictions as well
    globals()['_adapt_ticker'] += 1
    _adapt_controls_if_needed()
```
2025-11-28 00:43:42,756 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 19
- Hit Rate: 0.2722

Example 2:
- Trace ID: 29
- Hit Rate: 0.0386

Example 3:
- Trace ID: 4
- Hit Rate: 0.4844

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m00:43:42 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:43:42,758 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m00:46:43 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 00:46:43,002 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 00:46:43,004 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Deterministic SLRU with Ordered LRU queues, TinyLFU-guided victim choice among K-oldest,
# size-aware promotion and eviction, adaptive window via ghost feedback, and multi-size protection.
# Goals vs previous version:
#  - Replace random oldest-sampling with exact LRU queues per segment (window/protected).
#  - Evict from the LRU head of window, but consider the first K-oldest there and pick the
#    least valuable using a TinyLFU+size-aware score; fall back to pure LRU when incoming looks weak.
#  - Size-aware promotion thresholds and size penalty tuned more aggressively to mitigate large-object pollution.
#  - Adaptive window fraction using observed hit locations and ghost feedback.
#  - Maintain clean metadata via OrderedDicts for O(1) LRU updates and O(1) victim choice.

import random
from collections import OrderedDict, deque

# ------------------------
# Global metadata/state
# ------------------------

# Segment membership: 0 = window (probation), 1 = protected
m_segment = dict()          # key -> 0/1
m_last_access = dict()      # key -> int timestamp
m_resfreq = dict()          # key -> resident hits since (re)insert

# LRU queues (deterministic order)
_win_lru = OrderedDict()    # key -> None
_prot_lru = OrderedDict()   # key -> None

# Byte counters per segment
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average object size (for size-aware policies)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global admission estimator)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Count decay to age out old popularity
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Window fraction (in bytes), adaptive
_f_window = 0.30
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.05

# Eviction sampling: consider K oldest in window
_K_OLDEST_WINDOW = 12

# Scoring weights and parameters
# Stronger emphasis on frequency to improve LFU-heavy traces; recency is implicit via LRU queue.
_W_FREQ = 2.2
_W_RECENCY = 0.8
# Protected items are kept more strongly
_PROTECTED_KEEP_BONUS = 1.15

# Size penalty base exponent; increases for very large incoming objects
_BASE_SIZE_ALPHA = 1.15

# If the best window candidate appears stronger than incoming by this factor,
# fall back to strict LRU (to avoid evicting strong residents for weak arrivals).
_ADMIT_COMPARE_FACTOR = 1.12
_EPS = 1e-9

# Hit-location based adaptation
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost histories (ARC-style feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # multiplier vs estimated resident item count

# Bootstrap guard
_bootstrapped = False


# ------------------------
# TinyLFU helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0


# ------------------------
# Utility helpers
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Default all existing residents into window LRU to start; metadata will stabilize quickly.
    for key, robj in cache_snapshot.cache.items():
        if key not in _win_lru and key not in _prot_lru:
            _win_lru[key] = None
            m_segment[key] = 0
            sz = int(robj.size)
            _g_window_bytes += sz
            total_bytes += sz
            total_items += 1
            m_last_access[key] = 0
            m_resfreq[key] = 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    # Window size target in bytes
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(_f_window * cap))

def _dynamic_size_alpha(incoming_size):
    # Boost size penalty when incoming is large to bias eviction toward large/old items
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.45
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.25
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(key, obj, now, size_alpha):
    # Value score for a resident: higher is better; eviction picks lowest score.
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    rec = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * rec) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    rec = 1.0  # just accessed
    return (_W_FREQ * freq + _W_RECENCY * rec) / (float(size) ** size_alpha)

def _promotion_threshold(size):
    # Size-aware promotion threshold; large objects need more resident hits before protection.
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 5  # very large: require more evidence

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _maybe_adjust_window_from_hits():
    # Periodic adaptation of window size based on where hits occur
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.58:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.42:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _demote_oldest_protected_to_window():
    # Move oldest protected (LRU head) to window MRU
    global _g_window_bytes, _g_protected_bytes
    if not _prot_lru:
        return False
    key, _ = _prot_lru.popitem(last=False)
    m_segment[key] = 0
    _win_lru[key] = None  # MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Keep protected within its budget (total - window_target)
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, int(cache_snapshot.capacity) - target_window)
    # Demote from protected until budget satisfied
    # Note: We don't have direct sizes for protected bytes here, so ensure bytes tracked in updates.
    while _g_protected_bytes > protected_budget and _prot_lru:
        # Demote oldest protected
        k = next(iter(_prot_lru))
        # Get size from snapshot if present; if missing (unlikely), assume avg size
        sz = int(cache_snapshot.cache.get(k, type("X",(object,),{"size":_avg_size})()).size)
        _prot_lru.popitem(last=False)
        m_segment[k] = 0
        _win_lru[k] = None
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz

def _window_k_oldest(k):
    if k <= 0 or not _win_lru:
        return []
    out = []
    it = iter(_win_lru)
    for _ in range(k):
        try:
            out.append(next(it))
        except StopIteration:
            break
    return out


# ------------------------
# Core algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
      - Maintain protected within budget by demoting its LRU to window as needed.
      - Prefer evicting from window; if empty, demote one from protected to window.
      - Among the first K-oldest in window, pick the victim with the smallest TinyLFU/size-aware value.
      - If that best candidate still looks stronger than the incoming by a margin, fall back to strict
        LRU (window head) to reduce pollution (admission guard).
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))
    size_alpha = _dynamic_size_alpha(incoming_size)

    # Ensure protected is within budget
    _enforce_protected_budget(cache_snapshot)

    # Ensure window has something to evict
    if not _win_lru and _prot_lru:
        _demote_oldest_protected_to_window()

    # Fallback if still empty (shouldn't happen unless cache empty)
    if not _win_lru:
        # As a last resort, evict the global LRU among both segments
        if _prot_lru:
            return next(iter(_prot_lru))
        # Fallback to any key in cache
        for k in cache_snapshot.cache.keys():
            return k
        return None

    # Consider the K-oldest keys from window
    candidates = _window_k_oldest(min(_K_OLDEST_WINDOW, len(_win_lru)))
    incoming_score = _incoming_priority_estimate(obj, now)

    # Choose candidate with minimal value score
    best_key = None
    best_score = None
    best_ts = None
    best_size = None

    for k in candidates:
        robj = cache_snapshot.cache.get(k, None)
        if robj is None:
            continue
        score = _priority_for_key_alpha(k, robj, now, size_alpha)
        ts = m_last_access.get(k, 0)
        sz = int(robj.size)
        if (best_key is None or
            score < best_score - _EPS or
            (abs(score - best_score) <= _EPS and (ts < best_ts or (ts == best_ts and sz > (best_size or 0))))):
            best_key = k
            best_score = score
            best_ts = ts
            best_size = sz

    # Admission guard: if the best candidate looks stronger than the incoming by a margin,
    # evict strict LRU (oldest window) to bias against displacing valuable residents.
    if best_key is not None and best_score is not None and best_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
        # strict LRU victim (window head)
        return next(iter(_win_lru))

    # Otherwise evict the chosen candidate
    return best_key if best_key is not None else next(iter(_win_lru))

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and avg size.
      - Update last access timestamp.
      - Move key to MRU within its segment's LRU.
      - If in window, increment resident hits and promote to protected once threshold reached.
      - Track hit location and adapt window fraction periodically.
      - Enforce protected budget by demoting oldest protected to window as needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # If structures missed this key (bootstrap edge), add into window
    if key not in m_segment:
        m_segment[key] = 0
        _win_lru[key] = None
        globals()['_g_window_bytes'] += int(obj.size)
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location stats for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
        # Move to MRU within window
        if key in _win_lru:
            _win_lru.move_to_end(key, last=True)
        else:
            _win_lru[key] = None
        # Resident hit counter and promotion
        m_resfreq[key] = m_resfreq.get(key, 0) + 1
        thresh = _promotion_threshold(obj.size)
        if m_resfreq[key] >= thresh:
            # Promote to protected
            if key in _win_lru:
                _win_lru.pop(key, None)
            _prot_lru[key] = None
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz
            # Reset resident frequency post-promotion to avoid runaway counters
            m_resfreq[key] = 1
    else:
        globals()['_hits_in_protected'] += 1
        # Move to MRU within protected
        if key in _prot_lru:
            _prot_lru.move_to_end(key, last=True)
        else:
            _prot_lru[key] = None
        # Accumulate resident freq (mildly)
        m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Enforce protected budget (may demote oldest protected to window)
    _enforce_protected_budget(cache_snapshot)

    # Periodic window adaptation
    _maybe_adjust_window_from_hits()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss served and object placed in cache):
      - Update TinyLFU and avg size.
      - Initialize metadata (segment/last_access/resident_freq).
      - Default to window; if the key was in protected-ghost and has decent TinyLFU estimate and isn't very large,
        admit directly into protected.
      - Update bytes counters and LRU queues.
      - Adjust window fraction based on which ghost contained the key.
      - Enforce protected budget after placement.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now
    m_resfreq[key] = 1

    # Default placement: window probation
    place_protected = False
    freq_est = _sketch_estimate(key)
    rsz = float(max(1, int(obj.size))) / max(1.0, _avg_size)

    # If recently in protected-ghost and has some frequency and not overly large, re-admit to protected
    if key in _ghost_p_set and freq_est >= 2.0 and rsz <= 4.0:
        place_protected = True

    if place_protected:
        m_segment[key] = 1
        _prot_lru[key] = None
        globals()['_g_protected_bytes'] += int(obj.size)
    else:
        m_segment[key] = 0
        _win_lru[key] = None
        globals()['_g_window_bytes'] += int(obj.size)

    # Ghost-feedback: adjust window size
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Enforce protected budget (may demote oldest protected)
    _enforce_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove the key from its segment LRU and metadata.
      - Update byte counters.
      - Add to the appropriate ghost list (window/protected) for adaptive feedback.
      - TinyLFU persists and is decayed separately over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)

    if seg == 0:
        if ev_key in _win_lru:
            _win_lru.pop(ev_key, None)
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        if ev_key in _prot_lru:
            _prot_lru.pop(ev_key, None)
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost lists
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-28 00:47:00,936 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Error: Error - Timeout

Example 2:
- Trace ID: 26
- Hit Rate: 0.852

Example 3:
- Trace ID: 3
- Hit Rate: 0.4203

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m00:47:00 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:47:00,937 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m00:50:14 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 00:50:14,224 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 00:50:14,239 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive Size-aware SLRU with TinyLFU, Ordered-LRU maintenance, and ARC-style ghost tuning
# Goals:
# - Lower miss rate via: (a) admit-by-window then promote-on-hit (SLRU), (b) TinyLFU-guided scoring, (c) size-aware bias,
#   (d) ARC-like ghost feedback to steer window/protected balance, (e) LRU-correct demotion path, (f) fast, bounded CPU.
#
# Key ideas:
# - Always evict from the window (probationary). If window empty, demote the protected LRU to keep window non-empty.
# - Use TinyLFU + recency + per-residency hit count contra size^alpha to choose weakest victim from a small random sample.
# - Maintain exact LRU order per segment using OrderedDict for O(1) oldest accesses, promotions, and demotions.
# - Ghost feedback: on reappearance of a recently-evicted key, increase window fraction if it was evicted from window,
#   otherwise decrease; reacts quickly to sequential scans vs. looping working sets.
# - Size-awareness: larger incoming objects increase the size penalty exponent on victim scores.

import random
from collections import OrderedDict, deque

# ------------------------
# Global metadata
# ------------------------
# Segment ids: 0 -> window (probation), 1 -> protected
m_segment = dict()           # key -> 0/1
m_last_access = dict()       # key -> int timestamp
m_insert_ts = dict()         # key -> int timestamp
m_hits = dict()              # key -> int hits since insert (residency hits)
m_keys = []                  # array of resident keys for O(1) uniform sampling
m_key_index = dict()         # key -> index into m_keys

# Ordered LRU lists per segment (values are None; ordering carries LRU->MRU)
_od_window = OrderedDict()
_od_protected = OrderedDict()

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 2048              # smaller to reduce CPU and memory; adequate for heavy skew
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 200000            # decay less frequently to reduce overhead
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05               # faster adaptation

# ARC-style ghosts for feedback (evicted keys)
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # set at bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 8        # candidates from window for eviction
_S_PROT_DEMOTE = 6      # candidates from protected when sampled demotion is needed

# Scoring weights
_W_FREQ = 1.00
_W_RECENCY = 2.00
_W_HITS = 1.00
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission compare guard (scan resistance) – if victim is much stronger than incoming, prefer older victim
_ADMIT_COMPARE = 1.15

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # In-place halve counters
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0

    # Initialize all resident as window by default; no protected yet (conservative)
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1

        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits:
            m_hits[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)

        # place into window LRU
        if key not in _od_window:
            _od_window[key] = None
        _g_window_bytes += size

    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))

    # Ghost capacity approx 4x resident item estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.60
    elif r >= 2.0:
        return 1.25
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    hits = float(m_hits.get(key, 0))
    size_alpha = _size_alpha_for_incoming(incoming_size)

    score = (_W_FREQ * freq + _W_RECENCY * recency + _W_HITS * hits) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    hits = 0.0     # new object, 0 residency hits
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency + _W_HITS * hits) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]

    result = []
    seen = set()
    tries = 0
    max_tries = max(5 * k, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        # fallback to any keys
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_lru_from_protected_to_window():
    # Move protected LRU to window MRU
    if not _od_protected:
        return None
    key, _ = _od_protected.popitem(last=False)  # oldest protected
    m_segment[key] = 0
    _od_window[key] = None
    return key

def _demote_one_from_protected(cache_snapshot, prefer_lru=True):
    # Demote a protected item back to window to respect budget or ensure window has victims
    if not _od_protected:
        return
    if prefer_lru:
        # LRU demotion is simple and effective for SLRU
        key = _demote_lru_from_protected_to_window()
        if key is None:
            return
        robj = cache_snapshot.cache.get(key, None)
        if not robj:
            return
        size = int(robj.size)
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size
        return

    # Otherwise, pick a weak protected item via sampling
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    if key in _od_protected:
        _od_protected.pop(key, None)
        _od_window[key] = None
    m_segment[key] = 0
    robj = cache_snapshot.cache.get(key, None)
    if robj:
        size = int(robj.size)
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one protected LRU
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot, prefer_lru=True)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # The queue retains stale entries that will be cleaned as capacity is enforced; O(1) overhead.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Strategy:
      - Maintain protected budget by demoting LRU from protected if over budget.
      - Ensure window has at least one victim (demote protected LRU if needed).
      - Select a victim from a small sample of window keys via TinyLFU+recency+hits, size-aware.
      - Admission guard: if chosen victim looks stronger than incoming, evict oldest window key instead (scan resistance).
    """
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot, prefer_lru=True)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    if not candidates:
        # Fallback: if still nothing, globally choose the oldest among a few
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            # Pick the oldest among the same candidate set to avoid displacing a good key
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest
            else:
                # If window has LRU tracked, use it
                if _od_window:
                    victim_key = next(iter(_od_window))

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Maintain exact LRU order in the segment using OrderedDict.
      - Keep protected within budget via a single LRU demotion if needed.
      - Track per-residency hit count.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure indexed presence
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits:
        m_hits[key] = 0

    # Update timestamps and residency hits
    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)

    # Promote on hit: window -> protected
    if seg == 0:
        # Move in OrderedDicts
        if key in _od_window:
            _od_window.pop(key, None)
        _od_protected[key] = None
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size
    else:
        # Protected: refresh LRU
        if key in _od_protected:
            _od_protected.move_to_end(key, last=True)

    # Keep protected within budget by demoting LRU if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot, prefer_lru=True)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, per-residency hits=0.
      - Maintain index vector and OrderedDict (LRU).
      - Update bytes accounting.
      - Consume ghost feedback to adapt window fraction immediately.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Insert at window MRU
    if key in _od_protected:
        _od_protected.pop(key, None)
    _od_window[key] = None

    globals()['_g_window_bytes'] += size

    # If window too small due to prior promotions, ensure it has candidates (demote one if strictly empty)
    _ensure_window_has_candidate(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
      - Remove from OrderedDict and sampling index.
    """
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)

    # Remove from LRU structures
    if seg == 0:
        _od_window.pop(ev_key, None)
    else:
        _od_protected.pop(ev_key, None)

    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```
2025-11-28 00:51:26,446 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight SLRU + TinyLFU with sampling (LT-SLRU-TLFU)
# Goals:
# - Stable hit rate with low CPU: two segments (window/protected) and TinyLFU admission
# - Size-aware scoring and eviction via small random samples
# - Simple adaptive window fraction driven by where hits occur
# - Avoid heavy sets/ghosts/large-tracking to eliminate timeouts

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # keep small to avoid CPU spikes
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_S_WIN_EVICT = 10          # candidates to pick eviction from window
_S_PROT_DEMOTE = 8         # candidates when demoting from protected
_S_GLOBAL = 12             # global candidates if needed

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.2
_PROTECTED_KEEP_BONUS = 1.10
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.06

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger (frees space faster)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy (lightweight):
      1) Keep protected within budget via single demotion if needed.
      2) Prefer eviction from window (probationary) using sampling + size-aware score.
      3) Fall back to global sampling if window is undersized.
      4) Admission-compare guard: if incoming weaker than victim, evict the oldest window key to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected budget: demote one protected if over budget
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Collect candidates: prefer window segment
    if _g_window_bytes > target_window:
        candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)
    else:
        # If window is small, sample globally
        candidates = _sample_keys_from_segment(None, _S_GLOBAL)

    if not candidates:
        # Fallback: try oldest window; else any key
        alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, max(1, _S_WIN_EVICT // 2)))
        if alt is not None:
            return alt
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-compare guard: if incoming is weaker, prefer evicting an old window key
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt = _select_oldest_from_keys(cache_snapshot, _sample_keys_from_segment(0, _S_WIN_EVICT))
            if alt is not None:
                victim_key = alt

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and average size.
      - Update last access timestamp.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget with a single demotion if needed.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure key is indexed and in metadata
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Promote on first hit (simple SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within budget by demoting one if necessary
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust bytes accounting based on segment.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 2
- Hit Rate: 0.467

Example 2:
- Trace ID: 24
- Error: Error - Timeout

Example 3:
- Trace ID: 37
- Hit Rate: 0.6414

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m00:51:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 00:51:26,449 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:01:48,227 [INFO] openai._base_client: Retrying request to /chat/completions in 0.413462 seconds
[92m01:05:50 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:05:50,813 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:05:50,817 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved cache eviction algorithm:
# Segmented LRU with TinyLFU and size-aware scoring (S-LRU-TLFU-SA)
# Key ideas:
# - Two segments: window (probation) and protected (long-term), managed as arrays for O(1) updates and O(k) sampling
# - TinyLFU frequency sketch for reuse prediction (decayed)
# - Size-aware scoring for eviction; larger items must be hotter to stay
# - Adaptive window fraction based on where hits occur
# - Promotion guard: large objects require more hits before promotion
# - Admission guard: if incoming is weaker than victim, evict an old window item to resist scans

import random

# ------------------------
# Global metadata
# ------------------------
# Segment membership: 0 -> window, 1 -> protected
m_segment = dict()          # key -> 0/1
m_last_access = dict()      # key -> last access timestamp
m_insert_ts = dict()        # key -> insertion timestamp
m_local_hits = dict()       # key -> local hit count while resident

# Window/protected membership arrays for fast uniform sampling and O(1) removal
_win_keys = []              # list of keys in window
_win_index = dict()         # key -> index in _win_keys
_prot_keys = []             # list of keys in protected
_prot_index = dict()        # key -> index in _prot_keys

# Bytes accounting
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 2048               # keep compact for speed; depth*width small
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 50000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.65
_WINDOW_STEP = 0.03

# Sampling sizes
_S_WIN_EVICT = 10           # candidates to pick eviction from window
_S_PROT_DEMOTE = 8          # candidates when demoting from protected
_S_GLOBAL = 12              # global candidates if needed

# Scoring weights
_W_FREQ = 1.0
_W_LOCAL = 1.2
_W_RECENCY = 0.6
_PROTECTED_KEEP_BONUS = 1.15
_EPS = 1e-9

# Admission guard (scan resistance)
_ADMIT_COMPARE = 1.06

# Promotion thresholds (size-aware)
_PROMOTE_HUGE_MULT = 4.0    # > 4x avg => need 3 hits to promote
_PROMOTE_BIG_MULT = 2.0     # > 2x avg => need 2 hits to promote

# Adaptation based on hit location
_ADAPT_PERIOD = 3000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_local_hits:
            m_local_hits[key] = 0
        # Place in window arrays
        if key not in _win_index:
            _win_index[key] = len(_win_keys)
            _win_keys.append(key)
        _g_window_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.35
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _promotion_threshold_for_size(size):
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= _PROMOTE_HUGE_MULT:
        return 3
    elif r >= _PROMOTE_BIG_MULT:
        return 2
    else:
        return 1

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score => more valuable to keep
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    local = float(m_local_hits.get(key, 0))
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_LOCAL * local + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    local = 0.0  # new insert
    recency = 1.0  # now
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_LOCAL * local + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_from_list(keys_list, k):
    if not keys_list:
        return []
    k = min(k, len(keys_list))
    if k <= 0:
        return []
    # Sample by index to avoid copying keys_list
    idxs = random.sample(range(len(keys_list)), k)
    return [keys_list[i] for i in idxs]

def _select_oldest_from_list(keys_list):
    if not keys_list:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys_list:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Choose lowest score; tie-breaker: older, then larger (free more)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _win_add(key, size):
    if key in _win_index:
        return
    _win_index[key] = len(_win_keys)
    _win_keys.append(key)
    globals()['_g_window_bytes'] += int(size)
    m_segment[key] = 0

def _win_remove(key, size):
    idx = _win_index.pop(key, None)
    if idx is not None:
        last_idx = len(_win_keys) - 1
        last_key = _win_keys[last_idx]
        if idx != last_idx:
            _win_keys[idx] = last_key
            _win_index[last_key] = idx
        _win_keys.pop()
        globals()['_g_window_bytes'] -= int(size)

def _prot_add(key, size):
    if key in _prot_index:
        return
    _prot_index[key] = len(_prot_keys)
    _prot_keys.append(key)
    globals()['_g_protected_bytes'] += int(size)
    m_segment[key] = 1

def _prot_remove(key, size):
    idx = _prot_index.pop(key, None)
    if idx is not None:
        last_idx = len(_prot_keys) - 1
        last_key = _prot_keys[last_idx]
        if idx != last_idx:
            _prot_keys[idx] = last_key
            _prot_index[last_key] = idx
        _prot_keys.pop()
        globals()['_g_protected_bytes'] -= int(size)

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget
    if not _prot_keys:
        return
    candidates = _sample_from_list(_prot_keys, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    # Move from protected to window
    _prot_remove(key, size)
    _win_add(key, size)

def _adapt_controls_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total = _hits_in_window + _hits_in_protected
    if total > 0:
        frac_w = _hits_in_window / float(total)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      1) Keep protected within budget via a single demotion if needed.
      2) Prefer eviction from window using size-and-frequency-aware scoring on a small sample.
      3) Fallback to global sampling when window is undersized.
      4) Admission guard: if incoming is weaker than the chosen victim, evict the oldest window item to resist scans.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Maintain protected budget: demote one protected if over budget
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Collect candidates: prefer window segment if it is at/over target
    if _g_window_bytes >= target_window and _win_keys:
        candidates = _sample_from_list(_win_keys, _S_WIN_EVICT)
    else:
        # If window is small or empty, sample globally from both segments
        # Take half from window and half from protected
        k_w = _S_GLOBAL // 2
        k_p = _S_GLOBAL - k_w
        c_w = _sample_from_list(_win_keys, k_w) if _win_keys else []
        c_p = _sample_from_list(_prot_keys, k_p) if _prot_keys else []
        candidates = c_w + c_p

    # Fallback: if no candidates (should be rare), try oldest in window else oldest anywhere
    if not candidates:
        alt = _select_oldest_from_list(_win_keys) or _select_oldest_from_list(_prot_keys)
        return alt

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission guard: if incoming is weaker than victim by a margin, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt = _select_oldest_from_list(_win_keys)
            if alt is not None:
                victim_key = alt

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU, decay periodically, update avg size.
      - Update timestamps and local hit count.
      - Promotion: window -> protected when local_hits reaches a size-aware threshold.
      - Keep protected within budget by demoting one if necessary.
      - Track hit location and adapt window fraction periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Ensure basic metadata
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_local_hits:
        m_local_hits[key] = 0
    if key not in m_segment:
        # If missing (shouldn't happen on hit), treat as window
        m_segment[key] = 0
        _win_add(key, size)

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Count for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Local hit count for promotion gating
    m_local_hits[key] = m_local_hits.get(key, 0) + 1

    # Promotion logic: promote window item when hit count crosses threshold (size-aware)
    if seg == 0:
        need_hits = _promotion_threshold_for_size(size)
        if m_local_hits[key] >= need_hits:
            # Promote to protected
            _win_remove(key, size)
            _prot_add(key, size)
            m_segment[key] = 1

    # Keep protected within budget by demoting one if necessary
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Periodic adaptation
    _adapt_controls_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, timestamps, local hits=0.
      - Put into window and account bytes.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_local_hits[key] = 0

    _win_add(key, size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust bytes accounting based on segment.
      - Clean up from segment membership arrays.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.pop(ev_key, 0)

    # Remove from appropriate segment arrays and fix bytes
    if seg == 0:
        _win_remove(ev_key, ev_size)
    else:
        _prot_remove(ev_key, ev_size)

    # Remove other metadata
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_local_hits.pop(ev_key, None)
```
2025-11-28 01:09:54,750 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 44
- Hit Rate: 0.0419

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 14
- Hit Rate: 0.0222

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:09:54 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:09:54,752 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:12:01 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:12:01,766 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:12:01,768 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive Replacement Cache (ARC) with lazy-cleaned LRU queues
# - Two resident lists:
#     T1: recent (seen once, recency-focused)
#     T2: frequent (seen at least twice)
# - Two ghost lists:
#     B1: keys recently evicted from T1
#     B2: keys recently evicted from T2
# - Target p controls T1 size; adjusted using ARC's feedback rule:
#     * Miss on B1 -> increase p (favor recency)
#     * Miss on B2 -> decrease p (favor frequency)
# - Eviction picks LRU from T1 if |T1| >= p else from T2 (with fallbacks).
# - Deques allow duplicates; lazy-cleaning ensures O(1) per op.
# - All accounting is by item count (not bytes); eviction is by key only.
#
# Design notes:
# - Focuses on recency/frequency balance without expensive sampling or per-access scoring.
# - No reliance on object sizes (traces often behave count-limited in this framework).
# - Minimal metadata for speed and robustness across diverse workloads.

from collections import deque

# ------------------------
# Global metadata
# ------------------------
# Segments: 1 = T1 (recent), 2 = T2 (frequent)
m_segment = dict()          # key -> 1 (T1) or 2 (T2)
m_last_access = dict()      # key -> last access timestamp
m_insert_ts = dict()        # key -> insertion timestamp (for reference)

# Resident keys tracking (for fallbacks and lazy existence checks)
m_keys = []                 # list of resident keys
m_key_index = dict()        # key -> index in m_keys

# ARC queues (lazy-cleaned)
_t1 = deque()               # left = LRU, right = MRU
_t2 = deque()
_b1 = deque()               # ghosts for T1 evictions
_b2 = deque()               # ghosts for T2 evictions
_b1_set = set()
_b2_set = set()

# Target size for T1 (in items, adaptive)
_p_t1 = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _p_t1
    if _bootstrapped:
        return
    # Initialize: everything currently resident goes to T1 (recent)
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key in m_key_index:
            continue
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment[key] = 1
        m_insert_ts[key] = now
        m_last_access[key] = now
        _t1.append(key)
    # Start with small T1 target (quarter of residents, minimum 1)
    c = max(1, len(cache_snapshot.cache))
    _p_t1 = max(1, c // 4)
    _bootstrapped = True

def _clean_left(deq, want_seg):
    # Remove non-resident or wrong-segment keys from the left
    while deq:
        k = deq[0]
        if k in m_key_index and m_segment.get(k, 0) == want_seg:
            return k
        deq.popleft()
    return None

def _peek_lru_t1():
    return _clean_left(_t1, 1)

def _peek_lru_t2():
    return _clean_left(_t2, 2)

def _ensure_in_keys(key):
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _capacity_items(cache_snapshot):
    # We only need an estimate to bound ghosts. Use resident count (robust for both byte- and item-limited caches).
    return max(1, len(cache_snapshot.cache))

def _trim_ghosts(cache_snapshot):
    # Keep each ghost list <= 2C (like ARC); trim from LRU
    cap = 2 * _capacity_items(cache_snapshot)
    while len(_b1) > cap:
        old = _b1.popleft()
        _b1_set.discard(old)
    while len(_b2) > cap:
        old = _b2.popleft()
        _b2_set.discard(old)

def _arc_adjust_p_on_ghost_insert(cache_snapshot, key):
    # Called on insert: if the key is in B1 or B2, adjust p accordingly and clear from ghosts
    global _p_t1
    C = _capacity_items(cache_snapshot)
    if key in _b1_set:
        # Hit in B1 -> increase recency target
        denom = max(1, len(_b1))
        delta = max(1, len(_b2) // denom)
        _p_t1 = min(C, _p_t1 + delta)
        # Remove from B1
        _b1_set.discard(key)
        try:
            _b1.remove(key)
        except ValueError:
            pass
    elif key in _b2_set:
        # Hit in B2 -> decrease recency target (favor T2)
        denom = max(1, len(_b2))
        delta = max(1, len(_b1) // denom)
        _p_t1 = max(0, _p_t1 - delta)
        # Remove from B2
        _b2_set.discard(key)
        try:
            _b2.remove(key)
        except ValueError:
            pass
    # Trim ghosts if beyond bound
    _trim_ghosts(cache_snapshot)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC eviction policy:
      - If |T1| >= max(1, p), evict LRU from T1 (recency victims).
      - Else, evict LRU from T2 (frequency victims).
      - Fallbacks: if chosen list empty, use the other; otherwise any resident key.
    '''
    _bootstrap_if_needed(cache_snapshot)

    # Ensure T1 target is within [0, C]
    C = _capacity_items(cache_snapshot)
    global _p_t1
    if _p_t1 > C:
        _p_t1 = C
    if _p_t1 < 0:
        _p_t1 = 0

    # Choose victim according to ARC rule
    victim_key = None
    t1_head = _peek_lru_t1()
    t2_head = _peek_lru_t2()
    t1_size = 0
    t2_size = 0

    # Compute current sizes by scanning heads lazily; approximate sizes by counting segment map
    # (exact sizes are not strictly needed; we only compare >= p vs < p)
    # We compute sizes once per eviction for correctness.
    for k in list(m_key_index.keys()):
        seg = m_segment.get(k, 0)
        if seg == 1:
            t1_size += 1
        elif seg == 2:
            t2_size += 1

    if t1_size >= max(1, _p_t1):
        victim_key = t1_head if t1_head is not None else t2_head
    else:
        victim_key = t2_head if t2_head is not None else t1_head

    # Final fallbacks
    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update last access time.
      - If key in T1: promote to T2 (frequent).
      - If key in T2: move to MRU (touch).
      - If key not tracked (safety), add to T1.
    '''
    _bootstrap_if_needed(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count
    _ensure_in_keys(key)
    m_last_access[key] = now

    seg = m_segment.get(key, 0)
    if seg == 1:
        # Promote T1 -> T2
        m_segment[key] = 2
        _t2.append(key)  # MRU in T2
        # No need to remove from _t1 immediately; lazy-cleaning on access/eviction
    elif seg == 2:
        # Touch T2
        _t2.append(key)  # MRU in T2
    else:
        # Safety: unknown resident -> place in T1
        m_segment[key] = 1
        _t1.append(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Adjust p if key was in ghosts (ARC feedback), then clear from ghosts.
      - Insert into T1 as MRU (recent).
      - Initialize timestamps and bookkeeping.
      - Bound ghost sizes.
    '''
    _bootstrap_if_needed(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count

    # ARC feedback from ghosts
    _arc_adjust_p_on_ghost_insert(cache_snapshot, key)

    # Insert as recent (T1 MRU)
    _ensure_in_keys(key)
    m_segment[key] = 1
    m_insert_ts[key] = now
    m_last_access[key] = now
    _t1.append(key)

    # Keep ghosts reasonably bounded
    _trim_ghosts(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Add evicted key to appropriate ghost list (B1 if from T1; B2 if from T2).
      - Remove resident bookkeeping for evicted key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Record in ghost lists
    if seg == 1:
        if ev_key not in _b1_set:
            _b1.append(ev_key)
            _b1_set.add(ev_key)
    elif seg == 2:
        if ev_key not in _b2_set:
            _b2.append(ev_key)
            _b2_set.add(ev_key)

    _trim_ghosts(cache_snapshot)
```
2025-11-28 01:12:25,778 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 15
- Hit Rate: 0.0213

Example 2:
- Trace ID: 45
- Hit Rate: 0.069

Example 3:
- Trace ID: 18
- Hit Rate: 0.0231

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:12:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:12:25,780 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:15:08 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:15:08,563 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:15:08,564 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# GreedyDual-Size-Frequency (GDSF) with TinyLFU guidance and lazy min-heap
# - Eviction uses GreedyDual variant: each item has a priority H = L + cost.
#   On hit: H = max(H, L) + cost. On insert: H = L + base_cost.
#   Eviction removes the minimum-H item and sets L = H(evicted).
# - cost is size-aware (penalizes large items) and frequency-aware (TinyLFU provides an initial boost).
# - We keep a lazy min-heap of (priority, seq, key). Duplicates are allowed and cleaned lazily.
# - TinyLFU (count-min sketch) is decayed over time to age out old popularity.
# - This design emphasizes byte-efficient retention and adapts to changing access patterns
#   through the global age L and TinyLFU decay.

import heapq
import random

# ------------------------
# Global metadata
# ------------------------
# Priority structures
_pri = dict()            # key -> current priority (H)
_heap = []               # min-heap of (priority, seq, key), duplicates allowed (lazy deletion)
_seq = 0                 # tie-breaker sequence number for heap entries
_L = 0.0                 # global age (raised to evicted priority on each eviction)

# Basic per-key metadata
m_last_access = dict()   # key -> last access (timestamp)
m_resfreq = dict()       # key -> resident hits since (re)insert

# Average size tracking (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
_BASE_SIZE_ALPHA = 1.05     # base size penalty exponent; dynamic tweak per size
_FREQ_BOOST_INIT = 0.22     # weight for TinyLFU frequency in initial cost
_FREQ_BOOST_CAP = 8.0       # cap the TinyLFU estimate in initial boost
_HIT_COST_SCALE = 1.0       # scale for per-hit cost increment
_EPS = 1e-12

# Bootstrap flag
_bootstrapped = False

# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _dynamic_size_alpha(size):
    # Size-aware exponent: penalize very large objects more; small ones slightly less
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.30
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.15
    elif r <= 0.5:
        return max(0.92, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _cost_for_size(size, alpha):
    return _HIT_COST_SCALE / (float(max(1, int(size))) ** float(alpha))

def _initial_cost(size, freq_est):
    # Initial cost used on insert combines size penalty and a modest TinyLFU-based boost
    alpha = _dynamic_size_alpha(size)
    base = _cost_for_size(size, alpha)
    if freq_est <= 0.0:
        return base
    # Cap and scale TinyLFU estimate to avoid runaway boosts
    f = min(float(freq_est), _FREQ_BOOST_CAP)
    return base * (1.0 + _FREQ_BOOST_INIT * f)

def _push_heap(key, priority):
    global _seq
    _seq += 1
    heapq.heappush(_heap, (float(priority), _seq, key))

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _avg_size, _L
    if _bootstrapped:
        return
    # Initialize average size from snapshot
    total_bytes = 0
    total_items = 0
    for k, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize priorities for residents conservatively
    for k, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        init_cost = _initial_cost(size, 0.0)
        H = _L + init_cost
        _pri[k] = H
        _push_heap(k, H)
        m_last_access[k] = 0
        m_resfreq[k] = 0
    _bootstrapped = True

def _valid_heap_top(cache_snapshot):
    # Pop stale heap entries until the top matches current priority and is resident
    while _heap:
        pri, _, key = _heap[0]
        # Not resident anymore
        if key not in cache_snapshot.cache:
            heapq.heappop(_heap)
            continue
        # Priority stale
        current = _pri.get(key, None)
        if current is None or abs(current - pri) > _EPS:
            heapq.heappop(_heap)
            continue
        return key, pri
    return None, None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy: GreedyDual-Size-Frequency (GDSF)
      - Choose the resident item with minimum priority H.
      - H is maintained lazily in a min-heap; we pop stale entries until a valid top is found.
      - This approach optimizes for byte-efficiency and adapts to popularity shifts via global age L.
    """
    _bootstrap_if_needed(cache_snapshot)
    # Ensure some heap top is valid; if heap empty or all stale, fallback
    victim, _ = _valid_heap_top(cache_snapshot)
    if victim is not None:
        return victim
    # Fallback: pick a random resident key if heap was somehow exhausted
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache.keys()))
    return None

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch and decay periodically.
      - Update average size and last access timestamp.
      - Increase item's priority: H = max(H, L) + cost(size).
        This additive bump per hit lets frequent items float above others.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Compute per-hit cost increment
    alpha = _dynamic_size_alpha(obj.size)
    delta = _cost_for_size(obj.size, alpha)

    # Update priority
    old = _pri.get(key, 0.0)
    new_pri = max(old, _L) + delta
    _pri[key] = new_pri
    _push_heap(key, new_pri)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Update TinyLFU (for subsequent decisions) and decay periodically.
      - Initialize priority H = L + initial_cost(size, TinyLFU_estimate).
      - Record metadata.
    """
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now
    m_resfreq[key] = 1

    freq_est = _sketch_estimate(key)
    init_cost = _initial_cost(obj.size, freq_est)
    H = _L + init_cost
    _pri[key] = H
    _push_heap(key, H)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Raise global age L to evicted item's priority H(evicted), per GreedyDual.
      - Remove evicted key's metadata (heap is lazily cleaned on future evictions).
      - TinyLFU persists across evictions and is aged by decay.
    """
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    # Update global age L to evicted priority; if missing, leave L as is
    ev_pri = _pri.pop(ev_key, None)
    if ev_pri is not None:
        # L must be non-decreasing
        global _L
        if ev_pri > _L:
            _L = ev_pri

    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
```
2025-11-28 01:15:26,964 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware, 2Q/SLRU-style with TinyLFU guidance, adaptive window, ghost-feedback
# Key changes vs previous version:
#  - Promotion policy: promote from window -> protected only after enough resident hits
#    (threshold increases with object size; small objects promote faster).
#  - Eviction policy: strictly prefer evicting from window (probation). If window is empty
#    or protected is over budget, demote oldest protected to window first (ARC/SLRU style).
#  - Protected demotion uses oldest-protected heuristic; window victim uses oldest-of-window,
#    with size-aware tie-breaking via sampled TinyLFU score.
#  - Ghost-feedback retained to adapt window fraction; also allow direct insert into protected
#    on reappearance of protected-ghost items if TinyLFU supports it.
#  - Maintain resident-hit counters per key to avoid over-promotion on one-hit wonders.
#  - Size-aware promotion thresholds and dynamic size penalty exponent tuned for the incoming size.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
m_resfreq = dict()        # key -> resident hits since (re)insert

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01  # small EMA step

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384  # wider for better frequency estimation
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling
_SAMPLE_SIZE = 32           # candidates per decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected
_SAMPLE_SIZE_OLDEST = 24    # oldest-of sampling

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.08     # base size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 2.8            # weight for recency (more recency-sensitive in probation)
_PROTECTED_KEEP_BONUS = 1.12  # protected items harder to evict
_EPS = 1e-9

# Admit compare guard: if all sampled candidates appear stronger than incoming, keep scanning resistance
_ADMIT_COMPARE_FACTOR = 1.08

# Adaptive window ratio based on where hits happen
_ADAPT_PERIOD = 3500
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # ghost capacity factor vs estimated item capacity

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_resfreq:
            m_resfreq[key] = 1
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _dynamic_size_alpha(incoming_size):
    # Adjust size penalty based on incoming size relative to average
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.15)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # as if just accessed
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _window_oldest_candidate(cache_snapshot):
    # Sample from window and pick the oldest by last_access
    candidates = _sample_keys_from_segment(0, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _protected_oldest_candidate(cache_snapshot):
    # Sample from protected and pick the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _global_oldest_candidate(cache_snapshot):
    # Sample globally and pick the oldest
    candidates = _sample_keys_from_segment(None, _SAMPLE_SIZE_OLDEST)
    if not candidates:
        return None
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _maybe_demote_from_protected(cache_snapshot):
    # Demote oldest protected items until protected fits its byte budget
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget:
        cand = _protected_oldest_candidate(cache_snapshot)
        if cand is None:
            break
        robj = cache_snapshot.cache.get(cand, None)
        if robj is None:
            break
        if m_segment.get(cand, 0) == 1:
            m_segment[cand] = 0
            sz = int(robj.size)
            _g_protected_bytes -= sz
            _g_window_bytes += sz

def _demote_one_from_protected(cache_snapshot):
    # Helper: demote one oldest protected item into window; return True if demoted
    global _g_window_bytes, _g_protected_bytes
    cand = _protected_oldest_candidate(cache_snapshot)
    if cand is None:
        return False
    robj = cache_snapshot.cache.get(cand, None)
    if robj is None:
        return False
    if m_segment.get(cand, 0) == 1:
        m_segment[cand] = 0
        sz = int(robj.size)
        _g_protected_bytes = max(0, _g_protected_bytes - sz)
        _g_window_bytes += sz
        return True
    return False

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    # Estimate item capacity and scale for ghost lists
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    # Insert key into appropriate ghost set with trimming
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _promotion_threshold(size):
    # Size-aware promotion threshold (resident hits required to promote to protected)
    # Small <= 0.5x avg: 1 hit; medium <= 2x avg: 2 hits; large <= 4x: 3 hits; very large: 4 hits
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r <= 0.5:
        return 1
    elif r <= 2.0:
        return 2
    elif r <= 4.0:
        return 3
    else:
        return 4


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy (SLRU/TinyLFU hybrid):
      - Always prefer evicting from window (probation) using oldest-of-window heuristic.
      - Keep protected within a dynamic budget; if protected exceeds its budget, demote oldest
        protected items to window first.
      - If window is empty (or nearly empty), demote one oldest protected to window before evicting.
      - Within window, use oldest as primary victim; on ties, apply size-aware TinyLFU scoring over a
        small sample to pick least valuable.
      - Guard against polluting admission (if candidate looks stronger than the incoming)
        by picking an even older window candidate if available, or global oldest as fallback.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Update protected to respect its budget before choosing a victim
    _maybe_demote_from_protected(cache_snapshot)

    # Ensure there is something in window to evict; if empty, demote one from protected
    if _g_window_bytes <= 0:
        _demote_one_from_protected(cache_snapshot)

    # Try to pick a victim from window first
    window_victim = _window_oldest_candidate(cache_snapshot)

    # If still no window victim available, fallback to global oldest (rare)
    if window_victim is None:
        gv = _global_oldest_candidate(cache_snapshot)
        if gv is None:
            if m_keys:
                return m_keys[0]
            return None
        return gv

    # Improve victim choice within window using TinyLFU size-aware scoring
    candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    # Ensure the oldest is considered
    if window_victim not in candidates:
        candidates.append(window_victim)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    # Scan/pollution guard: if best window candidate still looks much stronger than incoming,
    # try to pick an even older window item (bias toward LRU in window).
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _window_oldest_candidate(cache_snapshot)
            if alt is not None:
                victim_key = alt
            else:
                g_old = _global_oldest_candidate(cache_snapshot)
                if g_old is not None:
                    victim_key = g_old

    if victim_key is None:
        # Last fallback
        if m_keys:
            return m_keys[0]
        return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch (admission estimator) and decay if needed.
      - Update running average size (EMA).
      - Update last access timestamp.
      - Increment resident-hit counter; promote from window -> protected when resident hits
        reach a size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Keep protected within budget by demoting oldest protected when needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata exists (in case of late init)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz
    if key not in m_resfreq:
        m_resfreq[key] = 0

    seg = m_segment.get(key, 0)

    # Count hit location for window adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Bump resident hit counter
    m_resfreq[key] = m_resfreq.get(key, 0) + 1

    # Size-aware promotion from window -> protected
    if seg == 0:
        thresh = _promotion_threshold(obj.size)
        if m_resfreq.get(key, 0) >= thresh:
            m_segment[key] = 1
            sz = int(obj.size)
            globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - sz)
            globals()['_g_protected_bytes'] += sz

    # Keep protected within budget
    _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access and decay.
      - Initialize metadata: segment = window (probation) and last_access = now.
      - If key is present in protected-ghost and TinyLFU indicates moderate frequency,
        admit directly into protected (size-aware threshold).
      - Track resident key index and bytes accounting.
      - Initialize resident-hit counter to 1 (this access).
      - Ghost-feedback: adjust window fraction based on which ghost contained the key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Default placement: window
    place_protected = False
    freq_est = _sketch_estimate(key)

    # If it was recently in protected-ghost and has some frequency, place into protected directly
    if key in _ghost_p_set:
        # Require a small TinyLFU estimate and avoid very large objects jumping straight in
        r = float(max(1, int(obj.size))) / max(1.0, _avg_size)
        if freq_est >= 2.0 and r <= 4.0:
            place_protected = True

    if place_protected:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now
    m_resfreq[key] = 1  # counts this access

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    sz = int(obj.size)
    if place_protected:
        globals()['_g_protected_bytes'] += sz
    else:
        globals()['_g_window_bytes'] += sz

    # Ghost-feedback adaptation on window fraction
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy removal from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Ensure protected budget constraint
    _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Add key to ghost history (segment-aware) to steer window/protected ratio.
      - TinyLFU persists across evictions; decay handles aging over time.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resfreq.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, globals()['_g_window_bytes'] - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, globals()['_g_protected_bytes'] - ev_size)

    # Add to ghost sets with capacity-based trimming
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 47
- Hit Rate: 0.1013

Example 2:
- Trace ID: 10
- Hit Rate: 0.0917

Example 3:
- Trace ID: 31
- Error: Error - Timeout

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:15:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:15:26,965 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:18:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:18:28,601 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:18:28,604 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight, fast SLRU with OrderedDict LRU, TinyLFU guidance, and ARC-style ghost feedback
# Goals:
#  - Strong recency through a window (probation) segment
#  - Strong frequency via quick promotion to protected segment
#  - O(1) metadata updates via OrderedDict (true LRU per segment)
#  - TinyLFU for admission guidance and mild eviction tie-breaking
#  - ARC-like ghost feedback to adapt window fraction
#  - Size-aware promotion gating to avoid pinning very large objects

import random
import itertools
from collections import OrderedDict, deque

# ------------------------
# Global metadata
# ------------------------
# Segments: window (probation) and protected; both as LRU queues
_window_od = OrderedDict()    # key -> None
_protected_od = OrderedDict() # key -> None

# Byte counters (kept consistent)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
# Kept small for speed; decay ages old history.
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 10000
_decay_ticker = 0

# Running average size (EMA) for size-aware promotion
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# Adaptive window fraction (bytes) with ghost feedback and hit-location feedback
_f_window = 0.28
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Hit-location adaptation
_ADAPT_PERIOD = 4000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like), bounded by estimated item capacity
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0  # multiplier on estimated item capacity

# Resident hits since insert (for size-aware promotion gating)
_m_hits_since_insert = dict()

# Bootstrap guard
_bootstrapped = False


# ------------------------
# TinyLFU helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est == float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # halve all counters
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0


# ------------------------
# Size helpers and bootstrap
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _get_size_from_cache(cache_snapshot, key, default=1):
    robj = cache_snapshot.cache.get(key, None)
    if robj is None:
        return default
    try:
        return int(robj.size)
    except Exception:
        return default

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize: place all existing items into window (probation) in iteration order
    for key, robj in cache_snapshot.cache.items():
        if key not in _window_od and key not in _protected_od:
            _window_od[key] = None
            sz = int(robj.size)
            _g_window_bytes += sz
            total_bytes += sz
            total_items += 1
            _m_hits_since_insert[key] = 0
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))


# ------------------------
# Segment maintenance
# ------------------------
def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    # Move oldest protected -> window tail
    if not _protected_od:
        return False
    key, _ = _protected_od.popitem(last=False)
    sz = _get_size_from_cache(cache_snapshot, key, 1)
    _protected_od_changed = True
    _window_od[key] = None
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    # Do not reset hits; keep whatever it had
    return True

def _rebalance_protected_budget(cache_snapshot):
    # Keep protected within budget by demoting oldest items
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    while _g_protected_bytes > protected_budget and _protected_od:
        _demote_one_from_protected(cache_snapshot)

def _promotion_threshold(size_bytes):
    # Size-aware gating: normal objects promote on first hit; very large need a bit more.
    r = float(max(1, int(size_bytes))) / max(1.0, _avg_size)
    if r <= 2.0:
        return 1
    elif r <= 8.0:
        return 2
    else:
        return 3

def _promote_to_protected(cache_snapshot, key, size_bytes=None):
    global _g_window_bytes, _g_protected_bytes
    if key not in _window_od:
        return
    if size_bytes is None:
        size_bytes = _get_size_from_cache(cache_snapshot, key, 1)
    # Remove from window, add to protected tail
    _window_od.pop(key, None)
    _protected_od[key] = None
    _g_window_bytes = max(0, _g_window_bytes - size_bytes)
    _g_protected_bytes += size_bytes

def _ensure_in_window_tail(cache_snapshot, key, size_bytes):
    global _g_window_bytes
    if key in _window_od:
        _window_od.move_to_end(key, last=True)
        return
    if key in _protected_od:
        # Move from protected to window (rare; used in bootstrap corrections)
        _protected_od.pop(key, None)
        _g_protected_bytes = max(0, _g_protected_bytes - size_bytes)
    _window_od[key] = None
    _g_window_bytes += size_bytes

def _ensure_in_protected_tail(cache_snapshot, key, size_bytes):
    global _g_protected_bytes
    if key in _protected_od:
        _protected_od.move_to_end(key, last=True)
        return
    if key in _window_od:
        _window_od.pop(key, None)
        _g_window_bytes = max(0, _g_window_bytes - size_bytes)
    _protected_od[key] = None
    _g_protected_bytes += size_bytes


# ------------------------
# Adaptation and ghosts
# ------------------------
def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.56:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.44:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
      - Maintain protected within its byte budget by demoting LRU protected -> window.
      - Evict from window LRU (true oldest). If window is empty, demote one from protected and evict it.
      - TinyLFU guidance: among the first few oldest window keys, evict the one with the lowest estimate.
        This improves resistance to evicting hot-but-old entries while keeping O(1) behavior.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _rebalance_protected_budget(cache_snapshot)

    # If no window items, demote one from protected to ensure a window victim exists
    if not _window_od and _protected_od:
        _demote_one_from_protected(cache_snapshot)

    # If still nothing, fall back to any cached key
    if not _window_od:
        # Rare; choose any key from cache (shouldn't happen often)
        for k in cache_snapshot.cache.keys():
            return k
        return None

    # Take a small slice of the oldest window candidates
    CAND_MAX = 8
    candidates = list(itertools.islice(_window_od.keys(), CAND_MAX))
    if not candidates:
        # Shouldn't happen as _window_od is non-empty
        for k in cache_snapshot.cache.keys():
            return k
        return None

    # Pick candidate with lowest TinyLFU estimate (tie-break by LRU order)
    best_key = None
    best_freq = None
    for k in candidates:
        f = _sketch_estimate(k)
        if best_key is None or f < best_freq:
            best_key = k
            best_freq = f

    return best_key


def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU counters and EMA size.
      - Move to MRU of its segment; if in window, promote to protected when hits exceed size-aware threshold.
      - Track hit locations for adaptive window sizing.
      - Rebalance protected to respect its budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # Count hit for adaptation
    global _adapt_ticker, _hits_in_window, _hits_in_protected
    _adapt_ticker += 1

    if key in _window_od:
        _hits_in_window += 1
        # Update hit counter
        _m_hits_since_insert[key] = _m_hits_since_insert.get(key, 0) + 1
        # Promote if threshold reached
        if _m_hits_since_insert[key] >= _promotion_threshold(sz):
            _promote_to_protected(cache_snapshot, key, sz)
            _protected_od.move_to_end(key, last=True)
        else:
            _window_od.move_to_end(key, last=True)
    elif key in _protected_od:
        _hits_in_protected += 1
        _protected_od.move_to_end(key, last=True)
    else:
        # Rare: metadata drift (e.g., after bootstrap). Put into window and handle like first hit.
        _ensure_in_window_tail(cache_snapshot, key, sz)
        _m_hits_since_insert[key] = _m_hits_since_insert.get(key, 0) + 1
        if _m_hits_since_insert[key] >= _promotion_threshold(sz):
            _promote_to_protected(cache_snapshot, key, sz)
            _protected_od.move_to_end(key, last=True)

    # Keep protected within budget
    _rebalance_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()


def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU and EMA size.
      - Place into window MRU by default; if present in protected-ghost (and not extremely large),
        place directly into protected MRU.
      - Reset resident hit counter to 0.
      - Ghost feedback to adjust window fraction.
      - Rebalance protected to budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    sz = int(obj.size)

    # Remove stale placements if any (safety)
    if key in _window_od:
        # Already present; move to MRU
        _window_od.move_to_end(key, last=True)
        _m_hits_since_insert[key] = 0
        return
    if key in _protected_od:
        _protected_od.move_to_end(key, last=True)
        _m_hits_since_insert[key] = 0
        return

    # Direct placement into protected if it was previously protected-ghost and not huge
    place_protected = False
    if key in _ghost_p_set:
        ratio = float(max(1, sz)) / max(1.0, _avg_size)
        if ratio <= 4.0:
            place_protected = True

    if place_protected:
        _ensure_in_protected_tail(cache_snapshot, key, sz)
    else:
        _ensure_in_window_tail(cache_snapshot, key, sz)

    _m_hits_since_insert[key] = 0

    # Ghost-feedback window fraction adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)  # lazy unlink from deque on trim
    if key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Keep protected within budget (may demote some old protected)
    _rebalance_protected_budget(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove key from the appropriate segment and bytes counters.
      - Drop its hit counter.
      - Add to ghost history (segment-aware).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_sz = int(evicted_obj.size)

    was_protected = False
    if ev_key in _protected_od:
        _protected_od.pop(ev_key, None)
        # Adjust bytes
        global _g_protected_bytes
        _g_protected_bytes = max(0, _g_protected_bytes - ev_sz)
        was_protected = True
    elif ev_key in _window_od:
        _window_od.pop(ev_key, None)
        global _g_window_bytes
        _g_window_bytes = max(0, _g_window_bytes - ev_sz)
    else:
        # Not in our structures (could be due to drift); nothing else to do.
        was_protected = False

    _m_hits_since_insert.pop(ev_key, None)

    # Add to ghost with capacity trimming
    _ghost_add(is_protected=was_protected, key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-28 01:18:46,665 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.0814

Example 2:
- Trace ID: 21
- Hit Rate: 0.0262

Example 3:
- Trace ID: 46
- Hit Rate: 0.4663

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:18:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:18:46,666 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:29:00,012 [INFO] openai._base_client: Retrying request to /chat/completions in 0.402015 seconds
2025-11-28 01:39:11,862 [INFO] openai._base_client: Retrying request to /chat/completions in 0.911224 seconds
[92m01:41:32 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:41:32,499 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:41:32,501 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Window TinyLFU (W-TinyLFU) with size-aware SLRU and density-guided selection
# - Three resident segments (all bytes-aware LRU via OrderedDict):
#     W: small admission window for recency (LRU)
#     P: probationary segment (new or demoted)
#     S: protected segment (frequently hit)
# - No ghosts; a global TinyLFU (count-min sketch with periodic decay) drives admission/eviction.
# - Eviction policy:
#     * Prefer evicting from W when it exceeds its window target.
#     * Otherwise evict from P.
#     * Within a chosen segment, select victim among the K oldest with minimal "value density":
#           score = TinyLFU_estimate / size^alpha(size)
#     * Admission control: when W is oversized and P has candidates, compare the incoming object's
#           density to the P-victim's density. If incoming <= victim_P, evict the incoming object
#           (i.e., return obj.key) to reject the admission. Otherwise evict from W.
# - Hits:
#     * W hit -> move to MRU of W.
#     * P hit -> promote to S (protected); if S exceeds its target, demote S's LRU to P.
#     * S hit -> move to MRU of S.
# - Inserts:
#     * Insert to W (MRU). TinyLFU updated on every access (hit+insert).
# - Evictions:
#     * Only W or P are evicted. S never directly evicted; it is kept in check by demotions.
#
# All accounting is size-aware (bytes). The TinyLFU density uses a dynamic size penalty alpha.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_W = OrderedDict()  # window (recency)   : key -> size
_P = OrderedDict()  # probationary       : key -> size
_S = OrderedDict()  # protected          : key -> size

# Byte accounting
_w_bytes = 0
_p_bytes = 0
_s_bytes = 0

# Targets (bytes)
_win_target_bytes = 0
_prot_target_bytes = 0  # max bytes in protected S

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized more; tiny objects slightly favored.
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 8.0:
        return _BASE_ALPHA + 0.55
    elif r >= 4.0:
        return _BASE_ALPHA + 0.35
    elif r >= 2.0:
        return _BASE_ALPHA + 0.18
    elif r <= 0.5:
        return max(0.88, _BASE_ALPHA - 0.12)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _choose_low_density_victim(od):
    # Among the K oldest keys of the given segment, choose the one with minimal density score.
    if not od:
        return None, None
    candidates = _segment_oldest_keys(od, min(_SAMPLE_K, len(od)))
    victim_key = None
    victim_score = None
    victim_size = None
    for k in candidates:
        sz = od.get(k, 1)
        sc = _score_value_density(k, sz)
        if victim_key is None or sc < victim_score:
            victim_key = k
            victim_score = sc
            victim_size = sz
    return victim_key, victim_size

def _ensure_targets(capacity):
    # Compute window and protected targets.
    # Window: ~5% of capacity, but at least 2 avg objects and at most 15% of capacity.
    global _win_target_bytes, _prot_target_bytes
    base_win = int(0.05 * capacity)
    min_win = int(max(2 * _avg_size, 0.02 * capacity))
    max_win = int(0.15 * capacity)
    _win_target_bytes = max(min_win, min(max_win, base_win))
    # Protected: ~85% of non-window region
    non_window = max(0, capacity - _win_target_bytes)
    _prot_target_bytes = int(0.85 * non_window)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _w_bytes, _p_bytes, _s_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize: place all current cache entries into probation (P) as we don't know history.
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_P, key, size)
        _p_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        # Initialize avg size
        global _avg_size
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _ensure_targets(cache_snapshot.capacity)
    _bootstrapped = True

def _demote_S_if_needed():
    # Demote from S to P to respect protected target.
    global _s_bytes, _p_bytes
    while _s_bytes > _prot_target_bytes and _S:
        k, sz = _odict_pop_lru(_S)  # LRU of S
        _s_bytes -= int(sz)
        _odict_add_mru(_P, k, sz)
        _p_bytes += int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    W-TinyLFU eviction and admission:
      - If window W exceeds target, candidate is from W.
        * If P non-empty: compare incoming obj's density vs P-victim's density.
          - If incoming <= victim_P and obj is resident, evict the incoming (reject).
          - Else evict from W.
        * If P empty: evict from W.
      - Else (W within target): evict from P (density-guided among K oldest).
      - Fallbacks always return a key from the cache if possible.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _ensure_targets(cache_snapshot.capacity)

    # Shortcuts
    global _w_bytes, _p_bytes, _s_bytes

    # If everything is empty in metadata, fallback to any cache key
    if (not _W) and (not _P) and (not _S):
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None

    # If window is oversized, consider admission decision
    if _w_bytes > _win_target_bytes and _W:
        # Candidate from W (prefer LRU among a small sample with density guidance)
        w_victim_key, _ = _choose_low_density_victim(_W)
        if not w_victim_key:
            # Fallback to any W key
            try:
                w_victim_key = next(iter(_W.keys()))
            except StopIteration:
                w_victim_key = None

        if _P:
            # P-victim for admission comparison (density-guided)
            p_victim_key, p_victim_sz = _choose_low_density_victim(_P)
            # Compute incoming object's density
            in_score = None
            if obj is not None:
                in_score = _score_value_density(obj.key, int(obj.size))
            # If incoming has weaker or equal density than the P-victim, reject it (if resident)
            if (obj is not None) and (obj.key in cache_snapshot.cache) and (p_victim_key is not None):
                p_victim_score = _score_value_density(p_victim_key, p_victim_sz)
                # Large-object safety: strongly discourage huge inserts
                huge_obj = int(obj.size) > max(1, int(0.5 * cache_snapshot.capacity))
                if huge_obj or (in_score is not None and in_score <= p_victim_score):
                    return obj.key  # reject admission by evicting the newly inserted object
        # Otherwise evict from W
        if w_victim_key is not None:
            return w_victim_key

    # Otherwise evict from P (preferred), falling back to W, then S if needed
    if _P:
        p_victim_key, _ = _choose_low_density_victim(_P)
        if p_victim_key is not None:
            return p_victim_key
    if _W:
        w_victim_key, _ = _choose_low_density_victim(_W)
        if w_victim_key is not None:
            return w_victim_key
    if _S:
        s_victim_key, _ = _choose_low_density_victim(_S)
        if s_victim_key is not None:
            return s_victim_key

    # Final fallback: any key from cache
    try:
        return next(iter(cache_snapshot.cache.keys()))
    except StopIteration:
        return None

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and avg size.
      - If in W: move to MRU of W.
      - If in P: promote to S; demote S's LRU to P if S exceeds target.
      - If in S: refresh MRU in S.
      - If not tracked, add to W as MRU (best-effort recovery).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)
    _ensure_targets(cache_snapshot.capacity)

    key = obj.key
    size = int(obj.size)
    global _w_bytes, _p_bytes, _s_bytes

    if key in _W:
        _odict_move_to_mru(_W, key)
    elif key in _P:
        # Promote P -> S
        old_sz = _P.pop(key)
        _p_bytes -= int(old_sz)
        _odict_add_mru(_S, key, size)
        _s_bytes += size
        _demote_S_if_needed()
    elif key in _S:
        _odict_move_to_mru(_S, key)
    else:
        # Not tracked (metadata drift), place into W
        _odict_add_mru(_W, key, size)
        _w_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and avg size.
      - Insert into Window W (MRU).
      - Targets recomputed based on capacity and avg size.
      - Note: Admission control enforced in evict() by possibly evicting the incoming object.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)
    _ensure_targets(cache_snapshot.capacity)

    key = obj.key
    size = int(obj.size)
    global _w_bytes

    # If already tracked, refresh its segment accordingly
    if key in _W:
        _odict_move_to_mru(_W, key)
        return
    if key in _P:
        # Treat as re-insert anomaly: promote to S (it's a reuse)
        old_sz = _P.pop(key)
        global _p_bytes, _s_bytes
        _p_bytes -= int(old_sz)
        _odict_add_mru(_S, key, size)
        _s_bytes += size
        _demote_S_if_needed()
        return
    if key in _S:
        _odict_move_to_mru(_S, key)
        return

    # Normal insert to window
    _odict_add_mru(_W, key, size)
    _w_bytes += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from whichever resident segment it belongs to.
      - No ghosts; TinyLFU persists globally (with periodic decay).
      - Maintain segment byte counters and protected demotions if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _ensure_targets(cache_snapshot.capacity)

    ev_key = evicted_obj.key
    global _w_bytes, _p_bytes, _s_bytes

    if ev_key in _W:
        sz = _W.pop(ev_key)
        _w_bytes -= int(sz)
    elif ev_key in _P:
        sz = _P.pop(ev_key)
        _p_bytes -= int(sz)
    elif ev_key in _S:
        sz = _S.pop(ev_key)
        _s_bytes -= int(sz)
    # If not in any segment, nothing to do (metadata drift).

    # Keep S within target by demoting if needed (rare after direct S eviction fallback)
    _demote_S_if_needed()
```
2025-11-28 01:41:57,725 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 39
- Hit Rate: 0.0909

Example 2:
- Trace ID: 41
- Hit Rate: 0.0725

Example 3:
- Trace ID: 0
- Hit Rate: 0.461

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:41:57 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:41:57,727 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:44:57 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:44:57,513 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:44:57,514 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Enhanced A-SS-TinyLFU with ARC-style ghost adaptation and size-aware victimization
# Key upgrades:
# - ARC-style ghost lists (for misses) adjust window/protected balance using evicted-key feedback.
# - Resident hit counter to sharpen in-cache popularity signal (with TinyLFU as global admission prior).
# - Size-aware victim scoring to avoid evicting many small hot items for a single large insert.
# - Stronger protected budget enforcement (looped demotion by oldest via sampling).
# - Mixed-segment sampling when choosing a victim (keeps protected honest without destabilizing it).
# - Size-aware promotion gate for very large items (avoid promoting large, marginal entries too eagerly).

import random
from collections import OrderedDict
import math

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_resident_hits = dict()  # key -> int (in-cache hits)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys

# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Adaptive window sizing (fraction of bytes devoted to window)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.02

# Sampling sizes
_SAMPLE_SIZE = 48            # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 32     # candidates to demote from protected (oldest approx)

# Priority scoring weights
_SIZE_ALPHA = 1.08           # size penalty exponent (slightly size-aware)
_W_FREQ = 1.0                # weight for global frequency (TinyLFU)
_W_RECENCY = 3.5             # weight for recency
_W_RESIDENT_HITS = 0.6       # weight for resident hit count (log-scaled)
_PROTECTED_KEEP_BONUS = 1.25 # protected items are harder to evict
_EPS = 1e-9

# ARC-style ghost caches (LRU of recently-evicted keys)
_g_ghost_w = OrderedDict()  # key -> (size, last_ts)
_g_ghost_p = OrderedDict()  # key -> (size, last_ts)
_g_ghost_w_bytes = 0
_g_ghost_p_bytes = 0
_GHOST_FRACTION = 1.0       # combined ghost budget as a fraction of capacity

# Adapt window fraction based on where hits happen (hit-driven adaptation)
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_resident_hits:
            m_resident_hits[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += int(robj.size)
        else:
            _g_protected_bytes += int(robj.size)
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _maybe_demote_from_protected(cache_snapshot):
    # If protected exceeds its byte budget, demote the oldest (approx via sampling) until within budget.
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    while _g_protected_bytes > protected_budget:
        candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
        if not candidates:
            break
        oldest_key = None
        oldest_ts = None
        for key in candidates:
            ts = m_last_access.get(key, 0)
            if oldest_key is None or ts < oldest_ts:
                oldest_key = key
                oldest_ts = ts
        if oldest_key is None:
            break
        robj = cache_snapshot.cache.get(oldest_key, None)
        if robj is None:
            # stale metadata; fix
            m_segment.pop(oldest_key, None)
            m_last_access.pop(oldest_key, None)
            m_resident_hits.pop(oldest_key, None)
            _remove_key_from_index(oldest_key)
            continue
        size = int(robj.size)
        if m_segment.get(oldest_key, 0) == 1:
            m_segment[oldest_key] = 0
            _g_protected_bytes -= size
            _g_window_bytes += size
        else:
            break  # already demoted

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_budget_bytes(capacity):
    return int(_GHOST_FRACTION * capacity)

def _ghost_trim_to_budget(capacity):
    global _g_ghost_w_bytes, _g_ghost_p_bytes
    budget = _ghost_budget_bytes(capacity)
    total = _g_ghost_w_bytes + _g_ghost_p_bytes
    if total <= budget:
        return
    # Evict from the ghost that exceeds its fair share more (simple heuristic)
    while _g_ghost_w_bytes + _g_ghost_p_bytes > budget:
        # Decide which ghost to evict from: remove from the one with larger byte share
        if _g_ghost_w_bytes >= _g_ghost_p_bytes:
            if not _g_ghost_w:
                break
            k, (sz, _) = _g_ghost_w.popitem(last=False)  # LRU
            _g_ghost_w_bytes -= sz
        else:
            if not _g_ghost_p:
                break
            k, (sz, _) = _g_ghost_p.popitem(last=False)
            _g_ghost_p_bytes -= sz

def _ghost_on_evict(cache_snapshot, key, size, seg, now):
    global _g_ghost_w_bytes, _g_ghost_p_bytes
    if seg == 0:
        # Window ghost
        if key in _g_ghost_w:
            old_sz, _ = _g_ghost_w.pop(key)
            _g_ghost_w_bytes -= old_sz
        _g_ghost_w[key] = (size, now)
        _g_ghost_w.move_to_end(key, last=True)
        _g_ghost_w_bytes += size
    else:
        # Protected ghost
        if key in _g_ghost_p:
            old_sz, _ = _g_ghost_p.pop(key)
            _g_ghost_p_bytes -= old_sz
        _g_ghost_p[key] = (size, now)
        _g_ghost_p.move_to_end(key, last=True)
        _g_ghost_p_bytes += size
    _ghost_trim_to_budget(cache_snapshot.capacity)

def _ghost_on_insert_adapt(cache_snapshot, key):
    # If this miss/insert touched a ghost, adapt window fraction like ARC
    global _f_window, _g_ghost_w_bytes, _g_ghost_p_bytes
    if key in _g_ghost_w:
        # Increase window; recency is useful
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP * 1.5)
        # Touch/move and remove entry to prevent repeated inflation on same key
        sz, ts = _g_ghost_w.pop(key)
        _g_ghost_w_bytes -= sz
    elif key in _g_ghost_p:
        # Decrease window; frequency/protection is more beneficial
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP * 1.5)
        sz, ts = _g_ghost_p.pop(key)
        _g_ghost_p_bytes -= sz
    # Keep within global ghost budget
    _ghost_trim_to_budget(cache_snapshot.capacity)

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size=None, incoming_freq=None):
    # Keep-score (higher => keep; lower => evict). We will select the minimum score as victim.
    # Base components
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    res_hits = m_resident_hits.get(key, 0)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))

    # Augment global frequency with resident signal (log to dampen)
    eff_freq = freq + _W_RESIDENT_HITS * math.log1p(res_hits)

    base = (_W_FREQ * eff_freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)

    # Protected items are slightly harder to evict
    if m_segment.get(key, 0) == 1:
        base *= _PROTECTED_KEEP_BONUS

    # Size-aware fit adjustment: when incoming is large, avoid evicting many small hot items
    if incoming_size is not None and cache_snapshot.capacity > 0:
        inc_ratio = float(incoming_size) / float(cache_snapshot.capacity)
        # Boost keep-score for very small items under large inserts
        smallness = min(5.0, (0.05 * cache_snapshot.capacity) / float(size))  # >1 for small items
        fit_multiplier = 1.0 + 0.75 * inc_ratio * smallness
        base *= fit_multiplier

    return base


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Victim selection:
      - Enforce window target by primarily evicting from window when over target.
      - Otherwise, sample from both segments (biased to window) and evict min keep-score.
      - Keep-score blends TinyLFU, resident hits, and recency; size-aware to avoid
        evicting many small hot items for a single large insertion.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    target_window = _current_window_target_bytes(cache_snapshot)
    window_over = _g_window_bytes > target_window

    # Incoming frequency to decide aggressiveness (optional)
    inc_freq = _sketch_estimate(obj.key)

    # Choose candidates via sampling
    if window_over:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        # Mixed sampling: bias to window but include protected
        k_w = min(len(m_keys), int(_SAMPLE_SIZE * 0.6))
        k_p = max(0, _SAMPLE_SIZE - k_w)
        cand_w = _sample_keys_from_segment(0, k_w)
        cand_p = _sample_keys_from_segment(1, k_p)
        candidates = cand_w + [k for k in cand_p if k not in set(cand_w)]
        if not candidates:
            candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    if not candidates:
        # ultimate fallback
        candidates = list(cache_snapshot.cache.keys())

    # Select victim with minimum keep-score (lower => easier to evict)
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    inc_size = int(obj.size)

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size=inc_size, incoming_freq=inc_freq)
        last_ts = m_last_access.get(key, 0)
        # Lower score preferred; tie-break by oldest last access, then by larger size
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and last access.
      - Increment resident hit counter.
      - Promote window->protected on hit, with mild size-aware gate for very large items.
      - Adapt window fraction periodically based on hit distribution.
      - Ensure protected stays within its budget (looped demotion by oldest via sampling).
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    cap = max(1, int(cache_snapshot.capacity))

    # Ensure resident metadata
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_resident_hits:
        m_resident_hits[key] = 0
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        sz = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += sz
        else:
            globals()['_g_protected_bytes'] += sz

    seg = m_segment.get(key, 0)

    # Hit accounting for adaptive window
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update metadata
    m_last_access[key] = now
    m_resident_hits[key] = m_resident_hits.get(key, 0) + 1

    # Size-aware promotion: promote on hit; for very large items require evidence
    if seg == 0:
        sz = int(obj.size)
        size_ratio = float(sz) / float(cap)
        allow_promote = True
        if size_ratio > 0.10:
            # Require either 2+ resident hits or decent global freq to promote very large items
            if m_resident_hits.get(key, 0) < 2 and _sketch_estimate(key) < 2.0:
                allow_promote = False
        if allow_promote:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= sz
            globals()['_g_protected_bytes'] += sz

            # Keep protected within its budget by demoting old protected keys if needed
            _maybe_demote_from_protected(cache_snapshot)

    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU for this access.
      - Initialize metadata: segment=window, last_access=now, resident_hits=0.
      - Track resident key for O(1) sampling and bytes accounting.
      - ARC-style ghost check to adjust window fraction based on where the miss reappeared.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_resident_hits[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += int(obj.size)

    # Adapt window fraction based on ghost feedback (ARC heuristic)
    _ghost_on_insert_adapt(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata and update bytes counters and sampling structures.
      - Add evicted key to ARC-style ghost list (segment-aware) to guide future adaptation.
      - Count-Min sketch persists (decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    now = cache_snapshot.access_count

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_resident_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Insert into ghost lists (for ARC-style window adaptation on future misses)
    _ghost_on_evict(cache_snapshot, ev_key, ev_size, seg, now)
```
2025-11-28 01:48:30,459 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Sampled, Size-aware TinyLFU (A-SS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: "window" (probation/recency) and "protected" (frequency).
# - Eviction is sampling-based to avoid full scans (prevents timeouts on large caches).
# - Dynamic window fraction adapts based on where hits occur.
# - Per-resident metadata kept compact; also maintain an indexable key list for O(1) sampling.

import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables (carefully chosen to balance hit rate and speed)
# - Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.02

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 16    # candidates to demote from protected (oldest)

# Priority scoring weights
_SIZE_ALPHA = 1.10          # size penalty exponent
_W_FREQ = 1.0               # weight for frequency
_W_RECENCY = 3.0            # weight for recency (higher -> keep more recent)
_PROTECTED_KEEP_BONUS = 1.2 # protected items are a bit harder to evict
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    # without converting dictionary keys to a list (uses m_keys and index).
    n = len(m_keys)
    if n == 0:
        return []
    if want_segment is None:
        k = min(k, n)
        # random.sample(range(n), k) is O(k) without building a list of size n
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(4 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    # Fallback: if none found (should be rare), return a few from the other set
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the oldest (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    # Sample candidates from protected and demote the oldest by last_access
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    oldest_key = None
    oldest_ts = None
    for key in candidates:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    if oldest_key is None:
        return
    # Perform demotion (no object removal, just segment move)
    # We need the size; access from snapshot
    robj = cache_snapshot.cache.get(oldest_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(oldest_key, 0) == 1:
        m_segment[oldest_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    # Remove key from m_keys/m_key_index via swap-with-last
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        # If more hits happen in window, increase window slightly; else decrease
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    # reset counters
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - If window byte-usage > target, sample only window keys and evict the min-score key.
      - Otherwise, sample globally and evict the min-score key (protected has a keep bonus).
      - Tie-break by oldest last access, then by largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = _g_window_bytes > target_window

    # Choose candidates via sampling
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    # As a final guard, if sampling picked nothing (shouldn't happen), evict an arbitrary key
    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            # No resident keys; nothing to evict
            return None

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update global TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Adapt window fraction periodically based on where hits occur.
      - If protected exceeds budget, demote an old protected item to window.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        # Assume window if unknown segment; update bytes counters conservatively
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            globals()['_g_window_bytes'] += _size
        else:
            globals()['_g_protected_bytes'] += _size

    # Count hit location before any promotion
    seg = m_segment.get(key, 0)
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        globals()['_g_window_bytes'] -= sz
        globals()['_g_protected_bytes'] += sz

        # Keep protected within its budget by demoting an old protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now.
      - Track resident key for O(1) sampling and bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count
    m_segment[key] = 0  # window
    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    globals()['_g_window_bytes'] += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.8913

Example 2:
- Trace ID: 43
- Hit Rate: 0.3667

Example 3:
- Trace ID: 9
- Hit Rate: 0.531

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:48:30 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:48:30,460 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:50:52 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:50:52,863 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:50:52,865 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved Adaptive, Sampled, Size-aware TinyLFU with Doorkeeper and ARC-style Ghosts (A-SS-TinyLFU+DG)
# Key upgrades:
# - Doorkeeper Bloom filter reduces CMS noise from one-time scans.
# - ARC-style ghost sets adapt window size using recency/frequency misses (not just hits).
# - Restricts eviction to window for cold admissions (scan resistance).
# - Smarter protected demotion (priority-based, not just oldest).
# - Frequency smoothing and tuned weights for better stability across traces.
# - Optional fast-track to protected on re-reference of a previously protected key (ghost hit).

import random
import math
from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_keys = []               # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()      # key -> index in m_keys
# Byte counters for segments (kept consistent by updates)
_g_window_bytes = 0
_g_protected_bytes = 0

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Doorkeeper Bloom filter (first-touch filter)
_DK_BITS = 1 << 18  # 262,144 bits ~ 32KB
_DK_BYTES = _DK_BITS // 8
_DK_MASK = _DK_BITS - 1
_doorkeeper = bytearray(_DK_BYTES)
# 2 independent hashes derived from SALTS
def _dk_pos(key, salt_idx):
    h = (hash(key) ^ _SALTS[salt_idx]) & 0xffffffffffffffff
    return h & _DK_MASK

def _dk_test_then_set(key):
    # Returns True if key likely seen before; otherwise sets bits and returns False
    p1 = _dk_pos(key, 0)
    p2 = _dk_pos(key, 1)
    b1 = ( _doorkeeper[p1 >> 3] >> (p1 & 7) ) & 1
    b2 = ( _doorkeeper[p2 >> 3] >> (p2 & 7) ) & 1
    seen = (b1 & b2) == 1
    if not b1:
        _doorkeeper[p1 >> 3] |= (1 << (p1 & 7))
    if not b2:
        _doorkeeper[p2 >> 3] |= (1 << (p2 & 7))
    return seen

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (bytes) is adaptive; initialize with modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.03

# Sampling
_SAMPLE_SIZE = 32           # candidates per eviction decision
_SAMPLE_SIZE_DEMOTE = 24    # candidates to demote from protected

# Priority scoring weights (tuned)
_SIZE_ALPHA = 1.06           # smoother size penalty
_W_FREQ = 1.0                # weight for frequency
_W_RECENCY = 1.75            # lower recency to avoid scan pollution
_PROTECTED_KEEP_BONUS = 1.35 # protected items more resistant to eviction
_EPS = 1e-9

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 10000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# ARC-style ghost caches (recent evictions by segment)
# - presence in ghost_window indicates recency-miss -> grow window
# - presence in ghost_protected indicates frequency-miss -> shrink window
_ghost_window = OrderedDict()    # key -> last seen timestamp
_ghost_protected = OrderedDict() # key -> last seen timestamp

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    # Doorkeeper: only count into CMS after second touch in an epoch
    if _dk_test_then_set(key):
        for d in range(_SKETCH_DEPTH):
            _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0.0
    # Smooth to reduce over-dominance of very hot items
    return math.sqrt(float(est))

def _sketch_decay_if_needed():
    global _decay_ticker, _doorkeeper
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        # Reset doorkeeper each epoch to keep "first touch" semantics fresh
        _doorkeeper = bytearray(_DK_BYTES)
        _decay_ticker = 0

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes
    if _bootstrapped:
        return
    # Initialize from current cache snapshot (if any preloaded keys exist)
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0  # default to window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        # Count bytes by segment
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += robj.size
        else:
            _g_protected_bytes += robj.size
    _bootstrapped = True

def _priority_for_key(cache_snapshot, key, obj, now, include_protected_bonus=True):
    # Weighted frequency+recency score divided by size penalty
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** _SIZE_ALPHA)
    if include_protected_bonus and m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _sample_keys_from_segment(want_segment, k):
    # Sample up to k unique keys from the requested segment (0 or 1)
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    # Segment-filtered sampling with bounded retries
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _maybe_demote_from_protected(cache_snapshot):
    # If protected segment exceeds its byte budget, demote the lowest priority item (approx via sampling).
    global _g_window_bytes, _g_protected_bytes
    target_window = _current_window_target_bytes(cache_snapshot)
    protected_budget = max(0, cache_snapshot.capacity - target_window)
    if _g_protected_bytes <= protected_budget:
        return
    candidates = _sample_keys_from_segment(1, _SAMPLE_SIZE_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    worst_key = None
    worst_score = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        # For intra-protected comparison, do not include protected keep bonus
        score = _priority_for_key(cache_snapshot, key, robj, now, include_protected_bonus=False)
        if worst_key is None or score < worst_score - _EPS:
            worst_key = key
            worst_score = score
    if worst_key is None:
        return
    robj = cache_snapshot.cache.get(worst_key, None)
    if robj is None:
        return
    size = int(robj.size)
    if m_segment.get(worst_key, 0) == 1:
        m_segment[worst_key] = 0
        _g_protected_bytes -= size
        _g_window_bytes += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_hits_if_needed():
    # Keep original hit-based adaptation (low frequency)
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.55:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.45:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_prune_if_needed():
    # Dynamic bound based on resident key count; between 1024 and 16384 keys each
    max_sz = max(1024, min(16384, 2 * max(1, len(m_keys))))
    while len(_ghost_window) > max_sz:
        _ghost_window.popitem(last=False)
    while len(_ghost_protected) > max_sz:
        _ghost_protected.popitem(last=False)

def _ghost_record_eviction(key, seg, now):
    # Record last eviction segment in ghost caches
    if seg == 0:
        if key in _ghost_window:
            _ghost_window.move_to_end(key)
        _ghost_window[key] = now
    else:
        if key in _ghost_protected:
            _ghost_protected.move_to_end(key)
        _ghost_protected[key] = now
    _ghost_prune_if_needed()

def _ghost_adapt_on_insert(cache_snapshot, key):
    # ARC-style: if the key was in window-ghost -> increase window; if in protected-ghost -> decrease window.
    global _f_window
    grew = False
    shrunk = False
    if key in _ghost_window:
        _ghost_window.pop(key, None)
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        grew = True
    if key in _ghost_protected:
        _ghost_protected.pop(key, None)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        shrunk = True
    # If returning both True is possible (if present in both), it means strong signal to rebalance;
    # allow both adjustments to apply. No return value needed.

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Sampled victim selection:
      - Restrict to window if it's over budget, or if incoming is cold (low estimate) -> scan resistance.
      - Otherwise, sample a mix (bias 3/4 window, 1/4 protected) to preserve protected segment.
      - Evict the min-score key among candidates; tie-break by oldest last access, then largest size.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count

    # Determine whether to restrict eviction to window (keep protected stable)
    target_window = _current_window_target_bytes(cache_snapshot)
    restrict_to_window = (_g_window_bytes > target_window)

    # Cold admission heuristic: if incoming has very low estimated frequency, avoid evicting protected
    est_new = _sketch_estimate(obj.key)
    if est_new <= 1.0:
        restrict_to_window = True

    # Choose candidates via sampling
    candidates = []
    if restrict_to_window:
        candidates = _sample_keys_from_segment(0, min(_SAMPLE_SIZE, len(m_keys)))
    else:
        # Mixed sampling: prioritize window candidates to preserve protected unless truly weak
        k = min(_SAMPLE_SIZE, len(m_keys))
        kw = int(0.75 * k)
        kp = max(0, k - kw)
        cw = _sample_keys_from_segment(0, kw)
        cp = _sample_keys_from_segment(1, kp)
        # Fill shortages from global pool if segments are small
        c = cw + cp
        if len(c) < k:
            c += _sample_keys_from_segment(None, k - len(c))
        candidates = c

    # Fallback: if sampling failed (extremely unlikely), scan all keys
    if not candidates:
        candidates = list(cache_snapshot.cache.keys())

    # Select victim among candidates
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and robj.size > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = robj.size

    if victim_key is None:
        if m_keys:
            victim_key = m_keys[0]
        else:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update Doorkeeper+TinyLFU sketch (admission estimator).
      - Update last access timestamp.
      - Promote from window -> protected on hit.
      - Track where hits occur to slowly adapt window fraction.
      - If protected exceeds budget, demote a low-priority protected item to window.
    '''
    global _g_window_bytes, _g_protected_bytes, _adapt_ticker, _hits_in_window, _hits_in_protected
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure resident metadata defaults if missing (rare)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_key_index:
        # If resident but not indexed, index it (conservative bytes update)
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        _size = int(obj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += _size
        else:
            _g_protected_bytes += _size

    seg = m_segment.get(key, 0)
    _adapt_ticker += 1
    if seg == 0:
        _hits_in_window += 1
    else:
        _hits_in_protected += 1

    # Update last access time
    m_last_access[key] = now

    # Promote to protected if currently in window
    if seg == 0:
        m_segment[key] = 1
        sz = int(obj.size)
        _g_window_bytes -= sz
        _g_protected_bytes += sz
        # Keep protected within its budget by demoting a low-priority protected key if needed
        _maybe_demote_from_protected(cache_snapshot)

    # Periodically adapt the window fraction based on hits
    _adapt_window_fraction_hits_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update Doorkeeper+TinyLFU counts for this access.
      - ARC-style ghost check:
          * If key found in window-ghost: increase window fraction.
          * If key found in protected-ghost: decrease window fraction and fast-track to protected.
      - Initialize metadata and sampling index.
      - Keep bytes accounting consistent.
      - If fast-tracked to protected, maintain protected budget by demoting low-priority protected items.
    '''
    global _g_window_bytes, _g_protected_bytes
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # ARC-style adaptation based on ghost hits
    was_in_protected_ghost = key in _ghost_protected
    _ghost_adapt_on_insert(cache_snapshot, key)

    # Decide initial segment: default window; if it was a protected-ghost, fast-track to protected
    if was_in_protected_ghost:
        m_segment[key] = 1
    else:
        m_segment[key] = 0

    m_last_access[key] = now

    # Track in sampling structures if not already tracked
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters
    sz = int(obj.size)
    if m_segment[key] == 0:
        _g_window_bytes += sz
    else:
        _g_protected_bytes += sz
        # Keep protected within its budget by demoting if needed
        _maybe_demote_from_protected(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove resident metadata for the evicted key.
      - Update bytes counters and sampling structures.
      - Record eviction into ghost cache for ARC-style adaptation.
      - Count-Min sketch persists (natural decay handles aging).
    '''
    global _g_window_bytes, _g_protected_bytes
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        _g_window_bytes -= ev_size
    else:
        _g_protected_bytes -= ev_size

    # Record in ghost caches for future adaptive decisions
    _ghost_record_eviction(ev_key, seg, cache_snapshot.access_count)
```
2025-11-28 01:53:56,373 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Lightweight Segmented TinyLFU with LRU Window and Protected queues (S-TinyLFU/LRU)
# Goals:
# - Reduce per-access overhead to avoid timeouts.
# - Preserve strong recency via an explicit LRU window.
# - Use TinyLFU frequency for global victim selection and tie-breaking.
# - Gentle, adaptive window sizing with ARC-like ghost history.
# - Simple "doomed" tagging for scan shielding without expensive sampling.
#
# Key simplifications vs previous version:
# - O(1) oldest-window victim selection via a lazy-cleaned deque (no oldest sampling).
# - Smaller candidate samples and cheaper "doomed" tagging (no quantile computation).
# - Faster protected demotion via a lazy-cleaned deque and budget enforcement.
# - Size penalty remains, but score computation is lighter on average.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned)
_window_q = deque()        # left = LRU, right = MRU; stores keys; duplicates allowed (lazy removal)
_protected_q = deque()     # left = LRU, right = MRU; duplicates allowed

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global, across all requests)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096  # power of two; smaller for speed
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Tunables
# Window fraction is adaptive; initialize with a modest recency bias.
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.05

# Sampling sizes (smaller for speed)
_SAMPLE_SIZE = 16            # candidates per eviction decision
_SAMPLE_SIZE_LARGE = 12      # from large set
_SAMPLE_SIZE_GLOBAL_OLDEST = 16

# Priority scoring weights (base)
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard factor (higher -> stricter)
_ADMIT_COMPARE_FACTOR = 1.25

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.70      # allow up to 70% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize from current cache content
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0  # unknown -> window
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        # size classification at bootstrap
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.20
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.08
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    # Remove non-resident or non-window keys from the left until head is a valid window key or deque empty
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    # Remove non-resident or non-protected keys from the left until head is a valid protected key or deque empty
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    # We will demote this key; do not pop here (lazy duplicates), but return k for demotion
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    # Safety: ensure still resident and protected
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # add as MRU in window
    return True

def _enforce_protected_budget(cache_snapshot):
    # Demote until protected bytes fit in budget
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    # Try to find an oldest "doomed" key in the window by scanning a few from the left
    steps = 0
    # First, ensure head is valid window
    _clean_window_left()
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # Not doomed; try next; we don't pop because it is a valid window key
        # Advance by moving this key to the right to avoid rechecking it too often (gentle aging)
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy:
      1) Enforce protected budget via simple demotions from oldest protected (lazy-cleaned deque).
      2) Prefer evicting an oldest "doomed" window item if any.
      3) Otherwise evict the oldest window item (LRU window).
      4) If large objects exceed cap, pick a weak large item from a small sample.
      5) Otherwise sample globally and evict the weakest by size-aware TinyLFU score.
      6) Admission-compare guard: if chosen victim is much stronger than incoming,
         prefer the oldest window item as an alternative, if available.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Admission guard estimate for incoming
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Try doomed-in-window first
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is None:
        # 2) Oldest window (LRU)
        victim_key = _peek_oldest_window()
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)

    # 3) Large overcap handling if no obvious window victim
    if victim_key is None:
        large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
        if _g_large_bytes > large_cap and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    # 4) Global sampling fallback
    if victim_key is None:
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, prefer oldest window
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            alt = _peek_oldest_window()
            if alt is not None and alt != victim_key:
                victim_key = alt

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU sketch and average size.
      - Update last access timestamp and resident hit counter.
      - Promote window->protected if hit threshold reached (1 for small, 2 for large).
      - Clear "doomed" tag upon hit.
      - Track hit location for window fraction adaptation.
      - Enforce protected budget via demotion of oldest protected.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: if missing, register as window by default
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Hit-location accounting for adaptation
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hit counters
    m_last_access[key] = now
    m_hits_resident[key] = min(4, m_hits_resident.get(key, 0) + 1)

    # Clear "doomed" tag on any reuse
    if key in m_doomed:
        m_doomed.discard(key)

    # Promotion policy: stricter for large items
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (miss path):
      - Update TinyLFU and average size.
      - Initialize metadata: segment=window, last_access=now, insert_ts=now, resident_hits=0.
      - Classify size (large vs small) and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction toward the segment where the key was last seen.
      - Simple scan-shielding: tag "doomed" for clearly cold, relatively large inserts.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification at insertion time
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)  # safety

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Simple scan-shielding ("doomed" tagging):
    # Tag as doomed if it's clearly cold (freq<=1), relatively large, and not recently in protected ghost.
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.8 * target_window))
    if (est_freq <= 1.0) and (size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.8)) and (key not in _ghost_p_set):
        if window_is_bloated or is_large:
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag (for adaptive window tuning).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    # Large-bytes accounting and tag cleanup
    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    # Add to ghost sets
    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 38
- Hit Rate: 0.1255

Example 2:
- Trace ID: 22
- Hit Rate: 0.0606

Example 3:
- Trace ID: 33
- Hit Rate: 0.6937

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:53:56 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:53:56,385 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m01:56:44 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 01:56:44,096 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 01:56:44,098 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved SLRU-Window TinyLFU (W-TinyLFU inspired) with better recency maintenance,
# stronger scan resistance, and safer large-object control.
#
# Key improvements:
# - Proper LRU order maintenance: touch on every hit (lazy duplicates).
# - Evict from oldest-window prefix (K oldest) using TinyLFU score to avoid
#   evicting strong-but-unpromoted items.
# - Large-object control happens before window eviction to prevent large blow-ups.
# - Tighter default window size (adaptable) for stronger scan resistance.
# - Stricter admission guard; on guard-trigger we tag incoming as doomed-like
#   (handled on insert) instead of evicting strong items.
# - Slightly stricter promotion for large items (3 hits) to avoid polluting protected.
# - Expanded "doomed" tagging for very cold inserts when window is bloated.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned); duplicates allowed, left=LRU, right=MRU
_window_q = deque()
_protected_q = deque()

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.15
_WINDOW_MIN = 0.02
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 16
_SAMPLE_SIZE_LARGE = 12
_SAMPLE_SIZE_GLOBAL_OLDEST = 16
_WINDOW_PREFIX_K = 6        # number of oldest-window candidates to consider first

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard
_ADMIT_COMPARE_FACTOR = 1.50

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                 # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.60       # allow up to 60% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 3            # stricter for large objects

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.25
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.10
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # MRU in window after demotion
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    _clean_window_left()
    steps = 0
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # gentle rotation to avoid repeatedly checking same head
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

def _collect_oldest_window_keys(k):
    # Collect up to k distinct valid window keys starting from LRU
    res = []
    seen = set()
    if k <= 0:
        return res
    for key in _window_q:
        if key in seen:
            continue
        if key in m_key_index and m_segment.get(key, 0) == 0:
            res.append(key)
            seen.add(key)
            if len(res) >= k:
                break
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (improved):
      1) Enforce protected budget via LRU demotions.
      2) If large bytes exceed cap, preferentially evict a weak large item (TinyLFU score).
      3) Prefer evicting a doomed window item (LRU scan shielding).
      4) Otherwise, consider the K oldest window items and evict the weakest by TinyLFU+recency score.
      5) If window empty, fallback to global sampling to evict the weakest.
      6) Admission guard: if chosen victim is much stronger than incoming,
         keep victim and tag incoming as doomed for future quick eviction.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    _enforce_protected_budget(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Large overcap handling first
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        if cands:
            vk, vs = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)
            if vk is not None:
                # Admission guard vs incoming: still allow if incoming is reasonably strong
                if vs >= _ADMIT_COMPARE_FACTOR * incoming_score:
                    # Incoming is weak -> tag it for quick exit after insert
                    m_doomed.add(obj.key)
                return vk

    # 2) Doomed in window, if any
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)
            return victim_key

    # 3) Oldest-window prefix candidates
    w_cands = _collect_oldest_window_keys(_WINDOW_PREFIX_K)
    if w_cands:
        victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, w_cands, incoming_size, now)
    else:
        victim_key, victim_score = None, None

    # 4) Fallbacks if needed
    if victim_key is None:
        # Try large again even if not overcap (if incoming itself is large, prefer trading a weak large)
        if (obj.size >= _LARGE_RATIO * max(1.0, _avg_size)) and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    if victim_key is None:
        # Global sampling fallback
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, keep victim and mark incoming as doomed
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            m_doomed.add(obj.key)
            return victim_key

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit (improved):
      - Update TinyLFU and avg size.
      - Update timestamps and resident hit counter.
      - Touch LRU position (append to segment deque).
      - Promote window->protected after threshold (1 small, 3 large).
      - Clear "doomed" tag on reuse.
      - Track hit location for window adaptation and enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: register if missing
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Adaptation accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hits
    m_last_access[key] = now
    m_hits_resident[key] = min(7, m_hits_resident.get(key, 0) + 1)

    # Clear doom on reuse
    m_doomed.discard(key)

    # Touch in LRU for the corresponding segment (maintain MRU order via lazy duplicates)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Promotion policy
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (improved):
      - Update TinyLFU and avg size.
      - Initialize metadata; set segment=window.
      - LRU touch in window.
      - Classify size and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction.
      - Scan-shielding: more aggressive doom tagging for clearly cold inserts
        especially when window is bloated.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging)
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.75 * target_window))
    # Tag as doomed if it's clearly cold and relatively large or window is bloated
    if (est_freq <= 1.0) and (key not in _ghost_p_set):
        if is_large or window_is_bloated or size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.7):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-28 01:59:44,180 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 8
- Hit Rate: 0.5399

Example 2:
- Trace ID: 16
- Hit Rate: 0.0199

Example 3:
- Trace ID: 7
- Hit Rate: 0.4986

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m01:59:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 01:59:44,181 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:02:52 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:02:52,022 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:02:52,051 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: ARC + SLRU with W-TinyLFU admission, byte-aware, and protected-segment enforcement
# - Resident segments:
#     T1: probationary (LRU)
#     T2: protected (LRU)
# - Ghost lists:
#     B1: recently evicted from T1 (key+size)
#     B2: recently evicted from T2 (key+size)
# - p (in bytes): target for T1. Adapted using B1/B2 hits (byte-aware, damped).
# - Eviction REPLACE:
#     • Choose base segment: evict from T1 if T1_bytes >= p (or T2 empty), else from T2.
#     • Within that segment, select victim among the K oldest by TinyLFU value-density (freq/size^alpha).
#     • Cross-check the other segment; if it has a much worse key, evict it instead (safety valve).
#     • W-TinyLFU admission: if incoming key's density < victim's density, reject admission (evict incoming).
# - Hits: T1 -> T2 promotion; T2 refresh to MRU. Enforce T2 size ≤ (capacity - p) by demoting LRU of T2 to T1.
# - Inserts: default to T1 MRU; B1 hit => increase p and insert into T2; B2 hit => decrease p and insert into T1.
# - Ghost budgets trimmed to ≤ capacity bytes each.
# - Count-Min Sketch with periodic decay for TinyLFU.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size (bytes)
_T2 = OrderedDict()  # key -> size (bytes)

# Ghost lists (store only key and size)
_B1 = OrderedDict()  # key -> size (bytes)
_B2 = OrderedDict()  # key -> size (bytes)

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 in bytes
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 24  # consider up to K oldest keys in a segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# W-TinyLFU admission threshold multiplier (admit if incoming_score >= gamma * victim_score)
_ADMISSION_GAMMA = 1.00

# Cross-segment override factor (if the other segment has a key with score <= factor * chosen_victim_score)
_CROSS_FACTOR = 0.60

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size, capacity=None):
    # Size-aware penalty; larger-than-average objects penalized more.
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    alpha = _BASE_ALPHA
    if r >= 4.0:
        alpha += 0.45
    elif r >= 2.0:
        alpha += 0.25
    elif r <= 0.5:
        alpha = max(0.90, alpha - 0.10)
    # Extra penalty if object is a large fraction of capacity
    if capacity:
        frac = float(size) / float(max(1, capacity))
        if frac >= 0.50:  # very large object
            alpha += 0.35
        elif frac >= 0.25:
            alpha += 0.15
    return alpha

def _score_value_density(key, size, capacity=None):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size, capacity)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation), unknown history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share (1/3 of capacity)
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)

def _enforce_t2_target(capacity):
    # Keep protected segment close to its target: T2_bytes <= capacity - p
    global _t1_bytes, _t2_bytes
    target_t2 = max(0, int(capacity) - int(_p_bytes))
    # Demote LRU of T2 to T1 until under target
    while _t2_bytes > target_t2 and _T2:
        k, sz = _odict_pop_lru(_T2)
        if k is None:
            break
        _t2_bytes -= int(sz)
        _odict_add_mru(_T1, k, sz)
        _t1_bytes += int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment and W-TinyLFU admission:
      - Choose base segment: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys in that segment, evict the key with lowest TinyLFU value density.
      - Safety valve: if the other segment has a much worse candidate (by factor), evict it instead.
      - Admission control: if incoming object's density < victim's density * gamma, reject new object.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    capacity = int(cache_snapshot.capacity)
    in_key = obj.key
    in_size = int(obj.size)

    # Reject objects that cannot possibly fit
    if in_size > capacity:
        return in_key  # do not admit oversized object

    # Decide base segment for REPLACE
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    base_seg = _T1 if from_T1 else _T2
    alt_seg = _T2 if from_T1 else _T1

    def _pick_victim_in_segment(seg):
        if not seg:
            return None, None, None
        candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
        victim_key = None
        victim_score = None
        victim_size = None
        for k in candidates:
            sz = seg.get(k, 1)
            sc = _score_value_density(k, sz, capacity)
            if victim_key is None or sc < victim_score:
                victim_key = k
                victim_score = sc
                victim_size = sz
        return victim_key, victim_size, victim_score

    # Pick victim in base segment
    v_key, v_size, v_score = _pick_victim_in_segment(base_seg)

    # Fallback: if base empty, try alternate; if both empty, fallback to any cached key
    if v_key is None:
        v_key, v_size, v_score = _pick_victim_in_segment(alt_seg)
        if v_key is None:
            # As a last resort, return any cached key if exists; else reject new object
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return in_key

    # Safety valve: consider worst in the other segment; if it's much worse, evict it instead
    alt_key, alt_size, alt_score = _pick_victim_in_segment(alt_seg)
    if alt_key is not None and alt_score is not None and v_score is not None:
        if alt_score < (v_score * _CROSS_FACTOR):
            v_key, v_size, v_score = alt_key, alt_size, alt_score

    # W-TinyLFU admission: compare incoming vs victim density (size-aware)
    incoming_score = _score_value_density(in_key, in_size, capacity)
    if v_score is not None and incoming_score < (_ADMISSION_GAMMA * v_score):
        # Reject admission: "evict" the incoming object (engine will skip inserting it)
        return in_key

    # Admit: evict chosen resident victim
    return v_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - Enforce T2 target size (capacity - p) by demoting LRU of T2 to T1 if needed.
      - If not tracked, add to T1 MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = int(cache_snapshot.capacity)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked (rare): add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    # Keep protected segment near its target
    _clamp_p(capacity)
    _enforce_t2_target(capacity)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss, if admitted):
      - Update TinyLFU and size EMA.
      - Default: insert into T1 (probation) MRU.
      - If key is in B1: increase p (favor recency) [damped], insert into T2.
      - If key is in B2: decrease p (favor protection) [damped], insert into T1.
      - Enforce T2 target size, and trim ghosts to budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = int(cache_snapshot.capacity)
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If admission was rejected, the object won't be in T1/T2; do nothing
    if (key not in _T1) and (key not in _T2) and (key not in cache_snapshot.cache):
        return

    # If already resident for some reason, refresh recency only
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        _enforce_t2_target(capacity)
        _trim_ghosts_to_budget(capacity)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        _enforce_t2_target(capacity)
        _trim_ghosts_to_budget(capacity)
        return

    # Ghost-guided adaptation (damped deltas to reduce oscillations)
    damp = 0.25
    if key in _B1:
        delta = int(damp * max(int(_avg_size), size))
        _p_bytes = min(capacity, _p_bytes + delta)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        delta = int(damp * max(int(_avg_size), size))
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default probationary admission
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _enforce_t2_target(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - If eviction is the incoming object (admission rejection), do not add to ghosts.
      - Otherwise, remove evicted key from resident segment and add to corresponding ghost:
          * From T1 -> B1
          * From T2 -> B2
      - Trim ghost lists to capacity budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    capacity = int(cache_snapshot.capacity)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Admission rejection: the incoming object was "evicted" but never resident
    if ev_key == obj.key and ev_key not in _T1 and ev_key not in _T2:
        # Do not record in ghosts; nothing to adjust in resident metadata
        _trim_ghosts_to_budget(capacity)
        return

    # Remove from resident segments and add to appropriate ghost
    if ev_key in _T1:
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Unknown (metadata drift); assume T1 origin
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```
2025-11-28 02:03:06,862 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 28
- Hit Rate: 0.0398

Example 2:
- Trace ID: 17
- Hit Rate: 0.0236

Example 3:
- Trace ID: 1
- Hit Rate: 0.4478

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:03:06 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:03:06,864 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:05:59 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:05:59,019 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:05:59,021 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Byte-aware ARC (Adaptive Replacement Cache) with LRU-within-segment sampling
# Design goals:
# - Strong recency via probationary segment (T1) + frequency via protected segment (T2)
# - Adaptive split between T1 and T2 using ARC-style ghost lists (B1, B2) measured in BYTES
# - LRU within each segment approximated by sampling and picking the oldest
# - Size-aware: bytes-based budgets, large-object friendly tie-breaking
# - Simple, stable, and fast (O(1) metadata updates; sampling for victim selection)

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
# Segment membership: 0 = T1 (probationary/recent), 1 = T2 (protected/frequent)
m_segment = dict()          # key -> 0 or 1
m_last_access = dict()      # key -> last access timestamp
m_insert_ts = dict()        # key -> insert timestamp (for diagnostics/ties)
m_keys = []                 # list of resident keys (for O(1) swap-delete index)
m_key_index = dict()        # key -> index in m_keys

# Resident bytes
_bytes_t1 = 0
_bytes_t2 = 0

# Target bytes for T1 (ARC "p" but in bytes)
_p_bytes = 0

# Ghost lists (ARC): store recently evicted keys from T1 (B1) and T2 (B2)
# We keep dicts for fast membership and sizes, and deques for LRU order.
_b1 = dict()                # key -> size
_b2 = dict()                # key -> size
_b1_q = deque()             # LRU order of B1 keys (left oldest)
_b2_q = deque()             # LRU order of B2 keys (left oldest)
_b1_bytes = 0
_b2_bytes = 0

# Running average resident size (EMA) to scale adaptation steps
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# Sampling parameters for candidate selection
_SAMPLE_T1 = 32
_SAMPLE_T2 = 24
_SAMPLE_GLOBAL = 32

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(int(sz))
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _bytes_t1, _bytes_t2, _p_bytes, _avg_size
    if _bootstrapped:
        return

    total_bytes = 0
    total_items = 0

    # Initialize all resident objects into T1 (probationary)
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        m_segment[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        _bytes_t1 += size

    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))

    # Start with half of capacity allocated to T1 (in bytes)
    _p_bytes = max(0, min(cache_snapshot.capacity, int(0.5 * cache_snapshot.capacity)))

    _bootstrapped = True

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _sample_keys_from_segment(seg, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if seg is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]

    # Reservoir-like uniform sampling with rejection if segment mismatches
    result = []
    seen = set()
    tries = 0
    max_tries = max(5 * k, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == seg:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result:
        # Fallback to global sampling
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    oldest_sz = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        sz = int(robj.size)
        if (
            oldest_key is None
            or ts < oldest_ts
            or (ts == oldest_ts and sz > oldest_sz)
        ):
            oldest_key = key
            oldest_ts = ts
            oldest_sz = sz
    return oldest_key

def _ghost_add(bdict, bq, bytes_counter_name, key, size):
    # Add to ghost dict, enqueue on right; lazily purge from deque when needed
    globals()[bytes_counter_name] += size
    bdict[key] = size
    bq.append(key)

def _ghost_remove_if_present(bdict, bq, bytes_counter_name, key):
    size = bdict.pop(key, None)
    if size is not None:
        globals()[bytes_counter_name] -= size
        if globals()[bytes_counter_name] < 0:
            globals()[bytes_counter_name] = 0  # safety

def _ghost_prune_to_budget(cache_snapshot):
    # Keep total ghost bytes <= capacity (ARC design keeps B1+B2 <= c)
    cap = cache_snapshot.capacity
    global _b1_bytes, _b2_bytes

    def pop_from(bdict, bq, bytes_counter_name):
        nonlocal _b1_bytes, _b2_bytes
        while bq:
            k = bq.popleft()
            sz = bdict.pop(k, None)
            if sz is not None:
                globals()[bytes_counter_name] -= sz
                if globals()[bytes_counter_name] < 0:
                    globals()[bytes_counter_name] = 0
                break

    while (_b1_bytes + _b2_bytes) > cap:
        # Evict from the larger ghost first to keep balance
        if _b1_bytes >= _b2_bytes and _b1_bytes > 0:
            pop_from(_b1, _b1_q, "_b1_bytes")
        elif _b2_bytes > 0:
            pop_from(_b2, _b2_q, "_b2_bytes")
        else:
            break

def _adjust_p_on_ghost_hit(cache_snapshot, key, incoming_size):
    # ARC-style adaptation for bytes:
    # If key ∈ B1: increase p (favor recency)
    # If key ∈ B2: decrease p (favor frequency)
    # Step scaled by avg size and relative ghost sizes
    global _p_bytes
    cap = cache_snapshot.capacity
    step_base = max(int(_avg_size), int(incoming_size))
    if key in _b1:
        ratio = (_b2_bytes / float(_b1_bytes)) if _b1_bytes > 0 else 1.0
        delta = int(step_base * max(1.0, ratio))
        _p_bytes = min(cap, _p_bytes + delta)
        _ghost_remove_if_present(_b1, _b1_q, "_b1_bytes", key)
    elif key in _b2:
        ratio = (_b1_bytes / float(_b2_bytes)) if _b2_bytes > 0 else 1.0
        delta = int(step_base * max(1.0, ratio))
        _p_bytes = max(0, _p_bytes - delta)
        _ghost_remove_if_present(_b2, _b2_q, "_b2_bytes", key)
    # Clamp
    _p_bytes = max(0, min(cap, _p_bytes))

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    ARC replacement (byte-aware):
      - Choose eviction segment using ARC rule:
          evict from T1 if bytes(T1) > p, or (bytes(T1) == p and obj in B2), else from T2.
      - Within chosen segment, sample keys and evict the oldest (LRU approximation).
      - If chosen segment is empty, fall back to the other or to a global oldest.
    """
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_key = obj.key

    # Decide segment to evict from
    t1_bytes = _bytes_t1
    t2_bytes = _bytes_t2
    p = _p_bytes

    # If candidate key was seen in B2, bias to evict from T1 when equal
    prefer_t1 = (t1_bytes > p) or (t1_bytes == p and incoming_key in _b2)

    chosen_seg = 0 if prefer_t1 else 1
    # If chosen segment is empty, try the other
    if (chosen_seg == 0 and t1_bytes <= 0) or (chosen_seg == 1 and t2_bytes <= 0):
        chosen_seg = 1 - chosen_seg

    # Sample candidates from chosen segment
    sample_k = _SAMPLE_T1 if chosen_seg == 0 else _SAMPLE_T2
    candidates = _sample_keys_from_segment(chosen_seg, sample_k)

    if not candidates:
        # Try other segment
        other = 1 - chosen_seg
        candidates = _sample_keys_from_segment(other, sample_k)
        if not candidates:
            # Fallback to global oldest among sampled keys
            global_candidates = _sample_keys_from_segment(None, min(_SAMPLE_GLOBAL, len(m_keys)))
            victim = _select_oldest_from_keys(cache_snapshot, global_candidates)
            if victim is not None:
                return victim
            # Last resort
            return m_keys[0] if m_keys else None

    victim = _select_oldest_from_keys(cache_snapshot, candidates)
    if victim is None:
        # Fallback: any key
        return m_keys[0] if m_keys else None
    return victim

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Refresh last access time and EMA of size.
      - Move from T1 -> T2 on hit (ARC promotion).
      - Keep bytes counters consistent.
    """
    _bootstrap_if_needed(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)
    _update_avg_size(size)

    # Ensure indexing (safety)
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    m_last_access[key] = now

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Promote to T2
        m_segment[key] = 1
        globals()['_bytes_t1'] -= size
        globals()['_bytes_t2'] += size
        if _bytes_t1 < 0:
            globals()['_bytes_t1'] = 0  # safety

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after miss):
      - Adjust target p using ghost feedback if the missed key was in B1 or B2.
      - Initialize metadata and place the key in T1.
      - Update bytes counters and timestamps.
    """
    _bootstrap_if_needed(cache_snapshot)
    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)
    _update_avg_size(size)

    # ARC-style adaptation on ghost hits (remove from ghost if present)
    _adjust_p_on_ghost_hit(cache_snapshot, key, size)
    _ghost_prune_to_budget(cache_snapshot)

    # Insert into T1
    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_bytes_t1'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Move evicted resident to appropriate ghost (B1 if from T1, B2 if from T2).
      - Remove resident metadata and adjust resident bytes counters.
      - Prune ghost lists to keep B1+B2 within capacity.
    """
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update resident bytes and add to appropriate ghost
    if seg == 0:
        globals()['_bytes_t1'] -= ev_size
        if _bytes_t1 < 0:
            globals()['_bytes_t1'] = 0
        _ghost_add(_b1, _b1_q, "_b1_bytes", ev_key, ev_size)
    else:
        globals()['_bytes_t2'] -= ev_size
        if _bytes_t2 < 0:
            globals()['_bytes_t2'] = 0
        _ghost_add(_b2, _b2_q, "_b2_bytes", ev_key, ev_size)

    _ghost_prune_to_budget(cache_snapshot)
```
2025-11-28 02:06:07,933 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 5
- Hit Rate: 0.4606

Example 2:
- Trace ID: 40
- Hit Rate: 0.0635

Example 3:
- Trace ID: 25
- Hit Rate: 0.3639

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:06:07 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:06:07,934 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:10:00 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:10:00,341 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:10:00,343 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# W-TinyLFU + SLRU (Caffeine-style) with ARC-like ghost adaptation
# Key ideas:
# - Two resident segments: window (probationary LRU) and main (protected LRU)
# - Hits promote from window -> main; main hits refresh recency
# - Evictions prefer window's oldest, but evaluate a few oldest window candidates using TinyLFU+recency+size
# - ARC-like ghost lists (B1/B2) adapt window fraction on reappearance
# - TinyLFU (Count-Min Sketch) with periodic decay; size-aware priority so large objects must earn their space
# - O(1) indexing using OrderedDicts; bounded CPU

import random
from collections import OrderedDict

# ------------------------
# Global metadata
# ------------------------
# Resident segments (LRU order: oldest at front, newest at end)
_win_od = OrderedDict()   # key -> size
_main_od = OrderedDict()  # key -> size

# Per-key metadata
_m_segment = dict()       # key -> 0 (window) or 1 (main/protected)
_m_last_access = dict()   # key -> last access time (int)
_m_insert_ts = dict()     # key -> insertion time (int)

# Bytes accounting
_win_bytes = 0
_main_bytes = 0

# Running average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay config
_DECAY_PERIOD = 10000
_decay_ticker = 0

# Window fraction (adaptive via ghosts)
_f_window = 0.20
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.80
_WINDOW_STEP = 0.05

# ARC-style ghost lists: B1 for window evictions, B2 for main evictions
_B1 = OrderedDict()  # keys evicted from window
_B2 = OrderedDict()  # keys evicted from main
_GHOST_CAP = 65536   # bounded by estimated item count (set at bootstrap)

# Candidate evaluation
_K_WIN_EVICT = 8     # evaluate up to K oldest window candidates
_W_FREQ = 1.0
_W_RECENCY = 1.5
_MAIN_KEEP_BONUS = 1.15
_ADMIT_COMPARE = 1.25
_EPS = 1e-9

_bootstrapped = False


# ------------------------
# Sketch helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(0.0 if est is float('inf') else est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0


# ------------------------
# Internal helpers
# ------------------------
def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(int(sz))
    except Exception:
        return
    s = max(1.0, s)
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * s

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _win_bytes, _main_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    # Initialize: place all existing items in window (probationary)
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        sz = int(robj.size)
        _win_od[key] = sz
        _m_segment[key] = 0
        _m_last_access[key] = 0
        _m_insert_ts[key] = 0
        _win_bytes += sz
        total_bytes += sz
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity ~ 4x estimated resident item count, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    tgt = int(_f_window * cap)
    # Ensure at least one average-sized item worth of window
    return max(int(0.5 * _avg_size), tgt)

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.45
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(key, size, now, incoming_size):
    # Higher score -> more valuable to keep
    freq = _sketch_estimate(key)
    age = max(0, now - _m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(max(1, size)) ** size_alpha)
    if _m_segment.get(key, 0) == 1:
        score *= _MAIN_KEEP_BONUS
    return score

def _incoming_priority_estimate(incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _rebalance_main_budget(cache_snapshot):
    global _win_bytes, _main_bytes
    # Maintain main within budget (demote oldest back to window as needed)
    budget_main = max(0, int(cache_snapshot.capacity) - _current_window_target_bytes(cache_snapshot))
    while _main_bytes > budget_main and _main_od:
        k, sz = _main_od.popitem(last=False)  # oldest from main
        _main_bytes -= sz
        _win_od[k] = sz
        _win_bytes += sz
        _m_segment[k] = 0  # now in window

def _ensure_window_has_victim(cache_snapshot):
    # If window empty but main non-empty, demote one oldest to window to guarantee a victim
    if not _win_od and _main_od:
        k, sz = _main_od.popitem(last=False)
        _main_bytes -= sz
        _win_od[k] = sz
        _win_bytes += sz
        _m_segment[k] = 0

def _ghost_hit_on_insert(key):
    # ARC-style adaptation: key appeared in ghost -> adjust window fraction
    global _f_window
    adjusted = False
    if key in _B1:
        # Reappear after window eviction: increase window
        _B1.pop(key, None)
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        adjusted = True
    elif key in _B2:
        # Reappear after main eviction: decrease window
        _B2.pop(key, None)
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        adjusted = True
    return adjusted

def _ghost_add(evicted_key, was_seg):
    # Keep bounded total ghost entries
    if was_seg == 0:
        if evicted_key in _B1:
            return
        _B1[evicted_key] = None
    else:
        if evicted_key in _B2:
            return
        _B2[evicted_key] = None
    # Bound ghosts
    while len(_B1) + len(_B2) > _GHOST_CAP:
        # Evict from the larger list first (ARC-ish)
        if len(_B1) >= len(_B2) and _B1:
            _B1.popitem(last=False)
        elif _B2:
            _B2.popitem(last=False)
        else:
            break

def _select_victim_from_window(incoming_obj, cache_snapshot):
    # Evaluate up to K oldest candidates from window using TinyLFU+recency+size
    if not _win_od:
        return None
    now = cache_snapshot.access_count
    incoming_size = max(1, int(incoming_obj.size))
    # Gather candidates
    cands = []
    i = 0
    for k in _win_od.keys():
        sz = _win_od[k]
        cands.append((k, sz))
        i += 1
        if i >= _K_WIN_EVICT:
            break
    # Pick weakest by score; tie-breaker: older first, then larger
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for k, sz in cands:
        sc = _priority_for_key(k, sz, now, incoming_size)
        ts = _m_last_access.get(k, 0)
        if (victim_key is None or
            sc < victim_score - _EPS or
            (abs(sc - victim_score) <= _EPS and (ts < (victim_ts if victim_ts is not None else ts) or
                                                 (ts == victim_ts and sz > (victim_size if victim_size is not None else 0))))):
            victim_key = k
            victim_score = sc
            victim_ts = ts
            victim_size = sz
    # Admission guard: if victim is clearly stronger than incoming, fall back to the oldest (pure LRU)
    if victim_key is not None:
        inc_sc = _incoming_priority_estimate(incoming_obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * inc_sc:
            # Oldest window key
            oldest_key = next(iter(_win_od.keys()))
            victim_key = oldest_key
    return victim_key


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Evict from window using an SLRU policy with TinyLFU scoring among the K oldest window keys.
    If window is empty, demote from main once to ensure a window victim. As a last resort,
    evict the global oldest from main.
    '''
    _bootstrap_if_needed(cache_snapshot)

    # Keep main within budget and ensure we have a window victim
    _rebalance_main_budget(cache_snapshot)
    _ensure_window_has_victim(cache_snapshot)

    # Prefer a window victim
    victim = _select_victim_from_window(obj, cache_snapshot)
    if victim is not None:
        return victim

    # If no window candidates, fall back to oldest in main
    if _main_od:
        return next(iter(_main_od.keys()))

    # Fallback: if both empty (shouldn't happen), return None
    return None


def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment TinyLFU and decay periodically; update size EMA
    - Refresh last access
    - Promote from window -> main on first hit (SLRU). Main hits refresh LRU order.
    - Keep main within its dynamic budget by demoting oldest back to window if necessary.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    sz = int(obj.size)
    _m_last_access[key] = now
    if key not in _m_segment:
        # Safety: if we missed tracking, treat as window resident
        _m_segment[key] = 0
        if key not in _win_od:
            _win_od[key] = sz
            globals()['_win_bytes'] += sz

    seg = _m_segment.get(key, 0)

    if seg == 0:
        # Promote to main
        if key in _win_od:
            # Remove from window
            try:
                val = _win_od.pop(key)
                globals()['_win_bytes'] -= val
            except KeyError:
                pass
        # Add to main MRU
        _main_od[key] = sz
        globals()['_main_bytes'] += sz
        _m_segment[key] = 1
    else:
        # Refresh main recency
        if key in _main_od:
            _main_od.move_to_end(key)
        else:
            # Safety: if missing in structure, add
            _main_od[key] = sz
            globals()['_main_bytes'] += sz
            _m_segment[key] = 1

    # Keep main within budget
    _rebalance_main_budget(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
    - Increment TinyLFU, decay, update size EMA
    - ARC-like ghost feedback to adjust window fraction
    - Insert into window MRU
    - Rebalance main budget if window fraction changed
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    sz = int(obj.size)

    # Ghost feedback
    adjusted = _ghost_hit_on_insert(key)

    # Insert into window MRU
    if key in _main_od:
        # Safety: if somehow present in main, refresh and return
        _main_od.move_to_end(key)
        _m_segment[key] = 1
    else:
        if key in _win_od:
            # Refresh existing window entry
            try:
                _win_od.pop(key)
            except KeyError:
                pass
        else:
            globals()['_win_bytes'] += sz
        _win_od[key] = sz
        _m_segment[key] = 0

    _m_last_access[key] = now
    _m_insert_ts[key] = now

    # If window fraction changed, ensure main respects the new (possibly smaller) budget
    if adjusted:
        _rebalance_main_budget(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Remove from resident structures and metadata; adjust bytes accounting
    - Record ghost entry in B1 (if from window) or B2 (if from main) to adapt window sizing
    '''
    _bootstrap_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_sz = int(evicted_obj.size)

    seg = _m_segment.pop(ev_key, 0)

    # Remove from resident structures
    if seg == 0:
        if ev_key in _win_od:
            try:
                val = _win_od.pop(ev_key)
                globals()['_win_bytes'] -= val
            except KeyError:
                pass
        else:
            # If missing, still account bytes conservatively
            globals()['_win_bytes'] = max(0, _win_bytes - ev_sz)
    else:
        if ev_key in _main_od:
            try:
                val = _main_od.pop(ev_key)
                globals()['_main_bytes'] -= val
            except KeyError:
                pass
        else:
            globals()['_main_bytes'] = max(0, _main_bytes - ev_sz)

    # Drop per-key metadata
    _m_last_access.pop(ev_key, None)
    _m_insert_ts.pop(ev_key, None)

    # Record in ghosts for adaptive control
    _ghost_add(ev_key, seg)
```
2025-11-28 02:10:08,648 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU (AHS-TinyLFU)
# - Global admission/priority uses a Count-Min Sketch with periodic aging (decay).
# - Two in-cache segments: window (recency/probation) and protected (frequency).
# - Adaptive window target (in bytes) via ARC-like ghost feedback.
# - Eviction is O(log n) using lazy min-heaps (global heap and window-only heap).
# - Compact per-resident metadata; stale heap entries are lazily discarded.
# - Safe, lightweight fallbacks if heaps are empty or highly stale.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # candidates for global eviction
_heap_window = []         # candidates restricted to the window segment

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.01
_MAX_WINDOW_FRAC = 0.60

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# Count-Min Sketch (TinyLFU)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration (sampled accesses to decay)
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0                # priority divisor exponent: size**alpha
_RECENCY_LAMBDA = 2.0            # multiplies (1 + lambda/(age+1)) term
_PROTECTED_KEEP_BONUS = 1.6      # factor to boost protected items' score (harder to evict)
_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    # Decay counters by halving every _DECAY_PERIOD accesses
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            # in-place halving
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    # Initialize state on first use or if capacity changes
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    # Reset everything if capacity changes or at first use
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _priority_for_key(cache_snapshot, key, obj, now):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))            # [~0, 1]
    mult = 1.0 + (_RECENCY_LAMBDA * recency)      # boosts more recent items

    score = (freq * mult) / (float(size) ** _SIZE_ALPHA)

    # Protected items receive a multiplicative boost (harder to evict).
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS

    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    if m_segment.get(key, 0) == 0:
        heappush(_heap_window, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    # Validate entry against current state; also check residency
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    # Pop until a valid entry is found or heap exhausted
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            # Return the key of a valid victim
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge too many stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    # Rebuild if heaps have grown too large compared to resident set and enough time has passed
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        # Default any missing metadata
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    # Insert key into ordered ghost dict, pop LRU if over capacity
    if key in ghost:
        # refresh position (move-to-end)
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        # pop oldest (LRU)
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if this miss (now inserting)
    # hits a ghost list from earlier evictions.
    cap = cache_snapshot.capacity
    k = obj.key
    step = max(obj.size, cap // 100)  # at least 1% of capacity or the object's size
    if k in _ghostW:
        # Recently evicted from window => need bigger window (recency working set)
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        # Recently evicted from protected => too much window, shrink it
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using heaps:
      1) If window byte-usage > adaptive target, evict from window via window-heap.
      2) Otherwise, evict the global minimum-score key (protected has bonus).
    Fall back to small random sampling if heaps are empty/stale.
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    # Decide if we must evict from window to respect budget
    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Try window heap first
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        # If window heap empty/stale, try global heap but restrict to window
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Not forced to evict from window: pick best global candidate
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=None)
        if victim_key is not None:
            return victim_key

    # Fallback: sample a few random keys to avoid a full scan
    # This also seeds the heaps for future evictions.
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None  # should not happen
    sample_sz = min(32, len(keys))
    # Prefer sampling window keys if restrict_to_window
    if restrict_to_window:
        window_keys = [k for k in keys if m_segment.get(k, 0) == 0]
        if window_keys:
            keys = window_keys
            sample_sz = min(sample_sz, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        # Ensure default metadata
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Also push entries to heaps to help future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global).
      - Update last access timestamp.
      - Promote from window -> protected on first hit after insert.
      - Push fresh heap entries; version bump invalidates stale entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    # Update last access
    m_last_access[key] = now

    # Promote to protected if currently in window
    if m_segment.get(key, 0) == 0:
        m_segment[key] = 1  # protected
        # Update window accounting
        _decr_window(obj.size)

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version=0->1.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert key into appropriate ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    # Record into ghost sets for adaptivity
    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    # Update window accounting if the evicted was in window
    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)

    # No need to touch heaps: lazy invalidation handles stale entries


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 30
- Hit Rate: 0.0456

Example 2:
- Trace ID: 6
- Hit Rate: 0.2667

Example 3:
- Trace ID: 11
- Hit Rate: 0.3615

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:10:08 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:10:08,649 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:20:28,514 [INFO] openai._base_client: Retrying request to /chat/completions in 0.440556 seconds
[92m02:23:49 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:23:49,140 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:23:49,141 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Adaptive, Heap-Accelerated Size-aware TinyLFU with Segmented Eviction (AHS-TinyLFU+SLRU)
# Major improvements over previous version:
# - Strict segment-quota eviction (ARC-like): evict from window if window > target; else evict from protected.
# - Separate heaps per segment for O(log n) victim selection without cross-segment interference.
# - Segment-specific scoring:
#     * Window (probation): recency-weighted, size-aware; prefer evicting old and large items.
#     * Protected (frequency): frequency and mild recency, size-aware.
# - Dual adaptation of window target:
#     * ARC-like ghost feedback (existing).
#     * Hit-driven drift: if hits occur mostly in window, grow the window; if mostly protected, shrink it.
# - Robust lazy heap invalidation via key versioning; periodic heap rebuild when stale.
# - Conservative TinyLFU decay and size-aware scoring constants for stability.

from heapq import heappush, heappop
import random

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()        # key -> 0 (window/probation) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_ver = dict()            # key -> small int (monotonic, for heap entry validation)

# Heaps: entries are (score, last_access, -size, key, ver)
_heap_global = []         # global candidates (used as a generic fallback)
_heap_window = []         # candidates in the window segment only
_heap_prot = []           # candidates in the protected segment only

# Window accounting and adaptation
_window_bytes = 0
_window_count = 0
_target_window_bytes = 0
_CAP_SEEN = None
_MIN_WINDOW_FRAC = 0.02
_MAX_WINDOW_FRAC = 0.70

# Ghost caches (ARC-inspired) to adapt the window target.
# Use insertion-ordered dicts as small LRU sets of recently evicted keys.
_ghostW = dict()  # evicted-from-window keys
_ghostP = dict()  # evicted-from-protected keys
_GHOST_CAP_KEYS = 16384  # max keys per ghost set

# TinyLFU Count-Min Sketch (with periodic decay)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096   # power of two; smaller width => faster decay and smaller memory
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 20000
_decay_ticker = 0
_last_rebuild_access = 0  # to rate-limit heap rebuilds

# Tunables
_DEFAULT_WINDOW_FRACTION = 0.20
_SIZE_ALPHA = 1.0
_PROTECTED_KEEP_BONUS = 1.25  # slight bonus; strict segment-quota handles most protection
# Segment-specific half-lives for recency contribution (in "access_count" units)
_HL_WINDOW = 512
_HL_PROT = 8192

# Hit-driven adaptation
_hitsW = 0
_hitsP = 0
_ADAPT_HIT_PERIOD = 1024  # adjust window target every this many hits
_last_adapt_hit_ts = 0

_EPS = 1e-9


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _ensure_init(cache_snapshot):
    global _CAP_SEEN, _target_window_bytes, _window_bytes, _window_count
    if _CAP_SEEN == cache_snapshot.capacity:
        return
    _reset_all_state(cache_snapshot)

def _reset_all_state(cache_snapshot):
    global m_segment, m_last_access, m_ver
    global _heap_global, _heap_window, _heap_prot
    global _window_bytes, _window_count, _target_window_bytes, _CAP_SEEN
    global _ghostW, _ghostP
    global _cm, _decay_ticker, _last_rebuild_access
    global _hitsW, _hitsP, _last_adapt_hit_ts

    m_segment.clear()
    m_last_access.clear()
    m_ver.clear()
    _heap_global.clear()
    _heap_window.clear()
    _heap_prot.clear()
    _ghostW.clear()
    _ghostP.clear()

    _window_bytes = 0
    _window_count = 0
    cap = cache_snapshot.capacity
    _CAP_SEEN = cap
    _target_window_bytes = max(1, int(_DEFAULT_WINDOW_FRACTION * cap))

    _decay_ticker = 0
    _last_rebuild_access = cache_snapshot.access_count
    _hitsW = 0
    _hitsP = 0
    _last_adapt_hit_ts = cache_snapshot.access_count

    # Reinitialize sketch
    for d in range(_SKETCH_DEPTH):
        for i in range(_SKETCH_WIDTH):
            _cm[d][i] = 0

    # If cache starts preloaded: assume all in window by default, last_access=0
    now = cache_snapshot.access_count
    for key, obj in cache_snapshot.cache.items():
        m_segment[key] = 0
        m_last_access[key] = 0
        m_ver[key] = 0
        _window_bytes += obj.size
        _window_count += 1
        _push_entry(key, obj, now)

def _clamp(v, lo, hi):
    return lo if v < lo else hi if v > hi else v

def _adjust_window_target(delta_bytes, cap):
    global _target_window_bytes
    min_b = int(_MIN_WINDOW_FRAC * cap)
    max_b = int(_MAX_WINDOW_FRAC * cap)
    _target_window_bytes = _clamp(_target_window_bytes + int(delta_bytes), min_b, max_b)

def _recency_factor(age, half_life):
    # Returns in (0,1], decays with age; half_life controls steepness.
    return 1.0 / (1.0 + (float(age) / float(half_life + 1)))

def _priority_for_key(cache_snapshot, key, obj, now):
    # Unified priority for eviction (lower => more evictable)
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    seg = m_segment.get(key, 0)
    if seg == 0:
        # Window: emphasize recency to keep short-term working set, still size-aware.
        rec = _recency_factor(age, _HL_WINDOW)
        score = (freq + 0.6) * rec / (float(size) ** _SIZE_ALPHA)
    else:
        # Protected: emphasize frequency with mild recency for demotion of long-idle items.
        rec = _recency_factor(age, _HL_PROT)
        score = (freq + 0.6) * rec / (float(size) ** _SIZE_ALPHA)
        score *= _PROTECTED_KEEP_BONUS
    return score

def _push_entry(key, obj, now):
    # Push key's current state onto heaps (lazy invalidation via version)
    ver = m_ver.get(key, 0)
    score = _priority_for_key(None, key, obj, now)  # 'cache_snapshot' not needed inside
    last_ts = m_last_access.get(key, 0)
    entry = (score, last_ts, -int(obj.size), key, ver)
    heappush(_heap_global, entry)
    seg = m_segment.get(key, 0)
    if seg == 0:
        heappush(_heap_window, entry)
    else:
        heappush(_heap_prot, entry)

def _valid_heap_entry(cache_snapshot, entry, seg_required=None):
    score, last_ts, neg_size, key, ver = entry
    if key not in cache_snapshot.cache:
        return False
    if m_ver.get(key, -1) != ver:
        return False
    if seg_required is not None and m_segment.get(key, 0) != seg_required:
        return False
    return True

def _pop_valid_from_heap(cache_snapshot, heap, seg_required=None):
    while heap:
        entry = heappop(heap)
        if _valid_heap_entry(cache_snapshot, entry, seg_required):
            return entry[3]
    return None

def _maybe_rebuild_heaps(cache_snapshot):
    # Occasionally rebuild heaps to purge stale entries
    global _last_rebuild_access, _heap_global, _heap_window, _heap_prot, _window_bytes, _window_count
    now = cache_snapshot.access_count
    n_res = len(cache_snapshot.cache)
    if now - _last_rebuild_access < 20000:
        return
    too_large = len(_heap_global) > (8 * max(1, n_res) + 1024)
    if not too_large:
        return

    _heap_global = []
    _heap_window = []
    _heap_prot = []
    _window_bytes = 0
    _window_count = 0
    for key, obj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_ver:
            m_ver[key] = 0
        _push_entry(key, obj, now)
        if m_segment.get(key, 0) == 0:
            _window_bytes += obj.size
            _window_count += 1

    _last_rebuild_access = now

def _ghost_admit(ghost, key):
    if key in ghost:
        ghost.pop(key, None)
    ghost[key] = 1
    if len(ghost) > _GHOST_CAP_KEYS:
        ghost.pop(next(iter(ghost)))

def _adaptive_on_insert(cache_snapshot, obj):
    # ARC-like feedback: adjust window target if insert (miss) hits a ghost set
    cap = cache_snapshot.capacity
    k = obj.key
    # Smaller step to avoid oscillation; size-aware
    step = max(min(obj.size, cap // 200), cap // 1024)
    if k in _ghostW:
        _ghostW.pop(k, None)
        _adjust_window_target(+step, cap)
    elif k in _ghostP:
        _ghostP.pop(k, None)
        _adjust_window_target(-step, cap)

def _maybe_adapt_on_hit(cache_snapshot):
    # Drift window target based on segment of hits, rate-limited and gentle
    global _hitsW, _hitsP, _last_adapt_hit_ts
    if (_hitsW + _hitsP) < _ADAPT_HIT_PERIOD:
        return
    cap = cache_snapshot.capacity
    step_base = max(1, cap // 512)
    # If window sees more hits, grow; else shrink
    if _hitsW > _hitsP:
        delta = step_base
    elif _hitsP > _hitsW:
        delta = -step_base
    else:
        delta = 0
    if delta != 0:
        _adjust_window_target(delta, cap)
    _hitsW = 0
    _hitsP = 0
    _last_adapt_hit_ts = cache_snapshot.access_count

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim with strict segment-quota enforcement:
      - If window byte-usage > adaptive target, evict from window via window-heap.
      - Otherwise, evict from protected via protected-heap.
    Fallbacks:
      - If the preferred segment heap is exhausted/stale, try the global heap with segment filter.
      - As last resort, random sampling (biased to the required segment).
    """
    _ensure_init(cache_snapshot)
    now = cache_snapshot.access_count

    restrict_to_window = (_window_bytes > _target_window_bytes) and (_window_count > 0)

    if restrict_to_window:
        # Evict from window (probation)
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_window, seg_required=0)
        if victim_key is not None:
            return victim_key
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=0)
        if victim_key is not None:
            return victim_key
    else:
        # Evict from protected
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_prot, seg_required=1)
        if victim_key is not None:
            return victim_key
        victim_key = _pop_valid_from_heap(cache_snapshot, _heap_global, seg_required=1)
        if victim_key is not None:
            return victim_key

    # Fallback: sample random keys (bias to required segment if possible)
    keys = list(cache_snapshot.cache.keys())
    if not keys:
        return None
    if restrict_to_window:
        cand = [k for k in keys if m_segment.get(k, 0) == 0]
        if cand:
            keys = cand
    else:
        cand = [k for k in keys if m_segment.get(k, 0) == 1]
        if cand:
            keys = cand

    sample_sz = min(32, len(keys))
    samples = random.sample(keys, sample_sz)
    best_k = None
    best_score = None
    best_last_ts = None
    best_size = None
    for k in samples:
        o = cache_snapshot.cache[k]
        if k not in m_segment:
            m_segment[k] = 0
        if k not in m_last_access:
            m_last_access[k] = 0
        if k not in m_ver:
            m_ver[k] = 0
        sc = _priority_for_key(cache_snapshot, k, o, now)
        ts = m_last_access.get(k, 0)
        if (best_k is None or sc < best_score - _EPS or
            (abs(sc - best_score) <= _EPS and (ts < best_last_ts or
                                               (ts == best_last_ts and o.size > best_size)))):
            best_k = k
            best_score = sc
            best_last_ts = ts
            best_size = o.size
        # Seed heaps for future calls
        _push_entry(k, o, now)

    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch (global) and decay if needed.
      - Update last access timestamp.
      - Hit-driven window target drift (gentle, periodic).
      - Promote from window -> protected on first hit (SLRU).
      - Version bump and fresh heap entries.
    """
    _ensure_init(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Ensure metadata exists
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = 0
    if key not in m_ver:
        m_ver[key] = 0

    seg_before = m_segment.get(key, 0)

    # Update last access
    m_last_access[key] = now

    # Count hits by segment for adaptive drift
    global _hitsW, _hitsP
    if seg_before == 0:
        _hitsW += 1
    else:
        _hitsP += 1

    # Promote to protected if currently in window
    if seg_before == 0:
        m_segment[key] = 1  # move to protected
        _decr_window(obj.size)  # update window accounting

    # Version bump and push a fresh heap entry
    m_ver[key] += 1
    _push_entry(key, obj, now)

    # Periodic drift adjustment and heap maintenance
    _maybe_adapt_on_hit(cache_snapshot)
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Adaptive tuning of window target using ghost feedback.
      - Update TinyLFU counts for this access.
      - Initialize metadata: segment=window, last_access=now, version bump.
      - Push heap entries.
    """
    _ensure_init(cache_snapshot)

    # Adapt window target based on ghosts (ARC-like)
    _adaptive_on_insert(cache_snapshot, obj)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0  # window/probation
    m_last_access[key] = now
    m_ver[key] = m_ver.get(key, 0) + 1

    # Window accounting
    _incr_window(obj.size)

    # Push entries
    _push_entry(key, obj, now)

    # Occasional heap maintenance if needed
    _maybe_rebuild_heaps(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Insert evicted key into ghost set (for adaptive window tuning).
      - Remove resident metadata for the evicted key.
      - Update window accounting if needed.
      - Heaps rely on lazy invalidation via version and residency checks.
    """
    _ensure_init(cache_snapshot)
    ev_key = evicted_obj.key

    seg = m_segment.get(ev_key, 0)
    if seg == 0:
        _ghost_admit(_ghostW, ev_key)
    else:
        _ghost_admit(_ghostP, ev_key)

    if seg == 0:
        _decr_window(evicted_obj.size)

    # Purge metadata (stale heap entries will be ignored)
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_ver.pop(ev_key, None)


# ------------------------
# Small helpers for window accounting
# ------------------------
def _incr_window(sz):
    global _window_bytes, _window_count
    _window_bytes += int(sz)
    _window_count += 1

def _decr_window(sz):
    global _window_bytes, _window_count
    _window_bytes = max(0, _window_bytes - int(sz))
    if _window_count > 0:
        _window_count -= 1
```
2025-11-28 02:26:19,876 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved SLRU-Window TinyLFU (W-TinyLFU inspired) with better recency maintenance,
# stronger scan resistance, and safer large-object control.
#
# Key improvements:
# - Proper LRU order maintenance: touch on every hit (lazy duplicates).
# - Evict from oldest-window prefix (K oldest) using TinyLFU score to avoid
#   evicting strong-but-unpromoted items.
# - Large-object control happens before window eviction to prevent large blow-ups.
# - Tighter default window size (adaptable) for stronger scan resistance.
# - Stricter admission guard; on guard-trigger we tag incoming as doomed-like
#   (handled on insert) instead of evicting strong items.
# - Slightly stricter promotion for large items (3 hits) to avoid polluting protected.
# - Expanded "doomed" tagging for very cold inserts when window is bloated.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_insert_ts = dict()       # key -> int (timestamp of insertion)
m_hits_resident = dict()   # key -> small int: resident hit counter since (re-)insert
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Recency queues (lazy-cleaned); duplicates allowed, left=LRU, right=MRU
_window_q = deque()
_protected_q = deque()

# Size classes and guards
m_large_keys = set()       # keys classified "large" at insertion time
m_doomed = set()           # keys tagged as low-value; evict first
_g_window_bytes = 0
_g_protected_bytes = 0
_g_large_bytes = 0         # total bytes of objects in m_large_keys (resident only)

# Running average size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.15
_WINDOW_MIN = 0.02
_WINDOW_MAX = 0.60
_WINDOW_STEP = 0.05

# Sampling sizes
_SAMPLE_SIZE = 16
_SAMPLE_SIZE_LARGE = 12
_SAMPLE_SIZE_GLOBAL_OLDEST = 16
_WINDOW_PREFIX_K = 6        # number of oldest-window candidates to consider first

# Priority scoring weights
_BASE_SIZE_ALPHA = 1.00
_W_FREQ = 1.0
_W_RECENCY = 0.5
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission-compare guard
_ADMIT_COMPARE_FACTOR = 1.50

# Adapt window fraction based on where hits happen
_ADAPT_PERIOD = 5000
_adapt_ticker = 0
_hits_in_window = 0
_hits_in_protected = 0

# Ghost history (ARC-like feedback)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_p = deque()
_ghost_p_set = set()
_GHOST_FACTOR = 2.0

# Size classification and caps
_LARGE_RATIO = 2.5                 # size >= 2.5x average -> "large"
_LARGE_BYTES_FRACTION = 0.60       # allow up to 60% of bytes to be large

# Doorkeeper thresholds (promotion on resident hits)
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 3            # stricter for large objects

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _g_large_bytes
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    for key, robj in cache_snapshot.cache.items():
        total_bytes += int(robj.size)
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits_resident:
            m_hits_resident[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        size = int(robj.size)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
            _window_q.append(key)
        else:
            _g_protected_bytes += size
            _protected_q.append(key)
        is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
        if is_large:
            m_large_keys.add(key)
            _g_large_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _protected_budget_bytes(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _dynamic_size_alpha(incoming_size):
    r = float(incoming_size) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_SIZE_ALPHA + 0.25
    elif r >= 2.0:
        return _BASE_SIZE_ALPHA + 0.10
    elif r <= 0.5:
        return max(0.90, _BASE_SIZE_ALPHA - 0.10)
    else:
        return _BASE_SIZE_ALPHA

def _priority_for_key_alpha(cache_snapshot, key, obj, now, size_alpha):
    size = max(int(obj.size), 1)
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    if key in m_doomed:
        score *= 0.65
    return score

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    return _priority_for_key_alpha(cache_snapshot, key, obj, now, _dynamic_size_alpha(incoming_size))

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size_alpha = _dynamic_size_alpha(incoming_obj.size)
    size = max(int(incoming_obj.size), 1)
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    return score

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _adapt_window_fraction_if_needed():
    global _adapt_ticker, _hits_in_window, _hits_in_protected, _f_window
    if _adapt_ticker < _ADAPT_PERIOD:
        return
    total_hits = _hits_in_window + _hits_in_protected
    if total_hits > 0:
        frac_w = _hits_in_window / float(total_hits)
        if frac_w > 0.60:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        elif frac_w < 0.40:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
    _adapt_ticker = 0
    _hits_in_window = 0
    _hits_in_protected = 0

def _ghost_capacity_items(cache_snapshot):
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    return max(1, int(_GHOST_FACTOR * est_items))

def _ghost_add(is_protected, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_protected:
        if key in _ghost_p_set:
            return
        _ghost_p.append(key)
        _ghost_p_set.add(key)
        while len(_ghost_p) > cap:
            old = _ghost_p.popleft()
            _ghost_p_set.discard(old)
    else:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)

def _clean_window_left():
    while _window_q:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            return k
        _window_q.popleft()
    return None

def _clean_protected_left():
    while _protected_q:
        k = _protected_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 1:
            return k
        _protected_q.popleft()
    return None

def _peek_oldest_window():
    return _clean_window_left()

def _pop_oldest_protected_for_demote():
    k = _clean_protected_left()
    if k is None:
        return None
    return k

def _demote_one_from_protected(cache_snapshot):
    global _g_window_bytes, _g_protected_bytes
    k = _pop_oldest_protected_for_demote()
    if k is None:
        return False
    if k not in m_key_index or m_segment.get(k, 0) != 1:
        return False
    robj = cache_snapshot.cache.get(k, None)
    if robj is None:
        return False
    sz = int(robj.size)
    m_segment[k] = 0
    _g_protected_bytes = max(0, _g_protected_bytes - sz)
    _g_window_bytes += sz
    _window_q.append(k)  # MRU in window after demotion
    return True

def _enforce_protected_budget(cache_snapshot):
    budget = _protected_budget_bytes(cache_snapshot)
    iter_guard = 0
    while _g_protected_bytes > budget and iter_guard < 8:
        if not _demote_one_from_protected(cache_snapshot):
            break
        iter_guard += 1

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    result = []
    seen = set()
    tries = 0
    max_tries = max(k * 6, k)
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if want_segment is None or m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _sample_from_keyset(keyset, k):
    if not keyset or k <= 0:
        return []
    candidates = [kk for kk in keyset if kk in m_key_index]
    if not candidates:
        return []
    if len(candidates) <= k:
        return list(candidates)
    return random.sample(candidates, k)

def _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        last_ts = m_last_access.get(key, 0)
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and int(robj.size) > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = int(robj.size)
    return victim_key, victim_score

def _find_oldest_doomed_in_window():
    _clean_window_left()
    steps = 0
    while _window_q and steps < 64:
        k = _window_q[0]
        if k in m_key_index and m_segment.get(k, 0) == 0:
            if k in m_doomed:
                return k
        # gentle rotation to avoid repeatedly checking same head
        _window_q.popleft()
        _window_q.append(k)
        steps += 1
    return None

def _collect_oldest_window_keys(k):
    # Collect up to k distinct valid window keys starting from LRU
    res = []
    seen = set()
    if k <= 0:
        return res
    for key in _window_q:
        if key in seen:
            continue
        if key in m_key_index and m_segment.get(key, 0) == 0:
            res.append(key)
            seen.add(key)
            if len(res) >= k:
                break
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction strategy (improved):
      1) Enforce protected budget via LRU demotions.
      2) If large bytes exceed cap, preferentially evict a weak large item (TinyLFU score).
      3) Prefer evicting a doomed window item (LRU scan shielding).
      4) Otherwise, consider the K oldest window items and evict the weakest by TinyLFU+recency score.
      5) If window empty, fallback to global sampling to evict the weakest.
      6) Admission guard: if chosen victim is much stronger than incoming,
         keep victim and tag incoming as doomed for future quick eviction.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    _enforce_protected_budget(cache_snapshot)

    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)

    # 1) Large overcap handling first
    large_cap = int(_LARGE_BYTES_FRACTION * cache_snapshot.capacity)
    if _g_large_bytes > large_cap and m_large_keys:
        cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
        if cands:
            vk, vs = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)
            if vk is not None:
                # Admission guard vs incoming: still allow if incoming is reasonably strong
                if vs >= _ADMIT_COMPARE_FACTOR * incoming_score:
                    # Incoming is weak -> tag it for quick exit after insert
                    m_doomed.add(obj.key)
                return vk

    # 2) Doomed in window, if any
    victim_key = _find_oldest_doomed_in_window()
    victim_score = None
    if victim_key is not None:
        robj = cache_snapshot.cache.get(victim_key, None)
        if robj is not None:
            victim_score = _priority_for_key(cache_snapshot, victim_key, robj, now, incoming_size)
            return victim_key

    # 3) Oldest-window prefix candidates
    w_cands = _collect_oldest_window_keys(_WINDOW_PREFIX_K)
    if w_cands:
        victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, w_cands, incoming_size, now)
    else:
        victim_key, victim_score = None, None

    # 4) Fallbacks if needed
    if victim_key is None:
        # Try large again even if not overcap (if incoming itself is large, prefer trading a weak large)
        if (obj.size >= _LARGE_RATIO * max(1.0, _avg_size)) and m_large_keys:
            cands = _sample_from_keyset(m_large_keys, _SAMPLE_SIZE_LARGE)
            if cands:
                victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, cands, incoming_size, now)

    if victim_key is None:
        # Global sampling fallback
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE, len(m_keys)))
        if candidates:
            victim_key, victim_score = _select_victim_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # 5) Admission guard: if victim is much stronger than incoming, keep victim and mark incoming as doomed
    if victim_key is not None and victim_score is not None:
        if victim_score >= _ADMIT_COMPARE_FACTOR * incoming_score:
            m_doomed.add(obj.key)
            return victim_key

    # Final fallbacks
    if victim_key is None:
        # Try global oldest by timestamp from a small sample
        candidates = _sample_keys_from_segment(None, min(_SAMPLE_SIZE_GLOBAL_OLDEST, len(m_keys)))
        if candidates:
            oldest = None
            oldest_ts = None
            for k in candidates:
                ts = m_last_access.get(k, 0)
                if oldest is None or ts < oldest_ts:
                    oldest = k
                    oldest_ts = ts
            victim_key = oldest

    if victim_key is None and m_keys:
        victim_key = m_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit (improved):
      - Update TinyLFU and avg size.
      - Update timestamps and resident hit counter.
      - Touch LRU position (append to segment deque).
      - Promote window->protected after threshold (1 small, 3 large).
      - Clear "doomed" tag on reuse.
      - Track hit location for window adaptation and enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    if key not in m_key_index:
        # Safety: register if missing
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
        m_segment.setdefault(key, 0)
        sz = int(obj.size)
        if m_segment[key] == 0:
            globals()['_g_window_bytes'] += sz
            _window_q.append(key)
        else:
            globals()['_g_protected_bytes'] += sz
            _protected_q.append(key)
        m_insert_ts[key] = now
        m_hits_resident[key] = 0

    seg = m_segment.get(key, 0)

    # Adaptation accounting
    globals()['_adapt_ticker'] += 1
    if seg == 0:
        globals()['_hits_in_window'] += 1
    else:
        globals()['_hits_in_protected'] += 1

    # Update timestamps and hits
    m_last_access[key] = now
    m_hits_resident[key] = min(7, m_hits_resident.get(key, 0) + 1)

    # Clear doom on reuse
    m_doomed.discard(key)

    # Touch in LRU for the corresponding segment (maintain MRU order via lazy duplicates)
    if seg == 0:
        _window_q.append(key)
    else:
        _protected_q.append(key)

    # Promotion policy
    size = int(obj.size)
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    needed_hits = _PROMOTE_HITS_LARGE if is_large else _PROMOTE_HITS_SMALL

    if seg == 0 and m_hits_resident[key] >= needed_hits:
        # Promote to protected
        m_segment[key] = 1
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - size)
        globals()['_g_protected_bytes'] += size
        _protected_q.append(key)

    # Keep protected within budget
    _enforce_protected_budget(cache_snapshot)

    # Periodically adapt window fraction
    _adapt_window_fraction_if_needed()

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (improved):
      - Update TinyLFU and avg size.
      - Initialize metadata; set segment=window.
      - LRU touch in window.
      - Classify size and update large-bytes accounting.
      - Ghost-feedback: adjust window fraction.
      - Scan-shielding: more aggressive doom tagging for clearly cold inserts
        especially when window is bloated.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    m_segment[key] = 0  # window
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    # Update bytes counters and queues
    globals()['_g_window_bytes'] += size
    _window_q.append(key)

    # Size classification
    is_large = (size >= _LARGE_RATIO * max(1.0, _avg_size))
    if is_large:
        if key not in m_large_keys:
            m_large_keys.add(key)
            globals()['_g_large_bytes'] += size
    else:
        if key in m_large_keys:
            m_large_keys.discard(key)
            globals()['_g_large_bytes'] = max(0, _g_large_bytes - size)

    # Ghost-feedback adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_p_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_p_set.discard(key)

    # Scan-shielding ("doomed" tagging)
    est_freq = _sketch_estimate(key)
    target_window = _current_window_target_bytes(cache_snapshot)
    window_is_bloated = (_g_window_bytes >= int(0.75 * target_window))
    # Tag as doomed if it's clearly cold and relatively large or window is bloated
    if (est_freq <= 1.0) and (key not in _ghost_p_set):
        if is_large or window_is_bloated or size >= max(2 * _avg_size, _LARGE_RATIO * _avg_size * 0.7):
            m_doomed.add(key)
        else:
            m_doomed.discard(key)
    else:
        m_doomed.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata for evicted key and adjust bytes counters.
      - Maintain large-bytes accounting and clean "doomed" tag.
      - Add key to ghost history with segment tag.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    if seg == 0:
        globals()['_g_window_bytes'] = max(0, _g_window_bytes - ev_size)
    else:
        globals()['_g_protected_bytes'] = max(0, _g_protected_bytes - ev_size)

    if ev_key in m_large_keys:
        m_large_keys.discard(ev_key)
        globals()['_g_large_bytes'] = max(0, _g_large_bytes - ev_size)
    m_doomed.discard(ev_key)

    _ghost_add(is_protected=(seg == 1), key=ev_key, cache_snapshot=cache_snapshot)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 12
- Hit Rate: 0.0568

Example 2:
- Trace ID: 2
- Hit Rate: 0.4382

Example 3:
- Trace ID: 16
- Hit Rate: 0.0206

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:26:19 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:26:19,877 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:28:41 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:28:41,452 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:28:41,456 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Lightweight W-TinyLFU with SLRU (Window + Probation + Protected) using item-count
# budgeting derived from average object size (works for byte- or item-capped caches).
# Focus: simple, robust admission/eviction with strong scan resistance and good reuse.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
# Segments: 0 = window (recency), 1 = probation (main, victimized first), 2 = protected (main)
m_segment = dict()         # key -> segment id
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int
m_hits_resident = dict()   # key -> small int: hits since (re-)insert into current segment

# Recency queues (lazy-cleaned); left=LRU, right=MRU
_window_q = deque()
_probation_q = deque()
_protected_q = deque()

# Segment counts (items)
_n_window = 0
_n_probation = 0
_n_protected = 0

# Running average size (EMA) to estimate item capacity if cache is byte-capped
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 20000
_decay_ticker = 0

# Target fractions (by estimated item count)
_f_window = 0.20        # default fraction of items in window
_f_protected = 0.80     # fraction of main (probation+protected) reserved for protected

# Adaptation via ghost history (ARC-like)
_ghost_w = deque()
_ghost_w_set = set()
_ghost_m = deque()
_ghost_m_set = set()
_GHOST_FACTOR = 2.0
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.50
_WINDOW_STEP = 0.05

# Admission compare margin
_ADMIT_COMPARE_FACTOR = 1.00  # accept into main if incoming >= victim

# Promotion thresholds
_PROMOTE_FROM_WINDOW_HITS = 1  # move window -> probation on first hit
# probation -> protected upon first hit

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        return 0.0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _est_item_capacity(cache_snapshot):
    # If capacity is bytes, estimate items as capacity/avg_size; if it's already item count, this still works well.
    cap = max(1, int(cache_snapshot.capacity))
    return max(1, int(round(cap / max(1.0, _avg_size))))

def _ghost_capacity_items(cache_snapshot):
    return max(1, int(_GHOST_FACTOR * _est_item_capacity(cache_snapshot)))

def _ghost_add(is_window, key, cache_snapshot):
    cap = _ghost_capacity_items(cache_snapshot)
    if is_window:
        if key in _ghost_w_set:
            return
        _ghost_w.append(key)
        _ghost_w_set.add(key)
        while len(_ghost_w) > cap:
            old = _ghost_w.popleft()
            _ghost_w_set.discard(old)
    else:
        if key in _ghost_m_set:
            return
        _ghost_m.append(key)
        _ghost_m_set.add(key)
        while len(_ghost_m) > cap:
            old = _ghost_m.popleft()
            _ghost_m_set.discard(old)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _n_window, _n_probation, _n_protected
    if _bootstrapped:
        return
    # Initialize from snapshot: place existing items into probation by default
    now = cache_snapshot.access_count
    _n_window = _n_probation = _n_protected = 0
    for key, robj in cache_snapshot.cache.items():
        m_segment[key] = 1
        m_last_access[key] = now
        m_insert_ts[key] = now
        m_hits_resident[key] = 0
        _probation_q.append(key)
        _n_probation += 1
        _update_avg_size(robj.size)
    _bootstrapped = True

def _peek_lru_of(deq, seg_id):
    # Clean lazy duplicates and return LRU key for the segment
    while deq:
        k = deq[0]
        if k in m_segment and m_segment.get(k, -1) == seg_id:
            return k
        deq.popleft()
    return None

def _append_mru_of(deq, key):
    deq.append(key)

def _move_to_segment(key, to_seg):
    # Move key to target segment; update counts and queues
    global _n_window, _n_probation, _n_protected
    from_seg = m_segment.get(key, None)
    if from_seg == to_seg:
        # Just touch in place
        if to_seg == 0:
            _append_mru_of(_window_q, key)
        elif to_seg == 1:
            _append_mru_of(_probation_q, key)
        elif to_seg == 2:
            _append_mru_of(_protected_q, key)
        return
    # Update counts
    if from_seg == 0:
        _n_window = max(0, _n_window - 1)
    elif from_seg == 1:
        _n_probation = max(0, _n_probation - 1)
    elif from_seg == 2:
        _n_protected = max(0, _n_protected - 1)
    # Add to target
    m_segment[key] = to_seg
    if to_seg == 0:
        _n_window += 1
        _append_mru_of(_window_q, key)
    elif to_seg == 1:
        _n_probation += 1
        _append_mru_of(_probation_q, key)
    elif to_seg == 2:
        _n_protected += 1
        _append_mru_of(_protected_q, key)
    # Reset segment-local hit counter
    m_hits_resident[key] = 0

def _demote_protected_lru():
    # Demote one from protected -> probation
    k = _peek_lru_of(_protected_q, 2)
    if k is None:
        return False
    _move_to_segment(k, 1)
    return True

def _enforce_protected_budget(cache_snapshot):
    # Keep protected fraction within budget relative to main (probation+protected)
    main_items = max(0, _n_probation + _n_protected)
    if main_items <= 0:
        return
    target_prot = int(round(_f_protected * main_items))
    iter_guard = 0
    while _n_protected > target_prot and iter_guard < 8:
        if not _demote_protected_lru():
            break
        iter_guard += 1

def _evict_choice_with_admission(cache_snapshot, incoming_key):
    # Decide whether to evict from Window or Probation based on TinyLFU admission compare.
    # Returns victim key or None if unavailable.
    cap_items = _est_item_capacity(cache_snapshot)
    window_target = max(1, int(round(_f_window * cap_items)))

    # Ensure we can evict from probation: if empty but protected has items, demote some
    if _n_probation == 0 and _n_protected > 0:
        _demote_protected_lru()

    w_lru = _peek_lru_of(_window_q, 0)
    p_lru = _peek_lru_of(_probation_q, 1)

    # If window is oversized, evict from window to keep recency buffer tight
    if _n_window > window_target and w_lru is not None:
        return w_lru

    # If no probation, fall back to window
    if p_lru is None:
        if w_lru is not None:
            return w_lru
        # As a last resort, evict from protected (should be rare after demotion)
        q_lru = _peek_lru_of(_protected_q, 2)
        return q_lru

    # Admission decision: compare incoming frequency vs probation LRU frequency
    f_in = _sketch_estimate(incoming_key)
    f_v = _sketch_estimate(p_lru)

    if f_in * _ADMIT_COMPARE_FACTOR < f_v:
        # Incoming is weak: evict from window if possible to protect main
        if w_lru is not None:
            return w_lru
        return p_lru
    else:
        # Incoming is strong enough: trade with probation victim
        return p_lru


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Eviction policy:
      - Maintain protected budget (demote protected LRU to probation if oversized).
      - Choose victim via W-TinyLFU admission comparison:
          * If window exceeds target size: evict window LRU.
          * Else compare incoming freq with probation LRU freq:
              - If incoming weaker -> evict window LRU (protect main).
              - Else -> evict probation LRU (admit incoming into main soon).
      - If probation empty, demote from protected to enable eviction from main.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_decay_if_needed()

    # Enforce protected budget before deciding
    _enforce_protected_budget(cache_snapshot)

    victim = _evict_choice_with_admission(cache_snapshot, obj.key)
    # As ultimate fallback, evict any available LRU across segments
    if victim is None:
        victim = (_peek_lru_of(_probation_q, 1) or
                  _peek_lru_of(_window_q, 0) or
                  _peek_lru_of(_protected_q, 2))
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Increment TinyLFU and avg size, timestamps.
      - Window: promote to probation on first hit; touch MRU.
      - Probation: promote to protected on hit; touch MRU.
      - Protected: touch MRU.
      - Enforce protected budget.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_segment.get(key, None)
    if seg is None:
        # Safety: treat as a probation resident if metadata missing
        m_segment[key] = 1
        m_insert_ts[key] = now
        m_hits_resident[key] = 0
        globals()['_n_probation'] += 1
        _probation_q.append(key)
        seg = 1

    if seg == 0:
        # Window hit: promote to probation upon first reuse
        m_hits_resident[key] = min(7, m_hits_resident.get(key, 0) + 1)
        if m_hits_resident[key] >= _PROMOTE_FROM_WINDOW_HITS:
            _move_to_segment(key, 1)
        else:
            _append_mru_of(_window_q, key)
    elif seg == 1:
        # Probation hit -> protected
        _move_to_segment(key, 2)
    else:
        # Protected hit: just touch
        _append_mru_of(_protected_q, key)

    # Keep protected fraction within budget
    _enforce_protected_budget(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
      - Increment TinyLFU and avg size.
      - Place into Window (recency).
      - Ghost feedback to adapt window fraction:
          * If key was in window-ghost -> increase window fraction.
          * If key was in main-ghost   -> decrease window fraction.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize metadata
    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now
    m_hits_resident[key] = 0
    globals()['_n_window'] += 1
    _window_q.append(key)

    # Ghost-based adaptation
    global _f_window
    if key in _ghost_w_set:
        _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        _ghost_w_set.discard(key)
    elif key in _ghost_m_set:
        _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        _ghost_m_set.discard(key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove metadata and adjust segment counts.
      - Add evicted key to ghost history (window vs main).
    '''
    _bootstrap_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    seg = m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits_resident.pop(ev_key, None)

    if seg == 0:
        globals()['_n_window'] = max(0, _n_window - 1)
        _ghost_add(is_window=True, key=ev_key, cache_snapshot=cache_snapshot)
    elif seg == 1:
        globals()['_n_probation'] = max(0, _n_probation - 1)
        _ghost_add(is_window=False, key=ev_key, cache_snapshot=cache_snapshot)
    elif seg == 2:
        globals()['_n_protected'] = max(0, _n_protected - 1)
        _ghost_add(is_window=False, key=ev_key, cache_snapshot=cache_snapshot)
```
2025-11-28 02:31:09,098 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Improved: Size-aware SLRU with TinyLFU, demote-before-evict, and ARC-style ghost adaptation
# Design goals:
# - Always evict from the window (probationary); protected demotes back to window first
# - TinyLFU-driven priorities (recency + freq) with size-aware normalization
# - ARC-like ghost feedback to adapt window fraction quickly and avoid pathological scans
# - Bounded CPU: small samples, O(1) indexing, periodic TinyLFU decay

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp)
m_insert_ts = dict()       # key -> int (timestamp)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive)
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP = 0.03

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 12       # window candidates to pick eviction from
_S_PROT_DEMOTE = 10     # protected candidates when demoting

# Scoring weights
_W_FREQ = 1.0
_W_RECENCY = 1.8
_PROTECTED_KEEP_BONUS = 1.12
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.10

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.40
    elif r >= 2.0:
        return 1.20
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))
    size_alpha = _size_alpha_for_incoming(incoming_size)
    score = (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)
    if m_segment.get(key, 0) == 1:
        score *= _PROTECTED_KEEP_BONUS
    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(5 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    last_key = m_keys[last_idx]
    if idx != last_idx:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    global _f_window
    seg = _ghost.pop(key, None)
    if seg is not None:
        # If it was evicted from window and reappears -> increase window
        # If it was evicted from protected and reappears -> decrease window
        if seg == 0:
            _f_window = min(_WINDOW_MAX, _f_window + _WINDOW_STEP)
        else:
            _f_window = max(_WINDOW_MIN, _f_window - _WINDOW_STEP)
        # Also remove from queue if present (best-effort)
        # We won't linearly search queue to remove; it will drop when encountered as stale.


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Always pick victim from window using TinyLFU+recency+size score (sampled).
      - Admission guard: if chosen victim looks stronger than incoming, switch to the oldest window item.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has at least one candidate
    _ensure_window_has_candidate(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    # Admission-guard: if victim is stronger than incoming, evict an old window key instead
    if victim_key is not None:
        incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
        if victim_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
            alt_oldest = _select_oldest_from_keys(candidates)
            if alt_oldest is not None:
                victim_key = alt_oldest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - Refresh last access.
      - Promote window -> protected on first hit (SLRU).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now

    seg = m_segment.get(key, 0)
    m_last_access[key] = now

    # Promote on hit (classic SLRU)
    if seg == 0:
        m_segment[key] = 1
        globals()['_g_window_bytes'] -= size
        globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Initialize metadata: segment=window, timestamps, indexing.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction immediately.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_last_access[key] = now
    m_insert_ts[key] = now

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
    else:
        globals()['_g_protected_bytes'] -= ev_size

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 46
- Hit Rate: 0.4663

Example 2:
- Trace ID: 0
- Hit Rate: 0.4722

Example 3:
- Trace ID: 8
- Hit Rate: 0.5384

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:31:09 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:31:09,100 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:33:13 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:33:13,586 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:33:13,614 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Size-aware SLRU + TinyLFU + reuse-distance + adaptive window momentum
# Key upgrades over previous version:
# - Size-aware promotions: large objects require more hits before moving to protected.
# - Reuse-distance aware prioritization: integrates inter-arrival time and age since insert.
# - Stronger TinyLFU weight, gentler recency weight (closer to W-TinyLFU behavior).
# - Admission-friendly eviction: try to avoid evicting items that are stronger than incoming.
# - ARC-style ghost with momentum: repeated ghost recalls scale the window fraction faster.
# - More robust "ensure window victim" via on-demand demotion and window refilling.

import random
from collections import deque

# ------------------------
# Global metadata
# ------------------------
m_segment = dict()         # key -> 0 (window) or 1 (protected)
m_last_access = dict()     # key -> int (timestamp of last access)
m_prev_access = dict()     # key -> int (previous last access)
m_last_gap = dict()        # key -> int (last reuse distance)
m_insert_ts = dict()       # key -> int (timestamp of insert)
m_hits = dict()            # key -> int (hits since insert)
m_keys = []                # list of resident keys (for O(1) uniform sampling)
m_key_index = dict()       # key -> index in m_keys

# Bytes accounting (sum of resident object sizes in each segment)
_g_window_bytes = 0
_g_protected_bytes = 0

# Running average resident size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch (global)
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 4096
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Decay configuration
_DECAY_PERIOD = 60000
_decay_ticker = 0

# Window fraction (adaptive) and momentum
_f_window = 0.25
_WINDOW_MIN = 0.05
_WINDOW_MAX = 0.70
_WINDOW_STEP_BASE = 0.03
_WINDOW_STEP_MAX = 0.12
_f_momentum = 0.0            # signed; positive -> increase window, negative -> decrease
_last_ghost_dir = 0          # +1 for window ghost, -1 for protected ghost, 0 for none
_ghost_streak = 0            # how many consecutive same-direction ghost consumes

# ARC-style ghosts to steer window size adaptively
_ghost = dict()        # key -> seg_at_eviction (0 window, 1 protected)
_ghost_q = deque()     # FIFO order to keep bounded
_GHOST_CAP = 65536     # updated on bootstrap based on capacity/avg size

# Sampling sizes
_S_WIN_EVICT = 16       # window candidates to pick eviction from
_S_PROT_DEMOTE = 12     # protected candidates when demoting

# Scoring weights
_W_FREQ = 2.2
_W_RECENCY = 1.0
_W_REUSE = 1.7
_PROTECTED_KEEP_BONUS = 1.08
_EPS = 1e-9

# Admission compare guard (scan resistance)
_ADMIT_COMPARE = 1.05   # if victim is >= 1.05x stronger than incoming, avoid evicting it

# Penalize cold window entries (never hit since insert)
_COLD_WINDOW_MULT = 0.70

# Bootstrap guard
_bootstrapped = False


# ------------------------
# Internal helpers
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _f_momentum
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        # Decay momentum slowly to avoid lock-in
        _f_momentum *= 0.92

def _update_avg_size(sz):
    global _avg_size
    try:
        szf = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, szf)

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _g_window_bytes, _g_protected_bytes, _avg_size, _GHOST_CAP
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Initialize all resident objects as window by default
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        total_bytes += size
        total_items += 1
        if key not in m_segment:
            m_segment[key] = 0
        if key not in m_last_access:
            m_last_access[key] = 0
        if key not in m_prev_access:
            m_prev_access[key] = 0
        if key not in m_last_gap:
            m_last_gap[key] = 1 << 20
        if key not in m_insert_ts:
            m_insert_ts[key] = 0
        if key not in m_hits:
            m_hits[key] = 0
        if key not in m_key_index:
            m_key_index[key] = len(m_keys)
            m_keys.append(key)
        if m_segment.get(key, 0) == 0:
            _g_window_bytes += size
        else:
            _g_protected_bytes += size
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Ghost capacity roughly 4x resident item count estimate, bounded
    est_items = max(1, int(cache_snapshot.capacity / max(1.0, _avg_size)))
    _GHOST_CAP = max(8192, min(131072, 4 * est_items))
    _bootstrapped = True

def _current_window_target_bytes(cache_snapshot):
    return max(1, int(_f_window * cache_snapshot.capacity))

def _size_alpha_for_incoming(incoming_size):
    r = float(max(1, int(incoming_size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 1.45
    elif r >= 2.0:
        return 1.22
    elif r <= 0.5:
        return 0.90
    else:
        return 1.05

def _promotion_threshold_for_size(size):
    # Large objects need more confirmations before protecting
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return 3
    elif r >= 2.0:
        return 2
    else:
        return 1

def _priority_for_key(cache_snapshot, key, obj, now, incoming_size):
    # Higher score -> more valuable (less likely to evict)
    size = max(1, int(obj.size))
    freq = _sketch_estimate(key)
    age = max(0, now - m_last_access.get(key, 0))
    recency = 1.0 / (1.0 + float(age))

    # Reuse distance since previous access (if available)
    gap = m_last_gap.get(key, None)
    if gap is None or gap <= 0:
        reuse_val = 0.0
    else:
        reuse_val = 1.0 / (1.0 + float(gap))

    size_alpha = _size_alpha_for_incoming(incoming_size)

    score = (_W_FREQ * freq + _W_RECENCY * recency + _W_REUSE * reuse_val) / (float(size) ** size_alpha)

    # Window cold entries (no hits since insert) are penalized to be evicted sooner
    if m_segment.get(key, 0) == 0:
        if m_hits.get(key, 0) <= 0:
            score *= _COLD_WINDOW_MULT
    else:
        # Protected bonus
        score *= _PROTECTED_KEEP_BONUS

    # Mild boost for very recent inserts that have already shown reuse quickly
    ins_ts = m_insert_ts.get(key, now)
    age_since_insert = max(0, now - ins_ts)
    if m_hits.get(key, 0) > 0 and age_since_insert <= 2:
        score *= 1.05

    return score

def _incoming_priority_estimate(cache_snapshot, incoming_obj, now):
    size = max(1, int(incoming_obj.size))
    freq = _sketch_estimate(incoming_obj.key)
    recency = 1.0  # incoming is "now"
    size_alpha = _size_alpha_for_incoming(size)
    # No reuse yet on incoming; TinyLFU + recency only
    return (_W_FREQ * freq + _W_RECENCY * recency) / (float(size) ** size_alpha)

def _sample_keys_from_segment(want_segment, k):
    n = len(m_keys)
    if n == 0 or k <= 0:
        return []
    if want_segment is None:
        k = min(k, n)
        idxs = random.sample(range(n), k)
        return [m_keys[i] for i in idxs]
    result = []
    seen = set()
    max_tries = max(6 * k, k)
    tries = 0
    while len(result) < k and tries < max_tries:
        i = random.randrange(n)
        key = m_keys[i]
        if key in seen:
            tries += 1
            continue
        if m_segment.get(key, 0) == want_segment:
            result.append(key)
            seen.add(key)
        tries += 1
    if not result and want_segment is not None:
        return _sample_keys_from_segment(None, min(k, n))
    return result

def _select_oldest_from_keys(keys):
    if not keys:
        return None
    oldest_key = None
    oldest_ts = None
    for key in keys:
        ts = m_last_access.get(key, 0)
        if oldest_key is None or ts < oldest_ts:
            oldest_key = key
            oldest_ts = ts
    return oldest_key

def _select_largest_from_keys(cache_snapshot, keys):
    if not keys:
        return None
    largest_key = None
    largest_size = -1
    for key in keys:
        robj = cache_snapshot.cache.get(key, None)
        if not robj:
            continue
        sz = int(robj.size)
        if sz > largest_size:
            largest_size = sz
            largest_key = key
    return largest_key

def _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now):
    victim_key = None
    victim_score = None
    victim_ts = None
    victim_size = None
    for key in candidates:
        robj = cache_snapshot.cache.get(key, None)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
        ts = m_last_access.get(key, 0)
        sz = int(robj.size)
        # Evict the lowest score; tie-breaker: older, then larger
        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (ts < victim_ts or (ts == victim_ts and sz > (victim_size or 0))))):
            victim_key = key
            victim_score = score
            victim_ts = ts
            victim_size = sz
    return victim_key, victim_score

def _demote_one_from_protected(cache_snapshot):
    # Demote a weak/old protected item back to window to respect budget or to ensure window has victims
    candidates = _sample_keys_from_segment(1, _S_PROT_DEMOTE)
    if not candidates:
        return
    now = cache_snapshot.access_count
    key, _ = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size=_avg_size, now=now)
    if key is None:
        return
    robj = cache_snapshot.cache.get(key, None)
    if not robj:
        return
    size = int(robj.size)
    if m_segment.get(key, 0) == 1:
        m_segment[key] = 0
        globals()['_g_protected_bytes'] -= size
        globals()['_g_window_bytes'] += size

def _remove_key_from_index(key):
    idx = m_key_index.pop(key, None)
    if idx is None:
        return
    last_idx = len(m_keys) - 1
    if last_idx < 0:
        return
    last_key = m_keys[last_idx]
    if key != last_key:
        m_keys[idx] = last_key
        m_key_index[last_key] = idx
    m_keys.pop()

def _current_protected_budget(cache_snapshot):
    return max(0, cache_snapshot.capacity - _current_window_target_bytes(cache_snapshot))

def _ensure_window_has_candidate(cache_snapshot):
    # If window empty but protected non-empty, demote one
    if _g_window_bytes <= 0 and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _refill_window_if_thin(cache_snapshot):
    # If window is too thin relative to target, demote one from protected (best-effort)
    target = _current_window_target_bytes(cache_snapshot)
    if _g_window_bytes < (target // 4) and _g_protected_bytes > 0:
        _demote_one_from_protected(cache_snapshot)

def _ghost_add(key, seg):
    # Bounded ghost recording (ARC-style feedback)
    if key in _ghost:
        return
    _ghost[key] = seg
    _ghost_q.append(key)
    if len(_ghost_q) > _GHOST_CAP:
        old = _ghost_q.popleft()
        _ghost.pop(old, None)

def _ghost_consume_on_insert(key):
    # Adaptive window adjust with momentum and streak scaling
    global _f_window, _f_momentum, _last_ghost_dir, _ghost_streak
    seg = _ghost.pop(key, None)
    if seg is None:
        return
    # Direction: +1 if it was evicted from window (we should increase window), -1 otherwise
    dirn = 1 if seg == 0 else -1
    if dirn == _last_ghost_dir:
        _ghost_streak += 1
    else:
        _ghost_streak = 1
        _last_ghost_dir = dirn

    step = min(_WINDOW_STEP_MAX, _WINDOW_STEP_BASE * (1.0 + 0.25 * min(8, _ghost_streak)))
    _f_momentum += dirn * step
    # Apply momentum immediately (clamped)
    _f_window = max(_WINDOW_MIN, min(_WINDOW_MAX, _f_window + _f_momentum * 0.5))
    # Momentum natural damping will occur during sketch decay


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Strategy:
      - Maintain protected budget by demoting at most one item if over budget.
      - Ensure window has at least one victim by demoting from protected if needed.
      - Prefer victims from window using size-aware TinyLFU + reuse-distance scoring (sampled).
      - Admission-guard: avoid evicting a victim that is clearly stronger than incoming; fall back to older window entries.
      - If window is too thin, opportunistically refill by a demotion to avoid protected evictions.
    '''
    _bootstrap_if_needed(cache_snapshot)
    now = cache_snapshot.access_count
    incoming_size = max(1, int(obj.size))

    # Keep protected within budget (single demotion per call)
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

    # Ensure window has candidates
    _ensure_window_has_candidate(cache_snapshot)
    _refill_window_if_thin(cache_snapshot)

    # Prefer candidates from window
    candidates = _sample_keys_from_segment(0, _S_WIN_EVICT)

    if not candidates:
        # If still nothing, fall back to global oldest
        alt_global = _select_oldest_from_keys(_sample_keys_from_segment(None, max(1, _S_WIN_EVICT)))
        if alt_global is not None:
            return alt_global
        return m_keys[0] if m_keys else None

    victim_key, victim_score = _select_weakest_from_candidates(cache_snapshot, candidates, incoming_size, now)

    if victim_key is None:
        # Fallback: oldest among candidates
        alt_oldest = _select_oldest_from_keys(candidates)
        if alt_oldest is not None:
            return alt_oldest
        return m_keys[0] if m_keys else None

    # Admission-guard: if victim is stronger than incoming, try alternate victim within candidates
    incoming_score = _incoming_priority_estimate(cache_snapshot, obj, now)
    if victim_score is not None and incoming_score is not None and victim_score >= _ADMIT_COMPARE * incoming_score:
        # Search within candidates for a weaker-than-incoming victim
        weaker_victim = None
        weaker_score = None
        for key in candidates:
            robj = cache_snapshot.cache.get(key, None)
            if not robj:
                continue
            s = _priority_for_key(cache_snapshot, key, robj, now, incoming_size)
            if s < incoming_score / _ADMIT_COMPARE:
                if weaker_victim is None or s < weaker_score:
                    weaker_victim = key
                    weaker_score = s
        if weaker_victim is not None:
            victim_key = weaker_victim
        else:
            # Fall back to an old window entry; if tie, evict the largest to free bytes
            oldest = _select_oldest_from_keys(candidates)
            if oldest is not None:
                victim_key = oldest
            else:
                largest = _select_largest_from_keys(cache_snapshot, candidates)
                if largest is not None:
                    victim_key = largest

    return victim_key if victim_key is not None else (m_keys[0] if m_keys else None)

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA; record reuse distance.
      - Refresh last access and hits-since-insert.
      - Size-aware promotion: window -> protected only after threshold hits (larger objects need more).
      - Keep protected within budget via a single demotion if needed.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # Safety: ensure indexed and metadata presence
    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_insert_ts:
        m_insert_ts[key] = now
    if key not in m_hits:
        m_hits[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_prev_access:
        m_prev_access[key] = now
    if key not in m_last_gap:
        m_last_gap[key] = 1 << 20

    # Update reuse gap using previous access timestamp
    prev = m_last_access.get(key, now)
    m_prev_access[key] = prev
    gap = max(1, now - prev)
    m_last_gap[key] = gap
    m_last_access[key] = now

    # Increase hit count since insert
    m_hits[key] = m_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)

    # Size-aware SLRU promotion
    if seg == 0:
        need = _promotion_threshold_for_size(size)
        if m_hits.get(key, 0) >= need:
            m_segment[key] = 1
            globals()['_g_window_bytes'] -= size
            globals()['_g_protected_bytes'] += size

    # Keep protected within its budget by demoting one if necessary
    protected_budget = _current_protected_budget(cache_snapshot)
    if _g_protected_bytes > protected_budget:
        _demote_one_from_protected(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA; initialize metadata including reuse tracking.
      - Initialize in window with zero hits.
      - Update bytes accounting.
      - Consume ghost feedback (ARC-style) to adapt window fraction with momentum.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    now = cache_snapshot.access_count
    size = int(obj.size)

    # ARC-style ghost feedback with momentum
    _ghost_consume_on_insert(key)

    m_segment[key] = 0
    m_prev_access[key] = now
    m_last_access[key] = now
    m_last_gap[key] = 1 << 20  # effectively "unknown/large"
    m_insert_ts[key] = now
    m_hits[key] = 0

    if key not in m_key_index:
        m_key_index[key] = len(m_keys)
        m_keys.append(key)

    globals()['_g_window_bytes'] += size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record a ghost entry with the segment to steer adaptive window sizing.
      - Remove metadata and adjust bytes accounting.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)

    seg = m_segment.pop(ev_key, 0)
    m_last_access.pop(ev_key, None)
    m_prev_access.pop(ev_key, None)
    m_last_gap.pop(ev_key, None)
    m_insert_ts.pop(ev_key, None)
    m_hits.pop(ev_key, None)
    _remove_key_from_index(ev_key)

    # Update bytes accounting
    if seg == 0:
        globals()['_g_window_bytes'] -= ev_size
        if _g_window_bytes < 0:
            globals()['_g_window_bytes'] = 0
    else:
        globals()['_g_protected_bytes'] -= ev_size
        if _g_protected_bytes < 0:
            globals()['_g_protected_bytes'] = 0

    # Record in ghost set for adaptive control
    _ghost_add(ev_key, seg)
```
2025-11-28 02:33:27,061 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.0271

Example 2:
- Trace ID: 22
- Hit Rate: 0.0587

Example 3:
- Trace ID: 20
- Hit Rate: 0.3648

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:33:27 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:33:27,065 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:36:47 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:36:47,287 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:36:47,288 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: W-TinyLFU + ARC-style adaptive window + size-aware, recency-aware, and
# inter-arrival-time-aware priority. Adds protected demotion on decay and ghost-informed
# reinsertion. Uses sampled eviction across both segments (window/protected) with
# soft window enforcement. Keeps O(1) updates and bounded overhead.

from collections import OrderedDict
import math

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)
m_ia_ewma = dict()        # key -> float (EWMA of inter-arrival time)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 32768           # increased to reduce collisions
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 5000            # faster aging for non-stationary traces
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.90
_WINDOW_FRAC_STEP = 0.03
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.75

# Recency multipliers per segment (stronger recency in window, weak in protected)
_REC_WIN_LAMBDA = 6.0
_REC_PROT_LAMBDA = 0.5

# Extra protection multiplier for protected segment (soft; allows stale eviction)
_PROTECTED_KEEP_BONUS = 1.15

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2
_MAX_RES_HITS = 255

# Blend resident hit counter with global TinyLFU estimate (log-scaled)
_FREQ_GLOBAL_WEIGHT = 0.70
_FREQ_RESIDENT_WEIGHT = 0.30

# Soonness (predicted time-to-next) weight in score
_SOONNESS_WEIGHT = 1.0
_EWMA_ALPHA = 0.30
_STALE_DEMOTE_FACTOR = 2.0   # demote protected if idle > factor * ewma
_DEMOTE_BUDGET = 64          # max protected demotions per decay period

# Eviction sampling
_SAMPLE_WIN = 8        # candidates sampled from window LRU tail
_SAMPLE_PROT = 6       # candidates sampled from protected LRU tail

# Soft window enforcement margin (bytes): if window exceeds target+margin -> prefer window
_WINDOW_SOFT_MARGIN_FRAC = 0.10

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        # per ghost budget ≈ cache size
        _GHOST_BUDGET_BYTES = cap

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _demote_protected_stale_if_needed(cache_snapshot):
    # On sketch decay, demote up to budget oldest protected entries that look stale
    if not _just_decayed:
        return
    if not _prot_q:
        return
    now = cache_snapshot.access_count
    moved = 0
    # Scan from oldest protected forward
    for k in list(_prot_q.keys()):
        if moved >= _DEMOTE_BUDGET:
            break
        la = m_last_access.get(k, 0)
        ew = m_ia_ewma.get(k)
        # No EWMA yet -> skip
        if ew is None or ew <= 0:
            continue
        # If the item stayed idle considerably longer than its expected inter-arrival -> demote
        if (now - la) > (_STALE_DEMOTE_FACTOR * ew + 1):
            # Move from protected -> window MRU
            _prot_q.pop(k, None)
            _win_q[k] = None
            _win_q.move_to_end(k, last=True)
            m_segment[k] = 0
            # Update bytes
            sz = int(cache_snapshot.cache[k].size) if k in cache_snapshot.cache else 0
            global _prot_bytes, _win_bytes
            _prot_bytes = max(0, _prot_bytes - sz)
            _win_bytes += sz
            # Require confirmation to re-promote
            m_promote_need[k] = max(m_promote_need.get(k, _PROMOTE_HITS_SMALL), 2)
            # Softly reduce resident hits to avoid immediate re-protect
            if k in m_res_hits:
                m_res_hits[k] = max(0, m_res_hits[k] >> 1)
            moved += 1

def _resident_decay_if_needed(cache_snapshot):
    # Triggered right after sketch decay; also demote stale protected
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _demote_protected_stale_if_needed(cache_snapshot)
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need
        if key not in m_ia_ewma:
            m_ia_ewma[key] = None

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    if key not in m_ia_ewma:
        m_ia_ewma[key] = None
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _log1p(x):
    try:
        return math.log1p(float(x))
    except Exception:
        return 0.0

def _predict_soonness(cache_snapshot, key, now):
    # Predict next arrival delay and convert to "soonness" in (0,1]
    last_ts = m_last_access.get(key, 0)
    ew = m_ia_ewma.get(key)
    if ew is None or ew <= 0:
        # unknown -> treat as not-so-soon
        return 0.0
    # Expected delay since now is roughly ew - elapsed_since_last if positive
    elapsed = max(0, now - last_ts)
    expected_delay = max(1.0, ew - elapsed) if ew > elapsed else 1.0
    soonness = 1.0 / (1.0 + expected_delay)
    return float(soonness)

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score ~ ((w1*log1p(TinyLFU) + w2*log1p(resident_hits))*(1+lambda_seg*recency) + wS*soonness) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)

    # Recency in (0,1]
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    # Frequency (log-scaled to balance with soonness)
    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * _log1p(freq_global)) + (_FREQ_RESIDENT_WEIGHT * _log1p(freq_res))

    # Soonness from inter-arrival EWMA
    soon = _predict_soonness(cache_snapshot, key, now)

    # Base score
    base = (freq * rec_mult + (_SOONNESS_WEIGHT * soon)) / (float(size) ** _SIZE_ALPHA)

    # Bonus for protected
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS

    return base

def _update_ia_on_hit(key, now):
    # Update EWMA inter-arrival on hit
    last = m_last_access.get(key)
    if last is None:
        m_ia_ewma[key] = None
        return
    delta = max(1.0, float(now - last))
    ew = m_ia_ewma.get(key)
    if ew is None:
        m_ia_ewma[key] = float(delta)
    else:
        m_ia_ewma[key] = (1.0 - _EWMA_ALPHA) * ew + _EWMA_ALPHA * delta

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Victim selection:
      - Always sample from both segments (window/protected) to avoid pinning stale protected entries.
      - Soft window enforcement: if window usage >> target, window items are more likely to be chosen
        (naturally due to lower protected bonus), but protected candidates are still considered.
      - Score combines global frequency (TinyLFU), resident hits, inter-arrival soonness, recency,
        and size penalty; protected has keep bonus.
      - Evict minimum score; tie-break by oldest last access, then by largest size.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    window_bytes = _win_bytes
    soft_margin = int(_WINDOW_SOFT_MARGIN_FRAC * max(1, cache_snapshot.capacity))
    window_overshoot = window_bytes > (target_window + soft_margin)

    # Build candidate list from both segments
    candidates = []
    win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
    prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)

    # If window grossly overshoots target, prioritize window keys by sampling more of them
    if window_overshoot and len(_win_q) > 0:
        # Bias: double the window keys if possible
        more_win = _sample_oldest_keys(_win_q, min(len(_win_q), _SAMPLE_WIN * 2))
        win_keys = more_win

    for k in win_keys:
        if k in cache_snapshot.cache:
            candidates.append(k)
    for k in prot_keys:
        if k in cache_snapshot.cache:
            candidates.append(k)

    # Fallback: if candidates empty, take any key
    if not candidates and cache_snapshot.cache:
        for k in cache_snapshot.cache.keys():
            candidates.append(k)
            break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # Ultimate fallback
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Decay resident metadata after sketch decay and demote stale protected if needed.
      - Update resident metadata: last_access, res_hits (capped), inter-arrival EWMA.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed(cache_snapshot)

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    # Inter-arrival update uses last access before updating it
    _update_ia_on_hit(key, now)

    # Update last access and resident hits
    m_last_access[key] = now
    m_res_hits[key] = min(_MAX_RES_HITS, m_res_hits.get(key, 0) + 1)

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes = max(0, _win_bytes - size)
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay and protected demotion).
      - Adaptive window target using ghost hits (ARC-style) and ghost-informed reinsertion:
          * If in protected-ghost -> insert directly into protected (re-protect).
          * If in window-ghost -> insert into window with a small boost.
      - Initialize metadata; insert into appropriate segment MRU; update bytes.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed(cache_snapshot)
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Capture ghost membership before adapt (it will pop)
    was_in_gw = key in _gw
    was_in_gp = key in _gp

    # Adaptive window size using ghost lists (and pop the ghost entry)
    _adapt_window_on_insert(cache_snapshot, key)

    # Initialize resident metadata
    cap = max(1, int(cache_snapshot.capacity))
    big_obj = int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap)

    if was_in_gp:
        # Re-protect on re-entry (ARC behavior)
        m_segment[key] = 1
        m_promote_need[key] = _PROMOTE_HITS_LARGE if big_obj else _PROMOTE_HITS_SMALL
        m_res_hits[key] = 1
        m_ia_ewma[key] = None
        m_last_access[key] = now
        # Place into protected MRU
        if key in _win_q:
            _win_q.pop(key, None)
        _prot_q[key] = None
        _prot_q.move_to_end(key, last=True)
        global _prot_bytes
        _prot_bytes += int(obj.size)
    else:
        # Default or window-ghost -> insert into window
        m_segment[key] = 0
        m_promote_need[key] = _PROMOTE_HITS_LARGE if big_obj else _PROMOTE_HITS_SMALL
        # If it was in window ghost, give a small boost to speed promotion
        m_res_hits[key] = 1 if was_in_gw else 0
        m_ia_ewma[key] = None
        m_last_access[key] = now
        # Place into window MRU
        if key in _prot_q:
            _prot_q.pop(key, None)
        _win_q[key] = None
        _win_q.move_to_end(key, last=True)
        global _win_bytes
        _win_bytes += int(obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
    m_ia_ewma.pop(ev_key, None)
```
2025-11-28 02:39:58,077 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Fast ASL-TinyLFU with O(1) ghost management and sampled eviction
# - Two in-cache segments using OrderedDict LRU queues:
#     0: window/probation (recency-biased)
#     1: protected (frequency-biased)
# - Global admission via TinyLFU Count-Min Sketch with periodic aging.
# - ARC-style adaptive window (bytes) using two ghost queues (OrderedDict) with O(1) pruning.
# - Eviction:
#     * If window bytes exceed target -> evict from window only (sample oldest few).
#     * Else -> sample oldest few from both segments; evict lowest composite priority.
# - Size-aware and recency-aware priority; protected has keep bonus.
# - Metadata maintained on hit/insert/evict with O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global metadata (resident)
# ------------------------
m_segment = dict()        # key -> 0 (window) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> int (hits required to promote from window)

# LRU queues per segment (oldest -> newest)
_win_q = OrderedDict()    # key -> None
_prot_q = OrderedDict()   # key -> None
_win_bytes = 0
_prot_bytes = 0
_od_initialized = False   # lazily initialize from snapshot if needed

# ------------------------
# Ghost caches for adaptive window sizing (ARC-style) with O(1) prune
# ------------------------
_gw = OrderedDict()       # window-ghost: key -> size (oldest -> newest)
_gp = OrderedDict()       # protected-ghost: key -> size
_gw_bytes = 0
_gp_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget (≈ cache capacity each)

# ------------------------
# TinyLFU Count-Min Sketch (global, across all requests)
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192            # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 10000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive window fraction (bytes): dynamically adjusted in [min, max]
_WINDOW_FRAC_MIN = 0.05
_WINDOW_FRAC_MAX = 0.80
_WINDOW_FRAC_STEP = 0.02
_window_frac = 0.25

# Size-aware priority divisor exponent
_SIZE_ALPHA = 0.9

# Recency multipliers per segment (stronger recency in window)
_REC_WIN_LAMBDA = 4.0
_REC_PROT_LAMBDA = 1.0

# Extra protection multiplier for protected segment
_PROTECTED_KEEP_BONUS = 1.35

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Eviction sampling
_SAMPLE_WIN = 6        # candidates sampled from window LRU tail
_SAMPLE_PROT = 4       # candidates sampled from protected LRU tail

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        # Halve resident hit counters on sketch decay
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Lazy initialization of LRU queues if cache is pre-populated
    global _od_initialized, _win_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        # Default unseen items to window
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now  # assume recent
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need

        if seg == 0:
            if key not in _win_q:
                _win_q[key] = None
                _win_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    # Initialize resident metadata if missing (e.g., newly inserted or first-time hit)
    if key not in m_segment:
        m_segment[key] = 0  # default to window
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    # Ensure presence in an LRU queue
    if key not in _win_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _win_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    # Add key to corresponding ghost queue's MRU side
    global _gw_bytes, _gp_bytes
    size = int(size)
    if segment == 0:
        if key in _gw:
            # refresh recency
            _gw.move_to_end(key, last=True)
        else:
            _gw[key] = size
            _gw_bytes += size
    else:
        if key in _gp:
            _gp.move_to_end(key, last=True)
        else:
            _gp[key] = size
            _gp_bytes += size

def _ghost_prune():
    # Keep each ghost <= budget bytes; prune by oldest entries in O(1)
    global _gw_bytes, _gp_bytes
    while _gw_bytes > _GHOST_BUDGET_BYTES and _gw:
        k, sz = _gw.popitem(last=False)
        _gw_bytes -= int(sz)
    while _gp_bytes > _GHOST_BUDGET_BYTES and _gp:
        k, sz = _gp.popitem(last=False)
        _gp_bytes -= int(sz)

def _adjust_window_fraction(delta):
    global _window_frac
    _window_frac = max(_WINDOW_FRAC_MIN, min(_WINDOW_FRAC_MAX, _window_frac + delta))

def _adapt_window_on_insert(cache_snapshot, key):
    # ARC-style adaptive control using ghost hits (O(1))
    global _gw_bytes, _gp_bytes
    if key in _gw:
        # Increase window target
        num = float(_gp_bytes)
        den = float(_gw_bytes if _gw_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(_WINDOW_FRAC_STEP * factor)
        # remove from ghost (now resident)
        sz = _gw.pop(key)
        _gw_bytes -= int(sz)
    elif key in _gp:
        # Decrease window target
        num = float(_gw_bytes)
        den = float(_gp_bytes if _gp_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_window_fraction(-_WINDOW_FRAC_STEP * factor)
        sz = _gp.pop(key)
        _gp_bytes -= int(sz)
    _ghost_prune()

def _priority_for_key(cache_snapshot, key, obj, now):
    # Composite priority: higher is better to keep
    # score = ((w1*TinyLFU + w2*resident_hits) * (1 + lambda_seg * recency)) / size^alpha
    # protected items get a multiplicative bonus
    size = max(1, int(obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    if seg == 0:
        rec_mult = 1.0 + (_REC_WIN_LAMBDA * recency)
    else:
        rec_mult = 1.0 + (_REC_PROT_LAMBDA * recency)

    freq_global = _sketch_estimate(key)
    freq_res = float(m_res_hits.get(key, 0))
    freq = (_FREQ_GLOBAL_WEIGHT * freq_global) + (_FREQ_RESIDENT_WEIGHT * freq_res)

    base = (freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    # Return up to k oldest keys from an OrderedDict without modifying it
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Choose a victim:
      - Maintain an adaptive window byte target (ARC-style). If window usage > target,
        evict the minimum-priority key among a small sample of oldest window entries.
      - Otherwise, sample oldest from both segments and evict global minimum priority
        (protected has keep bonus).
      - Tie-break by oldest last access, then by largest size.
    This design avoids full-cache scans and ghost O(n^2) pruning, preventing timeouts.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    # Ensure new incoming object's sketch gets considered in future (admission heuristic)
    # but do not increment sketch here; that's done on insert/hit hooks.

    # Compute window usage and target
    window_bytes = _win_bytes
    has_window = len(_win_q) > 0
    target_window = max(1, int(_window_frac * max(1, cache_snapshot.capacity)))
    restrict_to_window = has_window and (window_bytes > target_window)

    # Build candidate list
    candidates = []
    if restrict_to_window:
        sample_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        for k in sample_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        # Sample from both segments' oldest entries (favor more from window)
        win_keys = _sample_oldest_keys(_win_q, _SAMPLE_WIN)
        prot_keys = _sample_oldest_keys(_prot_q, _SAMPLE_PROT)
        for k in win_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in prot_keys:
            if k in cache_snapshot.cache:
                candidates.append(k)

        # If both segments are tiny (e.g., just started), fall back to any one key
        if not candidates and cache_snapshot.cache:
            # Pick any key deterministically
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for key in candidates:
        robj = cache_snapshot.cache.get(key)
        if robj is None:
            continue
        score = _priority_for_key(cache_snapshot, key, robj, now)
        last_ts = m_last_access.get(key, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = key
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    # As ultimate fallback, if something went wrong, evict any LRU from window then protected
    if victim_key is None:
        if _win_q:
            victim_key = next(iter(_win_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            # last resort: any key
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU (with decay).
      - Update resident metadata: last_access, res_hits.
      - Move key to MRU of its segment LRU queue.
      - Promote window -> protected once it reaches its promotion threshold (size-aware).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Move to MRU of window
        if key in _win_q:
            _win_q.move_to_end(key, last=True)
        else:
            _win_q[key] = None
        # Size-aware promotion
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote to protected
            if key in _win_q:
                _win_q.pop(key, None)
            m_segment[key] = 1
            _prot_q[key] = None
            _prot_q.move_to_end(key, last=True)
            # Update bytes counters
            size = int(cache_snapshot.cache[key].size) if key in cache_snapshot.cache else int(getattr(obj, 'size', 1))
            global _win_bytes, _prot_bytes
            _win_bytes -= size
            _prot_bytes += size
    else:
        # Protected hit: recency update
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Update TinyLFU (with decay and resident counter decay).
      - Initialize metadata: segment=window, last_access=now, res_hits=0, promote_need based on size.
      - Insert into window MRU; update bytes counters.
      - Adapt window target using ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    # Initialize resident metadata
    m_segment[key] = 0  # start in window/probation
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert into window MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _win_q:
        _win_q.move_to_end(key, last=True)
    else:
        _win_q[key] = None
    global _win_bytes
    _win_bytes += int(obj.size)

    # Adaptive window size using ghost lists
    _adapt_window_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Remove resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Record in ghost for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRU queues and adjust bytes
    global _win_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _win_q:
            _win_q.pop(ev_key, None)
        _win_bytes = max(0, _win_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.0433

Example 2:
- Trace ID: 35
- Hit Rate: 0.0741

Example 3:
- Trace ID: 14
- Hit Rate: 0.0207

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:39:58 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:39:58,085 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:42:29 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:42:29,322 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:42:29,324 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Improved: Admission-aware SLRU with TinyLFU and ARC-style adaptive probation size
# Key changes:
# - Eviction is admission-aware: compares incoming object's TinyLFU estimate vs candidates'.
# - Simpler, robust SLRU: two resident segments (probation/protected) using pure LRUs.
# - Immediate promotion of small objects on first hit; large objects require 2 hits.
# - ARC-style adaptation via compact ghost caches to tune probation fraction.
# - Size-aware priority and mild recency; protected gets a keep bonus.
# - O(1) LRU operations; small sampling set for victim choice (no scans).
#
# This design reduces churn, protects frequent objects, and biases eviction against
# objects that are less frequent than the incoming request (admission-aware choice).

from collections import OrderedDict

# ------------------------
# Global resident metadata
# ------------------------
m_segment = dict()        # key -> 0 (probation) or 1 (protected)
m_last_access = dict()    # key -> int (timestamp)
m_res_hits = dict()       # key -> int (resident hit counter, decayed)
m_promote_need = dict()   # key -> hits required to promote from probation

# LRU queues per segment (oldest -> newest)
_prob_q = OrderedDict()   # probation LRU
_prot_q = OrderedDict()   # protected LRU
_prob_bytes = 0
_prot_bytes = 0
_od_initialized = False

# ------------------------
# Ghost caches for ARC-style adaptation (O(1) prune)
# ------------------------
_g_prob = OrderedDict()   # probation-ghost: key -> size (oldest -> newest)
_g_prot = OrderedDict()   # protected-ghost: key -> size
_g_prob_bytes = 0
_g_prot_bytes = 0
_last_capacity_seen = 0
_GHOST_BUDGET_BYTES = 0   # per-ghost budget ≈ cache capacity

# ------------------------
# TinyLFU Count-Min Sketch
# ------------------------
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 8192                  # power of two
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# ------------------------
# Decay configuration
# ------------------------
_DECAY_PERIOD = 5000
_decay_ticker = 0
_just_decayed = False

# ------------------------
# Tunables
# ------------------------
# Adaptive probation fraction (bytes): dynamically adjusted in [min, max]
_PROB_FRAC_MIN = 0.05
_PROB_FRAC_MAX = 0.80
_PROB_FRAC_STEP = 0.02
_prob_frac = 0.25  # start with 25% probation, 75% protected

# Size-aware priority exponent
_SIZE_ALPHA = 0.85

# Recency multipliers per segment (stronger recency in probation)
_REC_PROB_LAMBDA = 3.0
_REC_PROT_LAMBDA = 0.75

# Protected segment keep bonus
_PROTECTED_KEEP_BONUS = 1.25

# Promotion policy: large objects need more hits to promote
_LARGE_PROMOTE_FRACTION = 0.05  # >5% of capacity => 2 hits to promote
_PROMOTE_HITS_SMALL = 1
_PROMOTE_HITS_LARGE = 2

# Blend resident hit counter with global TinyLFU estimate
_FREQ_GLOBAL_WEIGHT = 0.75
_FREQ_RESIDENT_WEIGHT = 0.25

# Admission-aware eviction bias (incoming vs existing frequency)
_ADMIT_BIAS_PROB = 0.90
_ADMIT_BIAS_PROT = 0.50

# Eviction sampling (oldest K in each segment)
_SAMPLE_PROB = 8
_SAMPLE_PROT = 6

# Numerical epsilon
_EPS = 1e-9

# ------------------------
# Internal helpers
# ------------------------
def _ensure_capacity_dependent_globals(cache_snapshot):
    global _last_capacity_seen, _GHOST_BUDGET_BYTES
    cap = max(1, int(cache_snapshot.capacity))
    if cap != _last_capacity_seen:
        _last_capacity_seen = cap
        _GHOST_BUDGET_BYTES = cap  # per-ghost budget ≈ cache size

def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    return float(est if est != float('inf') else 0.0)

def _sketch_decay_if_needed():
    global _decay_ticker, _just_decayed
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0
        _just_decayed = True

def _resident_decay_if_needed():
    global _just_decayed
    if _just_decayed:
        for k in list(m_res_hits.keys()):
            m_res_hits[k] >>= 1
        _just_decayed = False

def _init_od_from_snapshot_if_needed(cache_snapshot):
    # Initialize LRUs if cache pre-populated
    global _od_initialized, _prob_bytes, _prot_bytes
    if _od_initialized:
        return
    if not cache_snapshot.cache:
        _od_initialized = True
        return
    now = cache_snapshot.access_count
    for key, robj in cache_snapshot.cache.items():
        if key not in m_segment:
            m_segment[key] = 0
        seg = m_segment[key]
        if key not in m_last_access:
            m_last_access[key] = now
        if key not in m_res_hits:
            m_res_hits[key] = 0
        if key not in m_promote_need:
            cap = max(1, int(cache_snapshot.capacity))
            need = _PROMOTE_HITS_LARGE if int(robj.size) >= int(_LARGE_PROMOTE_FRACTION * cap) else _PROMOTE_HITS_SMALL
            m_promote_need[key] = need
        if seg == 0:
            if key not in _prob_q:
                _prob_q[key] = None
                _prob_bytes += int(robj.size)
        else:
            if key not in _prot_q:
                _prot_q[key] = None
                _prot_bytes += int(robj.size)
    _od_initialized = True

def _ensure_resident_metadata(cache_snapshot, key, obj=None):
    if key not in m_segment:
        m_segment[key] = 0
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count
    if key not in m_res_hits:
        m_res_hits[key] = 0
    if key not in m_promote_need:
        need = _PROMOTE_HITS_SMALL
        if obj is not None:
            cap = max(1, int(cache_snapshot.capacity))
            if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
                need = _PROMOTE_HITS_LARGE
        m_promote_need[key] = need
    if key not in _prob_q and key not in _prot_q:
        if m_segment.get(key, 0) == 0:
            _prob_q[key] = None
        else:
            _prot_q[key] = None

def _ghost_add(segment, key, size):
    global _g_prob_bytes, _g_prot_bytes
    size = int(size)
    if segment == 0:
        if key in _g_prob:
            _g_prob.move_to_end(key, last=True)
        else:
            _g_prob[key] = size
            _g_prob_bytes += size
    else:
        if key in _g_prot:
            _g_prot.move_to_end(key, last=True)
        else:
            _g_prot[key] = size
            _g_prot_bytes += size

def _ghost_prune():
    global _g_prob_bytes, _g_prot_bytes
    while _g_prob_bytes > _GHOST_BUDGET_BYTES and _g_prob:
        k, sz = _g_prob.popitem(last=False)
        _g_prob_bytes -= int(sz)
    while _g_prot_bytes > _GHOST_BUDGET_BYTES and _g_prot:
        k, sz = _g_prot.popitem(last=False)
        _g_prot_bytes -= int(sz)

def _adjust_prob_fraction(delta):
    global _prob_frac
    _prob_frac = max(_PROB_FRAC_MIN, min(_PROB_FRAC_MAX, _prob_frac + delta))

def _adapt_on_insert(cache_snapshot, key):
    # ARC-style: adjust probation fraction using ghost hits
    global _g_prob_bytes, _g_prot_bytes
    if key in _g_prob:
        # probation ghost hit -> increase probation target
        num = float(_g_prot_bytes)
        den = float(_g_prob_bytes if _g_prob_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_prob_fraction(_PROB_FRAC_STEP * factor)
        sz = _g_prob.pop(key)
        _g_prob_bytes -= int(sz)
    elif key in _g_prot:
        # protected ghost hit -> decrease probation target
        num = float(_g_prob_bytes)
        den = float(_g_prot_bytes if _g_prot_bytes > 0 else 1.0)
        factor = max(1.0, num / den)
        _adjust_prob_fraction(-_PROB_FRAC_STEP * factor)
        sz = _g_prot.pop(key)
        _g_prot_bytes -= int(sz)
    _ghost_prune()

def _blend_freq_for_key(key):
    return (_FREQ_GLOBAL_WEIGHT * _sketch_estimate(key)) + (_FREQ_RESIDENT_WEIGHT * float(m_res_hits.get(key, 0)))

def _evict_keep_score(cache_snapshot, key, existing_obj, now, incoming_est):
    # Keep score (higher is better to keep). We evict the minimal score.
    size = max(1, int(existing_obj.size))
    seg = m_segment.get(key, 0)
    last_ts = m_last_access.get(key, 0)
    age = max(0, now - last_ts)
    recency = 1.0 / (1.0 + float(age))
    rec_mult = 1.0 + ((_REC_PROB_LAMBDA if seg == 0 else _REC_PROT_LAMBDA) * recency)

    freq = _blend_freq_for_key(key)
    # Admission-aware bias: subtract a fraction of incoming estimate
    admit_bias = _ADMIT_BIAS_PROB if seg == 0 else _ADMIT_BIAS_PROT
    adj_freq = max(0.0, freq - admit_bias * float(incoming_est))

    base = (adj_freq * rec_mult) / (float(size) ** _SIZE_ALPHA)
    if seg == 1:
        base *= _PROTECTED_KEEP_BONUS
    return base

def _sample_oldest_keys(od, k):
    res = []
    for i, key in enumerate(od.keys()):
        if i >= k:
            break
        res.append(key)
    return res

def _promote_to_protected(cache_snapshot, key, size_hint=None):
    # Move key from probation to protected MRU and update bytes
    global _prob_bytes, _prot_bytes
    if key in _prob_q:
        _prob_q.pop(key, None)
    m_segment[key] = 1
    _prot_q[key] = None
    _prot_q.move_to_end(key, last=True)
    sz = size_hint
    if sz is None:
        if key in cache_snapshot.cache:
            sz = int(cache_snapshot.cache[key].size)
        else:
            sz = 1
    _prob_bytes = max(0, _prob_bytes - int(sz))
    _prot_bytes += int(sz)

# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    Admission-aware eviction:
      - Compute TinyLFU estimate of incoming object.
      - If probation bytes exceed target, sample oldest probation entries and evict the
        one with the lowest adjusted keep score.
      - Otherwise, sample from both segments' oldest and evict the global minimum keep score.
      - Tie-break by older last access, then larger size.
    '''
    now = cache_snapshot.access_count
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    incoming_est = _sketch_estimate(obj.key)

    # Targets by bytes
    cap_bytes = max(1, int(cache_snapshot.capacity))
    target_prob = max(1, int(_prob_frac * cap_bytes))
    restrict_to_prob = len(_prob_q) > 0 and (_prob_bytes > target_prob)

    candidates = []
    if restrict_to_prob:
        for k in _sample_oldest_keys(_prob_q, _SAMPLE_PROB):
            if k in cache_snapshot.cache:
                candidates.append(k)
    else:
        for k in _sample_oldest_keys(_prob_q, _SAMPLE_PROB):
            if k in cache_snapshot.cache:
                candidates.append(k)
        for k in _sample_oldest_keys(_prot_q, _SAMPLE_PROT):
            if k in cache_snapshot.cache:
                candidates.append(k)
        if not candidates and cache_snapshot.cache:
            for k in cache_snapshot.cache.keys():
                candidates.append(k)
                break

    victim_key = None
    victim_score = None
    victim_last_ts = None
    victim_size = None

    for k in candidates:
        robj = cache_snapshot.cache.get(k)
        if robj is None:
            continue
        score = _evict_keep_score(cache_snapshot, k, robj, now, incoming_est)
        last_ts = m_last_access.get(k, 0)
        sz = int(robj.size)

        if (victim_key is None or
            score < victim_score - _EPS or
            (abs(score - victim_score) <= _EPS and (last_ts < victim_last_ts or
                                                    (last_ts == victim_last_ts and sz > victim_size)))):
            victim_key = k
            victim_score = score
            victim_last_ts = last_ts
            victim_size = sz

    if victim_key is None:
        # Fallbacks in case of unforeseen states
        if _prob_q:
            victim_key = next(iter(_prob_q.keys()))
        elif _prot_q:
            victim_key = next(iter(_prot_q.keys()))
        else:
            for k in cache_snapshot.cache.keys():
                victim_key = k
                break

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU with decay and resident counter.
      - Refresh last_access and move to MRU of its segment.
      - Promote probation -> protected:
          * small objects: immediate on first hit,
          * large objects: require 2 hits.
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()

    key = obj.key
    _ensure_resident_metadata(cache_snapshot, key, obj)
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_res_hits[key] = m_res_hits.get(key, 0) + 1

    seg = m_segment.get(key, 0)
    if seg == 0:
        # Probation hit: recency
        if key in _prob_q:
            _prob_q.move_to_end(key, last=True)
        else:
            _prob_q[key] = None
        need = m_promote_need.get(key, _PROMOTE_HITS_SMALL)
        if m_res_hits.get(key, 0) >= need:
            # Promote
            _promote_to_protected(cache_snapshot, key, size_hint=int(getattr(obj, 'size', 1)))
    else:
        # Protected hit: just recency
        if key in _prot_q:
            _prot_q.move_to_end(key, last=True)
        else:
            _prot_q[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss was handled by evictions):
      - Update TinyLFU and resident decay.
      - Initialize metadata.
      - Insert into probation MRU; update bytes.
      - Adapt probation target with ghost hits (ARC-style).
    '''
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _resident_decay_if_needed()
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    key = obj.key
    now = cache_snapshot.access_count

    m_segment[key] = 0
    m_last_access[key] = now
    m_res_hits[key] = 0

    cap = max(1, int(cache_snapshot.capacity))
    if int(obj.size) >= int(_LARGE_PROMOTE_FRACTION * cap):
        m_promote_need[key] = _PROMOTE_HITS_LARGE
    else:
        m_promote_need[key] = _PROMOTE_HITS_SMALL

    # Insert probation MRU
    if key in _prot_q:
        _prot_q.pop(key, None)
    if key in _prob_q:
        _prob_q.move_to_end(key, last=True)
    else:
        _prob_q[key] = None
    global _prob_bytes
    _prob_bytes += int(obj.size)

    # Adaptive probation fraction using ghost lists
    _adapt_on_insert(cache_snapshot, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Record evicted key into the corresponding ghost list for adaptive sizing.
      - Remove from LRU queue and adjust bytes counters.
      - Drop resident metadata (sketch persists; decay handles aging).
    '''
    _ensure_capacity_dependent_globals(cache_snapshot)
    _init_od_from_snapshot_if_needed(cache_snapshot)

    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    seg = m_segment.get(ev_key, 0)

    # Ghost record for ARC-style adaptation
    _ghost_add(seg, ev_key, ev_size)
    _ghost_prune()

    # Remove from LRUs and adjust bytes
    global _prob_bytes, _prot_bytes
    if seg == 0:
        if ev_key in _prob_q:
            _prob_q.pop(ev_key, None)
        _prob_bytes = max(0, _prob_bytes - ev_size)
    else:
        if ev_key in _prot_q:
            _prot_q.pop(ev_key, None)
        _prot_bytes = max(0, _prot_bytes - ev_size)

    # Remove resident metadata
    m_segment.pop(ev_key, None)
    m_last_access.pop(ev_key, None)
    m_res_hits.pop(ev_key, None)
    m_promote_need.pop(ev_key, None)
```
2025-11-28 02:42:46,026 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Byte-aware ARC with TinyLFU guidance (OrderedDict-based SLRU) 
# - Two resident segments:
#     T1: recent/probation (LRU)
#     T2: protected (LRU)
# - Two ghost lists (store only keys+sizes):
#     B1: recently evicted from T1
#     B2: recently evicted from T2
# - Adaptive target p (in bytes) for T1 (as in ARC, but byte-aware)
#   p grows when we see B1 hits (need more recency), shrinks on B2 hits (need more protection).
# - Eviction REPLACE: evict from T1 if T1_bytes >= p; else from T2. Within the chosen segment,
#   select the lowest TinyLFU value density (freq/size^alpha) among a small set of oldest keys.
# - Hits: T1 -> T2 promotion; T2 stays in T2 and is moved to MRU.
# - Inserts: default to T1; if key present in a ghost, adjust p and place into the suggested segment.
# - TinyLFU: global count-min sketch with periodic decay, used only for eviction ranking.
# - All accounting in bytes; metadata uses OrderedDict for O(1) LRU updates.

from collections import OrderedDict

# ------------------------
# Global state
# ------------------------
# Resident segments
_T1 = OrderedDict()  # key -> size
_T2 = OrderedDict()  # key -> size

# Ghost lists (no data, only keys + size)
_B1 = OrderedDict()  # key -> size
_B2 = OrderedDict()  # key -> size

# Byte accounting
_t1_bytes = 0
_t2_bytes = 0
_b1_bytes = 0
_b2_bytes = 0

# ARC "p" target for T1 (bytes)
_p_bytes = 0

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 16  # number of oldest keys to consider within chosen segment

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized a bit more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    # Value density = TinyLFU frequency normalized by size^alpha
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _t1_bytes, _t2_bytes, _p_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in T1 (probation) as we don't know past history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    # Initialize p to a moderate recency share
    _p_bytes = int(0.33 * cache_snapshot.capacity)
    _bootstrapped = True

def _clamp_p(capacity):
    global _p_bytes
    if _p_bytes < 0:
        _p_bytes = 0
    if _p_bytes > capacity:
        _p_bytes = capacity

def _trim_ghosts_to_budget(capacity):
    # Keep each ghost list within at most 'capacity' bytes
    global _b1_bytes, _b2_bytes
    while _b1_bytes > capacity and _B1:
        k, sz = _odict_pop_lru(_B1)
        _b1_bytes -= int(sz)
    while _b2_bytes > capacity and _B2:
        k, sz = _odict_pop_lru(_B2)
        _b2_bytes -= int(sz)


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    ARC-style REPLACE with TinyLFU-guided victim within segment:
      - Choose segment to evict from: if T1_bytes >= p (or T2 empty), evict from T1; else from T2.
      - Among the K oldest keys of that segment, evict the one with lowest TinyLFU value density.
      - Fallbacks ensure a key is always returned.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _clamp_p(cache_snapshot.capacity)

    # Decide which segment to evict from
    from_T1 = (_t1_bytes >= max(1, _p_bytes)) or (len(_T2) == 0)
    seg = _T1 if from_T1 else _T2

    # If chosen segment empty, fallback to the other, else to any cached item
    if not seg:
        seg = _T2 if from_T1 else _T1
        if not seg:
            # As a last resort, return any key from the cache snapshot
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

    # Candidates: a small slice of the oldest keys in the chosen segment
    candidates = _segment_oldest_keys(seg, min(_SAMPLE_K, len(seg)))
    victim_key = None
    victim_score = None

    for k in candidates:
        size = seg.get(k, 1)
        score = _score_value_density(k, size)
        if victim_key is None or score < victim_score:
            victim_key = k
            victim_score = score

    if victim_key is None:
        try:
            return next(iter(cache_snapshot.cache.keys()))
        except StopIteration:
            return None
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in T1: promote to T2 (protected).
      - If in T2: move to MRU of T2.
      - If not tracked (shouldn't happen), add to T1 as MRU.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _t1_bytes, _t2_bytes

    if key in _T1:
        # Promote T1 -> T2
        old_size = _T1.pop(key)
        _t1_bytes -= int(old_size)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _T2:
        # Refresh recency
        _odict_move_to_mru(_T2, key)
    else:
        # Not tracked for some reason: add to T1
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update TinyLFU and size EMA.
      - Default place new key into T1 (MRU).
      - If key is in a ghost:
          * If in B1: increase p (favor recency), place in T2.
          * If in B2: decrease p (favor protection), place in T1.
      - Maintain ghost budgets.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    capacity = cache_snapshot.capacity
    global _p_bytes, _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # If already resident due to some sequence anomaly, treat as hit-like refresh
    if key in _T1:
        _odict_move_to_mru(_T1, key)
        return
    if key in _T2:
        _odict_move_to_mru(_T2, key)
        return

    # Ghost-guided adaptation
    if key in _B1:
        # Increase p: need more recency
        delta = max(int(_avg_size), size)
        _p_bytes = min(capacity, _p_bytes + delta)
        # Remove from ghost and place into protected (since it's a repeat)
        old_sz = _B1.pop(key)
        _b1_bytes -= int(old_sz)
        _odict_add_mru(_T2, key, size)
        _t2_bytes += size
    elif key in _B2:
        # Decrease p: need more protection
        delta = max(int(_avg_size), size)
        _p_bytes = max(0, _p_bytes - delta)
        old_sz = _B2.pop(key)
        _b2_bytes -= int(old_sz)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size
    else:
        # Default: insert to T1 (probation)
        _odict_add_mru(_T1, key, size)
        _t1_bytes += size

    _clamp_p(capacity)
    _trim_ghosts_to_budget(capacity)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from resident segment (T1 or T2).
      - Add to appropriate ghost (B1 if from T1, B2 if from T2).
      - Trim ghosts to capacity budgets.
      - TinyLFU is global and persists; only decay ages it.
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    capacity = cache_snapshot.capacity
    global _t1_bytes, _t2_bytes, _b1_bytes, _b2_bytes

    # Remove from resident segments
    if ev_key in _T1:
        # Evicted from T1 -> B1
        old_sz = _T1.pop(ev_key)
        _t1_bytes -= int(old_sz)
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size
    elif ev_key in _T2:
        # Evicted from T2 -> B2
        old_sz = _T2.pop(ev_key)
        _t2_bytes -= int(old_sz)
        _odict_add_mru(_B2, ev_key, ev_size)
        _b2_bytes += ev_size
    else:
        # Not found (metadata drift). Assume it was probationary.
        _odict_add_mru(_B1, ev_key, ev_size)
        _b1_bytes += ev_size

    _trim_ghosts_to_budget(capacity)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Hit Rate: 0.029

Example 2:
- Trace ID: 47
- Hit Rate: 0.0817

Example 3:
- Trace ID: 17
- Hit Rate: 0.0235

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m02:42:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gpt-5; provider = openai
2025-11-28 02:42:46,027 [INFO] LiteLLM: 
LiteLLM completion() model= gpt-5; provider = openai
[92m02:45:17 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-28 02:45:17,186 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-28 02:45:17,188 [INFO] gepa.adapters.caching_adapter_gpt5_0.caching_program_adapter: LM Response: ```python
# Windowed TinyLFU (W-TinyLFU) with byte-awareness and size-density eviction
# - Three resident segments (all byte-accounted, OrderedDict LRU):
#     W:  recency window (probation). Pure LRU, short-lived buffer for bursts.
#     P:  main probation (SLRU probation). Evict primarily from here using TinyLFU-guided density.
#     Q:  main protected (SLRU protected). Promotion target for repeat hits.
# - Admission and eviction:
#     * On miss: insert into W (MRU). We count the access for TinyLFU before making an eviction choice.
#     * Evict: 
#         - Prefer to evict from W if the candidate is not strong enough to displace a P-victim,
#           or whenever W is above its target budget.
#         - Else evict the weakest among a small sample of oldest entries in P by TinyLFU value density.
#         - Fall back to Q's LRU if P empty.
# - Hits:
#     * If in W or P: promote to Q (protected).
#     * If in Q: refresh recency within Q.
#     * Maintain a target for Q within main area and demote Q's LRU to P when Q exceeds target.
# - TinyLFU:
#     * Count-Min sketch with periodic decay; updated on every access (hit or miss).
#     * Eviction ranking uses value density = freq / size^alpha with adaptive alpha vs. average size.
# - Window size tuning:
#     * Adaptive W target fraction using hit locality: if many hits happen in W, grow window; otherwise shrink.
#     * Tuned periodically based on access_count.
#
# This design is robust on diverse traces: absorbs bursts (W), protects hot items (Q),
# and uses TinyLFU density to avoid letting large, low-frequency items crowd out hotter ones.

from collections import OrderedDict

# ------------------------
# Resident segments
# ------------------------
_W = OrderedDict()  # key -> size (bytes), recency window (LRU)
_P = OrderedDict()  # key -> size (bytes), main probation (SLRU probation)
_Q = OrderedDict()  # key -> size (bytes), main protected (SLRU protected)

# Byte accounting
_w_bytes = 0
_p_bytes = 0
_q_bytes = 0

# Targets
_win_target_frac = 0.18  # initial window share of total capacity (bytes)
_prot_target_frac = 0.80  # protected share within main (capacity - window)

# Hit locality tuning
_hits_in_w = 0
_hits_in_main = 0
_TUNE_PERIOD = 8192  # in accesses
_last_tune_access = 0
_MIN_WIN_FRAC = 0.05
_MAX_WIN_FRAC = 0.50
_TUNE_STEP = 0.02

# Average object size (EMA)
_avg_size = 1024.0
_AVG_SIZE_ALPHA = 0.01

# TinyLFU Count-Min Sketch
_SKETCH_DEPTH = 4
_SKETCH_WIDTH = 16384
_SKETCH_MASK = _SKETCH_WIDTH - 1
_cm = [[0] * _SKETCH_WIDTH for _ in range(_SKETCH_DEPTH)]
_SALTS = (0x9e3779b185ebca87, 0xc2b2ae3d27d4eb4f, 0x165667b19e3779f9, 0xd6e8feb86659fd93)

# Sketch decay
_DECAY_PERIOD = 15000
_decay_ticker = 0

# Eviction sampling
_SAMPLE_K = 32  # number of oldest keys to consider within P (and Q fallback)

# Size penalty
_BASE_ALPHA = 1.05  # base exponent for size penalty

# Bootstrap flag
_bootstrapped = False


# ------------------------
# Helper functions
# ------------------------
def _h(key, d):
    return (hash(key) ^ _SALTS[d]) & _SKETCH_MASK

def _sketch_increment(key):
    global _decay_ticker
    for d in range(_SKETCH_DEPTH):
        _cm[d][_h(key, d)] += 1
    _decay_ticker += 1

def _sketch_estimate(key):
    est = float('inf')
    for d in range(_SKETCH_DEPTH):
        est = min(est, _cm[d][_h(key, d)])
    if est == float('inf'):
        est = 0
    return float(est)

def _sketch_decay_if_needed():
    global _decay_ticker
    if _decay_ticker >= _DECAY_PERIOD:
        for d in range(_SKETCH_DEPTH):
            row = _cm[d]
            for i in range(_SKETCH_WIDTH):
                row[i] >>= 1
        _decay_ticker = 0

def _update_avg_size(sz):
    global _avg_size
    try:
        s = float(sz)
    except Exception:
        return
    _avg_size = (1.0 - _AVG_SIZE_ALPHA) * _avg_size + _AVG_SIZE_ALPHA * max(1.0, s)

def _dynamic_alpha(size):
    # Size-aware penalty: larger-than-average objects penalized more
    r = float(max(1, int(size))) / max(1.0, _avg_size)
    if r >= 4.0:
        return _BASE_ALPHA + 0.40
    elif r >= 2.0:
        return _BASE_ALPHA + 0.20
    elif r <= 0.5:
        return max(0.90, _BASE_ALPHA - 0.10)
    return _BASE_ALPHA

def _score_value_density(key, size):
    freq = _sketch_estimate(key)
    alpha = _dynamic_alpha(size)
    denom = float(max(1, int(size))) ** alpha
    return (freq + 0.25) / denom  # small offset to differentiate zero-frequency

def _odict_move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _odict_add_mru(od, key, size):
    od[key] = int(size)
    od.move_to_end(key, last=True)

def _odict_pop_lru(od):
    # Pop and return oldest (LRU) key, size
    if not od:
        return None, None
    k, sz = od.popitem(last=False)
    return k, sz

def _segment_oldest_keys(od, k):
    out = []
    cnt = 0
    for key in od.keys():
        out.append(key)
        cnt += 1
        if cnt >= k:
            break
    return out

def _ensure_targets(capacity):
    # Clamp window target and compute protected target in main
    global _win_target_frac
    if _win_target_frac < _MIN_WIN_FRAC:
        _win_target_frac = _MIN_WIN_FRAC
    if _win_target_frac > _MAX_WIN_FRAC:
        _win_target_frac = _MAX_WIN_FRAC
    w_target = int(max(0, min(capacity, _win_target_frac * capacity)))
    main_bytes = max(0, capacity - w_target)
    q_target = int(_prot_target_frac * main_bytes)
    return w_target, q_target

def _choose_weakest_in_segment_by_density(od, sample_k):
    # Among K oldest keys, return key with lowest density score
    if not od:
        return None, None, None
    candidates = _segment_oldest_keys(od, min(sample_k, len(od)))
    victim_key = None
    victim_score = None
    victim_size = None
    for k in candidates:
        sz = od.get(k, 1)
        sc = _score_value_density(k, sz)
        if victim_key is None or sc < victim_score:
            victim_key = k
            victim_score = sc
            victim_size = sz
    return victim_key, victim_score, victim_size

def _bootstrap_if_needed(cache_snapshot):
    global _bootstrapped, _p_bytes, _w_bytes, _q_bytes, _avg_size
    if _bootstrapped:
        return
    total_bytes = 0
    total_items = 0
    # Start with everything in P (main probation) as we don't know history
    for key, robj in cache_snapshot.cache.items():
        size = int(robj.size)
        _odict_add_mru(_P, key, size)
        _p_bytes += size
        total_bytes += size
        total_items += 1
    if total_items > 0:
        _avg_size = max(1.0, float(total_bytes) / float(total_items))
    _bootstrapped = True

def _tune_window_if_needed(cache_snapshot):
    global _last_tune_access, _hits_in_w, _hits_in_main, _win_target_frac
    acc = getattr(cache_snapshot, 'access_count', 0)
    if acc - _last_tune_access < _TUNE_PERIOD:
        return
    # Simple locality-based tuning: grow W if it yields more hits; otherwise shrink
    if _hits_in_w > _hits_in_main * 1.10:
        _win_target_frac = min(_MAX_WIN_FRAC, _win_target_frac + _TUNE_STEP)
    elif _hits_in_main > _hits_in_w * 1.10:
        _win_target_frac = max(_MIN_WIN_FRAC, _win_target_frac - _TUNE_STEP)
    # Reset
    _hits_in_w = 0
    _hits_in_main = 0
    _last_tune_access = acc


# ------------------------
# Algorithm hooks
# ------------------------
def evict(cache_snapshot, obj):
    '''
    W-TinyLFU eviction decision:
      1) Count the incoming access in TinyLFU (so admission uses up-to-date freq).
      2) If P has items, find its weakest (by TinyLFU density over a small LRU sample).
         Compare with incoming candidate's density:
           - If incoming <= weakest_P and W non-empty: evict W's LRU (keep main strong).
           - Else: evict the weakest in P (admit incoming to main sooner).
      3) If P empty:
           - Evict from W's LRU if available; else from Q's LRU; else any key.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _tune_window_if_needed(cache_snapshot)

    # Count the miss access for TinyLFU and decay sketch
    _sketch_increment(obj.key)
    _sketch_decay_if_needed()

    capacity = cache_snapshot.capacity
    w_target, _ = _ensure_targets(capacity)

    # Quick path: keep W within target when possible
    if _w_bytes > w_target and _W:
        k, _ = _odict_pop_lru_candidate = next(iter(_W.items())) if _W else (None, None)
        # Return strictly the LRU of W
        try:
            k, _ = next(iter(_W.items()))
            return k
        except StopIteration:
            pass  # fall through

    # Compute incoming candidate's density score
    cand_score = _score_value_density(obj.key, int(obj.size))

    # If we have probationary items, decide admission vs. replacement
    if _P:
        victim_p_key, victim_p_score, _ = _choose_weakest_in_segment_by_density(_P, _SAMPLE_K)
        if victim_p_key is None:
            # Fallback to W then Q
            if _W:
                try:
                    k, _ = next(iter(_W.items()))
                    return k
                except StopIteration:
                    pass
            if _Q:
                try:
                    k, _ = next(iter(_Q.items()))
                    return k
                except StopIteration:
                    pass
            # Last resort: any key
            try:
                return next(iter(cache_snapshot.cache.keys()))
            except StopIteration:
                return None

        # If the incoming candidate is weaker than the weakest in P, prefer to evict from W (if possible)
        if cand_score <= victim_p_score and _W:
            try:
                k, _ = next(iter(_W.items()))
                return k
            except StopIteration:
                # If W empty unexpectedly, evict the P victim
                return victim_p_key
        else:
            # Incoming is stronger, evict weakest in P
            return victim_p_key

    # If P is empty, evict from W else Q
    if _W:
        try:
            k, _ = next(iter(_W.items()))
            return k
        except StopIteration:
            pass
    if _Q:
        try:
            k, _ = next(iter(_Q.items()))
            return k
        except StopIteration:
            pass

    # As a last resort, return any key from the cache snapshot
    try:
        return next(iter(cache_snapshot.cache.keys()))
    except StopIteration:
        return None


def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Update TinyLFU and size EMA.
      - If in W or P: promote to Q (protected).
      - If in Q: move to MRU of Q.
      - Maintain Q target by demoting Q's LRU to P if Q exceeds its target.
      - Track locality for window tuning.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _tune_window_if_needed(cache_snapshot)

    _sketch_increment(obj.key)
    _sketch_decay_if_needed()
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _w_bytes, _p_bytes, _q_bytes, _hits_in_w, _hits_in_main

    w_target, q_target = _ensure_targets(cache_snapshot.capacity)

    if key in _W:
        # Promote from W to Q
        old_sz = _W.pop(key)
        _w_bytes -= int(old_sz)
        _odict_add_mru(_Q, key, size)
        _q_bytes += size
        _hits_in_w += 1
    elif key in _P:
        # Promote from P to Q
        old_sz = _P.pop(key)
        _p_bytes -= int(old_sz)
        _odict_add_mru(_Q, key, size)
        _q_bytes += size
        _hits_in_main += 1
    elif key in _Q:
        # Refresh recency in Q
        _odict_move_to_mru(_Q, key)
        _hits_in_main += 1
    else:
        # Not tracked (shouldn't happen); place in W
        _odict_add_mru(_W, key, size)
        _w_bytes += size
        _hits_in_w += 1

    # Enforce protected target: if Q too large, demote its LRU to P
    while _q_bytes > q_target and _Q:
        demote_k, demote_sz = _odict_pop_lru(_Q)
        if demote_k is None:
            break
        _q_bytes -= int(demote_sz)
        _odict_add_mru(_P, demote_k, demote_sz)
        _p_bytes += int(demote_sz)


def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after miss):
      - Update size EMA (sketch already updated in evict for the miss).
      - Place new key into W (MRU).
      - Optionally rebalance Q if it exceeds target by demoting its LRU to P.
      - Window tuning is handled periodically.
    '''
    _bootstrap_if_needed(cache_snapshot)
    _tune_window_if_needed(cache_snapshot)

    # Do NOT increment the sketch here to avoid double-counting the miss (evict() already did).
    _update_avg_size(obj.size)

    key = obj.key
    size = int(obj.size)
    global _w_bytes, _p_bytes, _q_bytes

    # If already tracked for some reason, refresh appropriately
    if key in _W:
        _odict_move_to_mru(_W, key)
        return
    if key in _P:
        # Treat as promotion due to reinsertion pattern
        old_sz = _P.pop(key)
        _p_bytes -= int(old_sz)
        _odict_add_mru(_Q, key, size)
        _q_bytes += size
        return
    if key in _Q:
        _odict_move_to_mru(_Q, key)
        return

    # Default new insert: into window W
    _odict_add_mru(_W, key, size)
    _w_bytes += size

    # Keep Q within its target (if Q drifted above due to earlier promotions)
    _, q_target = _ensure_targets(cache_snapshot.capacity)
    while _q_bytes > q_target and _Q:
        demote_k, demote_sz = _odict_pop_lru(_Q)
        if demote_k is None:
            break
        _q_bytes -= int(demote_sz)
        _odict_add_mru(_P, demote_k, demote_sz)
        _p_bytes += int(demote_sz)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
      - Remove evicted key from its resident segment (W, P, or Q).
      - No ghost lists; TinyLFU persists globally (with periodic decay).
    '''
    _bootstrap_if_needed(cache_snapshot)
    ev_key = evicted_obj.key
    ev_size = int(evicted_obj.size)
    global _w_bytes, _p_bytes, _q_bytes

    if ev_key in _W:
        old_sz = _W.pop(ev_key)
        _w_bytes -= int(old_sz)
    elif ev_key in _P:
        old_sz = _P.pop(ev_key)
        _p_bytes -= int(old_sz)
    elif ev_key in _Q:
        old_sz = _Q.pop(ev_key)
        _q_bytes -= int(old_sz)
    else:
        # Not tracked; nothing to adjust
        pass
```
