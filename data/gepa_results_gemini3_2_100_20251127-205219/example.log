GEPA artifacts will be saved to: /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/gepa_results
Dataset sizes -> train/val: 48 samples
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyaphgfz8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyqykf5r6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1ls05yrd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcvn9geg2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn99869rm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjrp4w3fg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbrighx66.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl3xrxar6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy5gkgzbx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppxn5b9dd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9fn0mvw3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0a08ovd1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpge_30vtg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnx7w7zoa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwjbg4mwd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsnzsx_k7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3ik8g6y4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpebg_j24r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg0uib_y7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjaa9s50e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpynqubfws.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdkd63wl6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl_ple39_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjt8595uy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkxmn0l1y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkjl6f1tk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx2cpwttt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdxioofur.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9_mscn2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf_ahxfwn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr815i7ma.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnxg0bcte.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6reyrxq0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmmw9c9t4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxgx31so4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzqy5ykpp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbikkxm43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpburijvz3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn34u5hx4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxlqrwyb_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph5y9x3kr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp76o00crs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi7o674a0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2f1jjoz8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq963h3i3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5vstdbfn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl6l2y7hn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqontnmo_.pickle

Iteration 0: Base program full valset score: 0.2066171041666667
Iteration 1: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgzozwfdd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9goiiig8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnu3w06oy.pickle

Iteration 1: Proposed new text for program: # Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9munfx5j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvtuiz0rr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpimvbthle.pickle

Iteration 1: New subsample score 0.71246 is better than old score 0.48805599999999993. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe6wvvevb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps5aj_wk1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy4pwdk6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprylqkgit.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2ukb2206.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw_m9tncd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg6n88q_0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg2r1722f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzxcajyty.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl9eawr1y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiamgholi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps9sqhosm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx6md4tad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbob95fzm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4jsdtfne.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5lntsum_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8sxx0gmw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplzqd9w61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv23q3_sn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4783kqqn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdltnp5uq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7n0fm173.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppx1z4kex.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp32y7q09s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppl0_6rym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkgl5glmb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8s17r_zh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx4yma8wm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp3j40h85.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmhnx88v4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyq1mxjun.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps3ejqcu1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2lcnfgai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2nmxhrgr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppzoxdnic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphjekl6wa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphrnljmnx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppegswh_6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9vcuu73a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8mpv7zh7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmponith3ih.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppdgmuvx_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgba1llv2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgkoaoymb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp49lz2ome.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5g_4sudo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpckcdu9bw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_5o5tjcw.pickle

Iteration 1: New program is on the linear pareto front
Iteration 1: Full valset score for new program: 0.22787797916666663
Iteration 1: Full train_val score for new program: 0.22787797916666663
Iteration 1: Individual valset scores for new program: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.456547, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.332169, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.700136, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 1: New valset pareto front scores: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.332169, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.725664, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 1: Full valset pareto front score: 0.2292864166666666
Iteration 1: Updated valset pareto front programs: [{1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {0}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}]
Iteration 1: Best valset aggregate score so far: 0.22787797916666663
Iteration 1: Best program as per aggregate score on train_val: 1
Iteration 1: Best program as per aggregate score on valset: 1
Iteration 1: Best score on valset: 0.22787797916666663
Iteration 1: Best score on train_val: 0.22787797916666663
Iteration 1: Linear pareto front program index: 1
Iteration 1: New program candidate index: 1
Iteration 2: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzqr8rlkv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyszd92rj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3yxfzkyo.pickle

Iteration 2: Proposed new text for program: import math

# Metadata dictionaries
# Tracks how many times an object has been accessed.
# We will use this to persist history even after eviction (Shadow Cache).
m_key_frequency = dict()

# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def get_eviction_score(key, current_time):
    '''
    Calculate a score for eviction. Lower score = higher likelihood of eviction.
    Combining Recency and Frequency with a decay factor.
    '''
    freq = m_key_frequency.get(key, 0)
    last_access = m_key_last_access.get(key, 0)
    
    # We use a logarithmic boost for frequency so that massive frequencies don't 
    # outweigh recency entirely.
    # The 'staleness' is how long ago it was accessed.
    staleness = current_time - last_access
    
    # Heuristic: Score = Log(Frequency) / Staleness
    # If an item is very stale (large denominator), score goes down -> Evict.
    # If an item is frequent (large numerator), score goes up -> Keep.
    # We add 1 to staleness to avoid division by zero.
    score = math.log(freq + 1) / (staleness + 1)
    
    return score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest combined Recency/Frequency score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # Find the object with the minimum score
    best_victim_key = None
    min_score = float('inf')
    
    # Optimization: If we have many items, strictly iterating all might be slow in Python,
    # but given the interface constraints, we must scan.
    for key in candidate_keys:
        score = get_eviction_score(key, current_time)
        
        if score < min_score:
            min_score = score
            best_victim_key = key
        # Tie-breaker: If scores are identical, pick the one with lower frequency (pure LFU fallback)
        elif score == min_score:
            if m_key_frequency.get(key, 0) < m_key_frequency.get(best_victim_key, 0):
                 best_victim_key = key

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, handle initialization.
    If the key is in our history (m_key_frequency), we restore its frequency 
    but give it a slight penalty (or reset logic) to prevent immediate dominance.
    '''
    global m_key_frequency, m_key_last_access
    
    # If it was in history (evicted previously), it retains some "heat"
    # but we don't want it to instantly dominate.
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        # Brand new item
        m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, we DO NOT delete the frequency metadata immediately.
    This creates a "Ghost Cache" or "Shadow History".
    This allows the algorithm to remember that this item was popular recently.
    
    However, to prevent memory leaks in the metadata, we can perform cleanup
    if the metadata dictionary grows too large relative to cache capacity.
    '''
    global m_key_frequency, m_key_last_access
    
    # Remove from recency tracking immediately as it's no longer in cache
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]

    # MAINTENANCE: Prune history if it gets too big (e.g., 2x cache capacity size approximation)
    # Since we don't know the exact count capacity, we use a heuristic or hard limit
    # to prevent the dictionary from growing infinitely in a long trace.
    if len(m_key_frequency) > 2 * len(cache_snapshot.cache):
        # Remove items with low frequency to save space
        # (Naive pruning for safety)
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1 and k not in cache_snapshot.cache:
                keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_key_frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7jj381d6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7j1ul8ky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeaqx_c5c.pickle

Iteration 2: New subsample score 0.090336 is better than old score 0.086738. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqhhcx886.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwdo3l6vw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe9r7imaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp82v9ehdx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph_y7i__j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmlonsuib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprf1l1fnt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph_psqoqs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3b40yynw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm8oua8nt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2z8zv7pi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqz0ae20s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg9ecvx8_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpotqliqh4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppmbihm9c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpedyhvb4a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprhnqkcli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeg7lap6p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwlo0oig7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqh3416bq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0e7gptwm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvwe7hdt8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8ozkl9kc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt1jjuk9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu8ut3txh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7y_7zas5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6j_3xicl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxyq_6hrq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0u_08hiy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi_65jlth.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj0kouwu2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps6cunan3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaxzal57s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjk_7l4dj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiwzo2o_k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgiqwnosy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnctgf97b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpstk4wn33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_a6q1ekf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp69xmgi5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpftb92b63.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprrhjj6op.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt2abeivc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1tq6077k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuoartkpb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5bymy_zp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0pikjnwy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsl96p3yh.pickle

Iteration 2: Full valset score for new program: 0.22630574999999994
Iteration 2: Full train_val score for new program: 0.22630574999999994
Iteration 2: Individual valset scores for new program: [0.470781, 0.451494, 0.458457, 0.416022, 0.467401, 0.460164, 0.261962, 0.498624, 0.538434, 0.531017, 0.066667, 0.323268, 0.023893, 0.0, 0.019255, 0.01916, 0.018581, 0.022069, 0.021094, 0.266728, 0.334317, 0.025184, 0.057382, 0.057382, 0.269775, 0.314516, 0.83714, 0.883621, 0.038934, 0.036364, 0.038724, 0.001233, 0.000959, 0.745745, 0.070175, 0.060018, 0.047188, 0.628027, 0.121771, 0.046576, 0.025046, 0.055925, 0.042763, 0.216667, 0.060151, 0.043759, 0.435583, 0.03268]
Iteration 2: New valset pareto front scores: [0.470781, 0.451494, 0.458457, 0.416022, 0.467401, 0.460164, 0.271531, 0.498624, 0.538434, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.332169, 0.314516, 0.83714, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.745745, 0.083333, 0.067961, 0.047188, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.060151, 0.074334, 0.466258, 0.081699]
Iteration 2: Full valset pareto front score: 0.2360832083333333
Iteration 2: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {1}, {0, 2}, {2}, {0, 1, 2}, {1}, {1}, {1}, {0, 1, 2}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {2}, {2}, {1}, {1}, {1}, {1}, {1}, {1}, {2}, {1}, {1}, {2}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {2}, {1}, {1}, {1}]
Iteration 2: Best valset aggregate score so far: 0.22787797916666663
Iteration 2: Best program as per aggregate score on train_val: 1
Iteration 2: Best program as per aggregate score on valset: 1
Iteration 2: Best score on valset: 0.22787797916666663
Iteration 2: Best score on train_val: 0.22787797916666663
Iteration 2: Linear pareto front program index: 1
Iteration 2: New program candidate index: 2
Iteration 3: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6b6rcutu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4n44m621.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8d3i1o0b.pickle

Iteration 3: Proposed new text for program: import math
import random

# Metadata dictionaries
# Count-Min Sketch parameters
m_cm_sketch_width = 256  # Width of the sketch matrix
m_cm_sketch_depth = 4    # Depth (number of hash functions)
m_cm_sketch_table = []   # The actual sketch table
m_total_accesses = 0     # Counter for aging the sketch

# We maintain an approximate LRU list via a dictionary mapping key -> access_time
# This is lighter than a full doubly-linked list but sufficient for finding LRU candidates
m_lru_times = dict()

# Constants for sketch aging
RESET_INTERVAL = 1000 # Halve counters after this many accesses

def _hash(key, seed):
    '''Simple hash function for Count-Min Sketch'''
    h = seed
    for char in str(key):
        h = (h * 31 + ord(char)) & 0xFFFFFFFF
    return h

def _estimate_frequency(key):
    '''Returns the estimated frequency of a key from the Count-Min Sketch'''
    if not m_cm_sketch_table:
        return 0
    
    min_count = float('inf')
    for i in range(m_cm_sketch_depth):
        idx = _hash(key, i) % m_cm_sketch_width
        count = m_cm_sketch_table[i][idx]
        if count < min_count:
            min_count = count
    return min_count

def _increment_sketch(key):
    '''Increments the frequency of a key in the Count-Min Sketch'''
    global m_cm_sketch_table, m_total_accesses
    
    # Initialize table if not exists
    if not m_cm_sketch_table:
        m_cm_sketch_table = [[0] * m_cm_sketch_width for _ in range(m_cm_sketch_depth)]

    for i in range(m_cm_sketch_depth):
        idx = _hash(key, i) % m_cm_sketch_width
        # Cap count at 15 to prevent overflow and encourage aging (4-bit counters usually)
        # Here we use standard ints but keep them reasonable
        if m_cm_sketch_table[i][idx] < 15: 
            m_cm_sketch_table[i][idx] += 1
            
    m_total_accesses += 1
    
    # Aging: Periodically halve all counters to favor recent popularity
    if m_total_accesses >= RESET_INTERVAL:
        _reset_sketch()

def _reset_sketch():
    '''Halves all counters in the sketch to simulate aging'''
    global m_cm_sketch_table, m_total_accesses
    for i in range(m_cm_sketch_depth):
        for j in range(m_cm_sketch_width):
            m_cm_sketch_table[i][j] //= 2
    m_total_accesses = 0

def evict(cache_snapshot, obj):
    '''
    Implements a TinyLFU-style eviction:
    1. Identify the LRU candidate from the cache.
    2. Compare the frequency of the LRU candidate vs the incoming object.
    3. If Incoming Freq <= LRU Candidate Freq:
       - The incoming object is not popular enough to displace the existing one.
       - However, the `evict` function contract implies we *must* return a victim 
         to make space.
       - In a strict TinyLFU, we might reject the new item. 
       - Since the interface requires an eviction to make space for `obj`, 
         we check if the `obj` is "worth" caching.
         If obj frequency is very low compared to the victim, we still return the victim
         (standard behavior), but in a real admission policy, we'd drop `obj`.
         
    Here, we will simply evict the LRU object, BUT logic in `update_after_evict`
    could be used to simulate rejection. However, given the standard `evict` interface usually
    implies "pick someone to die", we will pick the standard LRU victim.
    
    WAIT: Optimizing miss rates means we want to keep high frequency items.
    
    Strategy refinement:
    Find a candidate via LRU. Check if that candidate has higher frequency than
    the incoming object `obj`. 
    - If Candidate Freq > Incoming Freq: 
      Ideally, we shouldn't cache `obj`. But we are forced to return a key to evict.
      To hack this "admission control" into an eviction-only interface:
      We return the LRU candidate. The system will evict it.
      
    Actually, the pure TinyLFU benefit comes from *metadata* persisting even if objects are evicted.
    The Count-Min sketch persists.
    
    So, `evict` simply returns the LRU item. The magic happens because we track frequency 
    for items *even when they are not in the cache* (via the Sketch).
    '''
    
    # 1. Find the LRU object (Least Recently Used)
    # We iterate to find min access time.
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Finding the true LRU
    lru_key = None
    min_time = float('inf')

    # Optimization: We check a sample of keys if the cache is massive to save CPU,
    # but for accuracy, we scan all. Given the constraints, full scan is safer for hit rate.
    for key in candidate_keys:
        t = m_lru_times.get(key, 0)
        if t < min_time:
            min_time = t
            lru_key = key
            
    # Admission Control Logic (Simulated):
    # We compare the estimated frequency of the victim (lru_key) vs the incoming object (obj.key).
    # Ideally, if Incoming < Victim, we'd drop Incoming. But we must evict something.
    # We will stick to LRU eviction. The Hit Rate gains come from the sketch tracking
    # popularity over time, so if `lru_key` is actually popular, it will come back 
    # and have a high count, protecting it next time (or it would be MRU).
    
    return lru_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency sketch and LRU position.
    '''
    global m_lru_times
    
    # 1. Update global frequency sketch
    _increment_sketch(obj.key)
    
    # 2. Update LRU time (Make it MRU)
    m_lru_times[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    Update frequency sketch and initialize LRU position.
    '''
    global m_lru_times
    
    # 1. Update global frequency sketch
    # Even on insert, we increment. This builds the history.
    _increment_sketch(obj.key)
    
    # 2. Initialize LRU time
    m_lru_times[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up LRU tracking, but KEEP Sketch data.
    '''
    global m_lru_times
    
    # We remove the specific LRU tracking for the object
    if evicted_obj.key in m_lru_times:
        del m_lru_times[evicted_obj.key]
        
    # CRITICAL: We DO NOT remove data from m_cm_sketch_table.
    # This allows the algorithm to "remember" that an object was popular 
    # even if it was briefly evicted. This handles the "Trace 47" low hit rate scenario
    # where items might cycle in and out.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3p0zydvj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphef1o3ak.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_zpzrtjo.pickle

Iteration 3: New subsample score 1.346971 is not better than old score 1.434271, skipping
Iteration 4: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp68aw4cjr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5jdkeihq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbxmuztxs.pickle

Iteration 4: Proposed new text for program: # Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp88ilddmz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0z_r6tiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzwss9qxz.pickle

Iteration 4: New subsample score 1.3226360000000001 is better than old score 1.240016. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsttu2rb1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3w8i2hym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc4og4qh0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp9lwuesr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjssxn0zm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpodjxqgmd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp59f89pkp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9unhc4oy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpumxy0uoj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp13__0p61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4t1_v4r4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa65zmuix.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpltijng4n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxh_srdmw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv797oum4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2u1tc2ha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3polcqse.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9x74_iei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpupuqj0rm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplx861znn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8k2i1fwv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzourxivk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4ampizmt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmputhmyweo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgckb6r2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxtj1btd_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb2_hlte2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7ku3tuan.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmponh5zcu3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi8dd3vle.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbif8holr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbgmy0rtp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppkpgjpny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgz7wp3yx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2oecstvd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfvlmhhfb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbd6oc5l7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppuas6s2s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnensv8me.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7sudh9en.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprflmzykh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqiynrupl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpczwksg7s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm8p4g8jb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1j25iifs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4vj1wjbr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9j2_icgh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp25ef7fl9.pickle

Iteration 4: New program is on the linear pareto front
Iteration 4: Full valset score for new program: 0.24242306249999998
Iteration 4: Full train_val score for new program: 0.24242306249999998
Iteration 4: Individual valset scores for new program: [0.491552, 0.47887, 0.479244, 0.441451, 0.486709, 0.482993, 0.271531, 0.498624, 0.541294, 0.531017, 0.091667, 0.353908, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023615, 0.022641, 0.272227, 0.402163, 0.026164, 0.058672, 0.058672, 0.332168, 0.364919, 0.789474, 0.89235, 0.039533, 0.038636, 0.045558, 0.007003, 0.020672, 0.745065, 0.078947, 0.067079, 0.026001, 0.640907, 0.125461, 0.090657, 0.06292, 0.0723, 0.050987, 0.366667, 0.041778, 0.074053, 0.466258, 0.081699]
Iteration 4: New valset pareto front scores: [0.491552, 0.47887, 0.479244, 0.441451, 0.486709, 0.482993, 0.271531, 0.498624, 0.541294, 0.531017, 0.091667, 0.353908, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023615, 0.022641, 0.272227, 0.402163, 0.026164, 0.058672, 0.058672, 0.332169, 0.364919, 0.83714, 0.89235, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.745745, 0.083333, 0.067961, 0.047188, 0.640907, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.060151, 0.074334, 0.466258, 0.081699]
Iteration 4: Full valset pareto front score: 0.2444322916666666
Iteration 4: Updated valset pareto front programs: [{3}, {3}, {3}, {3}, {3}, {3}, {1, 3}, {0, 2, 3}, {3}, {0, 1, 2, 3}, {3}, {3}, {1, 3}, {0, 1, 2, 3}, {3}, {3}, {3}, {3}, {3}, {1, 3}, {3}, {1, 3}, {1, 3}, {1, 3}, {1}, {3}, {2}, {3}, {1}, {1, 3}, {1, 3}, {1, 3}, {1, 3}, {2}, {1}, {1}, {2}, {3}, {1, 3}, {1}, {1}, {1}, {1}, {3}, {2}, {1}, {1, 3}, {1, 3}]
Iteration 4: Best valset aggregate score so far: 0.24242306249999998
Iteration 4: Best program as per aggregate score on train_val: 3
Iteration 4: Best program as per aggregate score on valset: 3
Iteration 4: Best score on valset: 0.24242306249999998
Iteration 4: Best score on train_val: 0.24242306249999998
Iteration 4: Linear pareto front program index: 3
Iteration 4: New program candidate index: 3
Iteration 5: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqm9rl44z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5g9dacl3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpstvkivly.pickle

Iteration 5: Proposed new text for program: import math

# Metadata dictionaries
m_frequencies = dict()      # Estimated frequency count (Count-Min sketch approximation)
m_access_time = dict()      # Key -> Last Access Time (Logical Clock)
m_protected = set()         # Set of keys in the "Protected" (SLRU) segment

# Configuration
PROTECTED_RATIO = 0.8       # Max % of cache capacity dedicated to protected items
aging_counter = 0           # Counter for periodic aging
AGING_INTERVAL = 2000       # Interval to decay frequencies

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented LRU (SLRU) eviction policy.
    
    1. We try to evict from the "Probation" segment first.
    2. We pick the LRU item within that segment.
    3. If the Probation segment is empty, we must evict from Protected (demote).
    
    This protects frequently accessed items (Protected segment) from being flushed 
    by one-time scans (which land in Probation and are quickly evicted).
    '''
    global m_access_time, m_protected
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate candidates into Probation and Protected based on m_protected set
    probation_candidates = []
    protected_candidates = []
    
    for k in candidate_keys:
        # Check integrity: ensure key exists in access time map
        if k not in m_access_time:
            m_access_time[k] = 0
            
        if k in m_protected:
            protected_candidates.append(k)
        else:
            probation_candidates.append(k)
            
    # Strategy: Always prefer to evict from Probation (LRU).
    # If Probation is empty, evict from Protected (LRU).
    
    target_pool = probation_candidates if probation_candidates else protected_candidates
    
    # Find LRU in the target pool
    victim_key = None
    min_time = float('inf')
    
    for k in target_pool:
        t = m_access_time[k]
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def _decay_frequencies():
    '''
    Halves all frequency counts to prioritize recent popularity over historical popularity.
    This implements the "Window" aspect of Window-LFU.
    '''
    global m_frequencies
    
    # We create a list of keys to modify dictionary during iteration
    keys = list(m_frequencies.keys())
    for k in keys:
        m_frequencies[k] //= 2
        # Memory cleanup: If frequency drops to 0, remove to save memory
        if m_frequencies[k] == 0:
            del m_frequencies[k]

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency and manage SLRU segment transitions.
    '''
    global m_frequencies, m_access_time, m_protected, aging_counter, AGING_INTERVAL, PROTECTED_RATIO
    
    # 1. Update Recency
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    # We track frequency even for items not strictly in the protected set
    # to help future admission decisions or if they return to cache.
    m_frequencies[obj.key] = m_frequencies.get(obj.key, 0) + 1
    
    # 3. SLRU Promotion
    # If a Probation item is hit, it is promoted to Protected.
    if obj.key not in m_protected:
        m_protected.add(obj.key)
        
        # 4. Dynamic Segment Sizing
        # Ensure Protected segment doesn't consume 100% of cache.
        # If Protected grows too large, demote the LRU protected item back to Probation.
        # Note: We estimate count based on cache size.
        max_protected_count = int(cache_snapshot.capacity / (obj.size if obj.size > 0 else 1) * PROTECTED_RATIO)
        # Fallback if size calculation is weird, just cap at 80% of current object count
        if max_protected_count <= 0: 
             max_protected_count = int(len(cache_snapshot.cache) * PROTECTED_RATIO)
             
        if len(m_protected) > max_protected_count:
            # Find LRU in Protected to demote
            lru_key = None
            min_ts = float('inf')
            for k in m_protected:
                ts = m_access_time.get(k, 0)
                if ts < min_ts:
                    min_ts = ts
                    lru_key = k
            
            if lru_key:
                m_protected.remove(lru_key)
                # It is now in Probation (implicit by absence from set)

    # 5. Periodic Aging
    aging_counter += 1
    if aging_counter > AGING_INTERVAL:
        _decay_frequencies()
        aging_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    New items enter the "Probation" segment.
    '''
    global m_access_time, m_frequencies, m_protected
    
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # 1. Update Frequency
    # Even on insert, we increment frequency. If it was in history (ghost),
    # this bump helps it.
    m_frequencies[obj.key] = m_frequencies.get(obj.key, 0) + 1
    
    # 2. Assign Segment
    # New insertions always start in Probation (not in m_protected).
    # This filters out "one-hit wonders" (scan resistance).
    if obj.key in m_protected:
        m_protected.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    '''
    global m_access_time, m_protected, m_frequencies
    
    # Remove from cache tracking
    if evicted_obj.key in m_access_time:
        del m_access_time[evicted_obj.key]
        
    # Remove from protected set if present
    if evicted_obj.key in m_protected:
        m_protected.remove(evicted_obj.key)
        
    # Note: We intentionally DO NOT remove the key from m_frequencies immediately.
    # Keeping the frequency count allows the algorithm to "remember" this item
    # if it is accessed again soon (Frequency Sketching).
    # The _decay_frequencies function will eventually clean it up if it's not used.
    
    # Soft limit on frequency map size to prevent memory leaks
    if len(m_frequencies) > 10000: # Arbitrary large limit
         # Heuristic: Delete items with low frequency to make space
         keys_to_del = [k for k, v in m_frequencies.items() if v <= 1]
         for k in keys_to_del:
             del m_frequencies[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjmjq7u91.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpumdpzjzk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb47qr2l0.pickle

Iteration 5: New subsample score 1.094531 is not better than old score 1.217324, skipping
Iteration 6: Selected program 2 score: 0.22630574999999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp20jxkv4k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfvwzfhgh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw1xsyylk.pickle

Iteration 6: Proposed new text for program: import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqd07zjhl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdk85epgs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwmduy7e_.pickle

Iteration 6: New subsample score 0.534932 is better than old score 0.502421. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqfkpybg7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcgqhe7ps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz3mq7rx9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9bxvfto_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmzfvxzna.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp39o00acl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbqdxyb_l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzs7a_4cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6r56bjf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuh80l78o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvfd0ggcc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcr91k8t8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr98ci__f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp34f15_b_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfb3dhejv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx_zuipk9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_07ono7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkhds2iti.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuq6g4dx2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzv1172ju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyzhwum2u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprndwxe2i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu_fc4evx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqdwpxqvp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg2z4df8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1q1ge7j9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj0v492ui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy081j34s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmkfh5mlf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfamli5et.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg51dxn5d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxhvm0wws.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_9iqd2rs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnizv18eg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4k5x28fz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfx2o2qyh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsphtatnr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_ve5bwmu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9tqkrv89.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi4l19euv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4f5p3q4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd22ux_jd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp89rlghzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4fy59x4b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_409oef8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplarb417j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeo8s0l7i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp40of2fk2.pickle

Iteration 6: New program is on the linear pareto front
Iteration 6: Full valset score for new program: 0.2510750416666666
Iteration 6: Full train_val score for new program: 0.2510750416666666
Iteration 6: Individual valset scores for new program: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.272727, 0.478569, 0.539864, 0.531017, 0.075, 0.350799, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.359882, 0.026164, 0.058672, 0.058672, 0.269802, 0.356855, 0.857994, 0.892179, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.75034, 0.083333, 0.067961, 0.076964, 0.640392, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.283333, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 6: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.272727, 0.498624, 0.541294, 0.531017, 0.091667, 0.353908, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023615, 0.022641, 0.272227, 0.402163, 0.026164, 0.058672, 0.058672, 0.332169, 0.364919, 0.857994, 0.89235, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.75034, 0.083333, 0.067961, 0.076964, 0.640907, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 6: Full valset pareto front score: 0.2560361458333333
Iteration 6: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {4}, {0, 2, 3}, {3}, {0, 1, 2, 3, 4}, {3}, {3}, {1, 3, 4}, {0, 1, 2, 3, 4}, {3, 4}, {3, 4}, {3, 4}, {3}, {3, 4}, {1, 3, 4}, {3}, {1, 3, 4}, {1, 3, 4}, {1, 3, 4}, {1}, {3}, {4}, {3}, {4}, {1, 3, 4}, {1, 3, 4}, {4}, {4}, {4}, {1, 4}, {1, 4}, {4}, {3}, {1, 3, 4}, {4}, {4}, {4}, {1, 4}, {3}, {4}, {4}, {1, 3, 4}, {1, 3, 4}]
Iteration 6: Best valset aggregate score so far: 0.2510750416666666
Iteration 6: Best program as per aggregate score on train_val: 4
Iteration 6: Best program as per aggregate score on valset: 4
Iteration 6: Best score on valset: 0.2510750416666666
Iteration 6: Best score on train_val: 0.2510750416666666
Iteration 6: Linear pareto front program index: 4
Iteration 6: New program candidate index: 4
Iteration 7: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn73p8844.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn0tp3gjb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiv1xxwcj.pickle

Iteration 7: Proposed new text for program: import math

# Metadata dictionaries
m_freq = dict()         # Key -> Access Frequency
m_priority = dict()     # Key -> Current GDSF Priority Score
m_last_access = dict()  # Key -> Logical timestamp (for LRU tie-breaking)
m_ghost = dict()        # Key -> Boolean (Recently evicted registry)

# Global Inflation Factor (The "Aging" floor)
g_L = 0.0

# Configuration
MAX_GHOST_SIZE = 2000   # Limit ghost registry size

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Evict the item with the lowest Priority value.
    Priority = L + (Frequency / Size)
    
    Update L (inflation factor) to the Priority of the evicted item.
    '''
    global m_priority, m_last_access, g_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for the candidate with the lowest priority.
    # Tie-breaker: Least Recently Used (smallest access_count)
    victim_key = None
    min_priority = float('inf')
    min_access_time = float('inf')
    
    for key in candidate_keys:
        # Retrieve pre-calculated priority
        # Default to g_L if something went wrong, though keys should exist in m_priority
        p = m_priority.get(key, g_L)
        last_acc = m_last_access.get(key, 0)
        
        # Comparison logic:
        # 1. Primary: Lower Priority is worse
        # 2. Secondary: Lower Access Time (Older) is worse
        if p < min_priority:
            min_priority = p
            min_access_time = last_acc
            victim_key = key
        elif p == min_priority:
            if last_acc < min_access_time:
                min_access_time = last_acc
                victim_key = key
    
    # GDSF Aging Mechanism:
    # The system "ages" by raising the floor (L) to the value of the item just evicted.
    # This ensures new items (inserted at current L) are competitive with old items.
    if victim_key is not None:
        g_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Recency.
    3. Recalculate Priority based on CURRENT g_L. 
       This "resets" the object's age relative to the current inflation.
    '''
    global m_freq, m_priority, m_last_access, g_L
    
    key = obj.key
    
    # 1. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # 3. Recalculate Priority
    # Priority = L + (Freq / Size)
    # Note: We use max(1, size) to avoid division by zero.
    size = max(1, obj.size)
    m_priority[key] = g_L + (m_freq[key] / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency (check ghost registry for boost).
    2. Calculate Initial Priority.
    '''
    global m_freq, m_priority, m_last_access, m_ghost, g_L
    
    key = obj.key
    
    # 1. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # 2. Initialize Frequency
    if key in m_ghost:
        # Ghost Hit: It was recently evicted, so it's part of a loop or working set.
        # Give it a small boost so it survives longer this time.
        m_freq[key] = 2
        del m_ghost[key]
    else:
        # Cold Start
        m_freq[key] = 1
        
    # 3. Calculate Priority
    # New items start at the current inflation level g_L
    size = max(1, obj.size)
    m_priority[key] = g_L + (m_freq[key] / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata and update ghost registry.
    '''
    global m_freq, m_priority, m_last_access, m_ghost
    
    key = evicted_obj.key
    
    # Add to Ghost Registry to detect thrashing
    m_ghost[key] = True
    
    # Cleanup active metadata
    if key in m_freq:
        del m_freq[key]
    if key in m_priority:
        del m_priority[key]
    if key in m_last_access:
        del m_last_access[key]
        
    # Maintain Ghost Registry Size
    if len(m_ghost) > MAX_GHOST_SIZE:
        # Remove an arbitrary item (first one found) to keep memory usage low
        # In Python 3.7+, dicts maintain insertion order, so this acts as FIFO
        del m_ghost[next(iter(m_ghost))]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5jf5nw1y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmznekyhq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1fqamoql.pickle

Iteration 7: New subsample score 0.106879 is not better than old score 0.17557899999999999, skipping
Iteration 8: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_tfx2l9d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpikody8a2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd22jw8p8.pickle

Iteration 8: Proposed new text for program: import math

# Metadata dictionary
# Key -> [frequency, last_access_time]
# We use a list instead of a tuple so it's mutable
m_access_info = dict()

# Ghost registry (Shadow Cache)
# Key -> eviction_time
# Used to detect if we evicted something that was immediately needed again.
m_ghost_registry = dict()

# Constants for tuning
# HIT_BOOST: How much we add to frequency on a hit. 
# 1 is standard. Higher favors LFU more strongly.
HIT_BOOST = 1

# GHOST_BOOST: If an item is found in ghost registry, it gets this initial frequency.
# This helps items recover from premature eviction.
GHOST_BOOST = 2

def get_hyperbolic_score(key, current_time):
    '''
    Calculates the retention score. 
    LOWER score = Higher chance of being evicted.
    
    Formula: Frequency / (Time_Since_Last_Access)
    
    We want to EVICT the item with the LOWEST retention score.
    However, the provided interface asks for a "Victim Score" where 
    Higher Score = Higher chance of eviction.
    
    Therefore, we return: 1 / (Hyperbolic_Retention_Score) 
    Or simply: Time_Since_Last_Access / Frequency
    '''
    if key not in m_access_info:
        return float('inf') # Should be evicted immediately if no metadata
        
    freq, last_access = m_access_info[key]
    
    # Calculate staleness (time since last access)
    # Add a small epsilon (1) to avoid division by zero if queried in same tick
    staleness = (current_time - last_access) + 1
    
    # Standard Hyperbolic Caching retention score is (Frequency / Staleness).
    # We want to maximize retention score => minimize eviction score.
    # Eviction Score = Staleness / Frequency.
    #
    # Logic:
    # 1. Very old item (High Staleness), Low Freq => High Eviction Score (Bad)
    # 2. Very recent item (Low Staleness) => Low Eviction Score (Good)
    # 3. High Freq => Denominator large => Low Eviction Score (Good)
    
    return staleness / freq

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest "Staleness per Frequency" ratio.
    This effectively implements a smooth tradeoff between LRU and LFU.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # We want to find the object with the HIGHEST eviction score
    max_score = -1.0
    victim_key = None
    
    # Optimization: To avoid iterating the entire cache on every evict (O(N)),
    # we can sample if N is very large, but for standard competitive coding 
    # constraints, O(N) linear scan is usually acceptable.
    # If performance is an issue, a random sample of 64 keys usually finds 
    # a near-optimal victim. Let's do a linear scan for accuracy first.
    
    for key in candidate_keys:
        score = get_hyperbolic_score(key, current_time)
        
        if score > max_score:
            max_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency.
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        # Update existing
        meta = m_access_info[obj.key]
        meta[0] += HIT_BOOST      # Increase frequency
        meta[1] = current_time    # Update last access
    else:
        # Edge case: Key in cache but missing metadata (should not happen)
        m_access_info[obj.key] = [1, current_time]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Check Ghost Registry.
    If present: It's a "Ghost Hit". Initialize with higher frequency.
    If absent: Initialize with base frequency.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    if obj.key in m_ghost_registry:
        # We recently evicted this, and now it's back. 
        # This implies it's part of a loop or working set larger than cache.
        # Give it a boost so it stays longer this time.
        initial_freq = GHOST_BOOST
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = [initial_freq, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keeping the ghost registry too large wastes memory, too small reduces adaptability.
    # A size equal to the cache capacity is a standard heuristic (ARC).
    target_ghost_size = len(cache_snapshot.cache)
    if len(m_ghost_registry) > target_ghost_size:
        # Remove the oldest inserted item in the ghost registry.
        # Python 3.7+ dicts preserve insertion order. iter() gives the first (oldest).
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgfyf0ws0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmwjimsu8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ax0bcys.pickle

Iteration 8: New subsample score 0.8840889999999999 is not better than old score 0.935385, skipping
Iteration 9: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphxr3qpwd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9pbuk8vf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuj6gyoxj.pickle

Iteration 9: Proposed new text for program: from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Implementation
#
# ARC dynamically balances between Recency (T1) and Frequency (T2) by tracking 
# evicted items in "Ghost" lists (B1 and B2).
#
# Global Metadata:
# m_T1: Recent cache entries (OrderedDict for LRU)
# m_T2: Frequent cache entries (OrderedDict for LRU)
# m_B1: Ghost Recent (Keys evicted from T1)
# m_B2: Ghost Frequent (Keys evicted from T2)
# m_p:  Target size for T1 (Adaptive parameter)

m_T1 = OrderedDict()
m_T2 = OrderedDict()
m_B1 = OrderedDict()
m_B2 = OrderedDict()
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Selects a victim based on the ARC policy.
    ARC decides whether to evict from the Recent set (T1) or Frequent set (T2)
    based on the current adaptation parameter 'm_p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p

    # Safety: If one list is empty, we must evict from the other.
    if not m_T1 and not m_T2:
        # This technically shouldn't happen if the cache is reported full,
        # but as a fallback for potential state drift:
        candidate_keys = list(cache_snapshot.cache.keys())
        return candidate_keys[0] if candidate_keys else None
    
    if not m_T1:
        return next(iter(m_T2)) # LRU of T2
    if not m_T2:
        return next(iter(m_T1)) # LRU of T1

    # ARC Logic:
    # We check if the incoming object (obj) that caused this eviction 
    # is in the Ghost Frequent list (B2). This influences the eviction policy.
    # Logic: If T1 is larger than target 'p', we evict from T1 to shrink it.
    #        However, if the incoming item is in B2, we might be more aggressive on T1.
    
    incoming_in_B2 = obj.key in m_B2
    
    # Condition to evict from T1 (Recent):
    # 1. T1 has more items than its target size p.
    # 2. OR: The incoming item is a "Ghost Frequent" hit, and T1 is exactly at size p.
    if (len(m_T1) > m_p) or (incoming_in_B2 and len(m_T1) == m_p):
        return next(iter(m_T1)) # LRU of T1 (First item in OrderedDict)
    else:
        return next(iter(m_T2)) # LRU of T2 (First item in OrderedDict)

def update_after_hit(cache_snapshot, obj):
    '''
    Updates metadata on cache hit.
    In ARC, a hit implies the item is popular. It should move to (or stay in)
    the Frequent set (T2) and become the Most Recently Used (MRU).
    '''
    global m_T1, m_T2
    
    key = obj.key
    
    if key in m_T1:
        # Move from Recent (T1) to Frequent (T2)
        del m_T1[key]
        m_T2[key] = True # Add to MRU end
    elif key in m_T2:
        # Already in Frequent (T2), make it MRU
        m_T2.move_to_end(key)
    else:
        # Recovery: If metadata lost track (e.g., clear operations not captured),
        # treat as an insert into T2.
        m_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    Updates metadata after a new object is inserted.
    Handles "Ghost Hits" (Misses in cache, but hits in history).
    Adapts 'm_p' to favor Recency or Frequency based on which ghost list was hit.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # We estimate logical capacity 'c' as the current number of cached items.
    # Note: The object is not yet in our T1/T2 lists, so we add 1.
    current_c = len(m_T1) + len(m_T2) + 1

    # Case: Key was in Ghost Recent (B1)
    # Means we evicted a recent item too soon. We should expand T1 (increase p).
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
            
        m_p = min(m_p + delta, current_c)
        
        del m_B1[key]
        # Promoted to Frequent (T2) because it was seen recently (in T1) and now again.
        m_T2[key] = True

    # Case: Key was in Ghost Frequent (B2)
    # Means we evicted a frequent item too soon. We should shrink T1 (decrease p) to make room for T2.
    elif key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
            
        m_p = max(m_p - delta, 0)
        
        del m_B2[key]
        # Restored to Frequent (T2)
        m_T2[key] = True

    # Case: Totally new item (Cold miss)
    else:
        # Add to Recent (T1)
        m_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Updates metadata after an object is physically evicted.
    Moves the object from the active sets (T1/T2) to the ghost sets (B1/B2).
    '''
    global m_T1, m_T2, m_B1, m_B2
    
    key = evicted_obj.key
    
    if key in m_T1:
        del m_T1[key]
        # Add to Ghost Recent (B1)
        m_B1[key] = True
        
    elif key in m_T2:
        del m_T2[key]
        # Add to Ghost Frequent (B2)
        m_B2[key] = True
        
    # Housekeeping: Prevent ghost lists from growing infinitely.
    # Standard ARC suggests limits based on capacity 'c', but here we use a loose
    # heuristic based on current cache item count to handle variable sizes safeley.
    # Keep total history roughly 2x the current cache count.
    max_history = 2 * (len(m_T1) + len(m_T2) + 1)
    max_history = max(max_history, 100) # Minimum baseline
    
    if len(m_B1) > max_history:
        m_B1.popitem(last=False) # Remove oldest
        
    if len(m_B2) > max_history:
        m_B2.popitem(last=False) # Remove oldest
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7z9y5wr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_atgwgru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0srm56v7.pickle

Iteration 9: New subsample score 1.058401 is not better than old score 1.090026, skipping
Iteration 10: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp465dvim8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj7nd2tn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfpi44cug.pickle

Iteration 10: Proposed new text for program: import math

# --- Metadata Globals ---

# Stores simple metadata: key -> access_count (used to distinguish T1 vs T2)
# We don't need complex tuples here because the list membership defines the state.
m_access_count = dict()

# We maintain 4 LRU lists using standard Python dictionaries (ordered by insertion since Python 3.7+).
# Keys in T1/T2 are in the cache. Keys in B1/B2 are ghost entries (metadata only).
# Using dicts allows O(1) lookup and delete, and O(1) "move to end" (reinsert).
# 
# T1: Recently accessed once (Real Cache)
# T2: Accessed at least twice (Real Cache)
# B1: Evicted from T1 (Ghost)
# B2: Evicted from T2 (Ghost)

m_T1 = dict()
m_T2 = dict()
m_B1 = dict()
m_B2 = dict()

# Adaptation Parameter 'p'
# 0 <= p <= capacity
# 'p' is the target size for the T1 list.
m_p = 0

def move_to_mru(container, key):
    '''Helper: Refreshes an item in a python dict to be the most recent.'''
    val = container.pop(key)
    container[key] = val

def get_lru_key(container):
    '''Helper: Gets the LRU key (first item) from a python dict.'''
    return next(iter(container)) if container else None

def evict(cache_snapshot, obj):
    '''
    ARC Replace Subroutine.
    Decides which item to evict from T1 or T2 based on the current target size 'p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p

    # We need to free up space. The victim depends on the sizes of T1 vs p.
    # Note: len(T1) + len(T2) == Capacity (roughly).
    
    t1_len = len(m_T1)
    
    # Logic: 
    # If T1 is larger than our target 'p', we evict from T1 (LRU preference for Recency).
    # Otherwise, we evict from T2 (LRU preference for Frequency).
    # There is a subtle condition in ARC: if B1 is larger than B2, it influences eviction logic slightly
    # but the standard approximation `if t1_len > p` works well for fixed size caches.
    
    # Precise ARC condition for REPLACE(p):
    # if ( |T1| >= 1 and ( (|T1| > p) or (|B1| is referenced and implicit logic requires space) ) )
    # However, since `evict` is called strictly when cache is FULL:
    
    victim_key = None
    
    # If key is in B2, the original paper logic changes slightly, but we are inside 
    # the generic evict hook provided by the simulator.
    # We strictly follow the size constraints relative to p.
    
    if t1_len > 0 and (t1_len > m_p or (get_lru_key(m_B1) == obj.key and t1_len == m_p)):
        # Evict from T1 -> Move to B1
        victim_key = get_lru_key(m_T1)
        # The actual movement to B1 happens in `update_after_evict`
    else:
        # Evict from T2 -> Move to B2
        victim_key = get_lru_key(m_T2)
        if victim_key is None:
            # Fallback if T2 is empty (should not happen if cache full and T1 <= p)
            victim_key = get_lru_key(m_T1)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If in T1 or T2, move to MRU of T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Hit in T1 (Recently inserted, now accessed again -> upgrade to Frequent)
    if key in m_T1:
        # Remove from T1
        del m_T1[key]
        # Add to MRU of T2
        m_T2[key] = True
        
    # Case 2: Hit in T2 (Already frequent -> stay in Frequent, become MRU)
    elif key in m_T2:
        move_to_mru(m_T2, key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss processing):
    Check Ghosts (B1/B2) to adapt 'p'.
    Insert new item into T1 or T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # Case 1: Key is in B1 (Ghost Recent)
    # This implies we evicted a recent item too soon. We should increase T1 size (p).
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
        m_p = min(capacity, m_p + delta)
        
        # Remove from Ghost
        del m_B1[key]
        
        # Move to MRU of T2 (It has been seen twice now: once effectively in T1-history, now again)
        # Note: Standard ARC promotes ghost hits to T2.
        m_T2[key] = True
        return

    # Case 2: Key is in B2 (Ghost Frequent)
    # This implies we evicted a frequent item too soon. We should decrease T1 size (p) to make room for T2.
    if key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
        m_p = max(0, m_p - delta)
        
        # Remove from Ghost
        del m_B2[key]
        
        # Move to MRU of T2
        m_T2[key] = True
        return

    # Case 3: Completely new item (not in T1, T2, B1, B2)
    # Insert into T1 MRU.
    # Note: If cache was full, eviction happened before this.
    m_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T1/T2 to B1/B2 respectively.
    Ensure Ghost lists don't grow indefinitely.
    '''
    global m_T1, m_T2, m_B1, m_B2
    
    key = evicted_obj.key
    capacity = cache_snapshot.capacity
    
    # Determine where it came from and move to corresponding ghost list
    if key in m_T1:
        del m_T1[key]
        m_B1[key] = True # Add to B1 MRU
    elif key in m_T2:
        del m_T2[key]
        m_B2[key] = True # Add to B2 MRU
        
    # ARC Maintenance: Bounding the Ghost Lists (L1 + L2 <= 2*c)
    # Effectively, we usually want |T1| + |B1| <= c and |T2| + |B2| <= 2c
    # But a simpler bound often used in implementations is |B1| + |B2| <= Capacity
    
    # Prune B1 if too big
    if len(m_B1) > capacity:
        try:
            oldest = next(iter(m_B1))
            del m_B1[oldest]
        except StopIteration:
            pass

    # Prune B2 if too big
    if len(m_B2) > capacity:
        try:
            oldest = next(iter(m_B2))
            del m_B2[oldest]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbvxox39c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp22p1bg24.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiai0590l.pickle

Iteration 10: New subsample score 0.15401099999999998 is not better than old score 0.21971699999999997, skipping
Iteration 11: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd8y21mgl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5igdpzup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwe2gjl1t.pickle

Iteration 11: Proposed new text for program: import math

# Metadata dictionary
# Maps key -> { 'freq': int, 'score': float, 'last_access': int }
m_metadata = {}

# GDSF Priority Inflation Parameter (L)
# This value acts as a "watermark" for the cache's value. 
# It increases monotonically as items are evicted.
# New items enter at this baseline level.
m_inflation_L = 0.0

def get_gdsf_score(freq, size, L):
    '''
    Calculates the Greedy Dual Size Frequency score.
    Formula: Score = L + (Frequency / Size)
    
    - L: The aging factor.
    - Frequency: Promotes popular items.
    - Size: Penalizes large items (optimizing for hit density per byte).
    '''
    # Ensure size is at least 1 to prevent division by zero
    safe_size = max(1, size)
    return L + (float(freq) / float(safe_size))

def evict(cache_snapshot, obj):
    '''
    Strategy: GDSF (Greedy Dual Size Frequency)
    Evicts the object with the MINIMUM score.
    Tie-breaking: Least Recently Used (LRU) based on access_count.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
        
    victim_key = None
    # We want to find the lowest score to evict
    min_score = float('inf')
    min_last_access = float('inf')
    
    # Linear scan to find the victim
    for key in candidate_keys:
        # Retrieve metadata
        meta = m_metadata.get(key)
        
        # Safety check: if metadata is missing, evict this zombie item immediately
        if not meta:
            return key
            
        score = meta['score']
        last_access = meta['last_access']
        
        # Logic: Find Minimum Score
        if score < min_score:
            min_score = score
            min_last_access = last_access
            victim_key = key
        elif score == min_score:
            # Tie-breaker: If scores are equal, evict the older one (LRU)
            if last_access < min_last_access:
                min_last_access = last_access
                victim_key = key
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    3. "Re-value" the item using the CURRENT global L.
       This effectively resets the item's aging process, protecting it 
       from the rising tide of L caused by evictions.
    '''
    global m_metadata, m_inflation_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        meta = m_metadata[obj.key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        # Recalculate score with new frequency and current L
        meta['score'] = get_gdsf_score(meta['freq'], obj.size, m_inflation_L)
    else:
        # Fail-safe if metadata was lost
        m_metadata[obj.key] = {
            'freq': 1,
            'last_access': current_time,
            'score': get_gdsf_score(1, obj.size, m_inflation_L)
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Create metadata.
    2. Set start Frequency to 1.
    3. Calculate score based on current global L.
    '''
    global m_metadata, m_inflation_L
    
    current_time = cache_snapshot.access_count
    
    m_metadata[obj.key] = {
        'freq': 1,
        'last_access': current_time,
        'score': get_gdsf_score(1, obj.size, m_inflation_L)
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. CRITICAL: Update global L to the score of the evicted item.
       This ensures that for any existing item to survive, it must eventually
       prove it is more valuable than what was just evicted.
    2. Clean up metadata.
    '''
    global m_metadata, m_inflation_L
    
    if evicted_obj.key in m_metadata:
        # The inflation value L advances to the priority of the evicted object
        m_inflation_L = m_metadata[evicted_obj.key]['score']
        
        # Remove the evicted item from tracking
        del m_metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprmuhpe_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwcb4pyv2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptdys6f7t.pickle

Iteration 11: New subsample score 0.32259699999999997 is not better than old score 0.45189599999999996, skipping
Iteration 12: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5gr2knt9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqgrxo62m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0lx3yxzn.pickle

Iteration 12: Proposed new text for program: # Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp523dfa2f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp70lmd2zt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvuj30yjs.pickle

Iteration 12: New subsample score 0.628752 is better than old score 0.62675. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6abkzcm5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi97qiwf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpytqnkoh0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpllxidtum.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcoq_p0ts.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyi2u6r48.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmn3uq4is.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphz8lmovj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx1j1ftz_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3pjkf_0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2kfn9che.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgpik4jue.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgjizrbop.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0j3d5il0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl2gbu34y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxru7wint.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdytrfs6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0_wyd46l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp04ad5qst.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps5ugmkay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqyeqrkv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd0vy9p7y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4n7_n11u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqdo4a8lb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp0w1z6u5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxlf8_wk4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5ot866e1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp21zlduox.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphpn5vv24.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_pzw3siz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz50zw8ev.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf62ggy_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpue466g9z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyh2zwl32.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpijvy_lbp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjwlb68yb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp4ioa2n0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3sxwldre.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm2xuki4q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpchg7abk2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl9v2y5f0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkd5o45lx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmh97vkbj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq2ygoulo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpznsiqudf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn4sba4wy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpht3p4ctt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplk50z4bx.pickle

Iteration 12: Full valset score for new program: 0.23234020833333333
Iteration 12: Full train_val score for new program: 0.23234020833333333
Iteration 12: Individual valset scores for new program: [0.450926, 0.430948, 0.439549, 0.405619, 0.444296, 0.441139, 0.26555, 0.498624, 0.540579, 0.531017, 0.091667, 0.391208, 0.023893, 0.0, 0.019397, 0.019442, 0.018715, 0.02221, 0.021094, 0.272227, 0.369715, 0.02538, 0.058672, 0.058672, 0.288554, 0.359879, 0.796425, 0.888413, 0.039832, 0.038636, 0.045558, 4.8e-05, 3.6e-05, 0.749319, 0.083333, 0.067079, 0.018281, 0.639361, 0.125461, 0.041586, 0.039096, 0.043257, 0.052632, 0.366667, 0.041854, 0.041795, 0.466258, 0.078431]
Iteration 12: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.272727, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023615, 0.022641, 0.272227, 0.402163, 0.026164, 0.058672, 0.058672, 0.332169, 0.364919, 0.857994, 0.89235, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.75034, 0.083333, 0.067961, 0.076964, 0.640907, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 12: Full valset pareto front score: 0.2568132291666666
Iteration 12: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {4}, {0, 2, 3, 5}, {3}, {0, 1, 2, 3, 4, 5}, {3, 5}, {5}, {1, 3, 4}, {0, 1, 2, 3, 4, 5}, {3, 4}, {3, 4}, {3, 4}, {3}, {3, 4}, {1, 3, 4, 5}, {3}, {1, 3, 4}, {1, 3, 4, 5}, {1, 3, 4, 5}, {1}, {3}, {4}, {3}, {4}, {1, 3, 4, 5}, {1, 3, 4, 5}, {4}, {4}, {4}, {1, 4, 5}, {1, 4}, {4}, {3}, {1, 3, 4, 5}, {4}, {4}, {4}, {1, 4, 5}, {3, 5}, {4}, {4}, {1, 3, 4, 5}, {1, 3, 4}]
Iteration 12: Best valset aggregate score so far: 0.2510750416666666
Iteration 12: Best program as per aggregate score on train_val: 4
Iteration 12: Best program as per aggregate score on valset: 4
Iteration 12: Best score on valset: 0.2510750416666666
Iteration 12: Best score on train_val: 0.2510750416666666
Iteration 12: Linear pareto front program index: 4
Iteration 12: New program candidate index: 5
Iteration 13: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp94jptcp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjbrtoll3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp82jpo8vw.pickle

Iteration 13: Proposed new text for program: import math

# =============================================================================
# ARC (Adaptive Replacement Cache) State
# =============================================================================
# We emulate the 4 lists of ARC using Python's insertion-ordered dictionaries.
# T1: Recent (MRU) - Items accessed once recently.
# T2: Frequent (MRU) - Items accessed at least twice.
# B1: Ghost Recent - History of evicted T1 items.
# B2: Ghost Frequent - History of evicted T2 items.
# =============================================================================

m_t1 = dict()
m_t2 = dict()
m_b1 = dict()
m_b2 = dict()
m_p = 0  # Target size for the T1 list

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on ARC logic.
    Crucially, ARC's eviction policy depends on *where* the incoming object `obj` 
    was found (Ghost B1 vs Ghost B2 vs New).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    # Current Cache Size (in number of objects)
    # We use this as 'C' in ARC literature.
    cache_len = len(cache_snapshot.cache)
    
    # Safety: If cache is empty, we can't evict.
    if cache_len == 0:
        return None

    # Identify the incoming object's status (New, B1, or B2)
    # This influences whether we replace from T1 or T2.
    key = obj.key
    found_in_b2 = key in m_b2
    
    # ARC REPLACE Logic:
    # We evict from T1 (Recency) if:
    # 1. T1 contains more items than target 'p'.
    # 2. OR: The incoming item is in B2 (Ghost Freq) AND T1 has reached target 'p'.
    #    (This specific condition helps prioritize Frequency when Ghost Freq is hit).
    
    t1_len = len(m_t1)
    
    # We prioritize evicting from T1 if it exceeds the target p.
    # Logic derived from ARC paper "Replace(p)" subroutine.
    if (t1_len > 0) and ( (t1_len > m_p) or (found_in_b2 and t1_len == m_p) ):
        # Evict LRU from T1. 
        # Since Python dicts are insertion-ordered, iter() gives keys in insertion order.
        # The first key is the oldest (LRU).
        victim_key = next(iter(m_t1))
        return victim_key
    else:
        # Otherwise, evict LRU from T2.
        # If T2 is empty, fallback to T1 (should not happen if cache is full and logic holds, but implies safety).
        if len(m_t2) > 0:
            victim_key = next(iter(m_t2))
            return victim_key
        else:
            # Fallback for edge cases (e.g., p=Size, T2 empty)
            return next(iter(m_t1))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1, move to T2 (it has proven frequency).
    If item is in T2, move to MRU of T2.
    '''
    global m_t1, m_t2
    
    key = obj.key
    
    if key in m_t1:
        # Move from T1 (Recent) to T2 (Frequent)
        del m_t1[key]
        m_t2[key] = None # Insert at end (MRU)
    elif key in m_t2:
        # Update Position in T2 (MRU)
        del m_t2[key]
        m_t2[key] = None
    
    # Note: If cache state is inconsistent with our lists (rare simulation glitch),
    # we just ignore. The lists track the algorithmic view.

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost Lists (B1/B2) to adapt 'p'.
    2. Insert object into correct list (T1 or T2).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    
    # --- 1. Adaptation Step (Tuning p) ---
    if key in m_b1:
        # Hit in Ghost Recent (B1).
        # It means we evicted a recent item too soon. We should expand T1.
        delta = 1
        if len(m_b1) >= len(m_b2):
            delta = 1
        else:
            # Safety for division by zero
            delta = len(m_b2) // len(m_b1)
            
        # Increase p, cap at Cache Size
        current_capacity = len(cache_snapshot.cache) 
        m_p = min(m_p + delta, current_capacity)
        
        # Remove from ghost
        del m_b1[key]
        
        # Move to T2 (Frequency) as it has been seen twice (once in T1 history, now again)
        m_t2[key] = None
        
    elif key in m_b2:
        # Hit in Ghost Frequent (B2).
        # It means we evicted a frequent item too soon. We should shrink T1 (expand T2).
        delta = 1
        if len(m_b2) >= len(m_b1):
            delta = 1
        else:
            delta = len(m_b1) // len(m_b2)
            
        # Decrease p, cap at 0
        m_p = max(m_p - delta, 0)
        
        # Remove from ghost
        del m_b2[key]
        
        # Move to T2 (Frequency)
        m_t2[key] = None
        
    else:
        # Completely new item.
        # Insert into T1 (Recency - Probationary period)
        m_t1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from Active (T1/T2) to Ghost (B1/B2).
    Maintain Ghost List sizes.
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    key = evicted_obj.key
    current_capacity = len(cache_snapshot.cache) + 1 # +1 because we just evicted one to make space
    
    # Identify where it was evicted from and move to corresponding Ghost list
    if key in m_t1:
        del m_t1[key]
        m_b1[key] = None # Add to MRU of B1
    elif key in m_t2:
        del m_t2[key]
        m_b2[key] = None # Add to MRU of B2
        
    # --- Ghost Registry Maintenance ---
    # ARC property: |L1| + |L2| <= 2*c
    # i.e. (|T1| + |B1|) + (|T2| + |B2|) <= 2*c
    # We prune the ghost lists if they get too large.
    
    # Ideally, maintain |T1| + |B1| <= C and |T2| + |B2| <= 2C
    # But a simple bounded sum is usually sufficient and simpler.
    
    target_ghost_len = current_capacity
    
    while len(m_b1) > target_ghost_len:
        # Remove LRU of B1
        oldest = next(iter(m_b1))
        del m_b1[oldest]
        
    while len(m_b2) > target_ghost_len:
        # Remove LRU of B2
        oldest = next(iter(m_b2))
        del m_b2[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcmc6jfvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpju0zj62y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpms09bwmh.pickle

Iteration 13: New subsample score 0.7331719999999999 is not better than old score 0.803082, skipping
Iteration 14: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprwawxjuz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_1kg1tg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfehlorn0.pickle

Iteration 14: Proposed new text for program: import math

# --- Global Metadata ---
# m_metadata stores: key -> (is_frequent_segment, reference_bit)
# We track which segment an item belongs to:
#   False (0): T1 (Recency / Probation segment)
#   True (1):  T2 (Frequency / Protected segment)
m_metadata = dict()

# Ghost caches (store keys only)
# B1: Evicted from T1 (Recency ghosts)
# B2: Evicted from T2 (Frequency ghosts)
# Since we need fast lookups and order preservation is useful for cleaning, 
# we use dictionaries (Python 3.7+ preserves insertion order).
m_ghost_b1 = dict()
m_ghost_b2 = dict()

# Adaptive Parameter 'p'
# Target size for T1 (Recency list).
# Range: [0, cache_capacity]
m_target_p = 0

# --- Helper Logic ---

def clean_ghosts(cache_capacity):
    """
    Ensures the ghost lists don't grow indefinitely.
    ARC typically suggests |T1| + |B1| <= C and |T2| + |B2| <= 2C, 
    but roughly keeping |B1| + |B2| <= C is a safe memory bound.
    """
    global m_ghost_b1, m_ghost_b2
    
    # Simple cleanup strategy: If total ghosts > capacity, remove oldest
    # This prevents memory leaks in the simulation.
    total_ghosts = len(m_ghost_b1) + len(m_ghost_b2)
    
    # We allow ghosts to extend up to capacity to capture history
    if total_ghosts > cache_capacity:
        # Remove from B1 if it has elements, else B2
        if m_ghost_b1:
            # next(iter()) gets the oldest inserted key
            del m_ghost_b1[next(iter(m_ghost_b1))]
        elif m_ghost_b2:
            del m_ghost_b2[next(iter(m_ghost_b2))]

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic.
    Decides whether to evict from T1 (Recency) or T2 (Frequency) based on
    current sizes relative to the adaptive parameter 'p'.
    '''
    global m_metadata, m_target_p, m_ghost_b1, m_ghost_b2

    # Identify candidates present in the cache
    t1_candidates = []
    t2_candidates = []
    
    # Map cache contents to T1 or T2 based on metadata
    # Also need strict LRU order. Python dict iteration is insertion-ordered.
    # However, 'insertion' here means when it was added to the dictionary.
    # To do this strictly correct, we need access times. 
    # But for this simulation interface, we can rely on Python's dict order 
    # IF we manage re-insertions correctly in update_after_hit.
    
    # Let's iterate over cache snapshot to separate them
    for key in cache_snapshot.cache:
        # If key not in metadata, treat as T1 default (shouldn't happen often)
        is_t2 = False
        if key in m_metadata:
            is_t2, _ = m_metadata[key]
            
        if is_t2:
            t2_candidates.append(key)
        else:
            t1_candidates.append(key)
            
    # Count current sizes
    len_t1 = len(t1_candidates)
    
    victim_key = None
    
    # Logic:
    # If len(T1) > p, we usually evict from T1.
    # However, we must ensure we don't evict from an empty list.
    
    if len_t1 > 0 and (len_t1 > m_target_p or len(t2_candidates) == 0):
        # Evict LRU from T1
        # Since t1_candidates comes from cache_snapshot.cache keys iteration,
        # we need to find the "LRU" one. 
        # *Constraint*: We don't have an explicit LRU Linked List.
        # We must rely on an assumption or metadata.
        # To be safe, let's use the 'access_count' or insertion order. 
        # The previous code stored metadata. Let's look at m_metadata.
        # Actually, simpler: The simulator iterates `cache` dict. 
        # If we delete/re-insert on hit, the first item in iteration is LRU.
        
        # We will assume the simulator's cache keys are NOT ordered by LRU automatically
        # unless we enforced it. We can't enforce the simulator's dict order.
        # Optimization: We need to track LRU ourselves? 
        # Given the "Stateless" nature of 'evict' (it iterates keys), 
        # we should scan for the oldest access time if possible. 
        # However, the provided template doesn't let us store extra state easily 
        # unless we put it in m_metadata.
        # Let's add (last_access_time) to m_metadata.
        
        # Find LRU in T1
        victim_key = min(t1_candidates, key=lambda k: m_metadata[k][1])
        return victim_key
    else:
        # Evict LRU from T2
        if t2_candidates:
            victim_key = min(t2_candidates, key=lambda k: m_metadata[k][1])
            return victim_key
            
    # Fallback (should not be reached if cache not empty)
    return list(cache_snapshot.cache.keys())[0]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in T1, move to T2.
    2. If in T2, move to MRU of T2.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # Determine current state
    is_t2 = False
    if key in m_metadata:
        is_t2, _ = m_metadata[key]
        
    # On any hit, the item becomes "Frequency/Protected" (T2)
    # Update timestamp to make it MRU
    m_metadata[key] = (True, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost Lists (B1, B2).
    2. Adapt 'p' based on where the hit occurred (if it was a ghost).
    3. Insert new item into T1 (Recency) if strictly new, or T2 if rescued from ghost.
    '''
    global m_metadata, m_target_p, m_ghost_b1, m_ghost_b2
    
    current_time = cache_snapshot.access_count
    key = obj.key
    capacity = cache_snapshot.capacity # Note: This is in bytes in the object, but usually count in algorithms. 
    # *Assumption*: The simulator counts "capacity" in count (objects), or we approximate.
    # The prompt says capacity is "bytes", but standard caching algorithms count items.
    # We will assume the simulator handles the "cache full" logic based on bytes, 
    # but our logic for 'p' works on item counts usually. 
    # To be safe for byte-based caches, we treat 'p' as a generic scalar target.
    
    # ARC Adaptation Logic
    if key in m_ghost_b1:
        # Hit in B1 (Ghost Recency). T1 was too small. Increase p.
        # Delta: If B1 is bigger than B2, step smaller; else step 1.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) / len(m_ghost_b1)
            
        m_target_p = min(m_target_p + delta, len(cache_snapshot.cache) + 1)
        
        # Promote to T2 (Frequent) because it was seen recently (in T1) then accessed again
        m_metadata[key] = (True, current_time)
        del m_ghost_b1[key]
        
    elif key in m_ghost_b2:
        # Hit in B2 (Ghost Frequency). T2 was too small (so T1 was too big). Decrease p.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) / len(m_ghost_b2)
            
        m_target_p = max(m_target_p - delta, 0)
        
        # Re-insert into T2
        m_metadata[key] = (True, current_time)
        del m_ghost_b2[key]
        
    else:
        # Totally new item. Insert into T1 (Recency/Probation)
        m_metadata[key] = (False, current_time)

    # Clean up ghosts if they grew too large
    clean_ghosts(len(cache_snapshot.cache))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Identify where it was evicted from (T1 or T2).
    2. Add to corresponding Ghost List (B1 or B2).
    '''
    global m_metadata, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_metadata:
        is_t2, _ = m_metadata[key]
        del m_metadata[key]
        
        if is_t2:
            # Evicted from T2 -> Add to B2
            m_ghost_b2[key] = True # Value doesn't matter, existence does
        else:
            # Evicted from T1 -> Add to B1
            m_ghost_b1[key] = True
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi7jncnkt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe7u19i61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8w_c4ht6.pickle

Iteration 14: New subsample score 0.376457 is not better than old score 0.376457, skipping
Iteration 15: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp05ucr_lh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi7ucgout.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi3pfrggy.pickle

Iteration 15: Proposed new text for program: import math

# --- Global Metadata ---
# Tracks active cache items: key -> (access_count, type_flag)
# type_flag: 0 = L1 (Recency/Probation), 1 = L2 (Frequency/Protected)
m_meta = dict()

# Tracks access times for LRU calculation within the lists
# key -> last_access_time
m_access_time = dict()

# Ghost registries (Evicted items history)
# B1: Ghost list for L1 (Evicted from Recency)
# B2: Ghost list for L2 (Evicted from Frequency)
m_ghost_b1 = set()
m_ghost_b2 = set()

# --- Adaptive Parameter ---
# 'p' represents the target size of the L1 (Recency) list.
# 0 <= p <= capacity
global_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-inspired Eviction Strategy.
    
    We have two logical segments in the cache:
    - L1: Items accessed once (Recency heavy).
    - L2: Items accessed > once (Frequency heavy).
    
    The parameter `p` determines the ideal size of L1.
    If len(L1) > p, we evict from L1 (the LRU item among those accessed once).
    Else, we evict from L2 (the LRU item among those accessed frequently).
    '''
    global m_meta, m_access_time, global_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Classify current cached items into L1 (type 0) and L2 (type 1)
    l1_keys = []
    l2_keys = []
    
    for k in candidate_keys:
        # Default to L1 if missing metadata (safety fallback)
        meta = m_meta.get(k, (1, 0))
        if meta[1] == 0:
            l1_keys.append(k)
        else:
            l2_keys.append(k)

    # Determine which list to evict from based on target size 'p'
    # If the Recency list (L1) has grown beyond its target `p`, we trim it.
    # Otherwise, we trim the Frequency list (L2).
    if len(l1_keys) > 0 and (len(l1_keys) > global_p or len(l2_keys) == 0):
        victim_pool = l1_keys
    else:
        victim_pool = l2_keys
        
    # Find the LRU item within the chosen pool
    # We want the item with the smallest last_access_time
    victim_key = None
    min_time = float('inf')
    
    for k in victim_pool:
        t = m_access_time.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency (last access time).
    2. Move item to L2 (Frequency list) if it isn't already there.
    '''
    global m_meta, m_access_time
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    
    # Retrieve current metadata
    if obj.key in m_meta:
        count, type_flag = m_meta[obj.key]
        # It is now a frequent item, so type becomes 1 (L2)
        # We increment count just for statistics, though ARC relies mostly on the lists
        m_meta[obj.key] = (count + 1, 1)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost lists to adapt 'p'.
    2. Insert new item.
    '''
    global m_meta, m_access_time, m_ghost_b1, m_ghost_b2, global_p
    
    current_time = cache_snapshot.access_count
    capacity = len(cache_snapshot.cache) # Approximation of capacity (N)
    # Note: cache_snapshot.capacity is bytes, but len(cache) is count. 
    # ARC usually works on counts. We assume N is roughly current count.
    
    key = obj.key
    
    # CASE 1: Key is in Ghost L1 (B1) -> Recency list was too small
    if key in m_ghost_b1:
        # Adapt p: Increase L1 target size
        # If B1 size >= B2 size, increment by 1. Else, increment by larger delta.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) // len(m_ghost_b1)
        
        # Determine strict capacity count. Since we can't access `capacity` as count easily,
        # we use the current cache size as the baseline "N".
        N = len(cache_snapshot.cache)
        global_p = min(global_p + delta, N)
        
        # Promote to L2 immediately because it was recently seen (in ghost)
        # This restores a "wrongly evicted" recent item to the frequent list
        new_type = 1
        m_ghost_b1.remove(key)

    # CASE 2: Key is in Ghost L2 (B2) -> Frequency list was too small
    elif key in m_ghost_b2:
        # Adapt p: Decrease L1 target size (giving more room to L2)
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
            
        global_p = max(global_p - delta, 0)
        
        # Promote to L2
        new_type = 1
        m_ghost_b2.remove(key)

    # CASE 3: Totally new item
    else:
        # Insert into L1 (Recency)
        new_type = 0
    
    # Update Metadata
    m_meta[key] = (1, new_type)
    m_access_time[key] = current_time

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to appropriate Ghost List (B1 or B2) depending on where it came from.
    3. Maintain ghost list sizes.
    '''
    global m_meta, m_access_time, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_meta:
        _, type_flag = m_meta[key]
        del m_meta[key]
        del m_access_time[key]
        
        # If it was in L1 (Recency), move to Ghost B1
        if type_flag == 0:
            m_ghost_b1.add(key)
        # If it was in L2 (Frequency), move to Ghost B2
        else:
            m_ghost_b2.add(key)
            
    # Maintenance: Keep ghost lists bounded to roughly cache size (N)
    # In strict ARC, |L1| + |B1| = N and |L2| + |B2| = 2N, etc.
    # Here we simplify: if total ghosts > 2 * cache size, remove oldest.
    # Since we use Sets for O(1) lookups, we don't have perfect LRU order for ghosts easily
    # without a second data structure. To save compute/memory in this constraint, 
    # we just prune arbitrary items if it gets too big (or rely on Python's insertion order for iter).
    
    target_len = len(cache_snapshot.cache) 
    
    # Prune B1 if too big
    while len(m_ghost_b1) > target_len:
        # Remove first element (oldest insertion in modern Python dict/set implementation detail, usually)
        # Or just arbitrary pop.
        try:
            m_ghost_b1.pop() 
        except KeyError:
            break
            
    while len(m_ghost_b2) > target_len:
        try:
            m_ghost_b2.pop()
        except KeyError:
            break
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv8ifpkqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb1oppkx1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr_tywov_.pickle

Iteration 15: New subsample score 0.411665 is not better than old score 0.412406, skipping
Iteration 16: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg4z30a8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_bx17zjs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmehwnrcm.pickle

Iteration 16: Proposed new text for program: import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8kgpdgqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfisgrvae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuse7_wvs.pickle

Iteration 16: New subsample score 1.340873 is better than old score 1.254689. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt88a_yv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxgzld551.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptzdu4dda.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplmi5vcvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpva88fcsy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp373q62nh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe_o0f01v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwxd066xs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxlase16_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp84qe3fvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnkekyrwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl4_x14h7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmg_ddhzf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcmnao40p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp96py2ul8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkxd5qrsn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7pcimqs8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprcmfrzph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx6w9nbok.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvghayakd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsteiiq0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2tvbor_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwxd8lltf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxdefc3j4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1l1tiv0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn8ucmqu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpegwf47z3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpztr7epic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt7m5r4n5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr1390v8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl3tiskr4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt6xvxyv9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_7cmp695.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5a2iet3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpogm19wet.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn0nx3kjp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2owerdyp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfvecpohi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzqr4x1ml.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpso4p3wp_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpofo0cc84.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeiccp9dh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf9twaeus.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw05jg1ak.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_abpzeui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprh45sn8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqw49t3_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnweqx2kk.pickle

Iteration 16: Full valset score for new program: 0.23626714583333333
Iteration 16: Full train_val score for new program: 0.23626714583333333
Iteration 16: Individual valset scores for new program: [0.502152, 0.472508, 0.484395, 0.436027, 0.497353, 0.48674, 0.2811, 0.498624, 0.540937, 0.531017, 0.091667, 0.383215, 0.02486, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.388397, 0.026556, 0.058672, 0.058672, 0.286844, 0.388105, 0.836147, 0.894061, 0.020365, 0.038636, 0.04328, 9.6e-05, 3.6e-05, 0.736896, 0.078947, 0.067079, 0.010222, 0.641937, 0.125461, 0.022179, 0.021381, 0.023637, 0.044408, 0.366667, 0.021041, 0.023001, 0.466258, 0.039216]
Iteration 16: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.75034, 0.083333, 0.067961, 0.076964, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 16: Full valset pareto front score: 0.2575649791666666
Iteration 16: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6}, {3}, {0, 1, 2, 3, 4, 5, 6}, {3, 5, 6}, {5}, {1, 3, 4}, {0, 1, 2, 3, 4, 5, 6}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6}, {3}, {6}, {1, 3, 4, 5, 6}, {1, 3, 4, 5, 6}, {1}, {6}, {4}, {6}, {4}, {1, 3, 4, 5, 6}, {1, 3, 4, 5}, {4}, {4}, {4}, {1, 4, 5}, {1, 4}, {4}, {6}, {1, 3, 4, 5, 6}, {4}, {4}, {4}, {1, 4, 5}, {3, 5, 6}, {4}, {4}, {1, 3, 4, 5, 6}, {1, 3, 4}]
Iteration 16: Best valset aggregate score so far: 0.2510750416666666
Iteration 16: Best program as per aggregate score on train_val: 4
Iteration 16: Best program as per aggregate score on valset: 4
Iteration 16: Best score on valset: 0.2510750416666666
Iteration 16: Best score on train_val: 0.2510750416666666
Iteration 16: Linear pareto front program index: 4
Iteration 16: New program candidate index: 6
Iteration 17: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyc4x5d23.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxm3v_yrt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnflsnk2b.pickle

Iteration 17: Proposed new text for program: # Metadata dictionaries
m_key_frequency = dict()      # Tracks access frequency
m_key_last_access = dict()    # Tracks absolute access time (logical time)
m_ghost_registry = dict()     # Tracks keys of "valuable" items recently evicted

# Constants
GHOST_CAPACITY = 2000         # Max size for ghost registry
PROTECTED_BOOST = 10000.0     # Score boost for items with Freq > 1

def evict(cache_snapshot, obj):
    '''
    Evicts based on a Segmented Hyperbolic Score.
    
    Logic:
    1. Items with Frequency 1 (Probationary) are evicted first, using LRU logic.
    2. Items with Frequency > 1 (Protected) are evicted only if no Freq 1 items exist.
    3. Within the Protected tier, items are ranked by (Frequency / Age).
    '''
    global m_key_frequency, m_key_last_access, PROTECTED_BOOST
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate Age (Time since last access)
        # We add 1 to avoid division by zero and smooth very recent accesses
        age = (current_time - last_access) + 1.0
        
        if freq == 1:
            # Probationary Segment:
            # Score is purely based on recency (Younger = Higher Score).
            # As Age increases, Score -> 0.
            # Max possible score here is 1.0 (Age 1).
            score = 1.0 / age
        else:
            # Protected Segment:
            # Give a massive boost so these are considered "better" than any Probationary item.
            # Within this tier, we use Hyperbolic Caching:
            # - Higher Frequency = Higher Score
            # - Higher Age = Lower Score
            hyperbolic_val = freq / age
            score = PROTECTED_BOOST + hyperbolic_val
        
        # We evict the item with the LOWEST score
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update recency.
    '''
    global m_key_frequency, m_key_last_access
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment frequency
    # We allow frequency to grow naturally to distinguish heavy hitters in the Protected tier
    curr_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata for new items.
    Check Ghost Registry to see if this is a returning "Protected" item.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost_registry:
        # It was valuable before! Promote immediately to Protected tier.
        # This prevents it from being evicted by the Probationary LRU cleaner.
        m_key_frequency[obj.key] = 2
        del m_ghost_registry[obj.key]
    else:
        # Standard insertion: Start in Probationary tier.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    Only add to Ghost Registry if the item had proven value (Freq > 1).
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, GHOST_CAPACITY
    
    evicted_key = evicted_obj.key
    freq = m_key_frequency.get(evicted_key, 1)
    
    # Smart Ghosting:
    # If Freq was 1, it was likely "Scan" noise. Don't ghost it.
    # If Freq > 1, it was part of our working set but we ran out of space. Ghost it.
    if freq > 1:
        m_ghost_registry[evicted_key] = True
        
        # Manage Ghost Registry Size (Simple FIFO if full)
        if len(m_ghost_registry) > GHOST_CAPACITY:
            # Removing an arbitrary element (first available in iterator) is O(1) in Python dicts
            # This keeps overhead low.
            oldest_ghost = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost]

    # Clean up active metadata
    if evicted_key in m_key_frequency:
        del m_key_frequency[evicted_key]
    if evicted_key in m_key_last_access:
        del m_key_last_access[evicted_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq3b3t_1e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpggmaw1sh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpic9zhdkg.pickle

Iteration 17: New subsample score 0.680506 is not better than old score 0.794856, skipping
Iteration 18: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfv7brrwz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxakse6i9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcqez4936.pickle

Iteration 18: Proposed new text for program: import collections

# Global State for the Adaptive Algorithm
# T1: Recently accessed once (LRU Cache part) - stores keys
_t1 = collections.OrderedDict()
# T2: Frequently accessed (LFU/LRU Cache part) - stores keys
_t2 = collections.OrderedDict()
# B1: Ghost entries for T1 (evicted from T1) - stores keys
_b1 = collections.OrderedDict()
# B2: Ghost entries for T2 (evicted from T2) - stores keys
_b2 = collections.OrderedDict()

# Adaptation parameter: Target size for T1
_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction logic.
    We don't calculate a score for every item. Instead, we determine WHICH list (T1 or T2)
    must sacrifice an item to maintain the constraints relative to target _p.
    '''
    global _t1, _t2, _b1, _b2, _p
    
    # We must return a key to be evicted from the actual cache.
    # The actual cache content is the union of keys in _t1 and _t2.
    
    # In a standard ARC implementation, the "replace" logic runs before insertion.
    # Here, the framework calls `evict` when the cache IS full.
    # We need to decide whether to kill the LRU of T1 or the LRU of T2.
    
    t1_len = len(_t1)
    
    # Logic to decide victim:
    # If len(T1) > p, we usually evict from T1.
    # However, we must ensure T1 isn't empty if p > 0.
    
    candidate_key = None
    
    # Case 1: T1 is larger than the target `p`. 
    # We evict the LRU of T1.
    if t1_len > 0 and (t1_len > _p or (_b2 and t1_len == _p)):
        # popitem(last=False) pops the FIRST item (FIFO order = LRU)
        candidate_key, _ = _t1.popitem(last=False)
        # Move to B1 (Ghost Recent)
        _b1[candidate_key] = True
    else:
        # Case 2: We evict from T2.
        if _t2:
            candidate_key, _ = _t2.popitem(last=False)
            # Move to B2 (Ghost Frequent)
            _b2[candidate_key] = True
        else:
            # Fallback if T2 is empty (rare, but possible if p=capacity)
            if _t1:
                candidate_key, _ = _t1.popitem(last=False)
                _b1[candidate_key] = True

    # Maintenance: Keep ghost lists bounded to Cache Capacity (C)
    capacity = cache_snapshot.capacity # Note: ARC usually defines C as element count, here it's generic capacity
    # Assuming capacity is roughly number of items for this logic, or just a safe upper bound.
    # To be safe against memory leaks, we restrict B1 and B2 length arbitrarily to the current cache count * 2
    max_ghosts = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 1000
    
    while len(_b1) > max_ghosts:
        _b1.popitem(last=False)
    while len(_b2) > max_ghosts:
        _b2.popitem(last=False)

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it is in T1, move to T2 (MRU).
    If it is in T2, move to T2 (MRU).
    '''
    global _t1, _t2
    
    key = obj.key
    
    if key in _t1:
        # Was in Recent, now it's Frequent. Move T1 -> T2.
        del _t1[key]
        _t2[key] = True # Add to MRU end
    elif key in _t2:
        # Already Frequent. Refresh position in T2 (MRU).
        del _t2[key]
        _t2[key] = True
    else:
        # Edge case: It's in the snapshot cache but our tracking lost it 
        # (shouldn't happen in strict ARC, but possible in distributed/external state).
        # Treat as new frequent.
        _t2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was in Ghost lists (B1 or B2) to adjust parameter P.
    Then add to T1 (MRU).
    '''
    global _t1, _t2, _b1, _b2, _p
    
    key = obj.key
    capacity = cache_snapshot.capacity 
    # Since capacity is in bytes, we estimate element count C roughly by current count
    # However, ARC logic relies on 'C' being the count limit. 
    # We will treat the current cache length as the effective size for the ratio.
    current_cache_size = len(cache_snapshot.cache)
    # Avoid div by zero
    if current_cache_size == 0: current_cache_size = 1 
    
    # 1. ADAPTATION (Ghost Hits)
    if key in _b1:
        # Hit in Ghost Recent (B1). We made T1 too small. Increase p.
        delta = 1
        if len(_b1) >= len(_b2) and len(_b2) > 0:
            delta = 1
        elif len(_b2) > len(_b1):
            delta = len(_b2) / len(_b1)
        
        _p = min(current_cache_size, _p + delta)
        del _b1[key]
        
        # It's a "hit" in history, so it goes to T2
        _t2[key] = True
        return

    elif key in _b2:
        # Hit in Ghost Frequent (B2). We made T1 too big (T2 too small). Decrease p.
        delta = 1
        if len(_b2) >= len(_b1) and len(_b1) > 0:
            delta = 1
        elif len(_b1) > len(_b2):
            delta = len(_b1) / len(_b2)
            
        _p = max(0, _p - delta)
        del _b2[key]
        
        # It's a "hit" in history, so it goes to T2
        _t2[key] = True
        return

    # 2. NEW INSERTION (Pure Miss)
    # Add to T1 (MRU)
    _t1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    The `evict` function already handled moving keys from T1/T2 to B1/B2.
    This function is primarily for cleanup if the framework forces an eviction
    different from what we suggested (though usually it follows `evict`).
    '''
    # We rely on `evict` having done the list manipulation.
    # However, to be robust: ensure the evicted object is actually gone from T1/T2
    # and present in B1/B2.
    global _t1, _t2, _b1, _b2
    
    key = evicted_obj.key
    
    # Ensure removed from active lists
    if key in _t1:
        del _t1[key]
        # If we didn't move it to ghost in `evict` (unexpected), do it here
        if key not in _b1: _b1[key] = True
            
    if key in _t2:
        del _t2[key]
        if key not in _b2: _b2[key] = True
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7qro4ix3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv8uk4__v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1tlfkaqb.pickle

Iteration 18: New subsample score 0.923455 is not better than old score 0.93394, skipping
Iteration 19: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppmz21p7m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeolm9uwu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmns0cw8f.pickle

Iteration 19: Proposed new text for program: import random

# Metadata
# Tracks estimated frequency. We do NOT delete keys immediately upon eviction
# to retain history for "Admission" logic.
m_sketch_freq = dict()

# Tracks insertion order/recency for LRU approximation
m_lru_map = dict()

# Constants for frequency aging
m_access_counter = 0
RESET_INTERVAL = 1000  # Halve frequencies every N accesses to handle shifts in popularity

def _get_freq(key):
    return m_sketch_freq.get(key, 0)

def _aging_process():
    '''
    Periodically halves all frequencies to ensure that
    formerly popular items eventually decay (Aging).
    '''
    global m_sketch_freq
    # Create a new dict to remove 0 entries after division
    new_freq = {}
    for k, v in m_sketch_freq.items():
        new_v = v // 2
        if new_v > 0:
            new_freq[k] = new_v
    m_sketch_freq = new_freq

def evict(cache_snapshot, obj):
    '''
    Implements a simplified TinyLFU-style eviction.
    1. Select a set of random candidates (to avoid iterating the whole cache).
    2. Among candidates, find the one with the Lowest Frequency.
    3. If frequencies match, use LRU (simulated by insertion order/recency).
    
    This approximates LFU without the heavy sorting overhead and utilizes
    historical data for better decision making.
    '''
    # Optimizing for speed and efficacy: Random Sampling LFU
    # Instead of iterating the whole list, we sample K items.
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
        
    # Sample size: usually 5-10 is statistically sufficient to approximate global LFU
    sample_size = min(len(candidate_keys), 8)
    sample_keys = random.sample(candidate_keys, sample_size)
    
    victim = None
    min_freq = float('inf')
    min_lru = float('inf')

    for key in sample_keys:
        freq = _get_freq(key)
        lru = m_lru_map.get(key, 0)
        
        # We look for the item with the smallest frequency
        if freq < min_freq:
            victim = key
            min_freq = freq
            min_lru = lru
        elif freq == min_freq:
            # Tie-breaker: LRU (smallest timestamp is oldest)
            if lru < min_lru:
                victim = key
                min_lru = lru

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update recency.
    Check for aging trigger.
    '''
    global m_access_counter, m_sketch_freq, m_lru_map
    
    m_access_counter += 1
    
    # Update Sketch Frequency
    curr = m_sketch_freq.get(obj.key, 0)
    # Cap frequency to prevent integer overflow or excessive weight
    if curr < 15: 
        m_sketch_freq[obj.key] = curr + 1
        
    # Update Recency
    m_lru_map[obj.key] = cache_snapshot.access_count

    # Aging mechanism
    if m_access_counter >= RESET_INTERVAL:
        _aging_process()
        m_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize frequency (or increment if history exists) and set recency.
    '''
    global m_access_counter, m_sketch_freq, m_lru_map
    
    m_access_counter += 1

    # In standard LFU, new items start at 1. 
    # Here, if we have seen it before (and evicted it), it retains some 'heat'.
    # If it's totally new, it starts at 1.
    curr = m_sketch_freq.get(obj.key, 0)
    if curr < 15:
        m_sketch_freq[obj.key] = curr + 1
    
    m_lru_map[obj.key] = cache_snapshot.access_count

    # Aging mechanism
    if m_access_counter >= RESET_INTERVAL:
        _aging_process()
        m_access_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Crucial change: We do NOT remove frequency data for the evicted object immediately.
    This allows the algorithm to remember "Ghost" items that are thrashing in and out.
    We only remove the recency data to keep the LRU map clean for current cache items.
    
    To prevent memory leaks, the _aging_process implicitly handles cleanup of low-frequency ghosts.
    '''
    global m_lru_map
    
    # We maintain m_sketch_freq[evicted_obj.key] deliberately.
    
    # We clean up the LRU map because the object is no longer in cache
    if evicted_obj.key in m_lru_map:
        del m_lru_map[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuzo2t_kk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt6nl8j1r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxum50z5a.pickle

Iteration 19: New subsample score 0.912304 is better than old score 0.800386. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdpfe3p4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp13rd3noj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6gm6uq_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphh5y4qep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmdpygakv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph7e50q7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcya2v286.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp46wn6ln8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqts2in7f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxhth8utd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd6l0uvq4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp53uujt7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe5j4fdne.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1cvhtqva.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa8ib6kk8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp09e368lo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp88pkgstc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpevb197_l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp91j56et.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjzke2e65.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxgs_kwou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp57bpz1ty.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc91yio16.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptl9th_9x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp8r4gct8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq_1z5mad.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpscy97l_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd3yphgkw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxk42o3q_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp45bjp54a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1nrupcal.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyg4rtl99.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbuvwodcc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgg8x0g2f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0hiy0vhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpby_bdben.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6jobsvn7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpysostsub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu1oaieyp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpydwwiqeu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo0zq0t7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6c35wzfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkv4vmcek.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsq8ueawg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4hlxpzdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi9hgvko_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppqdxiapj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpio62vn_k.pickle

Iteration 19: Full valset score for new program: 0.2305623541666666
Iteration 19: Full train_val score for new program: 0.2305623541666666
Iteration 19: Individual valset scores for new program: [0.451033, 0.431123, 0.437428, 0.403841, 0.445473, 0.440044, 0.272727, 0.498624, 0.539864, 0.531017, 0.091667, 0.37611, 0.023893, 0.0, 0.018689, 0.018879, 0.018446, 0.021507, 0.020953, 0.272227, 0.381514, 0.024988, 0.057382, 0.057382, 0.276879, 0.346774, 0.816286, 0.88345, 0.029051, 0.038636, 0.045558, 3.6e-05, 0.0, 0.749489, 0.083333, 0.067961, 0.014718, 0.639361, 0.125461, 0.030219, 0.032376, 0.030125, 0.052632, 0.366667, 0.027064, 0.031417, 0.466258, 0.078431]
Iteration 19: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.75034, 0.083333, 0.067961, 0.076964, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 19: Full valset pareto front score: 0.2575649791666666
Iteration 19: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7}, {3}, {0, 1, 2, 3, 4, 5, 6, 7}, {3, 5, 6, 7}, {5}, {1, 3, 4}, {0, 1, 2, 3, 4, 5, 6, 7}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7}, {3}, {6}, {1, 3, 4, 5, 6}, {1, 3, 4, 5, 6}, {1}, {6}, {4}, {6}, {4}, {1, 3, 4, 5, 6, 7}, {1, 3, 4, 5, 7}, {4}, {4}, {4}, {1, 4, 5, 7}, {1, 4, 7}, {4}, {6}, {1, 3, 4, 5, 6, 7}, {4}, {4}, {4}, {1, 4, 5, 7}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7}, {1, 3, 4}]
Iteration 19: Best valset aggregate score so far: 0.2510750416666666
Iteration 19: Best program as per aggregate score on train_val: 4
Iteration 19: Best program as per aggregate score on valset: 4
Iteration 19: Best score on valset: 0.2510750416666666
Iteration 19: Best score on train_val: 0.2510750416666666
Iteration 19: Linear pareto front program index: 4
Iteration 19: New program candidate index: 7
Iteration 20: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7keoe1t6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpurt6005i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkljyf2u5.pickle

Iteration 20: Proposed new text for program: # Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Constants
MAX_FREQ = 50           # Cap frequency to prevent historical domination
PROTECTED_THRESHOLD = 1 # Items with freq > this are "Protected"
CLEANUP_THRESHOLD_RATIO = 5 # Clean metadata when it exceeds capacity * this

def evict(cache_snapshot, obj):
    '''
    Evicts based on Segmented LRU (SLRU).
    We divide the cache into "Probation" (low freq) and "Protected" (high freq).
    We prefer to evict LRU items from Probation. 
    If Probation is empty, we evict LRU from Protected.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Step 1: Identify the victim
    victim_key = None
    victim_time = float('inf')
    
    # We first try to find a victim in the Probationary segment (Freq <= 1)
    probation_victim_key = None
    probation_victim_time = float('inf')
    
    # We also track the LRU of the Protected segment as a fallback
    protected_victim_key = None
    protected_victim_time = float('inf')
    
    found_probationary = False

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        if freq <= PROTECTED_THRESHOLD:
            # Item is in Probation
            found_probationary = True
            if last_access < probation_victim_time:
                probation_victim_time = last_access
                probation_victim_key = key
        else:
            # Item is Protected
            if last_access < protected_victim_time:
                protected_victim_time = last_access
                protected_victim_key = key
    
    # Policy: Evict Probationary LRU if available, otherwise Protected LRU
    if found_probationary and probation_victim_key is not None:
        return probation_victim_key
    else:
        return protected_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    # We promote items on hits, pushing them into the Protected segment
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Handle Ghost entries with decay.
    '''
    global m_key_frequency, m_key_last_access
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic (Decaying History):
    # If the key is in metadata, it's a "Ghost". 
    # We restore it, but with a penalty (decay).
    # This ensures looping items (Freq 1) reset to 0/1 (Probation),
    # while Hot items (Freq 10) restore to ~5 (Protected).
    if obj.key in m_key_frequency:
        old_freq = m_key_frequency[obj.key]
        # Decay factor: Halve the frequency. 
        # Ensure at least 0 (or 1 effectively since it's an insert)
        new_freq = old_freq // 2
        # If it was previously 0 or 1, it stays low. If it was 10, it becomes 5.
        m_key_frequency[obj.key] = max(0, new_freq)
    else:
        # Brand new item. Start with frequency 0 (Probation).
        # It needs a Hit to reach frequency 1 or 2.
        m_key_frequency[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Manage metadata cleanup to prevent leaks.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately keep metadata for the evicted object (Ghost Cache).
    
    # Periodic Cleanup Strategy:
    # If metadata grows too large relative to cache capacity, purge old entries.
    # This prevents memory leaks in long-running simulations.
    capacity = cache_snapshot.capacity # Note: capacity is in bytes, but usually proportional to count in these problems
    # Using a heuristic for cleanup trigger based on dictionary size
    if len(m_key_frequency) > cache_snapshot.size * CLEANUP_THRESHOLD_RATIO and len(m_key_frequency) > 1000:
        
        # We need to remove old ghosts.
        # Calculate a cutoff time. Remove items accessed long ago that are NOT in cache.
        # To avoid iterating everything, we can just remove a batch of random keys or
        # do a full sweep. A full sweep is safer for correctness.
        
        current_time = cache_snapshot.access_count
        # Sort keys by access time (approximate LRU of metadata)
        # Only keep the most recent N items in metadata
        
        # Optimization: Just remove keys that haven't been seen in a long time
        # Threshold: keys older than 2 * the time it takes to fill the cache?
        # Simpler: Just keep the top X most recent keys.
        
        target_size = int(cache_snapshot.size * 2) 
        if target_size < 100: target_size = 100 # Safety floor
        
        # Get the keys sorted by access time (oldest first)
        sorted_keys = sorted(m_key_last_access.keys(), key=lambda k: m_key_last_access[k])
        
        # Identify keys to delete (oldest ones), but ensure we don't delete keys currently in cache
        keys_in_cache = set(cache_snapshot.cache.keys())
        deleted_count = 0
        num_to_delete = len(m_key_frequency) - target_size
        
        for k in sorted_keys:
            if deleted_count >= num_to_delete:
                break
            if k not in keys_in_cache:
                del m_key_frequency[k]
                del m_key_last_access[k]
                deleted_count += 1
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph0u__cbv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdjq40uew.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6krqrpj3.pickle

Iteration 20: New subsample score 1.095925 is not better than old score 1.132082, skipping
Iteration 21: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph23pg3jt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp35jn4fil.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx4p4u8uo.pickle

Iteration 21: Proposed new text for program: # Metadata dictionaries
# Maps key -> {'freq': int, 'last': int}
m_metadata = dict() 
# Maps key -> int (frequency at eviction). Used to detect thrashing/loops.
m_ghosts = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Size-Aware Hyperbolic Score.
    
    Score Formula: (Frequency ^ 1.5) / (Size * Age)
    
    Why this works:
    1. Maximizes Hit Rate by penalizing Size: Evicting one large object makes room 
       for multiple small objects.
    2. Adaptive Aging: The 'Age' denominator naturally decays the score of items 
       that haven't been accessed recently, transitioning from LFU to LRU behavior automatically.
    3. Frequency Bias: The 1.5 exponent protects the 'hot set' (Trace 27) from being 
       flushed by temporary scans.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        meta = m_metadata.get(key)
        
        # Defensive check against desync, though unlikely
        if not meta:
            continue
            
        freq = meta['freq']
        last_access = meta['last']
        
        # Calculate Age (Time since last access). 
        # Add 1 to avoid division by zero (if eviction happens in same tick as insert)
        age = (current_time - last_access) + 1
        
        # Get object size from snapshot
        obj_size = cache_snapshot.cache[key].size
        
        # Calculate Score
        # We want to EVICT the item with the LOWEST score.
        # High Freq -> High Score (Keep)
        # Small Size -> High Score (Keep)
        # Low Age (Recent) -> High Score (Keep)
        score = (freq ** 1.5) / (obj_size * age)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update last access time.
    '''
    global m_metadata
    
    if obj.key in m_metadata:
        m_metadata[obj.key]['freq'] += 1
        m_metadata[obj.key]['last'] = cache_snapshot.access_count
    else:
        # Fallback for data integrity
        m_metadata[obj.key] = {'freq': 1, 'last': cache_snapshot.access_count}

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata for new object.
    Leverage Ghost Registry to detect returning items (Loop/Thrash protection).
    '''
    global m_metadata, m_ghosts
    
    current_time = cache_snapshot.access_count
    
    # Determine initial frequency
    initial_freq = 1
    
    # If key is in ghost registry, it means it was evicted recently.
    # Restore its "heat" so it isn't immediately evicted again.
    if obj.key in m_ghosts:
        initial_freq = m_ghosts[obj.key] + 1
        del m_ghosts[obj.key]
    
    m_metadata[obj.key] = {
        'freq': initial_freq,
        'last': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up active metadata and move vital stats to ghost registry.
    '''
    global m_metadata, m_ghosts
    
    if evicted_obj.key in m_metadata:
        # Save frequency to ghost registry
        freq = m_metadata[evicted_obj.key]['freq']
        del m_metadata[evicted_obj.key]
        
        m_ghosts[evicted_obj.key] = freq
        
        # Maintain Ghost Registry Size
        # A limit of 2000 is a heuristic to balance memory vs history.
        # Python dicts are ordered by insertion, so this acts as FIFO.
        if len(m_ghosts) > 2000:
            oldest_key = next(iter(m_ghosts))
            del m_ghosts[oldest_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuke5msie.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_u09mgbp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6b9zq7vh.pickle

Iteration 21: New subsample score 0.934003 is not better than old score 0.957743, skipping
Iteration 22: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwvlrjv_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkmhrwluf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp38hcgojb.pickle

Iteration 22: Proposed new text for program: # Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjd1kosgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps4d917b9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptyncozq_.pickle

Iteration 22: New subsample score 0.53198 is better than old score 0.477706. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv_aom7hw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiq3gn13w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpne3mt9cf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpze7s43xf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9fm33_k9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx5cih_g0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1bqzxty6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy0qul4ou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1yiod193.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt0prpcmz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprw376ur0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwbf08c59.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7vg4a64v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmq3hhrpr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp07pi7ina.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_foxhyj1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp63fw5h8i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpah58s1dx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy8a5rc8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4hs4rrbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphm5tx3is.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1ouv4dmk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpllv8745m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5anned13.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbni6b774.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp54uodhs_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8o7_jngt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjryzz0rr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk1qkap8s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprui4mpvv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7tv74ev6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8n6ta8mp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphtkdw77h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9wdojfkv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0l_9yl4d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ss3jbaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk7siw9lv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkvljurgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8p_w79vu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplr9j7po4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo49s83ck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphaiqpmku.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdm_zitcs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpud6ime7i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ghy1lj3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ofww7sz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsls34ct_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl6yqo0fb.pickle

Iteration 22: Full valset score for new program: 0.2443585416666667
Iteration 22: Full train_val score for new program: 0.2443585416666667
Iteration 22: Individual valset scores for new program: [0.508125, 0.483073, 0.491061, 0.441362, 0.50575, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.333925, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.269799, 0.373992, 0.849057, 0.891494, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.640392, 0.125461, 0.066815, 0.092242, 0.057161, 0.052632, 0.316667, 0.093314, 0.061711, 0.466258, 0.081699]
Iteration 22: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 22: Full valset pareto front score: 0.25813520833333325
Iteration 22: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {3, 5, 6, 7}, {5}, {8, 1, 3, 4}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8}, {3}, {6}, {1, 3, 4, 5, 6, 8}, {1, 3, 4, 5, 6, 8}, {1}, {6}, {4}, {6}, {8}, {1, 3, 4, 5, 6, 7, 8}, {1, 3, 4, 5, 7, 8}, {8, 4}, {8, 4}, {8}, {1, 4, 5, 7, 8}, {8, 1, 4, 7}, {8}, {6}, {1, 3, 4, 5, 6, 7, 8}, {4}, {4}, {4}, {1, 4, 5, 7, 8}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8}, {8, 1, 3, 4}]
Iteration 22: Best valset aggregate score so far: 0.2510750416666666
Iteration 22: Best program as per aggregate score on train_val: 4
Iteration 22: Best program as per aggregate score on valset: 4
Iteration 22: Best score on valset: 0.2510750416666666
Iteration 22: Best score on train_val: 0.2510750416666666
Iteration 22: Linear pareto front program index: 4
Iteration 22: New program candidate index: 8
Iteration 23: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqstd59wf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwy249erq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphh6ax4on.pickle

Iteration 23: Proposed new text for program: import math

# Global Metadata
# Tracks the frequency of keys (including "ghost" keys that were evicted)
# This serves as our long-term memory.
g_frequency_map = {}

# Tracks the current priority (GDSF score) of items currently IN the cache.
g_priority_map = {}

# The "Inflation Value" (L).
# This acts as a logical clock that ratchets up as items are evicted.
g_inflation_L = 0.0

# Configuration
MAX_HISTORY_SIZE = 50000 # Max size for metadata to prevent leaks

def _calculate_priority(freq, size):
    '''
    Calculates the GDSF priority.
    Score = L + (Frequency / Cost)
    Where Cost is a function of size.
    We use log(size) to favor small items (improving hit rate) 
    without completely starving large items.
    '''
    global g_inflation_L
    # Logarithmic size penalty is smoother than linear size penalty.
    # We add 1 to size to avoid div by zero or log(0).
    size_cost = math.log2(size + 2.0) 
    if size_cost < 1.0: size_cost = 1.0
    
    return g_inflation_L + (freq / size_cost)

def _cleanup_metadata(cache_snapshot):
    '''
    Keeps the frequency map from growing indefinitely.
    '''
    global g_frequency_map
    
    if len(g_frequency_map) > MAX_HISTORY_SIZE:
        # If we exceed history limit, we prune.
        # We want to remove keys that are NOT in the cache and have LOW frequency.
        
        # Identify keys in cache (set lookup is fast)
        cached_keys = set(cache_snapshot.cache.keys())
        
        # Collect victims: keys not in cache.
        # We sort roughly by frequency to remove the least useful history.
        # Limiting the sort to a subset or threshold is faster, but for Python sim
        # a simple list comprehension filtering is usually okay up to 50k items.
        
        keys_to_purge = []
        count = 0
        
        # First pass: try to remove items with frequency 1 (noise)
        for k, v in g_frequency_map.items():
            if k not in cached_keys and v <= 1:
                keys_to_purge.append(k)
                count += 1
                if count >= 5000: break # Purge in chunks
        
        # Execute purge
        for k in keys_to_purge:
            del g_frequency_map[k]
            
        # Hard cap fallback: if still too big, random/arbitrary deletion of non-cached
        if len(g_frequency_map) > MAX_HISTORY_SIZE:
             for k in list(g_frequency_map.keys()):
                if k not in cached_keys:
                    del g_frequency_map[k]
                    if len(g_frequency_map) <= MAX_HISTORY_SIZE * 0.9:
                        break

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the object with the smallest Priority value.
    Updates the global inflation parameter L to that minimum value.
    '''
    global g_priority_map, g_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority score.
    # While O(N), this allows perfect accuracy for the algorithm.
    victim_key = None
    min_priority = float('inf')
    
    # Optimization: If the cache is synced with g_priority_map, 
    # we iterate over that instead of the snapshot keys for potentially faster access.
    
    for key in candidate_keys:
        p = g_priority_map.get(key, -1.0)
        
        # Safety fallback if key missing from priority map
        if p == -1.0:
            p = 0.0
            
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # CRITICAL STEP in GDSF:
    # The system "ages" by updating L to the priority of the evicted object.
    # All future inserted objects will start at this new, higher L baseline.
    if victim_key is not None:
        g_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. "Refresh" the priority.
       The object is brought up to the CURRENT L value + its frequency weight.
       This resets its aging clock, protecting it from immediate eviction.
    '''
    global g_frequency_map, g_priority_map, g_inflation_L
    
    key = obj.key
    
    # Update Frequency
    curr_freq = g_frequency_map.get(key, 0)
    g_frequency_map[key] = curr_freq + 1
    
    # Update Priority
    # Note: We use the *current* g_inflation_L. This effectively brings the
    # item to the "front" of the recency queue relative to the aging factor.
    g_priority_map[key] = _calculate_priority(g_frequency_map[key], obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Cache (History).
    2. Calculate initial priority based on CURRENT L.
    '''
    global g_frequency_map, g_priority_map, g_inflation_L
    
    key = obj.key
    
    # Frequency handling
    if key in g_frequency_map:
        # Ghost Hit! It was here before.
        # We do NOT reset frequency. We accumulate it.
        # This helps detect working sets that are slightly larger than cache capacity.
        g_frequency_map[key] += 1
    else:
        # Brand new item
        g_frequency_map[key] = 1
        
    # Calculate Priority
    # New items start at the current inflation floor (L) plus their inherent value.
    g_priority_map[key] = _calculate_priority(g_frequency_map[key], obj.size)
    
    # Maintenance
    if len(g_frequency_map) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from current priority map (it's no longer in cache).
    2. Keep in frequency map (Ghost Cache) for future history.
    '''
    global g_priority_map
    
    # The eviction logic in evict() already updated g_inflation_L.
    # Here we just clean up the "In-Cache" metadata.
    if evicted_obj.key in g_priority_map:
        del g_priority_map[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5xprmae4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4h3joh_o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy1wzvai8.pickle

Iteration 23: New subsample score 1.039707 is not better than old score 1.056725, skipping
Iteration 24: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz_voi3f6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8htfzym6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1xvyvt1a.pickle

Iteration 24: Proposed new text for program: # Metadata dictionaries
# m_metadata stores active cache info: Key -> {'freq': int, 'priority': float, 'last_access': int}
m_metadata = dict()
# m_ghost_registry stores frequency history for evicted keys: Key -> int (frequency)
m_ghost_registry = dict()
# g_inflation_L is the "Clock" or "Aging" factor for the GDSF algorithm
g_inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority Score (GDSF).
    Score = Inflation_L + (Frequency / Size)
    Tie-breaker: Least Recently Used (LRU)
    '''
    global m_metadata, g_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')

    # Iterate to find the candidate with the lowest priority score
    for key in candidate_keys:
        # Retrieve metadata safely
        meta = m_metadata.get(key)
        
        if not meta:
            # Fallback for safety (should ideally not happen)
            priority = -1.0
            last_access = 0
        else:
            priority = meta['priority']
            last_access = meta['last_access']

        # Selection Logic: Minimize Priority
        if priority < min_priority:
            min_priority = priority
            min_last_access = last_access
            victim_key = key
        elif priority == min_priority:
            # Tie-breaker: If priorities are equal, use LRU (smaller timestamp is older)
            if last_access < min_last_access:
                min_last_access = last_access
                victim_key = key
    
    # GDSF Aging Mechanism:
    # Update the global inflation L to the priority of the evicted item.
    # This raises the bar for all future insertions/updates, effectively aging out 
    # items that haven't been accessed recently enough to justify their frequency.
    if victim_key and min_priority != float('inf') and min_priority > g_inflation_L:
        g_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and re-calculate priority using current L.
    This effectively "brings the item to the present".
    '''
    global m_metadata, g_inflation_L
    
    current_time = cache_snapshot.access_count
    key = obj.key
    # Ensure size is at least 1 to avoid division by zero
    size = max(1, obj.size) 
    
    if key not in m_metadata:
        freq = 1
    else:
        freq = m_metadata[key]['freq'] + 1
    
    # GDSF Priority Calculation
    # New Priority = Current_Inflation + (Frequency / Size)
    priority = g_inflation_L + (freq / size)
    
    m_metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency from Ghost if available.
    '''
    global m_metadata, m_ghost_registry, g_inflation_L
    
    current_time = cache_snapshot.access_count
    key = obj.key
    size = max(1, obj.size)
    
    # Check Ghost Registry (Admission Control / History)
    if key in m_ghost_registry:
        # Object has been seen before; restore its frequency to give it a fighting chance
        freq = m_ghost_registry[key]
        # Cleanup ghost entry as it is now active
        del m_ghost_registry[key]
    else:
        # Truly new object
        freq = 1

    # Calculate initial priority
    priority = g_inflation_L + (freq / size)
    
    m_metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move vital statistics (Frequency) to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    key = evicted_obj.key
    
    # Preserve frequency in ghost registry
    if key in m_metadata:
        m_ghost_registry[key] = m_metadata[key]['freq']
        del m_metadata[key]
        
    # Cap the Ghost Registry size to prevent memory leaks
    # Using a simple FIFO eviction for the ghost registry if it gets too large
    if len(m_ghost_registry) > 2000:
        # Remove the oldest inserted key in the ghost registry
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy0l4rbmc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk8n0mdba.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphgbl8iaq.pickle

Iteration 24: New subsample score 0.30179 is not better than old score 0.476326, skipping
Iteration 25: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3x8dv7bv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpujkfqquu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmopv8ycx.pickle

Iteration 25: Proposed new text for program: # Metadata dictionaries to store state efficiently
m_freq = {}          # Key -> Frequency (int)
m_priority = {}      # Key -> Priority Score (float)
m_last_access = {}   # Key -> Access Count (int) - for LRU tie-breaking
m_ghosts = {}        # Key -> Frequency (int) - History of evicted items

# Global state for LFU-DA
m_L = 0.0            # Dynamic Aging Factor (Value of the last evicted item's priority)
GHOST_LIMIT = 2000   # Max size for ghost registry to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority score (LFU-DA).
    Tie-breakers: 
    1. Size (Desc): Evict larger items to free more byte-space.
    2. Recency (Asc): Evict Least Recently Used.
    Updates the global aging factor m_L.
    '''
    global m_priority, m_last_access, m_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    victim_key = None
    
    # We want to minimize Priority
    min_priority = float('inf')
    
    # Track tie-breaker attributes for the current best candidate
    cand_priority = float('inf')
    cand_size = -1
    cand_last_access = float('inf')

    # Single pass scan to find the best victim
    for key in candidate_keys:
        # Retrieve metadata (default to 0 if missing to prevent errors)
        p = m_priority.get(key, 0)
        last_acc = m_last_access.get(key, 0)
        size = cache_snapshot.cache[key].size
        
        # Comparison Logic
        if p < cand_priority:
            # Found a new minimum priority
            victim_key = key
            cand_priority = p
            cand_size = size
            cand_last_access = last_acc
        elif p == cand_priority:
            # Priority Tie: Prefer evicting LARGER items
            # This is a safe heuristic: if cache is byte-limited, this is optimal.
            # If cache is count-limited, size doesn't matter, so it's neutral.
            if size > cand_size:
                victim_key = key
                cand_priority = p
                cand_size = size
                cand_last_access = last_acc
            elif size == cand_size:
                # Size Tie: Prefer evicting OLDER items (LRU)
                if last_acc < cand_last_access:
                    victim_key = key
                    cand_priority = p
                    cand_size = size
                    cand_last_access = last_acc

    # LFU-DA Aging: Update L to the priority of the victim
    if victim_key is not None:
        m_L = cand_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and recalculate Priority = Freq + L
    '''
    global m_freq, m_priority, m_last_access, m_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Access Time
    m_last_access[key] = current_time
    
    # Update Frequency
    new_freq = m_freq.get(key, 0) + 1
    m_freq[key] = new_freq
    
    # Update Priority (LFU-DA formula)
    m_priority[key] = new_freq + m_L

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency if in ghost registry.
    Priority = Initial_Freq + L
    '''
    global m_freq, m_priority, m_last_access, m_ghosts, m_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Check Ghost Registry for frequency restoration
    if key in m_ghosts:
        initial_freq = m_ghosts[key]
        del m_ghosts[key]
    else:
        initial_freq = 1
        
    m_freq[key] = initial_freq
    m_last_access[key] = current_time
    
    # Calculate Priority
    # New items start at L + Freq. Since L is the min priority of the cache (approx),
    # this gives new items a fair chance to survive at least one eviction cycle.
    m_priority[key] = initial_freq + m_L

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move frequency to ghost registry and cleanup active metadata.
    '''
    global m_freq, m_priority, m_last_access, m_ghosts, GHOST_LIMIT
    
    key = evicted_obj.key
    
    # Save frequency to ghost registry
    if key in m_freq:
        m_ghosts[key] = m_freq[key]
        
    # Clean up active metadata
    if key in m_freq: del m_freq[key]
    if key in m_priority: del m_priority[key]
    if key in m_last_access: del m_last_access[key]
    
    # Maintain Ghost Registry size limit (FIFO)
    if len(m_ghosts) > GHOST_LIMIT:
        # dict in Python 3.7+ preserves insertion order, so this removes the oldest inserted ghost
        try:
            oldest_key = next(iter(m_ghosts))
            del m_ghosts[oldest_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpci7f09q5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp87n2cyqf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprjmb5t44.pickle

Iteration 25: New subsample score 1.4061379999999999 is better than old score 1.4015429999999998. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp42z0pcxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9xh945ht.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8nww_seq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpigorpwxd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvyrohn1t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp11t3ay0i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcvnuvzu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1mdjg0r4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoghm51ks.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfg66ldaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzt8tcnoi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiwxviqmc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm4y6h2de.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqipkggo3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw136uc6i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5wvifzce.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvdz28t9n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppc52m21i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpup4mof5c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj9bk4zuv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwe8vvl1e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp9an3s1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzvm_4_yz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpib8an8uf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb9xr_4tm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp67webkbm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbc5bxjr9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdnsk5u94.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphz8xyo9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp72f3gd6r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpev8knmwq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt99td6j6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeempn4w1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk2mqaodb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaq5oiakk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdofo74ns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2haprtkc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ghmsdfj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptw43dalc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqz2b1jvx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_jgo36dm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy8o27u4o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2m6ne0wh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxccm5jo8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphjp_aevj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyzkiznt2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp_l7g_wz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt88o1g3a.pickle

Iteration 25: Full valset score for new program: 0.22390564583333336
Iteration 25: Full train_val score for new program: 0.22390564583333336
Iteration 25: Individual valset scores for new program: [0.46158, 0.442038, 0.449549, 0.39824, 0.459485, 0.452323, 0.264354, 0.498624, 0.537719, 0.531017, 0.066667, 0.337478, 0.023967, 0.0, 0.01968, 0.019442, 0.018715, 0.02221, 0.021375, 0.266728, 0.339233, 0.02538, 0.057382, 0.057382, 0.288316, 0.266129, 0.794439, 0.885333, 0.038335, 0.036364, 0.038724, 0.000108, 6e-05, 0.74966, 0.072368, 0.062665, 0.024156, 0.627512, 0.125461, 0.051566, 0.035431, 0.063803, 0.044408, 0.2, 0.041397, 0.043198, 0.441718, 0.045752]
Iteration 25: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 25: Full valset pareto front score: 0.25813520833333325
Iteration 25: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {3, 5, 6, 7}, {5}, {8, 1, 3, 4}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8}, {3}, {6}, {1, 3, 4, 5, 6, 8}, {1, 3, 4, 5, 6, 8}, {1}, {6}, {4}, {6}, {8}, {1, 3, 4, 5, 6, 7, 8}, {1, 3, 4, 5, 7, 8}, {8, 4}, {8, 4}, {8}, {1, 4, 5, 7, 8}, {8, 1, 4, 7}, {8}, {6}, {1, 3, 4, 5, 6, 7, 8, 9}, {4}, {4}, {4}, {1, 4, 5, 7, 8}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8}, {8, 1, 3, 4}]
Iteration 25: Best valset aggregate score so far: 0.2510750416666666
Iteration 25: Best program as per aggregate score on train_val: 4
Iteration 25: Best program as per aggregate score on valset: 4
Iteration 25: Best score on valset: 0.2510750416666666
Iteration 25: Best score on train_val: 0.2510750416666666
Iteration 25: Linear pareto front program index: 4
Iteration 25: New program candidate index: 9
Iteration 26: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjx17rwyn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvnrorg23.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd2f7vzmf.pickle

Iteration 26: Proposed new text for program: # Metadata dictionaries
# Tracks the 'priority' value (H) of an object.
# H = L + Frequency. This combines frequency with an aging factor.
m_key_priority = dict()

# Tracks the frequency of the object (needed to recalculate Priority).
m_key_frequency = dict()

# Tracks the order of insertion/update to handle tie-breaking (LRU).
# We use a simple counter or the access_count provided by snapshot.
m_key_last_access = dict()

# The global aging factor 'L'.
# This increases monotonically, helping new objects compete with old high-frequency objects.
aging_factor_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (GDSF-like value).
    Formula: Priority = aging_factor_L + Frequency
    Tie-breaking: LRU (Least Recently Used)
    
    This strategy (LFU with dynamic aging) prevents "cache pollution" where
    old, high-frequency items stick around forever despite no longer being needed.
    '''
    global aging_factor_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority value.
    # Initialize best candidate with the first item.
    best_key = candidate_keys[0]
    min_priority = m_key_priority.get(best_key, 0.0)
    min_last_access = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_priority = m_key_priority.get(key, 0.0)
        curr_last_access = m_key_last_access.get(key, 0)

        # Primary Criteria: Lowest Priority value
        if curr_priority < min_priority:
            best_key = key
            min_priority = curr_priority
            min_last_access = curr_last_access
        # Secondary Criteria: LRU (if priorities are equal, evict the older one)
        elif curr_priority == min_priority:
            if curr_last_access < min_last_access:
                best_key = key
                min_priority = curr_priority
                min_last_access = curr_last_access
    
    # CRITICAL STEP for Aging:
    # Update the global aging factor L to the priority of the evicted object.
    # This effectively "lowers" the relative value of existing high-frequency items
    # compared to new items entering the cache in the future.
    aging_factor_L = min_priority
            
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, we increase frequency and re-calculate priority based on the current L.
    This effectively "resets" the object's aging relative to the current baseline.
    '''
    global m_key_frequency, m_key_priority, m_key_last_access, aging_factor_L
    
    # Increment Frequency
    curr_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = curr_freq
    
    # Update Priority: L + Frequency
    # By adding current L, we bring this object to the "front" of the aging curve.
    m_key_priority[obj.key] = aging_factor_L + curr_freq
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, we give the new object a fighting chance.
    Priority = current L + initial Frequency (1).
    '''
    global m_key_frequency, m_key_priority, m_key_last_access, aging_factor_L
    
    # Initialize Frequency
    m_key_frequency[obj.key] = 1
    
    # Initialize Priority
    # New objects start at the current baseline (L) + 1. 
    # This prevents them from being immediately evicted if older objects have high freq,
    # but ensures they are evicted soon if they don't get hits (since older objects likely have freq > 1).
    m_key_priority[obj.key] = aging_factor_L + 1.0
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    '''
    global m_key_frequency, m_key_priority, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
    
    if evicted_obj.key in m_key_priority:
        del m_key_priority[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpleiz1ik9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7ed2ldch.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm52wx29d.pickle

Iteration 26: New subsample score 0.344592 is not better than old score 0.369077, skipping
Iteration 27: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp2dil115.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj4u54eo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbj8e618i.pickle

Iteration 27: Proposed new text for program: import math

# Metadata dictionaries
# Tracks estimated frequency. We do not delete keys immediately on eviction to maintain history.
m_key_frequency = dict()
# Tracks the last access time (Recency) to break ties in frequency.
m_key_last_access = dict()
# A global counter to trigger the aging process
global_access_counter = 0
# Threshold to trigger frequency decay (halving)
DECAY_THRESHOLD = 10000 

def _apply_aging():
    '''
    Helper function to decay frequencies. 
    Divides all frequencies by 2 to prioritize recent popularity over historical popularity.
    '''
    global m_key_frequency
    keys_to_remove = []
    for k in m_key_frequency:
        m_key_frequency[k] //= 2
        if m_key_frequency[k] == 0:
            keys_to_remove.append(k)
    
    # Clean up zero-frequency items to keep memory usage somewhat bounded
    for k in keys_to_remove:
        del m_key_frequency[k]
        if k in m_key_last_access:
            del m_key_last_access[k]

def evict(cache_snapshot, obj):
    '''
    Evicts the object based on a dynamic frequency policy with LRU tie-breaking.
    Because we use aging, this acts similarly to Window TinyLFU.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We are looking for the key with the Lowest Frequency.
    # Tie-breaker: Least Recently Used.
    
    # Initialize with the first key
    victim_key = candidate_keys[0]
    min_freq = m_key_frequency.get(victim_key, 0)
    min_time = m_key_last_access.get(victim_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Frequency Check (Lower is better for eviction)
        if curr_freq < min_freq:
            victim_key = key
            min_freq = curr_freq
            min_time = curr_time
        # 2. Recency Check (If frequencies are equal, older is better for eviction)
        elif curr_freq == min_freq:
            if curr_time < min_time:
                victim_key = key
                min_freq = curr_freq
                min_time = curr_time
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update recency.
    Check if we need to apply aging.
    '''
    global m_key_frequency, m_key_last_access, global_access_counter
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    # Cap frequency to prevent integer overflows and excessive dominance
    m_key_frequency[obj.key] = min(current_freq + 1, 100000)
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Periodic Aging
    global_access_counter += 1
    if global_access_counter >= DECAY_THRESHOLD:
        _apply_aging()
        global_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: 
    If the object was previously seen (in ghost metadata), restore its frequency (incremented).
    If it's new, start with frequency 1.
    '''
    global m_key_frequency, m_key_last_access, global_access_counter
    
    # Check history. If it exists in m_key_frequency, it was a "ghost" entry.
    # We revive it.
    old_freq = m_key_frequency.get(obj.key, 0)
    
    # Logic: If it was evicted recently, it might still have a freq > 0.
    # If it's a brand new object, freq is 0.
    # We increment it to indicate the new access.
    m_key_frequency[obj.key] = old_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Periodic Aging check (also needed on insert)
    global_access_counter += 1
    if global_access_counter >= DECAY_THRESHOLD:
        _apply_aging()
        global_access_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    CRITICAL CHANGE: Do NOT delete metadata immediately.
    We keep it to serve as "Ghost" entries.
    However, to prevent unlimited memory growth, we rely on the `_apply_aging` 
    function to clean up items that drop to frequency 0.
    '''
    # We deliberately do nothing here regarding m_key_frequency.
    # The evicted object's frequency remains in the dictionary.
    # It will eventually decay to 0 and be removed by _apply_aging,
    # or it will be re-inserted and regain its status.
    
    # We can optionally remove recency to save space, as frequency is the primary metric for ghosts.
    # However, keeping it helps tie-breaking if it is immediately re-inserted.
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4i79l0y6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphrj2vr0v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp60d_v8po.pickle

Iteration 27: New subsample score 0.549347 is not better than old score 0.578545, skipping
Iteration 28: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9fxkbqp3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd7j09apm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgt_t16ai.pickle

Iteration 28: Proposed new text for program: # Metadata dictionaries
# Tracks the specific segment an object belongs to: 0 for Probationary, 1 for Protected
m_key_segment = dict()
# Tracks the last access time (Recency) necessary for LRU behavior within segments
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented LRU eviction policy.
    1. Try to evict the Least Recently Used item from the Probationary segment (Segment 0).
    2. If Probationary is empty, evict the Least Recently Used item from the Protected segment (Segment 1).
    '''
    global m_key_segment, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the LRU item in segment 0, or if none exist, LRU in segment 1.
    victim_key = None
    victim_time = float('inf')
    
    # First pass: Look for victim in Probationary Segment (0)
    found_in_probationary = False
    
    # We iterate to find the absolute LRU in Probationary
    for key in candidate_keys:
        segment = m_key_segment.get(key, 0) # Default to 0 if missing (safety)
        if segment == 0:
            found_in_probationary = True
            access_time = m_key_last_access.get(key, 0)
            if access_time < victim_time:
                victim_time = access_time
                victim_key = key

    # Second pass: If nothing found in Probationary, look in Protected Segment (1)
    if not found_in_probationary:
        victim_time = float('inf') # Reset for second pass
        for key in candidate_keys:
            # We treat everything remaining as effectively protected or fallback
            access_time = m_key_last_access.get(key, 0)
            if access_time < victim_time:
                victim_time = access_time
                victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update Last Access Time (Recency).
    2. Promote to Protected Segment (Segment 1) regardless of current segment.
    '''
    global m_key_segment, m_key_last_access
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Promotion: If you are hit, you are important. Move to Protected.
    m_key_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize Last Access Time.
    2. Place in Probationary Segment (Segment 0).
    '''
    global m_key_segment, m_key_last_access
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Insertion starts in Probationary
    m_key_segment[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global m_key_segment, m_key_last_access
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    if evicted_obj.key in m_key_segment:
        del m_key_segment[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcvnznnuc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf_lus67b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0pzj8tvi.pickle

Iteration 28: New subsample score 0.174267 is not better than old score 0.174267, skipping
Iteration 29: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt5szxn_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt0v0ofi4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph5i_mtwu.pickle

Iteration 29: Proposed new text for program: # Metadata dictionaries
# T1: Resident Recency (Recently accessed once) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_t1 = dict()

# T2: Resident Frequency (Accessed at least twice) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_t2 = dict()

# B1: Ghost Recency (Evicted from T1) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_b1 = dict()

# B2: Ghost Frequency (Evicted from T2) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_b2 = dict()

# Adaptive parameter 'p'
# Represents the target size (number of items) for the T1 list.
p = 0.0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Policy.
    Decides whether to evict from the Recency set (T1) or the Frequency set (T2)
    based on the adaptation parameter 'p' and the ghost history of the incoming object.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2, p
    
    # If resident lists are empty (startup/error), fallback to cache snapshot
    if not meta_t1 and not meta_t2:
        return next(iter(cache_snapshot.cache))
    
    incoming_key = obj.key
    len_t1 = len(meta_t1)
    
    # ARC "Replace" Logic
    # We evict from T1 if it exceeds the target size 'p'.
    # Special Case: If the incoming item is in B2 (Ghost Frequency), we are even 
    # more inclined to evict T1 (to grow T2), so we treat len_t1 == p as sufficient to evict T1.
    
    evict_from_t1 = False
    
    if meta_t1 and len_t1 > p:
        evict_from_t1 = True
    elif meta_t1 and (incoming_key in meta_b2 and len_t1 == int(p)):
        evict_from_t1 = True
        
    # Boundary checks: if one list is empty, must evict from the other
    if not meta_t2:
        evict_from_t1 = True
    elif not meta_t1:
        evict_from_t1 = False

    if evict_from_t1:
        # Return LRU of T1 (first key in dict)
        return next(iter(meta_t1))
    else:
        # Return LRU of T2 (first key in dict)
        return next(iter(meta_t2))

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    - If in T1 (Recency), move to T2 (Frequency).
    - If in T2 (Frequency), move to MRU position in T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from T1 -> T2
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Make MRU in T2 (delete and re-insert)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # Synchronization fallback: if hit in cache but not in T1/T2, assume T2
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    - Detect "Ghost Hits" (hits in B1 or B2) to adjust 'p'.
    - Insert object into T1 or T2 based on history.
    - Prune ghost lists to keep history bounded.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2, p
    
    key = obj.key
    # Approximate current capacity in number of objects (N)
    current_count = len(meta_t1) + len(meta_t2)
    
    # 1. Adapt 'p' based on where the miss occurred (B1 or B2)
    if key in meta_b1:
        # Hit in Ghost T1: We should have made T1 larger. Increase p.
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            # Safety check for divide by zero
            delta = len(meta_b2) / len(meta_b1) if len(meta_b1) > 0 else 1
            
        p = p + delta
        # Cap p at roughly the total number of items
        if p > current_count + 1:
            p = current_count + 1
            
        # Move from Ghost T1 -> Resident T2 (second access)
        del meta_b1[key]
        meta_t2[key] = None
        
    elif key in meta_b2:
        # Hit in Ghost T2: We should have made T2 larger (T1 smaller). Decrease p.
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            delta = len(meta_b1) / len(meta_b2) if len(meta_b2) > 0 else 1
            
        p = p - delta
        if p < 0:
            p = 0
            
        # Move from Ghost T2 -> Resident T2
        del meta_b2[key]
        meta_t2[key] = None
        
    else:
        # Completely new item -> Insert into Resident T1
        meta_t1[key] = None

    # 2. Prune Ghost Lists (B1/B2)
    # They shouldn't grow indefinitely. Standard ARC limits:
    # len(T1) + len(B1) <= C
    # len(T2) + len(B2) <= 2C
    # We use current_count as a proxy for Capacity (C)
    
    c = max(1, current_count)
    
    while len(meta_b1) > c:
        del meta_b1[next(iter(meta_b1))]
        
    while len(meta_b2) > 2 * c:
        del meta_b2[next(iter(meta_b2))]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move the evicted object from Resident (T) to Ghost (B).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpennw7uct.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxj_4rqdf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk708bhzb.pickle

Iteration 29: New subsample score 0.6482049999999999 is better than old score 0.61644. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0yroxm1w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwtberlwt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjhnjohkz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7e0vi6ld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps4f7tbdv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfkhfeo3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp40swvxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp87lgv7gq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3_63qjpg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw5gmc6ap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp65y7c1jz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc09foijj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3d2laiio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5eqt9mdu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0e__ajrf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp99qqfe0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbtq2y9q1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu8tm54oy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpweea1fq4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvj1ew7zk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf138a4aj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptvn_1f7q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfdfzq2fn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm9yttr2z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp205yp5yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyjga6619.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcs65m81n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt_hy8jff.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz4ykjhdk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkz40swyt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuvhqdd40.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsc5oonnw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp94f_idxx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv0ygo2h3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppq_tp550.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpla3rg119.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_qnduvz7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl2e31z7_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaz5wcmqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6i0jef3g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfz7hqd4k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwmp88ct1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpucvnnx7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3qo01z_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn5k62b43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbmsdk6b6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9il27wnq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdujfe9h_.pickle

Iteration 29: Full valset score for new program: 0.2458522291666667
Iteration 29: Full train_val score for new program: 0.2458522291666667
Iteration 29: Individual valset scores for new program: [0.508125, 0.483248, 0.491122, 0.441629, 0.505643, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.334813, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.342183, 0.026164, 0.058672, 0.058672, 0.269801, 0.371976, 0.85005, 0.891494, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.080676, 0.124007, 0.057624, 0.052632, 0.3, 0.152321, 0.060028, 0.466258, 0.081699]
Iteration 29: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 29: Full valset pareto front score: 0.25813520833333325
Iteration 29: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {3, 5, 6, 7}, {5}, {1, 3, 4, 8, 10}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8, 10}, {3}, {6}, {1, 3, 4, 5, 6, 8, 10}, {1, 3, 4, 5, 6, 8, 10}, {1}, {6}, {4}, {6}, {8}, {1, 3, 4, 5, 6, 7, 8, 10}, {1, 3, 4, 5, 7, 8, 10}, {8, 10, 4}, {8, 10, 4}, {8}, {1, 4, 5, 7, 8, 10}, {1, 4, 7, 8, 10}, {8}, {6}, {1, 3, 4, 5, 6, 7, 8, 9, 10}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10}, {1, 3, 4, 8, 10}]
Iteration 29: Best valset aggregate score so far: 0.2510750416666666
Iteration 29: Best program as per aggregate score on train_val: 4
Iteration 29: Best program as per aggregate score on valset: 4
Iteration 29: Best score on valset: 0.2510750416666666
Iteration 29: Best score on train_val: 0.2510750416666666
Iteration 29: Linear pareto front program index: 4
Iteration 29: New program candidate index: 10
Iteration 30: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnuk62cgf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxc_lgbcs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps9wnnlvs.pickle

Iteration 30: Proposed new text for program: # Metadata Global State
# T1: LRU list for recent items (in cache). Key -> Object
# T2: LRU list for frequent items (in cache). Key -> Object
# B1: LRU list for recent evictions (ghost). Key -> None
# B2: LRU list for frequent evictions (ghost). Key -> None
# Note: In this simulation environment, 'cache_snapshot.cache' holds the actual object storage.
# We use auxiliary lists (Python lists acting as queues) to track order.
# To make lookups O(1), we maintain sets or dicts for membership testing.

meta_t1_keys = []    # Keys in T1 (Recent)
meta_t2_keys = []    # Keys in T2 (Frequent)
meta_b1_keys = []    # Keys in B1 (Ghost Recent)
meta_b2_keys = []    # Keys in B2 (Ghost Frequent)

# Sets for fast O(1) lookups
set_t1 = set()
set_t2 = set()
set_b1 = set()
set_b2 = set()

# Adaptation parameter
# p targets the size of T1. 
# 0 <= p <= capacity
target_p = 0

def _replace(cache_snapshot, key_to_insert_if_ghost_hit=None):
    """
    Subroutine to determine which item to evict based on the state of T1, T2, and P.
    """
    global meta_t1_keys, meta_t2_keys, meta_b1_keys, meta_b2_keys
    global set_t1, set_t2, set_b1, set_b2, target_p

    victim = None
    
    t1_len = len(meta_t1_keys)
    
    # Logic to decide whether to evict from T1 or T2
    # If T1 is larger than target_p, we generally evict from T1 (the LRU end).
    # However, if the operation was triggered by a B2 hit, we might be more aggressive on T1.
    
    evict_from_t1 = False
    
    if t1_len > 0 and (t1_len > target_p or (key_to_insert_if_ghost_hit in set_b2 and t1_len == target_p)):
        evict_from_t1 = True
    else:
        # Evict from T2
        evict_from_t1 = False
    
    # Perform the eviction logic on metadata
    if evict_from_t1:
        # Pop LRU of T1
        victim = meta_t1_keys.pop(0) 
        set_t1.remove(victim)
        
        # Move to B1 (Ghost Recent)
        meta_b1_keys.append(victim)
        set_b1.add(victim)
    else:
        # Pop LRU of T2
        # Safety check: if T2 is empty, we must evict T1 (should rarely happen if logic is sound)
        if not meta_t2_keys:
            victim = meta_t1_keys.pop(0)
            set_t1.remove(victim)
            meta_b1_keys.append(victim)
            set_b1.add(victim)
        else:
            victim = meta_t2_keys.pop(0)
            set_t2.remove(victim)
            
            # Move to B2 (Ghost Frequent)
            meta_b2_keys.append(victim)
            set_b2.add(victim)
            
    return victim

def evict(cache_snapshot, obj):
    """
    Determines the victim using ARC logic (T1 vs T2 split based on P).
    """
    global target_p
    
    # Typically, ARC performs the eviction inside the insertion logic loop.
    # However, this framework asks for a specific 'evict' function call.
    # We use the _replace helper logic to find the victim.
    
    # Note: We don't change metadata state *here* usually, but since the framework
    # separates `evict` (get victim) and `update_after_evict` (do cleanup), 
    # we need to be careful.
    
    # In this specific framework flow:
    # 1. evict() is called. We must return a key.
    # 2. Framework removes key from cache_snapshot.cache.
    # 3. update_after_evict is called.
    
    # Because _replace() modifies our internal lists (moves T to B), we must return the result
    # of _replace(). The framework will then perform the physical removal.
    
    # Context check: is the object being inserted (obj) in B2?
    # This affects the eviction decision in ARC.
    key_in_b2 = obj.key in set_b2
    
    victim_key = _replace(cache_snapshot, obj.key if key_in_b2 else None)
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    Handles a cache hit.
    Move to MRU of T2.
    """
    global meta_t1_keys, meta_t2_keys, set_t1, set_t2
    
    key = obj.key
    
    # Case 1: Key is in T1 (Recent) -> Move to T2 (Frequent)
    if key in set_t1:
        # Remove from T1
        meta_t1_keys.remove(key)
        set_t1.remove(key)
        
        # Add to T2 (MRU)
        meta_t2_keys.append(key)
        set_t2.add(key)
        
    # Case 2: Key is in T2 (Frequent) -> Move to MRU of T2
    elif key in set_t2:
        meta_t2_keys.remove(key)
        meta_t2_keys.append(key)
        # set remains same

def update_after_insert(cache_snapshot, obj):
    """
    Handles inserting a new object.
    Could be a totally new object, or a ghost hit (B1 or B2).
    """
    global meta_t1_keys, meta_t2_keys, meta_b1_keys, meta_b2_keys
    global set_t1, set_t2, set_b1, set_b2, target_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # We assume capacity is count-based for ARC logic usually, but here bytes.
    # Approximation: use current key count as capacity reference if needed, 
    # but strictly ARC manages the lists.
    # Since the framework handles "Full" checks, we just manage list movements.
    
    # Case 1: Ghost Hit in B1 (Was recently evicted from T1)
    if key in set_b1:
        # Adapt P: Favor T1 (Recency)
        delta = 1
        if len(set_b1) < len(set_b2):
            delta = len(set_b2) // len(set_b1)
        target_p = min(cache_snapshot.capacity, target_p + delta) # Clamp to max capacity
        
        # Move from B1 to T2 (It has been seen twice now: once in T1, evicted, now seen again)
        meta_b1_keys.remove(key)
        set_b1.remove(key)
        
        meta_t2_keys.append(key)
        set_t2.add(key)
        return

    # Case 2: Ghost Hit in B2 (Was recently evicted from T2)
    if key in set_b2:
        # Adapt P: Favor T2 (Frequency)
        delta = 1
        if len(set_b2) < len(set_b1):
            delta = len(set_b1) // len(set_b2)
        target_p = max(0, target_p - delta)
        
        # Move from B2 to T2 (Reinforce Frequency)
        meta_b2_keys.remove(key)
        set_b2.remove(key)
        
        meta_t2_keys.append(key)
        set_t2.add(key)
        return

    # Case 3: Totally new item
    # Insert into T1 (MRU)
    meta_t1_keys.append(key)
    set_t1.add(key)

    # Note: The `evict` function handles the necessary clearing of space (moving T->B).
    # However, we must ensure the Ghost lists (B1/B2) don't grow infinitely.
    # Standard ARC rule: Len(T1+B1) <= C and Len(T2+B2) <= 2C.
    # Simplified cleanup:
    
    total_keys_count = len(cache_snapshot.cache) # Approximate C
    
    # Ensure B1 + T1 <= C (roughly)
    # Actually, simplistic ARC cleanup: just ensure B1+B2+T1+T2 isn't massive.
    # The pure algorithm removes LRU of B1 if len(T1)+len(B1) > C
    
    if len(meta_b1_keys) > total_keys_count:
        removed = meta_b1_keys.pop(0)
        set_b1.remove(removed)
        
    if len(meta_b2_keys) > total_keys_count + len(meta_b1_keys): # Looser bound for B2
        removed = meta_b2_keys.pop(0)
        set_b2.remove(removed)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Framework calls this after evict() returns a victim and it is removed.
    ARC's logic for moving items to Ghost lists was handled inside evict() -> _replace()
    because we needed to select the specific victim based on list states.
    
    We just perform a consistency check here.
    """
    # In our implementation of evict(), we pop from T1/T2 and push to B1/B2.
    # So the metadata is already updated.
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdxulvzjl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyv0bxdhh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp36oor01_.pickle

Iteration 30: New subsample score 0.36282899999999996 is better than old score 0.362228. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg5a_rr48.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmayb2jvb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1h5rdlsk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9hpw9lm8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7y1wjznf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqny3qoiw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyzmq3zj5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpse2gzctm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzawhveb0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkdcuxis8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv_u0jt7z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp66vh3qh9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkkwfebgc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzavkdq6d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbuwddvg9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptosil20w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptr9__stu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwhio19rg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzjalig1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaasizgg7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp19u0lqog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr60dmeuc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdw6yf8h0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0_e_6icp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps0olhnfn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppwpvso_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpha9bfanq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq718e19x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1du04k6r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvzi9wmhy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyf3q1gww.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfq8_9qui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpni431rdy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzxwcfs3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy45ebwgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmvehv22l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps177psxj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps2va43qb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbp_w4o5v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyhlgxdn1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwu4o1m61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcu5end89.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7z_d1fu6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwss5ei84.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1qy6s9k5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4o25w83n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjo_1bknu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp33ge0lkf.pickle

Iteration 30: Full valset score for new program: 0.24149552083333328
Iteration 30: Full train_val score for new program: 0.24149552083333328
Iteration 30: Individual valset scores for new program: [0.508125, 0.482489, 0.490819, 0.44154, 0.505857, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.332593, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.342183, 0.026164, 0.058672, 0.058672, 0.269799, 0.362903, 0.85005, 0.891494, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.640392, 0.125461, 0.052953, 0.050703, 0.048509, 0.052632, 0.316667, 0.052527, 0.040112, 0.466258, 0.081699]
Iteration 30: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 30: Full valset pareto front score: 0.25813520833333325
Iteration 30: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {3, 5, 6, 7}, {5}, {1, 3, 4, 8, 10, 11}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {3}, {6}, {1, 3, 4, 5, 6, 8, 10, 11}, {1, 3, 4, 5, 6, 8, 10, 11}, {1}, {6}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 5, 7, 8, 10, 11}, {8, 10, 11, 4}, {8, 10, 11, 4}, {8, 11}, {1, 4, 5, 7, 8, 10, 11}, {1, 4, 7, 8, 10, 11}, {8, 11}, {6}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 8, 10, 11}]
Iteration 30: Best valset aggregate score so far: 0.2510750416666666
Iteration 30: Best program as per aggregate score on train_val: 4
Iteration 30: Best program as per aggregate score on valset: 4
Iteration 30: Best score on valset: 0.2510750416666666
Iteration 30: Best score on train_val: 0.2510750416666666
Iteration 30: Linear pareto front program index: 4
Iteration 30: New program candidate index: 11
Iteration 31: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0vcp4ebq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyqq7r94g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp94kcm0gl.pickle

Iteration 31: Proposed new text for program: import math

# -- Global State for ARC Implementation --

# m_meta stores metadata for currently cached items.
# Key -> {'type': 0 (T1 - Recency) or 1 (T2 - Frequency), 'time': access_timestamp}
m_meta = dict()

# m_ghosts stores keys of recently evicted items (Shadow Cache).
# Key -> type (0 for B1/Ghost Recency, 1 for B2/Ghost Frequency)
# Python dicts (3.7+) preserve insertion order, allowing us to use this as a FIFO queue.
m_ghosts = dict()

# m_state stores the adaptive parameter 'p'.
# 'p' represents the target number of items the cache should dedicate to the T1 (Recency) list.
m_state = {'p': 0}

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict using ARC logic.
    1. Checks if the incoming object is in the Ghost Registry to adapt 'p'.
    2. Determines whether to evict from T1 (Recency) or T2 (Frequency) based on 'p'.
    3. Selects the LRU victim from the chosen list.
    '''
    global m_meta, m_ghosts, m_state
    
    # -- Step 1: Adaptivity (Ghost Hit Check) --
    # If the incoming object (obj) corresponds to a ghost, we know we made a mistake 
    # by evicting it earlier. We adjust 'p' to correct this for the future.
    if obj.key in m_ghosts:
        ghost_type = m_ghosts[obj.key]
        
        # Calculate the size of Ghost Recency (B1) and Ghost Frequency (B2)
        b1_size = sum(1 for v in m_ghosts.values() if v == 0)
        b2_size = len(m_ghosts) - b1_size
        
        delta = 1
        # Current logical capacity roughly equals the number of items currently in cache
        current_capacity = len(cache_snapshot.cache)
        
        if ghost_type == 0: 
            # Hit in B1 (Recency Ghost): We should have kept T1 larger.
            # Delta calculation favors B2 size to converge faster if B1 is small
            if b1_size >= b2_size:
                delta = 1
            else:
                delta = b2_size / max(b1_size, 1) # Avoid div/0
            
            # Increase target p (grow T1)
            m_state['p'] = min(current_capacity, m_state['p'] + delta)
            
        else: 
            # Hit in B2 (Frequency Ghost): We should have kept T2 larger.
            if b2_size >= b1_size:
                delta = 1
            else:
                delta = b1_size / max(b2_size, 1) # Avoid div/0
            
            # Decrease target p (shrink T1, implicitly growing T2)
            m_state['p'] = max(0, m_state['p'] - delta)

    # -- Step 2: Identify Candidates and Victim Selection --
    
    candidates = []
    t1_count = 0
    
    # Scan cache to identify which items are T1 (Recency) and which are T2 (Frequency)
    # This also helps us find the LRU of each set.
    for k in cache_snapshot.cache:
        # Maintenance: Ensure every key in cache has metadata
        if k not in m_meta:
            m_meta[k] = {'type': 0, 'time': 0}
            
        meta = m_meta[k]
        if meta['type'] == 0:
            t1_count += 1
        candidates.append((k, meta))
            
    # Decision: Do we evict from T1 or T2?
    target_p = m_state['p']
    
    # If we have more T1 items than our target 'p', we must evict from T1.
    # Otherwise, we have space in T1 (or need space for it), so we evict from T2.
    evict_from_t1 = (t1_count > target_p)
    
    best_victim_key = None
    min_time = float('inf')
    found_preferred = False
    
    # Pass 1: Find LRU in the preferred list
    for k, meta in candidates:
        if (evict_from_t1 and meta['type'] == 0) or (not evict_from_t1 and meta['type'] == 1):
            if meta['time'] < min_time:
                min_time = meta['time']
                best_victim_key = k
                found_preferred = True
                
    # Pass 2: Fallback (if preferred list was empty)
    if not found_preferred:
        for k, meta in candidates:
            if meta['time'] < min_time:
                min_time = meta['time']
                best_victim_key = k
                
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update last access time.
    2. Move item to T2 (Frequency List). 
       Any item hit more than once is considered "Frequent".
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    if obj.key in m_meta:
        m_meta[obj.key]['type'] = 1 # Promote to T2
        m_meta[obj.key]['time'] = current_time
    else:
        # Fallback for sync issues
        m_meta[obj.key] = {'type': 1, 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If it was a Ghost, it enters as T2 (Frequency).
    3. If it is new, it enters as T1 (Recency).
    '''
    global m_meta, m_ghosts
    current_time = cache_snapshot.access_count
    
    new_type = 0 # Default: T1 (Recency)
    
    if obj.key in m_ghosts:
        # It was recently evicted, so it's a returning frequent item -> T2
        new_type = 1
        del m_ghosts[obj.key]
        
    m_meta[obj.key] = {'type': new_type, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (B1 if it was T1, B2 if it was T2).
    3. Keep Ghost Registry size bounded.
    '''
    global m_meta, m_ghosts
    
    # 1. Capture type before deleting
    evicted_type = 0
    if evicted_obj.key in m_meta:
        evicted_type = m_meta[evicted_obj.key]['type']
        del m_meta[evicted_obj.key]
        
    # 2. Add to Ghosts
    # m_ghosts acts as a FIFO queue because we insert new ghosts at the end
    m_ghosts[evicted_obj.key] = evicted_type
    
    # 3. Maintain Ghost Size
    # ARC typically maintains |Ghosts| <= |Cache Capacity|
    # We use current cache length as a proxy for capacity count.
    # We allow a small buffer (+1) to account for the object currently being inserted.
    max_ghosts = len(cache_snapshot.cache) + 1
    
    if len(m_ghosts) > max_ghosts:
        try:
            # Remove the oldest inserted ghost (FIFO)
            oldest_ghost = next(iter(m_ghosts))
            del m_ghosts[oldest_ghost]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp92rd2h0m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpifqqzu_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphpdugbfe.pickle

Iteration 31: New subsample score 0.328278 is not better than old score 0.328278, skipping
Iteration 32: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfuer_nbm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpejw4w00b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpovlhv3qi.pickle

Iteration 32: Proposed new text for program: import math

# Metadata Global Storage
# Stores the "value" (frequency + aging bias) of keys
m_key_values = dict()
# The global "inflation" value (similar to L in LFU-DA) to handle aging
m_global_age = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the minimum value.
    Value = Frequency + Global_Age_At_Entry
    
    This implements an LFU-DA (Dynamic Aging) approach. 
    - LFU: We evict the item with the least estimated utility.
    - DA: We add a 'cache age' factor to new items so they don't start at 0 
      and immediately die against old, high-frequency items.
      
    When an eviction happens, the global age increases to the value of the victim.
    This effectively "ages out" old high-frequency items because new items come in 
    with a higher baseline score.
    '''
    global m_key_values, m_global_age
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We search for the key with the lowest tracked value in the cache
    victim_key = None
    min_val = float('inf')
    
    # Tie-breaking optimization:
    # If values are equal, we prefer to evict the one inserted/accessed earliest (LRU tie-breaker).
    # Since Python 3.7+, dicts maintain insertion order. If we iterate, we might implicitly
    # hit older items first or last depending on update logic.
    # However, for pure LFU-DA, just finding the min value is sufficient.
    
    for key in candidate_keys:
        val = m_key_values.get(key, 0.0)
        if val < min_val:
            min_val = val
            victim_key = key
            
    # LFU-DA Logic Update:
    # The Global Age advances to the value of the evicted item.
    # This prevents the "frequency counter pollution" where old items stay forever.
    if victim_key is not None:
        m_global_age = min_val

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment the object's value.
    '''
    global m_key_values
    
    key = obj.key
    current_val = m_key_values.get(key, 0.0)
    
    # Standard LFU reinforcement. 
    # We add 1.0 to the frequency.
    m_key_values[key] = current_val + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Initialize value using Ghost Cache logic + Dynamic Aging.
    '''
    global m_key_values, m_global_age
    
    key = obj.key
    
    # Ghost Cache Check:
    if key in m_key_values:
        # The key is in metadata history (it was evicted previously).
        # We assume it has retained some 'heat', but we must ensure 
        # it is at least comparable to the current global age.
        # Logic: If it was popular before, it starts with that previous popularity.
        # However, if that popularity is now lower than the current global age (cache minimum),
        # we bump it up to Global Age + Initial_Boost to give it a fighting chance.
        
        saved_val = m_key_values[key]
        if saved_val < m_global_age:
             # Reset to current baseline + small boost for being a return visitor
            m_key_values[key] = m_global_age + 1.0
        else:
            # It was very popular, keep its high score + boost
            m_key_values[key] = saved_val + 1.0
            
    else:
        # New Item (never seen).
        # In LFU-DA, a new item enters with value = Global_Age + Initial_Frequency.
        # This allows it to survive long enough to prove its worth.
        m_key_values[key] = m_global_age + 1.0
    
    # Maintenance: Prune metadata if it grows too large to prevent memory leaks.
    # We only prune items NOT in the cache.
    if len(m_key_values) > 50000: # Heuristic cap
        # Remove entries that are significantly below global age (likely won't be admitted/survive)
        # unless they are currently in the cache.
        keys_to_purge = []
        cached_keys = set(cache_snapshot.cache.keys())
        
        # We accept O(N) cleanup occasionally for better hit rate performance
        count = 0
        for k, v in m_key_values.items():
            if k not in cached_keys and v < m_global_age:
                keys_to_purge.append(k)
                count += 1
                if count > 1000: break # Cleanup in chunks
        
        for k in keys_to_purge:
            del m_key_values[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Update metadata but do NOT delete the key.
    '''
    # We keep the key in m_key_values. 
    # This serves as the "Ghost Cache". 
    # If this item is requested again, we know it was recently popular.
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0rdqxbfc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzjizldx_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3_fa7u5c.pickle

Iteration 32: New subsample score 0.8027709999999999 is not better than old score 0.914353, skipping
Iteration 33: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgqbq82e8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppy9eb9i3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0olvqxoy.pickle

Iteration 33: Proposed new text for program: import math

# Metadata dictionaries
# m_key_metadata stores a tuple or object: (frequency, insertion_time, is_protected)
m_key_metadata = dict()

# Ghost cache to track recently evicted items. 
# We store insertion_time or a boolean to know it was recently seen.
# Using a set for O(1) lookups.
m_ghost_keys = set()
m_ghost_fifo = [] # To manage the size of the ghost cache
MAX_GHOST_SIZE = 0 # Will be dynamically set based on cache capacity approximation

# Global counter for logical time
current_time = 0

def get_score(key, access_count):
    """
    Calculate a utility score for eviction. Lower score = better victim.
    Score components: Frequency tier + Recency.
    """
    if key not in m_key_metadata:
        return -1
    
    freq, last_access, protected = m_key_metadata[key]
    
    # Hierarchy of importance:
    # 1. Protected status (High Score)
    # 2. Frequency (Medium Score influence)
    # 3. Recency (Tie-breaker)
    
    # Base score: Protected items get a massive boost (e.g., +1,000,000)
    # Frequency acts as a multiplier or additive tier.
    # Recency is the fine-grained differentiator.
    
    # We invert the logic for eviction: We want the MINIMUM score.
    # So, Recency should increase the score (newer = higher score).
    
    base = 1000000 if protected else 0
    
    # Logarithmic frequency to prevent historical domination, but reward popularity
    freq_score = min(freq, 100) * 1000
    
    # Time delta
    # We want newer items to have higher scores.
    # Since access_count grows, 'last_access' is larger for newer items.
    
    return base + freq_score + last_access

def cleanup_ghosts():
    """Ensure ghost cache doesn't grow indefinitely."""
    global m_ghost_keys, m_ghost_fifo, MAX_GHOST_SIZE
    
    # Lazy cleanup: remove oldest ghosts if exceeding size limit
    while len(m_ghost_fifo) > MAX_GHOST_SIZE:
        old_key = m_ghost_fifo.pop(0)
        if old_key in m_ghost_keys:
            m_ghost_keys.remove(old_key)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest score (Least Valuable).
    Prioritizes evicting unprotected (probationary) items with low frequency and old access time.
    '''
    global m_key_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for the key with the MINIMUM score.
    # Logic: Unprotected < Protected. Old < New. Low Freq < High Freq.
    
    victim = None
    min_score = float('inf')
    
    # Optimization: We can scan to find the first unprotected item. 
    # If we find unprotected items, we restrict our search to them to ensure
    # we don't evict protected items unless absolutely necessary.
    
    # 1. Try to find victim in Probation (Unprotected) list
    probation_candidates = [k for k in candidate_keys if k in m_key_metadata and not m_key_metadata[k][2]]
    
    search_space = probation_candidates if probation_candidates else candidate_keys
    
    for key in search_space:
        # Default metadata if missing (shouldn't happen often)
        if key not in m_key_metadata:
            return key 
            
        freq, last_access, protected = m_key_metadata[key]
        
        # Calculate Score
        # Primary Weight: Protection Status (0 or 1)
        # Secondary Weight: Frequency
        # Tertiary Weight: Recency (Access Time)
        
        # We want to evict: Not Protected -> Low Freq -> Oldest
        
        # Composite score calculation for comparison
        # (Protected, Frequency, Last_Access)
        # Python compares tuples element by element.
        # We want to minimize this tuple.
        score = (1 if protected else 0, freq, last_access)
        
        if score < min_score:
            min_score = score
            victim = key
            
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Promote to Protected status.
    '''
    global m_key_metadata
    
    curr_freq = 0
    is_protected = False
    
    if obj.key in m_key_metadata:
        curr_freq, _, is_protected = m_key_metadata[obj.key]
    
    # Increment frequency
    new_freq = curr_freq + 1
    
    # Hit implies value -> Promote to Protected
    new_protected = True
    
    m_key_metadata[obj.key] = (new_freq, cache_snapshot.access_count, new_protected)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Cache.
    2. If in Ghost, insert as Protected (high priority) + restore frequency.
    3. If new, insert as Probation (low priority).
    '''
    global m_key_metadata, m_ghost_keys, MAX_GHOST_SIZE
    
    # Dynamic sizing for ghost cache based on observed capacity
    # We estimate capacity by current count. 
    if MAX_GHOST_SIZE == 0 and len(cache_snapshot.cache) > 0:
        # Heuristic: Ghost cache size same as actual cache size
        MAX_GHOST_SIZE = len(cache_snapshot.cache) 
        
    freq = 1
    protected = False
    
    # Check if this is a "Resurrection"
    if obj.key in m_ghost_keys:
        # It was recently evicted and needed again. 
        # This is a strong signal it belongs in the working set.
        freq = 2 # Boost frequency slightly
        protected = True # Enter directly into protected mode
        # Remove from ghost
        m_ghost_keys.remove(obj.key)
    else:
        # Brand new item
        freq = 1
        protected = False # Starts in Probation
        
    m_key_metadata[obj.key] = (freq, cache_snapshot.access_count, protected)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Cache to remember we saw it recently.
    '''
    global m_key_metadata, m_ghost_keys, m_ghost_fifo
    
    # Clean up active metadata
    if evicted_obj in m_key_metadata:
        del m_key_metadata[evicted_obj]
        
    # Add to ghost cache logic
    # We track that this key existed recently.
    if evicted_obj not in m_ghost_keys:
        m_ghost_keys.add(evicted_obj)
        m_ghost_fifo.append(evicted_obj)
        
    cleanup_ghosts()
Subprocess stdout: Error in subprocess: '<' not supported between instances of 'tuple' and 'float'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxq66kt3k.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 290, in evict
    candid_obj_key = self.evict_func(self.snapshot, obj)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/My.py", line 106, in evict
    if score < min_score:
       ^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'tuple' and 'float'

Subprocess stdout: Error in subprocess: '<' not supported between instances of 'tuple' and 'float'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdp4ebn5b.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 290, in evict
    candid_obj_key = self.evict_func(self.snapshot, obj)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/My.py", line 106, in evict
    if score < min_score:
       ^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'tuple' and 'float'

Subprocess stdout: Error in subprocess: '<' not supported between instances of 'tuple' and 'float'
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuu9x66xb.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/cache_simulate.py", line 290, in evict
    candid_obj_key = self.evict_func(self.snapshot, obj)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/My.py", line 106, in evict
    if score < min_score:
       ^^^^^^^^^^^^^^^^^
TypeError: '<' not supported between instances of 'tuple' and 'float'

Iteration 33: New subsample score 0.0 is not better than old score 0.874893, skipping
Iteration 34: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8whiz0zg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph8ei6gf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa5frl833.pickle

Iteration 34: Proposed new text for program: import math

# Metadata dictionaries
# Tracks how many times an object has been accessed.
m_key_frequency = dict()
# Tracks the last access time of an object.
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest utility score.
    Utility is calculated using a Hyperbolic Caching inspired function:
    Score = Frequency / (Time Since Last Access)
    
    This balances LFU (keeping popular items) and LRU (discarding old items).
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Current "time" is represented by access_count
    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    min_score = float('inf')

    # We are looking for the object with the LOWEST score to evict.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta. Add 1 to avoid division by zero if accessed same tick.
        time_delta = (current_time - last_access) + 1
        
        # Hyperbolic Score:
        # Items accessed recently (small delta) get HIGH score.
        # Items accessed frequently (high freq) get HIGH score.
        # We want to evict the item with the LOWEST score.
        score = freq / time_delta
        
        # Optimization: Tie-breaking
        # If scores are identical, evict the one with the lowest frequency (pure LFU fallback)
        # or the one that is largest (Cost-benefit, larger items free up more space).
        # Here we just strictly minimize score.
        if score < min_score:
            min_score = score
            best_victim_key = key
        elif score == min_score:
            # Tie-breaker: Evict the one that was accessed longest ago (Classic LRU logic)
            # This handles cases where frequencies and deltas are proportional.
            if last_access < m_key_last_access.get(best_victim_key, 0):
                best_victim_key = key

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    # Linearly incrementing creates strong pollution.
    # We use a standard increment here, relying on the 'Time Delta' 
    # in the evict function to age out old heavy hitters.
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1
    
    # Update Recency to current access count
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency
    # We start at 1. The eviction logic protects this item briefly 
    # because (CurrentTime - LastAccess) will be very small, resulting in a high Score.
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx1xqent5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9r_1al9u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkyulvr7y.pickle

Iteration 34: New subsample score 0.6245809999999999 is not better than old score 0.6767050000000001, skipping
Iteration 35: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvn5qvsxu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx27iqdzr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0d9rt1ns.pickle

Iteration 35: Proposed new text for program: import math

# Global metadata storage
# Key -> { 'freq': int, 'size': int, 'l_value': float }
m_metadata = dict()

# Global inflation value (L). 
# This represents the "cost" or "priority" threshold of the cache at the current moment.
# It increases monotonically as items are evicted.
m_inflation_l = 0.0

def get_priority_score(key, obj_size):
    '''
    Calculate the priority (GDSF-like score).
    Higher Score = More valuable (KEEP).
    Lower Score = Less valuable (EVICT).
    
    Formula: H(p) = L + (Frequency / Size)
    
    We modify this slightly for stability:
    Score = L + (Frequency * CostWeight) / Size
    
    Where CostWeight is a constant to balance the magnitude of freq/size vs L.
    '''
    if key not in m_metadata:
        return -1.0
    
    data = m_metadata[key]
    freq = data['freq']
    # Use the stored L value from when the object was last updated/inserted
    # This acts as the "Time" component (Recency).
    base_l = data['l_value'] 
    
    # Weighting:
    # We want small items to have high scores.
    # We want frequent items to have high scores.
    # We calculate density: value per byte.
    
    # Avoid division by zero
    safe_size = max(1, obj_size)
    
    # Priority = The inflation value at time of insertion/update + (Freq / Size)
    # This combines Recency (L value increases over time), Frequency, and Size.
    priority = base_l + (freq / safe_size)
    
    return priority

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the LOWEST priority score.
    Returns the key of the victim.
    '''
    global m_inflation_l
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    min_score = float('inf')
    victim_key = None
    
    # Find the object with the minimum priority score
    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        score = get_priority_score(key, cached_obj.size)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    # CRITICAL GDSF STEP:
    # Update the global Inflation Value (L) to the score of the evicted item.
    # This ensures that future items must have a higher merit to survive than the item we just killed.
    if victim_key is not None:
        m_inflation_l = min_score
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset its 'L' value to the current global inflation L.
       (This effectively marks it as "Recently Accessed").
    '''
    global m_metadata, m_inflation_l
    
    if obj.key not in m_metadata:
        # Should not happen on hit, but safety fallback
        m_metadata[obj.key] = {'freq': 1, 'size': obj.size, 'l_value': m_inflation_l}
    else:
        # Increment frequency
        m_metadata[obj.key]['freq'] += 1
        # Update L value to current global L (Restores Recency)
        m_metadata[obj.key]['l_value'] = m_inflation_l
        # Ensure size is accurate (though usually static)
        m_metadata[obj.key]['size'] = obj.size

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Set L value to current global inflation L.
    '''
    global m_metadata, m_inflation_l
    
    # New object starts with frequency 1
    # It inherits the current system "temperature" (L)
    m_metadata[obj.key] = {
        'freq': 1, 
        'size': obj.size, 
        'l_value': m_inflation_l
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata to save memory.
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptwud6xvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpolbx50uh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7r0ss62w.pickle

Iteration 35: New subsample score 0.027743 is not better than old score 0.096891, skipping
Iteration 36: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiesm1gls.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpokconjy4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppdlw7hv3.pickle

Iteration 36: Proposed new text for program: import math

# Metadata dictionaries
# Tracks the frequency of access for keys
m_key_frequency = dict()
# Tracks the specific timestamp (access_count) of the last access
m_key_last_access = dict()

# Heuristics
# A cap prevents long-lived objects from becoming "immortal"
MAX_FREQ = 100 
# We maintain a dynamic average frequency to help new items survive
avg_cache_frequency = 1.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the Minimum Frequency.
    Ties are broken by Recency (Least Recently Used among min-frequency items).
    '''
    global m_key_frequency, m_key_last_access

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We want to find the victim with the lowest score.
    # Score tuple: (Frequency, Last_Access_Time)
    # Python compares tuples element by element.
    # Lower frequency is evicted first. If frequencies equal, older time is evicted.
    
    victim_key = None
    min_score = (float('inf'), float('inf'))

    for k in candidate_keys:
        freq = m_key_frequency.get(k, 1)
        access_time = m_key_last_access.get(k, 0)
        
        # We add a slight "age" penalty to frequency if the item is very old
        # This helps evict high-frequency items that have stopped being accessed.
        # However, purely relying on the tuple (freq, access_time) is usually robust enough
        # if the update logic handles decay correctly.
        score = (freq, access_time)
        
        if score < min_score:
            min_score = score
            victim_key = k

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency (subject to a cap).
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    current_freq = m_key_frequency.get(key, 0)
    
    # Linear increment with a hard cap
    if current_freq < MAX_FREQ:
        m_key_frequency[key] = current_freq + 1
        
    # Update global average frequency heuristic slightly
    # (Simple exponential moving average)
    avg_cache_frequency = 0.95 * avg_cache_frequency + 0.05 * m_key_frequency[key]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Set Initial Frequency.
       If the item was in our "Ghost" history (recently evicted), we restore its old frequency.
       If it's new, we give it a "fighting chance" frequency (e.g., floor of avg_freq)
       so it isn't evicted immediately by older, established items.
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in m_key_frequency:
        # Ghost Hit: This item was evicted but requested again.
        # This is a strong signal of popularity. Boost it.
        # We restore it, perhaps slightly decayed or just incremented.
        new_freq = min(m_key_frequency[key] + 1, MAX_FREQ)
        m_key_frequency[key] = new_freq
    else:
        # Cold Start:
        # Instead of starting at 1, we start at a baseline relative to the cache's current state.
        # This prevents the "new item churn" loop where new items enter with freq 1 and are
        # immediately evicted because everyone else is freq > 1.
        # However, we don't want to make it too high, or we pollute the cache.
        # Using 1 is safe for strict LFU, but taking min(1, int(avg_cache_frequency // 2)) helps adapt.
        # For this specific implementation, starting at 1 is standard, 
        # but relying on the "Age" logic in evict handles the rest.
        # Let's stick to 1 for simplicity but allow the ghost logic to handle re-entries.
        m_key_frequency[key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. We DO NOT delete metadata for the evicted object immediately (Ghost Cache).
    2. However, to prevent frequency inflation over time, we apply a global decay 
       periodically or based on max frequency thresholds.
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    # Adaptive Decay Strategy:
    # If the average frequency in the cache gets too high, or the evicted item
    # had a very high frequency, it implies our counters are saturated.
    # We decrement all counters periodically to simulate "forgetting".
    # Here, we trigger decay if the evicted item had a high frequency (e.g., > 10).
    
    evicted_freq = m_key_frequency.get(evicted_obj.key, 1)
    
    # If we are evicting something that was somewhat popular, it means the cache 
    # is under pressure. We should decay everyone slightly to allow new trends to emerge.
    if evicted_freq > 5:
        # Decay all frequencies by integer division or subtraction
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            # If item is not in cache (it's a ghost), decay faster
            is_ghost = k not in cache_snapshot.cache
            
            if is_ghost:
                new_v = v // 2 
            else:
                # If in cache, decay slower (decrement)
                new_v = v - 1 if v > 1 else 1
            
            if new_v <= 0:
                keys_to_remove.append(k)
            else:
                m_key_frequency[k] = new_v
        
        # Cleanup zeroed out ghosts
        for k in keys_to_remove:
            del m_key_frequency[k]
            if k in m_key_last_access:
                del m_key_last_access[k]
                
    # Note: We keep the evicted object in m_key_frequency for now (as a ghost)
    # unless it was removed in the cleanup loop above.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1f3gz560.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp889i3l6h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbiyj98pr.pickle

Iteration 36: New subsample score 1.003782 is not better than old score 1.005558, skipping
Iteration 37: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeemj07vn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7_lnzage.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplp4qt7vu.pickle

Iteration 37: Proposed new text for program: # Metadata dictionaries
m_key_frequency = dict()     # Stores access frequency: F(k)
m_key_last_access = dict()   # Stores logical timestamp: T(k)

# Global State for Dynamic Aging
# L represents the 'minimum' value required to stay in the cache.
# Instead of decaying old items, we inflate the cost of new items.
m_cache_min_priority = 0.0   

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority.
    
    Priority P(k) = F(k) + L
    where:
      F(k) is the frequency of key k
      L is the dynamic aging factor (m_cache_min_priority) at the time of insertion/update.
      
    Note: Since L is applied during updates, the stored frequency m_key_frequency[key]
    effectively represents P(k).
    
    Tie-breaking: Least Recently Used (LRU) is used if priorities are equal.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    min_timestamp = float('inf')

    # We iterate to find the item with the smallest Priority.
    # If priorities match, we pick the one with the smallest Last Access Time (LRU).
    for key in candidate_keys:
        # In this implementation, m_key_frequency[key] stores the computed Priority directly
        priority = m_key_frequency.get(key, 0)
        timestamp = m_key_last_access.get(key, 0)
        
        if priority < min_priority:
            min_priority = priority
            min_timestamp = timestamp
            victim_key = key
        elif priority == min_priority:
            # Tie-breaker: LRU
            if timestamp < min_timestamp:
                min_timestamp = timestamp
                victim_key = key
    
    # Update the global aging factor L to the priority of the evicted item.
    # This ensures new items must meet this threshold to survive.
    if victim_key is not None:
        m_cache_min_priority = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency (timestamp).
    2. Increment Priority.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Standard LFU-DA update: P_new = P_old + 1
    # Note: Some variations reset P to L + Freq + 1, but simple increment works 
    # well to preserve accumulated history while allowing the global L floor to rise.
    current_priority = m_key_frequency.get(obj.key, m_cache_min_priority)
    m_key_frequency[obj.key] = current_priority + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Set recency.
    2. Initialize Priority to L + 1.
       This gives the new item a "fighting chance" against existing items
       because it inherits the base cost (L) of the cache.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Initialize Priority
    # The new item starts with the current global aging factor (L) plus an initial count.
    # This prevents the "Cold Start" problem where new items with freq=1 are immediately evicted.
    m_key_frequency[obj.key] = m_cache_min_priority + 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpto_tpava.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp53ipbxq1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp33fsc5hc.pickle

Iteration 37: New subsample score 0.49903299999999995 is not better than old score 0.602604, skipping
Iteration 38: Selected program 5 score: 0.23234020833333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpso5ezwqj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpikf8vw_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpilghkqvh.pickle

Iteration 38: Proposed new text for program: # Metadata dictionaries
# Tracks the last access time (Recency) for all items
m_access_time = dict()
# Tracks items considered "Protected" (High priority)
m_protected = set()

# Configuration
PROTECTED_RATIO = 0.8  # Max 80% of items can be protected

def evict(cache_snapshot, obj):
    '''
    Evicts based on Segmented LRU (SLRU).
    Priority 1: Evict LRU from the Probation (non-protected) segment.
    Priority 2: If no probation items exist, evict LRU from Protected segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate candidates into Probation and Protected lists
    probation_candidates = []
    protected_candidates = []
    
    for key in candidate_keys:
        if key in m_protected:
            protected_candidates.append(key)
        else:
            probation_candidates.append(key)
            
    victim_key = None
    
    # 1. Try to evict from Probation segment (Filter out one-hit wonders)
    if probation_candidates:
        # Find LRU in probation
        victim_key = probation_candidates[0]
        min_time = m_access_time.get(victim_key, 0)
        
        for key in probation_candidates[1:]:
            t = m_access_time.get(key, 0)
            if t < min_time:
                min_time = t
                victim_key = key
    
    # 2. If Probation is empty, evict LRU from Protected
    elif protected_candidates:
        victim_key = protected_candidates[0]
        min_time = m_access_time.get(victim_key, 0)
        
        for key in protected_candidates[1:]:
            t = m_access_time.get(key, 0)
            if t < min_time:
                min_time = t
                victim_key = key
                
    # Fallback (should not be reached if cache is not empty)
    if victim_key is None:
        victim_key = candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency.
    2. If item is in Probation, promote to Protected.
    3. Balance segments if Protected becomes too large.
    '''
    global m_access_time, m_protected
    
    # Update global recency
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # Promotion Logic
    if obj.key not in m_protected:
        m_protected.add(obj.key)
        
    # Balancing Logic: Ensure Protected segment doesn't starve Probation
    # If Protected set is larger than specific ratio of total cache count
    current_cache_size = len(cache_snapshot.cache)
    if len(m_protected) > current_cache_size * PROTECTED_RATIO:
        # We need to demote the LRU item within the Protected set back to Probation.
        # Note: We only look at keys currently in cache AND in protected set.
        protected_in_cache = [k for k in m_protected if k in cache_snapshot.cache]
        
        if protected_in_cache:
            lru_protected_key = protected_in_cache[0]
            min_ts = m_access_time.get(lru_protected_key, 0)
            
            for k in protected_in_cache[1:]:
                ts = m_access_time.get(k, 0)
                if ts < min_ts:
                    min_ts = ts
                    lru_protected_key = k
            
            # Demote: Remove from protected set (effectively moves to Probation)
            m_protected.remove(lru_protected_key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Update recency.
    2. New items always start in Probation (not in m_protected).
    '''
    global m_access_time, m_protected
    
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # Ensure it starts in Probation
    if obj.key in m_protected:
        m_protected.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Cleanup metadata to prevent memory leaks.
    '''
    global m_access_time, m_protected
    
    # Clean up the evicted object's metadata
    if evicted_obj.key in m_access_time:
        del m_access_time[evicted_obj.key]
        
    if evicted_obj.key in m_protected:
        m_protected.remove(evicted_obj.key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp88n1yehh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8bo4tlj1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpegbbptoa.pickle

Iteration 38: New subsample score 0.782551 is not better than old score 0.807444, skipping
Iteration 39: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplclx7fbg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu_5y5zxq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptonfrx32.pickle

Iteration 39: Proposed new text for program: import math

# --- Metadata Globals ---

# Stores active cache metadata.
# Key -> Node(type, last_access_time)
# Types: 'T1' (Recent/Probation), 'T2' (Frequent/Protected)
m_meta = dict()

# Ghost Registries (Evicted keys)
# We store just the key presence. In a full ARC, these are strict queues, 
# but here we use dicts for O(1) lookups and use access time to approximate LRU behavior.
m_ghost_b1 = dict() # Ghost Recent: Evicted from T1
m_ghost_b2 = dict() # Ghost Frequent: Evicted from T2

# Adaptive Parameter 'p'
# Target size for the T1 (Recent) list.
# 0 <= p <= capacity.
m_p = 0

def get_lru_key(candidate_keys, key_subset_filter=None):
    """
    Helper to find the LRU key among candidates, optionally filtering by a set/type.
    """
    lru_key = None
    min_time = float('inf')
    
    for k in candidate_keys:
        # If a filter is provided (e.g., only look at T1 items), skip others
        if key_subset_filter and m_meta.get(k, {}).get('type') != key_subset_filter:
            continue
            
        # Standard LRU search
        access_time = m_meta.get(k, {}).get('time', 0)
        if access_time < min_time:
            min_time = access_time
            lru_key = k
            
    return lru_key

def evict(cache_snapshot, obj):
    '''
    ARC-like eviction logic.
    We decide whether to evict from T1 (Recent) or T2 (Frequent) based on target `p`.
    '''
    global m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Classify current cache content
    t1_keys = [k for k in candidate_keys if m_meta.get(k, {}).get('type') == 'T1']
    t2_keys = [k for k in candidate_keys if m_meta.get(k, {}).get('type') == 'T2']
    
    len_t1 = len(t1_keys)
    
    # ARC Eviction Logic:
    # We want T1 to have roughly 'm_p' items.
    # If len(T1) > p, we have too many recent items, evict LRU from T1.
    # Else, we satisfy the recent quota, so we evict LRU from T2 to make space.
    
    victim_key = None
    
    # Robustness check: if T1 is empty, must evict T2. If T2 empty, must evict T1.
    if len_t1 > 0 and (len_t1 > m_p or len(t2_keys) == 0):
        # Evict LRU from T1
        victim_key = get_lru_key(t1_keys) # t1_keys is the filter
        # Fallback if logic fails (shouldn't happen given len_t1 > 0)
        if not victim_key: victim_key = get_lru_key(candidate_keys)
    else:
        # Evict LRU from T2
        victim_key = get_lru_key(t2_keys)
        # Fallback
        if not victim_key: victim_key = get_lru_key(candidate_keys)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item was in T1 (Recent), move to T2 (Frequent).
    If item was in T2, update time (stay in T2).
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    if obj.key in m_meta:
        # Update timestamp
        m_meta[obj.key]['time'] = current_time
        
        # If it was in T1 (Probation), a second hit promotes it to T2 (Protected)
        if m_meta[obj.key]['type'] == 'T1':
            m_meta[obj.key]['type'] = 'T2'
    else:
        # Edge case: Metadata missing (should not happen in consistent simulation)
        m_meta[obj.key] = {'type': 'T2', 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Adapt 'p' based on Ghost hits.
    2. Add new item to T1 (Recent).
    '''
    global m_meta, m_ghost_b1, m_ghost_b2, m_p
    
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: Sim treats capacity as count usually, but provided as bytes/size. Assuming size=1 count logic or utilizing capacity purely for calculation boundaries.
    
    # Check Ghost Hits (Did we evict something we shouldn't have?)
    if obj.key in m_ghost_b1:
        # Hit in Ghost Recent (B1). 
        # Means we evicted a recent item too soon. We should expand T1 size (p).
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) // len(m_ghost_b1)
        m_p = min(m_p + delta, len(cache_snapshot.cache) + 1) # Bound by current size (approx capacity)
        
        del m_ghost_b1[obj.key]
        # Promoted straight to T2 because it's a recalled item (Frequency > 1 logic)
        new_type = 'T2' 
        
    elif obj.key in m_ghost_b2:
        # Hit in Ghost Frequent (B2).
        # Means we evicted a frequent item too soon. We should shrink T1 size (p) to make room for T2.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
        m_p = max(m_p - delta, 0)
        
        del m_ghost_b2[obj.key]
        # Promoted straight to T2
        new_type = 'T2'
        
    else:
        # Brand new item, never seen or long forgotten.
        # Enters T1 (Recent)
        new_type = 'T1'

    # Insert Metadata
    m_meta[obj.key] = {'type': new_type, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted metadata to the corresponding Ghost Registry.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2
    
    if evicted_obj.key in m_meta:
        evicted_type = m_meta[evicted_obj.key]['type']
        
        # Move to Ghost lists
        if evicted_type == 'T1':
            m_ghost_b1[evicted_obj.key] = True
        else:
            m_ghost_b2[evicted_obj.key] = True
            
        del m_meta[evicted_obj.key]
    
    # Maintenance: Limit Ghost Registry Size to prevent memory leaks
    # Standard ARC keeps ghost size roughly equal to cache capacity.
    current_cache_len = len(cache_snapshot.cache)
    
    # Simple cleanup strategy: If ghosts exceed 2x Capacity (conservative), prune.
    # Since we can't easily iter-delete without order, we rely on the fact that
    # Python dicts are insertion ordered.
    if len(m_ghost_b1) > current_cache_len:
        try:
            oldest = next(iter(m_ghost_b1))
            del m_ghost_b1[oldest]
        except: pass
        
    if len(m_ghost_b2) > current_cache_len:
        try:
            oldest = next(iter(m_ghost_b2))
            del m_ghost_b2[oldest]
        except: pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg6xtnsv0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoesp8m_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn51x44pm.pickle

Iteration 39: New subsample score 1.02751 is not better than old score 1.029898, skipping
Iteration 40: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvj05iwuj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwtj7xgb0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8n02vw0k.pickle

Iteration 40: Proposed new text for program: # Segmented LRU (SLRU) Implementation with Ghost Cache

# Metadata dictionaries behaving as Ordered Sets
# meta_protected: 'Protected' segment for popular items. Keys mapped to None. LRU at front.
meta_protected = dict()
# meta_probation: 'Probation' segment for new items. Keys mapped to None. LRU at front.
meta_probation = dict()
# meta_ghost: 'Ghost' cache to track recently evicted items. Keys mapped to None. LRU at front.
meta_ghost = dict()

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Strategy.
    
    We maintain two segments:
    1. Probation: For new items. They are evicted first unless accessed again.
    2. Protected: For popular items (accessed >= 2 times).
    
    Strategy:
    1. Balance segments: Ensure Protected doesn't exceed a specific ratio (e.g., 80%) of the total cache count.
       If it does, demote the LRU of Protected to the MRU of Probation.
    2. Pick Victim: Choose the LRU of Probation. If Probation is empty, choose LRU of Protected.
    '''
    global meta_protected, meta_probation
    
    # Determine current operational size (number of objects)
    # When evict is called, the cache is effectively full.
    current_count = len(cache_snapshot.cache)
    
    # Define target capacity for Protected segment (e.g., 80% of total items)
    target_protected = int(current_count * 0.8)
    
    # 1. Segment Balancing
    # If Protected is too big, demote its LRU items to Probation (give them a second chance)
    while len(meta_protected) > target_protected:
        # Get LRU from Protected
        try:
            demoted_key = next(iter(meta_protected))
            del meta_protected[demoted_key]
            
            # Move to Probation (as MRU, giving it a grace period)
            # We check if it's in cache to ensure metadata consistency
            if demoted_key in cache_snapshot.cache:
                meta_probation[demoted_key] = None
        except StopIteration:
            break
            
    # 2. Select Victim
    victim_key = None
    
    # Preference 1: Evict from Probation (LRU)
    if meta_probation:
        # Iterate to find the first key that is effectively in the cache
        # (Handles potential rare async state drifts)
        for k in list(meta_probation.keys()):
            if k in cache_snapshot.cache:
                victim_key = k
                break
            else:
                # Clean up stale metadata if found
                del meta_probation[k]
    
    # Preference 2: Evict from Protected (LRU) - only if Probation is empty/invalid
    if victim_key is None and meta_protected:
        for k in list(meta_protected.keys()):
            if k in cache_snapshot.cache:
                victim_key = k
                break
            else:
                del meta_protected[k]
                
    # Fallback (Should not happen in consistent state)
    if victim_key is None:
        victim_key = list(cache_snapshot.cache.keys())[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If object is in Probation, promote it to Protected (Frequency > 1).
    - If object is in Protected, update its LRU position (move to MRU).
    '''
    global meta_protected, meta_probation
    
    key = obj.key
    
    if key in meta_probation:
        # Promote: Probation -> Protected
        del meta_probation[key]
        meta_protected[key] = None
    elif key in meta_protected:
        # Update usage: Move to MRU of Protected
        del meta_protected[key]
        meta_protected[key] = None
    else:
        # Safety net: If hit but not in metadata, assume Protected
        meta_protected[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - If object key is in Ghost cache (recently evicted), it means it's a recurring item.
      Insert directly into Protected segment.
    - Otherwise, it's a new item. Insert into Probation segment.
    '''
    global meta_protected, meta_probation, meta_ghost
    
    key = obj.key
    
    if key in meta_ghost:
        # Was seen recently, now back -> Protected
        del meta_ghost[key]
        meta_protected[key] = None
    else:
        # First access / Cold -> Probation
        meta_probation[key] = None
        
    # Maintain Ghost size limits
    # Ghost list prevents "one-hit wonders" from polluting Protected, 
    # but needs to be bounded to avoid memory leaks.
    # We use a heuristic limit relative to cache count.
    max_ghost_size = max(100, len(cache_snapshot.cache))
    
    if len(meta_ghost) > max_ghost_size:
        # Evict LRU from Ghost
        try:
            oldest = next(iter(meta_ghost))
            del meta_ghost[oldest]
        except StopIteration:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Remove the victim from the metadata segments.
    - Add the victim key to the Ghost cache (history) to track recency.
    '''
    global meta_protected, meta_probation, meta_ghost
    
    key = evicted_obj.key
    
    if key in meta_probation:
        del meta_probation[key]
        # Track in Ghost cache (MRU)
        meta_ghost[key] = None
    elif key in meta_protected:
        del meta_protected[key]
        # Even if evicted from Protected, we track in Ghost
        meta_ghost[key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbnfsoluy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg7e8_ndz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbc3ssngs.pickle

Iteration 40: New subsample score 0.661171 is not better than old score 0.680242, skipping
Iteration 41: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbnh9n350.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk0n28g54.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3rxvqa1k.pickle

Iteration 41: Proposed new text for program: # Metadata dictionaries
# m_key_state: Maps key -> state (0: T1 (Recency), 1: T2 (Frequency), 2: B1 (Ghost Recency), 3: B2 (Ghost Frequency))
m_key_state = dict()

# We track a target size for the T1 (Recency) list.
# p: Target size for the recency list (adaptive parameter)
adaptive_p = 0

# Constants for states to improve readability
STATE_T1 = 0 # In Cache, Recent
STATE_T2 = 1 # In Cache, Frequent
STATE_B1 = 2 # Ghost, Recent
STATE_B2 = 3 # Ghost, Frequent

# Track separate LRU lists to find victims efficiently.
# Since we cannot modify the cache object structure, we maintain our own ordering lists.
list_t1 = [] # Keys in T1 (Recency)
list_t2 = [] # Keys in T2 (Frequency)
list_b1 = [] # Keys in B1 (Ghost Recency)
list_b2 = [] # Keys in B2 (Ghost Frequency)

def _remove_from_list(l, key):
    """Helper to remove key from a list if present."""
    if key in l:
        l.remove(key)

def _move_to_mru(l, key):
    """Helper to move a key to the end (MRU position) of a list."""
    if key in l:
        l.remove(key)
    l.append(key)

def evict(cache_snapshot, obj):
    '''
    Evicts an object based on the Adaptive Replacement Cache (ARC) logic.
    We decide whether to evict from T1 (Recent) or T2 (Frequent) based on the 
    adaptive parameter `p` and the current size of T1.
    '''
    global list_t1, list_t2, list_b1, list_b2, adaptive_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Identify which list to evict from
    # Logic: If len(T1) > p, we have too many recent items, evict from T1.
    #        Otherwise, evict from T2.
    # Note: We must ensure the victim is actually in the cache snapshot.
    
    t1_size = len(list_t1)
    victim_key = None
    
    # We prefer to evict from T1 if it exceeds the target size 'p'
    if t1_size > 0 and (t1_size > adaptive_p or not list_t2):
        # Evict LRU of T1
        # Validate that the item is actually in the cache (sync check)
        for k in list_t1:
            if k in cache_snapshot.cache:
                victim_key = k
                break
        
        # If we found a victim in T1, we treat it as moving to B1 (Ghost Recency)
        # The actual move happens in update_after_evict usually, but we need to know WHO here.
        
    # If we didn't pick from T1, or T1 was empty, pick from T2
    if victim_key is None and list_t2:
        for k in list_t2:
            if k in cache_snapshot.cache:
                victim_key = k
                break

    # Fallback: if our lists are out of sync with the cache for some reason, 
    # pick the first available key (should rarely happen)
    if victim_key is None:
        victim_key = candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    If it's in T1, move to T2 (it has been seen twice).
    If it's in T2, move to MRU of T2.
    '''
    global list_t1, list_t2, m_key_state
    key = obj.key
    
    state = m_key_state.get(key)

    if state == STATE_T1:
        # Promotion: Recent -> Frequent
        _remove_from_list(list_t1, key)
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2
        
    elif state == STATE_T2:
        # Update: Frequent -> MRU Frequent
        _move_to_mru(list_t2, key)
        # State remains T2

    # If state is None or Ghost (B1/B2), it technically counts as a miss in the
    # pure ARC model, but here `update_after_hit` implies it WAS in `cache_snapshot.cache`.
    # This usually means it was already in T1 or T2.

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (Cache Miss):
    Check if it was in Ghost lists (B1 or B2) to adapt `p`.
    Then insert into T1 (Recency).
    '''
    global list_t1, list_t2, list_b1, list_b2, m_key_state, adaptive_p
    
    key = obj.key
    capacity = cache_snapshot.capacity // obj.size if obj.size > 0 else 100 # Approx capacity in count
    # Since we only get total capacity in bytes, we estimate count capacity roughly or 
    # rely on the lists. For ARC, 'c' is usually the cache count capacity.
    # We will use the current number of items + 1 as a proxy for capacity if needed,
    # or just cap our ghost lists.
    
    state = m_key_state.get(key)

    if state == STATE_B1:
        # Ghost Hit on Recency History -> We need a larger Recency list.
        # Increase p
        delta = 1
        if len(list_b1) >= len(list_b2) and len(list_b2) > 0:
            delta = 1
        elif len(list_b2) > 0:
            delta = len(list_b1) / len(list_b2)
            
        adaptive_p = adaptive_p + delta
        # remove from B1
        _remove_from_list(list_b1, key)
        
        # Move to T2 (since it was seen recently, and now seen again)
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2

    elif state == STATE_B2:
        # Ghost Hit on Frequency History -> We need a larger Frequency list (smaller p).
        # Decrease p
        delta = 1
        if len(list_b2) >= len(list_b1) and len(list_b1) > 0:
            delta = 1
        elif len(list_b1) > 0:
            delta = len(list_b2) / len(list_b1)
            
        adaptive_p = adaptive_p - delta
        # remove from B2
        _remove_from_list(list_b2, key)
        
        # Move to T2
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2

    else:
        # Totally new item.
        # Insert into T1 (Recency)
        _move_to_mru(list_t1, key)
        m_key_state[key] = STATE_T1

    # Clamp p
    current_cache_size = len(cache_snapshot.cache)
    if adaptive_p < 0: adaptive_p = 0
    if adaptive_p > current_cache_size: adaptive_p = current_cache_size
    
    # Initialize p if it's 0 and we are just starting (heuristic)
    if adaptive_p == 0 and current_cache_size > 0 and not list_b1 and not list_b2:
        adaptive_p = current_cache_size / 2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Move the victim from T1/T2 to B1/B2 (Ghost lists).
    '''
    global list_t1, list_t2, list_b1, list_b2, m_key_state
    
    key = evicted_obj.key
    state = m_key_state.get(key)
    
    if state == STATE_T1:
        _remove_from_list(list_t1, key)
        # Move to B1
        _move_to_mru(list_b1, key)
        m_key_state[key] = STATE_B1
    elif state == STATE_T2:
        _remove_from_list(list_t2, key)
        # Move to B2
        _move_to_mru(list_b2, key)
        m_key_state[key] = STATE_B2
        
    # Maintain Ghost List Sizes
    # To prevent memory leaks, we restrict the size of B1 and B2.
    # Standard ARC sets |T1| + |B1| = c and |T2| + |B2| = 2c.
    # We will simply cap them at the current cache size.
    current_cache_count = len(cache_snapshot.cache)
    max_ghost_size = max(current_cache_count, 100) # Ensure at least some history
    
    while len(list_b1) > max_ghost_size:
        k = list_b1.pop(0) # Remove LRU of B1
        if k in m_key_state: del m_key_state[k]

    while len(list_b2) > max_ghost_size:
        k = list_b2.pop(0) # Remove LRU of B2
        if k in m_key_state: del m_key_state[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwfsz91co.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_xswszd2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfcn9fqtk.pickle

Iteration 41: New subsample score 0.169448 is better than old score 0.169424. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp38j45psu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvu1gpuyw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplpbazy0z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprd0pvet9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi2bh3izn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfy25ggdm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_ixy9_b4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcn1bqdpi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8oz3k9dn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw2kjbldu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpef_cz7_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy_lfzj0r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpouc9y3cb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp01rzeaah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdreffkzw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpthlny997.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmlt6841f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxcwp_zxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoqth8c4n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgt3y1urf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3jh79iwu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5ooposw_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp64kis_9s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe_jccagv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkn5zrhy3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6h9l6jd3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0q_8xbvm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpenqe6bqi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2f0jv_87.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphop_zqt5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu5ekrncg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4twzmgis.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpats3nol7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_14kv9ey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf19qxbuw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfe1r7snm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzuxe0m12.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyh6_giqq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcyj6t6v1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3__o3zed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbqh1pgc0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz1wtv1t_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjwyqbe4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpge6f8uv1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4gcfktio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsljt2drn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpso0vltxp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp541si4j3.pickle

Iteration 41: Full valset score for new program: 0.22436612499999994
Iteration 41: Full train_val score for new program: 0.22436612499999994
Iteration 41: Individual valset scores for new program: [0.487785, 0.466262, 0.474274, 0.42269, 0.484944, 0.475383, 0.272727, 0.498034, 0.539864, 0.531017, 0.091667, 0.30373, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.351032, 0.026164, 0.058672, 0.058672, 0.269775, 0.305444, 0.781529, 0.884135, 0.021563, 0.036364, 0.045558, 0.000144, 8.4e-05, 0.729578, 0.083333, 0.067961, 0.009183, 0.640392, 0.125461, 0.023011, 0.022602, 0.024255, 0.052632, 0.116667, 0.021346, 0.023282, 0.466258, 0.055556]
Iteration 41: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 41: Full valset pareto front score: 0.25813520833333325
Iteration 41: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {3, 5, 6, 7, 12}, {5}, {1, 3, 4, 8, 10, 11, 12}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12}, {3}, {6}, {1, 3, 4, 5, 6, 8, 10, 11, 12}, {1, 3, 4, 5, 6, 8, 10, 11, 12}, {1}, {6}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 5, 7, 8, 10, 11, 12}, {8, 10, 11, 4}, {8, 10, 11, 4}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12}, {1, 4, 7, 8, 10, 11, 12}, {8, 11}, {6}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12}, {1, 3, 4, 8, 10, 11}]
Iteration 41: Best valset aggregate score so far: 0.2510750416666666
Iteration 41: Best program as per aggregate score on train_val: 4
Iteration 41: Best program as per aggregate score on valset: 4
Iteration 41: Best score on valset: 0.2510750416666666
Iteration 41: Best score on train_val: 0.2510750416666666
Iteration 41: Linear pareto front program index: 4
Iteration 41: New program candidate index: 12
Iteration 42: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn6xcpc6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg3p_5uvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_dn9jk4e.pickle

Iteration 42: Proposed new text for program: import math

# Global Metadata
# m_map: Stores metadata for items currently in cache or ghost lists.
# Structure: key -> {'list_type': 'T1'/'T2'/'B1'/'B2', 'access_time': int}
# T1: Recent (Probation) | T2: Frequent (Protected)
# B1: Ghost Recent       | B2: Ghost Frequent
m_map = dict()

# Adaptive Parameter 'p': Target size for the T1 (Probation) list.
# 0 <= p <= capacity
m_p = 0
m_capacity_tracker = 0 # To store capacity from snapshot for logic

def get_victim_score(key, current_time):
    '''
    We implement the eviction logic inside the `evict` function itself using the ARC strategy.
    However, the framework requires this function to return a score.
    To bridge the gap, we will use this function to identifying which list (T1 or T2)
    the key belongs to and return a score that aligns with the decision made in `evict`.
    
    Higher score = High priority to evict.
    '''
    global m_map, m_p, m_capacity_tracker
    
    if key not in m_map:
        return -1.0

    meta = m_map[key]
    list_type = meta['list_type']
    access_time = meta['access_time'] # Smaller means older (LRU)

    # ARC Logic determines whether we evict from T1 or T2 based on the target 'p'.
    # We count how many items are currently in T1.
    t1_count = sum(1 for k, v in m_map.items() if v['list_type'] == 'T1')
    
    # We want to evict the LRU of T1 if len(T1) > p
    # We want to evict the LRU of T2 if len(T1) <= p
    
    # Construction of Score:
    # 1. Base Score separates the preferred victim list from the safe list.
    # 2. Tie-breaker is staleness (Time - access_time). Oldest gets highest score.
    
    staleness = current_time - access_time
    
    # If T1 is overflowing (len(T1) > p), we prefer evicting from T1.
    # If T1 is under quota, we prefer evicting from T2.
    evict_t1_preference = t1_count > m_p
    
    if evict_t1_preference:
        if list_type == 'T1':
            # Highest priority: Base 2e12 + staleness
            return 2_000_000_000_000 + staleness
        elif list_type == 'T2':
            # Lower priority: Base 0 + staleness
            return 0 + staleness
    else:
        if list_type == 'T2':
            # Highest priority: Base 2e12 + staleness
            return 2_000_000_000_000 + staleness
        elif list_type == 'T1':
             # Lower priority: Base 0 + staleness
            return 0 + staleness

    return 0.0

def evict(cache_snapshot, obj):
    '''
    Selects the victim. 
    The logic is encoded in `get_victim_score` which dynamically adjusts
    priorities based on the adaptive parameter `p` and current list sizes.
    '''
    global m_capacity_tracker
    
    # Update capacity tracker for use in logic
    m_capacity_tracker = cache_snapshot.capacity
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0

    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1 (Probation), move to T2 (Protected).
    If item is in T2, move to MRU position of T2.
    '''
    global m_map
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    if key in m_map:
        # Move to T2 (Frequent) regardless of whether it was T1 or T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time
    else:
        # Edge case: Hit in cache but missing metadata (should not happen usually)
        m_map[key] = {'list_type': 'T2', 'access_time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if it was in Ghost lists (B1 or B2).
    2. Adapt parameter 'p' based on ghost hits.
    3. Insert into T2 if it came from ghost, or T1 if brand new.
    '''
    global m_map, m_p, m_capacity_tracker
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # Ensure capacity is known
    capacity = cache_snapshot.capacity
    
    # Case 1: Brand new item (Not in T1, T2, B1, B2)
    if key not in m_map:
        # Insert into T1 (MRU)
        m_map[key] = {'list_type': 'T1', 'access_time': current_time}
        return

    # Case 2: History Hit (Ghost List)
    list_type = m_map[key]['list_type']
    
    # Sizes of ghost lists
    b1_size = sum(1 for v in m_map.values() if v['list_type'] == 'B1')
    b2_size = sum(1 for v in m_map.values() if v['list_type'] == 'B2')
    
    if list_type == 'B1':
        # Hit in Ghost Recent. We should have kept T1 larger.
        # Delta logic from ARC
        delta = 1 if b1_size >= b2_size else b2_size / b1_size
        m_p = min(capacity, m_p + delta)
        
        # Promote to T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time
        
    elif list_type == 'B2':
        # Hit in Ghost Frequent. We should have kept T2 larger (T1 smaller).
        delta = 1 if b2_size >= b1_size else b1_size / b2_size
        m_p = max(0, m_p - delta)
        
        # Promote to T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time

    # If it was somehow already T1 or T2 (should be handled by hit), just update time
    else:
        m_map[key]['access_time'] = current_time

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted item from Active list (T1/T2) to Ghost list (B1/B2).
    Prune Ghost lists if they get too huge.
    '''
    global m_map
    
    key = evicted_obj.key
    if key not in m_map:
        return
        
    prev_type = m_map[key]['list_type']
    
    if prev_type == 'T1':
        m_map[key]['list_type'] = 'B1' # Ghost Recent
    elif prev_type == 'T2':
        m_map[key]['list_type'] = 'B2' # Ghost Frequent
        
    # Garbage Collection for Metadata
    # ARC generally suggests maintaining B1+B2 size roughly equal to Cache Capacity.
    # Total tracked items (T1+T2+B1+B2) <= 2 * Capacity
    
    # Get all keys in ghost lists
    ghost_keys = [k for k, v in m_map.items() if v['list_type'] in ('B1', 'B2')]
    
    if len(ghost_keys) > cache_snapshot.capacity:
        # Find oldest ghost to remove
        # This scan is linear, but metadata size is constrained, usually acceptable.
        oldest_key = None
        min_time = float('inf')
        
        for k in ghost_keys:
            if m_map[k]['access_time'] < min_time:
                min_time = m_map[k]['access_time']
                oldest_key = k
        
        if oldest_key:
            del m_map[oldest_key]
Killing subprocess...
Killing subprocess...
Killing subprocess...
Iteration 42: New subsample score 0.0 is not better than old score 1.3151890000000002, skipping
Iteration 43: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0b2peqkv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpomszma7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5nxgfuew.pickle

Iteration 43: Proposed new text for program: import math

# --- Metadata Globals ---
# Tracks the access frequency of a key (Frequency)
m_freq = {}
# Tracks the computed GDSF priority value
m_priority = {}
# Tracks the last access time (for LRU tie-breaking)
m_last_access = {}

# The "Inflation" value (Logical Clock) for GDSF
# This value rises over time, acting as the aging mechanism.
g_inflation = 0.0

# --- Constants ---
# Limit metadata size to prevent memory leaks in long simulations
MAX_METADATA_SIZE = 100000 

def _get_size_cost(size):
    '''
    Returns the cost denominator.
    We ensure size is at least 1 to avoid division by zero.
    We interpret size strictly to maximize object hit rate.
    '''
    return max(1, size)

def _cleanup_metadata(current_cache_keys):
    '''
    Removes old metadata to prevent memory leaks.
    Only removes keys that are NOT currently in the cache.
    '''
    global m_freq, m_priority, m_last_access
    
    if len(m_freq) > MAX_METADATA_SIZE:
        # Identify keys not in cache
        cache_set = set(current_cache_keys)
        deletion_candidates = []
        
        # We want to remove items that are not in cache AND have low priority/frequency
        # To be fast, we just sample or iterate once.
        # Here we just iterate and remove the first N non-cached candidates found
        # to bring size down slightly.
        count = 0
        keys = list(m_freq.keys())
        for k in keys:
            if k not in cache_set:
                deletion_candidates.append(k)
                count += 1
                # Prune in batches of 1000 to keep it responsive
                if count >= 1000:
                    break
        
        for k in deletion_candidates:
            m_freq.pop(k, None)
            m_priority.pop(k, None)
            m_last_access.pop(k, None)

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the object with the lowest Priority value.
    Updates the global Inflation value (g_inflation) to the priority of the evicted object.
    '''
    global m_priority, m_last_access, g_inflation
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim = None
    min_p = float('inf')
    # We use min_access for tie-breaking (LRU)
    min_access = float('inf') 

    # Linear scan to find victim (Allowed per context)
    for key in candidate_keys:
        p = m_priority.get(key, 0.0)
        access_time = m_last_access.get(key, 0)
        
        # We look for the smallest Priority
        if p < min_p:
            min_p = p
            victim = key
            min_access = access_time
        elif p == min_p:
            # Tie-breaker: Evict the Least Recently Used among those with equal priority
            if access_time < min_access:
                victim = key
                min_access = access_time

    # GDSF Aging Mechanism:
    # Update the global inflation factor to the priority of the item being evicted.
    # This ensures that future insertions start at a higher base priority,
    # effectively aging out existing items that don't get refreshed.
    if victim is not None:
        g_inflation = min_p

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Recency.
    3. Recalculate Priority based on new Frequency and current Inflation.
    '''
    global m_freq, m_priority, m_last_access, g_inflation
    
    key = obj.key
    
    # Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Update Priority
    # Formula: Priority = Inflation + (Frequency / Size)
    # Note: We use current g_inflation. When an item is hit, it is "brought up to date"
    # with the current aging clock.
    size_cost = _get_size_cost(obj.size)
    m_priority[key] = g_inflation + (m_freq[key] / size_cost)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Restore Frequency (Ghost Cache logic).
    2. Set Recency.
    3. Calculate initial Priority.
    '''
    global m_freq, m_priority, m_last_access, g_inflation
    
    key = obj.key
    
    # Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Frequency Logic (Ghost Cache)
    if key in m_freq:
        # We've seen this before (recently evicted). 
        # Increase frequency to indicate it's a returning visitor.
        m_freq[key] += 1
    else:
        # Brand new item
        m_freq[key] = 1
        
    # Calculate Priority
    # Even for new items, we add g_inflation. This puts them on the "current playing field"
    # rather than at 0.
    size_cost = _get_size_cost(obj.size)
    m_priority[key] = g_inflation + (m_freq[key] / size_cost)
    
    # Cleanup if needed
    if len(m_freq) > MAX_METADATA_SIZE:
        _cleanup_metadata(cache_snapshot.cache.keys())

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    We do NOT delete the metadata immediately. 
    Keeping the metadata allows us to implement "Ghost Cache" behavior 
    (remembering frequency if the object is re-inserted later).
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzuey0ifg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7v0w17vg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3wwd0mbm.pickle

Iteration 43: New subsample score 1.031397 is not better than old score 1.105232, skipping
Iteration 44: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqaogqz33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5hm6k6kw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphftpw004.pickle

Iteration 44: Proposed new text for program: import math

# Metadata dictionary
# Key -> (frequency, last_access_time)
m_access_info = dict()

# Ghost registry to track recently evicted items (Key -> eviction_time)
# Used to give a "boost" to items that are evicted but immediately requested again (thrashing protection).
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculates a score determining likelihood of eviction.
    Higher Score = Better Victim (Likely to be evicted).
    
    Formula concept:  TimeDelta / (Frequency ^ Weight)
    
    1. If an item is very old (high TimeDelta), it should be evicted.
    2. If an item is very frequent (high Frequency), the score reduces, protecting it.
    '''
    if key not in m_access_info:
        return float('inf') # Safety fallback
        
    freq, last_access = m_access_info[key]
    
    # Time since last access
    # We add 1 to avoid division by zero or zero-staleness issues
    staleness = (current_time - last_access) + 1
    
    # We apply a logarithmic dampener to frequency. 
    # This ensures that an item accessed 100 times isn't 100x harder to evict than an item accessed once,
    # but maybe 4-5x harder. This prevents "cache pollution" by historically hot items that are now cold.
    # We use base 2 log. freq=1 -> val=1. freq=2 -> val=2. freq=4 -> val=3.
    freq_weight = math.log2(freq + 1) + 1
    
    # The Score:
    # High staleness (old) -> High Score (Evict)
    # High frequency -> Low Score (Keep)
    #
    # Items with Freq=1 (Scans) will have freq_weight ~ 1.58.
    # Items with Freq=High will have higher denominator, reducing the score drastically,
    # allowing them to survive longer periods of inactivity.
    return staleness / freq_weight

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest victim score (Oldest and Least Frequent).
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # Optimization: To avoid O(N) overhead on large caches every single evict,
    # we can employ a random sampling strategy if N is large.
    # However, to guarantee the *best* victim for the requirement, we scan all.
    # Given the constraints of the problem description usually imply logic over raw speed,
    # we will stick to a full scan for maximum hit-rate accuracy.
    
    best_victim_key = None
    max_score = -1.0
    
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time to current.
    2. Increment Frequency.
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    # Retrieve existing data
    # Default (0, current) shouldn't happen on hit, but safe coding
    freq, _ = m_access_info.get(obj.key, (0, current_time))
    
    # Update frequency
    new_freq = freq + 1
    
    m_access_info[obj.key] = (new_freq, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry. If present, this is a 'false negative' eviction (thrashing).
       Initialize with higher frequency to protect it immediately.
    2. Otherwise, initialize with base frequency.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Adaptive Logic:
    # If this key was recently evicted (it's in the ghost registry), it means our cache 
    # was too small or we evicted it prematurely. We want to "latch" it into the cache 
    # so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Boost frequency. A boost to 4 roughly simulates it having "survived" the probation period.
        initial_freq = 4
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # Remove from active stats
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prevent Ghost Registry from growing infinitely.
    # A good rule of thumb is keeping the ghost registry size proportional to cache size.
    # Here we limit it to roughly the number of items in the cache to keep memory overhead low
    # while retaining enough history to be useful.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest inserted key in the ghost registry (Python 3.7+ dicts preserve insertion order)
        # next(iter()) gets the first key efficiently.
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp18j5i34u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprvubp12m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpad_vmg8u.pickle

Iteration 44: New subsample score 1.144812 is not better than old score 1.250512, skipping
Iteration 45: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwyy7pg8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpddcsl0jz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp_raz__k.pickle

Iteration 45: Proposed new text for program: # Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
# Now persists even after eviction to handle cyclic patterns better.
m_key_frequency = dict()

# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

# Tracks the "frequency inflation" baseline. 
# This represents the minimum frequency currently in the cache, allowing new items to enter with competitive scores.
m_dynamic_aging_base = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a Dynamic LFU policy.
    Primary Metric: Frequency (with aging/inflation considerations).
    Secondary Metric: Recency (LRU).
    '''
    global m_dynamic_aging_base

    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    best_key = None
    min_score = float('inf')
    min_time = float('inf')

    # Iterate to find the object with the lowest score (frequency)
    # Tie-break with oldest timestamp
    for key in candidate_keys:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        if curr_freq < min_score:
            min_score = curr_freq
            min_time = curr_time
            best_key = key
        elif curr_freq == min_score:
            if curr_time < min_time:
                min_time = curr_time
                best_key = key
    
    # Update the dynamic aging base to the frequency of the item we are about to evict.
    # This ensures new items inserted later start relative to this baseline.
    if best_key is not None:
        m_dynamic_aging_base = m_key_frequency.get(best_key, 0)

    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Standard LFU increment
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, we use the Dynamic Aging Base.
    If the object has history (was evicted previously), we restore it + inflation.
    If it's new, it starts at the inflation base + 1.
    '''
    global m_key_frequency, m_key_last_access, m_dynamic_aging_base
    
    # Check if we have historical frequency data (Ghost entry)
    historical_freq = m_key_frequency.get(obj.key, 0)
    
    if historical_freq > 0:
        # If we remember this object, give it a boost based on current cache pressure
        # But ensure it doesn't just dominate immediately if it's been gone a long time.
        # Max(historical, base + 1) ensures it's at least competitive with current items.
        new_freq = max(historical_freq, m_dynamic_aging_base + 1)
        m_key_frequency[obj.key] = new_freq
    else:
        # Totally new object. Give it a fighting chance by starting at the current baseline + 1
        m_key_frequency[obj.key] = m_dynamic_aging_base + 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, we strictly REMOVE the Last Access Time (Recency) to save space,
    BUT we KEEP the Frequency metadata. 
    
    This creates a "Ghost Cache" effect where we remember how popular an item was 
    even if it's temporarily evicted.
    '''
    global m_key_last_access
    
    # We purposefully do NOT delete from m_key_frequency here.
    # We only delete recency to mark it as not currently in cache for LRU logic.
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    # Optional: To prevent infinite memory growth of m_key_frequency, 
    # one might implement a cleanup mechanism if the dict gets too large,
    # but for typical competitive coding/trace contexts, retaining it is usually preferred.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5m1kkcrq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0iyfrl_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmozs66xd.pickle

Iteration 45: New subsample score 0.10931700000000001 is not better than old score 0.161042, skipping
Iteration 46: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnmqjgw5z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp3j1esid.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2j9mu1jg.pickle

Iteration 46: Proposed new text for program: import math

# --- Metadata ---
# Tracks how many times a key has been accessed.
m_frequency = dict()
# Tracks the specific timestamp (access_count) of the last access.
m_last_access = dict()
# Tracks keys that were evicted. Value is True if it was evicted from T1 (Recent), False if T2 (Frequent).
m_ghosts = dict()

# --- Adaptive Parameters ---
# Target size for the "Recent/T1" logical list. 
# It adapts between 0 and capacity.
m_p = 0 
# Current count of items in the cache that effectively belong to T1 (freq=1).
m_t1_size = 0

# Constants for cleanup
MAX_GHOSTS = 100000

def evict(cache_snapshot, obj):
    '''
    Eviction Logic inspired by Adaptive Replacement Cache (ARC).
    
    We categorize items currently in cache into two logical sets:
    1. T1 (Recent): Items accessed exactly once (frequency == 1).
    2. T2 (Frequent): Items accessed more than once (frequency > 1).
    
    We rely on the adaptive parameter `m_p`, which represents the ideal size of T1.
    
    The Logic:
    - If len(T1) > m_p: We prefer evicting from T1 (the LRU item among those with freq=1).
    - If len(T1) < m_p: We prefer evicting from T2 (the LRU item among those with freq>1).
    
    Since we cannot maintain strict sorted lists in this restricted environment easily, 
    we simulate this via a scoring function that penalizes the "oversized" set.
    '''
    global m_frequency, m_last_access, m_p, m_t1_size
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Recalculate T1 size just to be safe (ensure consistency)
    m_t1_size = sum(1 for k in candidate_keys if m_frequency.get(k, 1) == 1)
    
    victim_key = None
    # We want to evict the item with the MINIMUM score.
    min_score = float('inf')
    
    # Determine which set we want to evict from based on P
    # If we have too many T1 items compared to our target P, we want to evict T1.
    evict_t1 = m_t1_size > m_p
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_acc = m_last_access.get(key, 0)
        
        # Primary sort: Classify into T1 (freq=1) or T2 (freq>1)
        is_t1 = (freq == 1)
        
        # Scoring Logic:
        # We construct a score such that:
        # 1. The preferred eviction set has significantly lower scores than the protected set.
        # 2. Within the same set, strictly LRU applies (smaller last_acc -> smaller score).
        
        # Base score is the access time (LRU behavior)
        score = last_acc
        
        if evict_t1:
            if is_t1:
                # T1 is the target. Keep score as raw timestamp (very small).
                # This ensures T1 items are evicted before T2 items.
                pass 
            else:
                # T2 is protected. Add a massive constant to timestamp to ensure 
                # it is greater than any T1 timestamp.
                score += 10**15
        else:
            # We want to evict T2 (Frequent), protecting T1.
            if is_t1:
                # T1 is protected.
                score += 10**15
            else:
                # T2 is the target.
                pass
                
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. Update frequency.
    2. Update recency.
    3. If item moves from T1 (freq=1) to T2 (freq=2), update m_t1_size.
    '''
    global m_frequency, m_last_access, m_t1_size
    
    key = obj.key
    curr_freq = m_frequency.get(key, 0)
    
    # Update Metadata
    m_last_access[key] = cache_snapshot.access_count
    m_frequency[key] = curr_freq + 1
    
    # If it was in T1, it is now in T2
    if curr_freq == 1:
        m_t1_size -= 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Check Ghost lists to adapt `m_p`.
    2. Initialize metadata.
    '''
    global m_frequency, m_last_access, m_ghosts, m_p, m_t1_size
    
    key = obj.key
    capacity = cache_snapshot.capacity # We approximate capacity using size here or assume count based
    # Note: Since capacity is bytes and we deal with objects, the logic is slightly abstract,
    # but the counts work for the algorithm logic.
    
    # Adaptation: Did we miss something recently evicted?
    if key in m_ghosts:
        was_t1_ghost = m_ghosts[key]
        
        # Delta determines how fast we adapt. 
        # If we have many T1 ghosts, B1 is large, we increment P slowly.
        # If we have many T2 ghosts, B2 is large, we decrement P slowly.
        # Simplified ARC delta:
        delta = 1 # simplified step
        
        if was_t1_ghost:
            # We evicted a T1 item too soon. We need to increase P (size of T1).
            m_p = min(len(cache_snapshot.cache) + len(m_ghosts), m_p + delta)
        else:
            # We evicted a T2 item too soon. We need to decrease P (shrink T1, grow T2).
            m_p = max(0, m_p - delta)
            
        # Remove from ghost since it's back in cache
        del m_ghosts[key]
    
    # Initialize new item
    m_frequency[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_t1_size += 1
    
    # Housekeeping
    if len(m_ghosts) > MAX_GHOSTS:
        # Prune oldest ghosts
        # In a real ARC implementation this is strict, here we just prevent memory leaks
        keys_to_del = list(m_ghosts.keys())[:100]
        for k in keys_to_del:
            del m_ghosts[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict:
    1. Update internal T1 counter.
    2. Add to Ghost list.
    '''
    global m_frequency, m_last_access, m_ghosts, m_t1_size
    
    key = evicted_obj.key
    freq = m_frequency.get(key, 1)
    
    # Track as ghost
    # True if it was T1 (freq=1), False if T2 (freq > 1)
    is_t1 = (freq <= 1)
    m_ghosts[key] = is_t1
    
    # Remove strict metadata to save memory, logic relies on Ghosts for history now
    if key in m_frequency: del m_frequency[key]
    if key in m_last_access: del m_last_access[key]
    
    if is_t1:
        m_t1_size -= 1
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4hdob8hs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ytcf7ae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo2_1wq74.pickle

Iteration 46: New subsample score 0.35860200000000003 is not better than old score 0.5175379999999999, skipping
Iteration 47: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3_sdfnqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp35nioynk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfizxdysg.pickle

Iteration 47: Proposed new text for program: # Metadata definitions
# T1: Recent keys (LRU logic for items seen once)
# T2: Frequent keys (LRU logic for items seen >1 times)
# B1: Ghost keys evicted from T1 (Recency history)
# B2: Ghost keys evicted from T2 (Frequency history)
# p: Target size for the T1 list (adaptation parameter)

m_T1 = []
m_T2 = []
m_B1 = []
m_B2 = []
m_p = 0
m_access_map = dict() # Fast lookup to find which list a key is in

def evict(cache_snapshot, obj):
    '''
    Eviction logic implementing the REPLACE subroutine of ARC.
    Decides which key to evict based on the current sizes of T1, T2 
    and the adaptive parameter 'p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_map
    
    # If the cache is empty, nothing to evict
    if not cache_snapshot.cache:
        return None

    victim_key = None
    
    # Logic to determine which list (T1 or T2) to evict from
    # We evict from T1 if it has grown larger than the target `p`.
    # Or, if T1 is within `p` but L1 (T1+B1) is massive, we might still evict from T1 
    # to make room for T2 items if the incoming item is a B2 hit.
    
    # However, standard ARC `REPLACE(p)` logic is usually called *before* insertion 
    # to make space. Since the framework here asks "who to evict" *when full*, 
    # we simulate the decision:
    
    # Basic Rule:
    # if len(T1) >= 1 and ((incoming_is_in_B2 and len(T1) == p) or (len(T1) > p)):
    #   pop from T1
    # else:
    #   pop from T2
    
    # Note: 'obj' is the new object causing the eviction. We need to know if 
    # it is in B2 to strictly follow ARC, but usually checking T1 > p is sufficient 
    # for the eviction constraint.
    
    # Check if the incoming object is in B2 (Ghost Frequency)
    in_B2 = obj.key in m_B2
    
    if len(m_T1) > 0 and (
        len(m_T1) > m_p or (in_B2 and len(m_T1) == m_p)
    ):
        # Evict LRU from T1
        victim_key = m_T1[0]
    else:
        # Evict LRU from T2
        if len(m_T2) > 0:
            victim_key = m_T2[0]
        elif len(m_T1) > 0:
            # Fallback if T2 is empty
            victim_key = m_T1[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Updates lists when an object is found in the cache (T1 or T2).
    Moves the object to the MRU position of T2 (Frequency list).
    '''
    global m_T1, m_T2, m_access_map
    
    key = obj.key
    
    # Identify where the key is currently
    # It must be in T1 or T2 since it's a hit.
    
    if key in m_T1:
        m_T1.remove(key)
        m_T2.append(key) # Move to T2 MRU
        m_access_map[key] = 'T2'
    elif key in m_T2:
        m_T2.remove(key)
        m_T2.append(key) # Move to T2 MRU (renew)
        m_access_map[key] = 'T2'
    else:
        # Theoretical edge case: cache says hit, but metadata lost it.
        # Treat as new insert to T2.
        m_T2.append(key)
        m_access_map[key] = 'T2'

def update_after_insert(cache_snapshot, obj):
    '''
    Updates lists when a new object is inserted.
    This handles three cases:
    1. Key was in Ghost List B1 (Recency miss) -> Adapt p, move to T2.
    2. Key was in Ghost List B2 (Frequency miss) -> Adapt p, move to T2.
    3. Key is totally new -> Add to T1.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_map
    
    key = obj.key
    capacity = cache_snapshot.capacity # We assume capacity is roughly max count of items
    # If capacity is bytes, we approximate max items based on current cache length 
    # or just rely on the list lengths relative to each other.
    # ARC generally relies on count of items (c). We use len(cache) as c.
    c = len(cache_snapshot.cache) 
    if c == 0: c = 100 # Safety
    
    # Case 1: Recency Hit (Ghost B1)
    if key in m_B1:
        # Adapt p: Increase T1 target size
        # We favor Recency because we just missed something that was recently evicted.
        delta = 1
        if len(m_B1) >= len(m_B2) and len(m_B2) > 0:
            delta = 1
        elif len(m_B2) > 0:
            delta = len(m_B1) / len(m_B2)
            
        m_p = min(c, m_p + delta)
        
        # Move to T2 (promote to frequent)
        m_B1.remove(key)
        m_T2.append(key)
        m_access_map[key] = 'T2'
        
    # Case 2: Frequency Hit (Ghost B2)
    elif key in m_B2:
        # Adapt p: Decrease T1 target size (Favor Frequency)
        delta = 1
        if len(m_B2) >= len(m_B1) and len(m_B1) > 0:
            delta = 1
        elif len(m_B1) > 0:
            delta = len(m_B2) / len(m_B1)
            
        m_p = max(0, m_p - delta)
        
        # Move to T2 (promote to frequent)
        m_B2.remove(key)
        m_T2.append(key)
        m_access_map[key] = 'T2'
        
    # Case 3: Totally New
    else:
        # Add to T1 MRU
        m_T1.append(key)
        m_access_map[key] = 'T1'

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Maintains the Ghost lists (B1 and B2).
    When an item is evicted from cache (T1 or T2), it moves to the corresponding ghost list.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_access_map
    
    victim_key = evicted_obj.key
    
    # The evict() function decided which key to kill, but didn't remove it from lists.
    # We remove it now and push to Ghost lists.
    
    if victim_key in m_T1:
        m_T1.remove(victim_key)
        m_B1.append(victim_key) # Add to B1 MRU
        # Cap B1 size (History shouldn't grow forever)
        # Standard ARC caps L1+B1 <= 2*c, we just cap B1 to roughly cache size
        if len(m_B1) > cache_snapshot.capacity: # loosely using capacity as limit
             # or better, simple fixed limit if capacity is bytes and huge
             if len(m_B1) > len(cache_snapshot.cache) + 100:
                 m_B1.pop(0) 
                 
        if victim_key in m_access_map: del m_access_map[victim_key]

    elif victim_key in m_T2:
        m_T2.remove(victim_key)
        m_B2.append(victim_key) # Add to B2 MRU
        # Cap B2 size
        if len(m_B2) > len(cache_snapshot.cache) + 100:
            m_B2.pop(0)

        if victim_key in m_access_map: del m_access_map[victim_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcdws7xio.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt384ee88.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp80_t79ll.pickle

Iteration 47: New subsample score 0.08881099999999999 is better than old score 0.072797. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbv_314ak.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzqbx6efj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppygctr6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm16dgnxi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc6ob_rvy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnpozt83x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc9fu7ag7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv3h71tar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5e23r5ak.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu_57a_1x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4bu465qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp386lwcnh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw_u7_y6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpulz94bit.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps59k87_8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2it9a24n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaal5rslp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq5bc09tx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp6veb2jl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpymk7suzj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpojgf2x4z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzygq008q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb2pyuziq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb2s6pxer.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7ob6_os5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdtxbmzwq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy21ko_u4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps1lhj5fi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplo5zg3ei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdkhuk31e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbr57bs9v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdjsl90d5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpusiikgep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvozgt0nj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph1bfwkhr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpar1cglld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy3auu2lr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptvw722oj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgc3n0t3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb7nwctn1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp89com7ot.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ygelcwk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgvgez8rd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0lj73w2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa6j_9id0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzc04gitn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp56ift8iy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsz8npuu1.pickle

Iteration 47: Full valset score for new program: 0.22411064583333326
Iteration 47: Full train_val score for new program: 0.22411064583333326
Iteration 47: Individual valset scores for new program: [0.48644, 0.464394, 0.471911, 0.419756, 0.483554, 0.472962, 0.270335, 0.498034, 0.540937, 0.531017, 0.091667, 0.284192, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.352016, 0.026164, 0.058672, 0.058672, 0.269774, 0.283266, 0.774578, 0.88345, 0.021563, 0.027273, 0.045558, 0.000144, 8.4e-05, 0.721749, 0.083333, 0.067961, 0.009183, 0.640392, 0.125461, 0.023011, 0.022602, 0.024255, 0.052632, 0.183333, 0.021346, 0.023282, 0.466258, 0.055556]
Iteration 47: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 47: Full valset pareto front score: 0.25813520833333325
Iteration 47: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {3, 5, 6, 7, 12, 13}, {5}, {1, 3, 4, 8, 10, 11, 12, 13}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13}, {3}, {6}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13}, {1}, {6}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13}, {8, 10, 11, 4}, {8, 10, 11, 4}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {1, 4, 7, 8, 10, 11, 12, 13}, {8, 11}, {6}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13}, {1, 3, 4, 8, 10, 11}]
Iteration 47: Best valset aggregate score so far: 0.2510750416666666
Iteration 47: Best program as per aggregate score on train_val: 4
Iteration 47: Best program as per aggregate score on valset: 4
Iteration 47: Best score on valset: 0.2510750416666666
Iteration 47: Best score on train_val: 0.2510750416666666
Iteration 47: Linear pareto front program index: 4
Iteration 47: New program candidate index: 13
Iteration 48: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj_jbs_k0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3758wu2y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyi600bjv.pickle

Iteration 48: Proposed new text for program: # Metadata dictionaries
m_key_access_time = dict()  # Tracks logical timestamp (for LRU within segments)
m_key_segment = dict()      # 0 = Probation (T1), 1 = Protected (T2)
m_ghost_registry = dict()   # Stores segment info of evicted keys: key -> segment_id
m_p = 0                     # Target size of the Probation segment (T1)

# Constants
GHOST_CAPACITY_FACTOR = 2   # How many ghosts to keep relative to cache count

def evict(cache_snapshot, obj):
    '''
    ARC-inspired Eviction Policy.
    Decides whether to evict from Probation (T1) or Protected (T2) based on 
    the adaptive parameter `m_p`.
    '''
    global m_key_access_time, m_key_segment, m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Separate candidates into T1 (Probation) and T2 (Protected)
    t1_candidates = []
    t2_candidates = []
    
    for k in candidate_keys:
        seg = m_key_segment.get(k, 0) # Default to 0 if unknown
        if seg == 0:
            t1_candidates.append(k)
        else:
            t2_candidates.append(k)

    # ARC Logic to decide which region to victimize:
    # We evict from T1 if T1 size exceeds target `p`.
    # However, if T1 is empty, we must evict from T2 (and vice versa).
    
    evict_from_t1 = False
    
    if len(t1_candidates) > 0 and len(t1_candidates) > m_p:
        evict_from_t1 = True
    elif len(t1_candidates) > 0 and len(t2_candidates) == 0:
        evict_from_t1 = True
    else:
        evict_from_t1 = False
        
    # Select the victim list
    candidates = t1_candidates if evict_from_t1 else t2_candidates
    
    # If for some reason the chosen list is empty (edge case), fallback to the other
    if not candidates:
        candidates = t2_candidates if evict_from_t1 else t1_candidates

    # Find LRU in the selected segment
    victim_key = None
    min_time = float('inf')
    
    for k in candidates:
        t = m_key_access_time.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. Update Recency.
    2. If item was in Probation (T1), promote to Protected (T2).
    '''
    global m_key_access_time, m_key_segment
    
    # Update Access Time (LRU)
    m_key_access_time[obj.key] = cache_snapshot.access_count
    
    # Promotion Logic
    # If it was in T1 (Probation), move to T2 (Protected)
    if m_key_segment.get(obj.key, 0) == 0:
        m_key_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. New items start in Probation (T1).
    2. Check Ghost Registry to adapt `m_p`.
    '''
    global m_key_access_time, m_key_segment, m_ghost_registry, m_p
    
    # Initialize metadata
    m_key_access_time[obj.key] = cache_snapshot.access_count
    m_key_segment[obj.key] = 0 # Insert into Probation (T1)
    
    # Adaptive Logic (ARC Parameter Tuning)
    if obj.key in m_ghost_registry:
        ghost_segment = m_ghost_registry[obj.key]
        
        # Calculate Delta
        # If we had a hit in T1's ghost (B1), we needed a larger T1. Increase p.
        # If we had a hit in T2's ghost (B2), we needed a larger T2. Decrease p.
        
        # Estimating current ghost sizes for delta calculation (Simplified ARC delta)
        # Note: In a strict implementation, we track B1/B2 lengths. 
        # Here we use a safe increment of 1 to ensure stability.
        
        delta = 1
        
        if ghost_segment == 0:
            # Hit in B1 (Recency Ghost) -> Increase Probation Size
            # Limit p to total number of items currently in cache
            current_count = len(cache_snapshot.cache)
            m_p = min(current_count, m_p + delta)
        else:
            # Hit in B2 (Frequency Ghost) -> Decrease Probation Size (Increase Protected)
            m_p = max(0, m_p - delta)
            
        # Promote immediately to T2 because it's a "known" item returning
        m_key_segment[obj.key] = 1
        
        # Remove from ghost
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict:
    1. Move victim to Ghost Registry.
    2. Cleanup active metadata.
    '''
    global m_key_access_time, m_key_segment, m_ghost_registry, GHOST_CAPACITY_FACTOR
    
    victim_key = evicted_obj.key
    segment = m_key_segment.get(victim_key, 0)
    
    # Add to Ghost Registry
    m_ghost_registry[victim_key] = segment
    
    # Clean up active metadata
    if victim_key in m_key_access_time:
        del m_key_access_time[victim_key]
    if victim_key in m_key_segment:
        del m_key_segment[victim_key]
        
    # Maintain Ghost Registry Size
    # If the ghost registry grows too large, remove oldest entries to save memory.
    # Using a heuristic based on cache count.
    max_ghosts = len(cache_snapshot.cache) * GHOST_CAPACITY_FACTOR
    if max_ghosts < 100: max_ghosts = 100 # Minimum safety floor
    
    if len(m_ghost_registry) > max_ghosts:
        # Remove arbitrary old item (FIFO behavior of dict in Python 3.7+)
        # This keeps the overhead low compared to tracking LRU for ghosts strictly.
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9rbn6gk4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp46o346d3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz7gxtxsk.pickle

Iteration 48: New subsample score 1.13857 is not better than old score 1.146534, skipping
Iteration 49: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvh3km382.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5nzppzdy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfxporrin.pickle

Iteration 49: Proposed new text for program: import collections

# Global State Management
# m_metadata stores: key -> [frequency, priority, insertion_tick]
m_metadata = dict()

# Global Priority Inflation Value (L)
# This represents the "minimum worth" required to stay in the cache.
m_inflation_L = 0.0

# Ghost Registry to remember frequency of recently evicted items.
# Maps key -> frequency
m_ghost_freq = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest priority.
    Strategy: LFU-DA (Least Frequently Used with Dynamic Aging).
    
    We look for the item with the minimum Priority score.
    Priority = (Frequency) + L (at time of insertion/update)
    
    The global 'L' increases as we evict items, naturally aging out
    items that haven't been updated recently.
    '''
    global m_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
    
    min_priority = float('inf')
    victim_key = None
    
    # We want to evict the item with the LOWEST priority.
    # Tie-breaking: If priorities are equal, evict the LRU one (smallest insertion_tick).
    # Since Python 3.7+, dict order is insertion order, but we track access ticks explicitly.
    
    # Optimization: To avoid O(N) scan every time, usually heaps are used, 
    # but given the constraints, we scan.
    
    victim_entry = None # Will store (priority, insertion_tick)
    
    for key in candidate_keys:
        if key in m_metadata:
            freq, priority, tick = m_metadata[key]
            
            # Comparison: Lower priority is better victim.
            # If priorities equal, lower tick (older) is better victim.
            if priority < min_priority:
                min_priority = priority
                victim_key = key
                victim_entry = (priority, tick)
            elif priority == min_priority:
                # Tie-breaker: LRU
                if victim_entry and tick < victim_entry[1]:
                    victim_key = key
                    victim_entry = (priority, tick)
    
    # Fallback if metadata is desync'd (should not happen)
    if victim_key is None:
        return candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on current L.
    3. Update Access Tick (for LRU tie-breaking).
    '''
    global m_metadata, m_inflation_L
    
    current_tick = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, old_priority, _ = m_metadata[obj.key]
        new_freq = freq + 1
        
        # LFU-DA Update Rule: Priority = New_Frequency + Current_L
        # This brings the item "up to date" with the current inflation level
        new_priority = new_freq + m_inflation_L
        
        m_metadata[obj.key] = [new_freq, new_priority, current_tick]
    else:
        # Should not happen on hit, but self-heal if necessary
        m_metadata[obj.key] = [1, 1.0 + m_inflation_L, current_tick]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine Frequency (1 if new, restored if Ghost).
    2. Calculate Priority = Frequency + L.
    '''
    global m_metadata, m_ghost_freq, m_inflation_L
    
    current_tick = cache_snapshot.access_count
    
    # Check ghost registry for history
    if obj.key in m_ghost_freq:
        initial_freq = m_ghost_freq[obj.key]
        # Remove from ghost
        del m_ghost_freq[obj.key]
    else:
        initial_freq = 1
        
    # Calculate Priority
    # New items enter with Priority = Freq + L.
    # This prevents them from being immediately evicted if L is high,
    # provided their frequency justifies it.
    priority = initial_freq + m_inflation_L
    
    m_metadata[obj.key] = [initial_freq, priority, current_tick]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Update Global L (Aging factor) to the priority of the evicted item.
    2. Save frequency to Ghost Registry.
    3. Clean up metadata.
    '''
    global m_metadata, m_ghost_freq, m_inflation_L
    
    if evicted_obj.key in m_metadata:
        freq, priority, _ = m_metadata[evicted_obj.key]
        
        # UPDATE L: L becomes the priority of the evicted object.
        # This is the standard GDSF / LFU-DA aging mechanism.
        # It ensures that only items with priority > this victim survive long term.
        m_inflation_L = priority
        
        # Save to Ghost: Remember the frequency so if it comes back, it resumes importance
        m_ghost_freq[evicted_obj.key] = freq
        
        del m_metadata[evicted_obj.key]
        
    # Maintenance: Cap the ghost registry size to prevent memory leaks.
    # A good rule of thumb is capacity * 2 or a fixed limit.
    # Since we don't have max keys, we use len(cache) as a proxy for desired scale.
    cache_capacity_count = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100
    if len(m_ghost_freq) > cache_capacity_count:
        # Remove arbitrary item (FIFO-ish in Python 3.7+)
        try:
            it = iter(m_ghost_freq)
            del m_ghost_freq[next(it)]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpta66bsj1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbhyxx8qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8p624umv.pickle

Iteration 49: New subsample score 1.1830349999999998 is not better than old score 1.3246570000000002, skipping
Iteration 50: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_s1ostdg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp387gzhze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi_vw1zwh.pickle

Iteration 50: Proposed new text for program: from collections import OrderedDict

# Metadata containers
# T1: Recency set (Currently cached, accessed once recently) -> OrderedDict[key, None]
# T2: Frequency set (Currently cached, accessed at least twice) -> OrderedDict[key, None]
# B1: Ghost Recency (Evicted from T1) -> OrderedDict[key, None]
# B2: Ghost Frequency (Evicted from T2) -> OrderedDict[key, None]
meta_t1 = OrderedDict()
meta_t2 = OrderedDict()
meta_b1 = OrderedDict()
meta_b2 = OrderedDict()

# Adaptive parameter 'p': Target size (count) for T1
p = 0

def _ensure_metadata_sync(cache_snapshot):
    '''
    Safety utility to ensure metadata isn't empty if the cache is populated.
    This handles initialization or drift cases.
    '''
    global meta_t1, meta_t2
    # If cache has items but our tracking lists are empty, rebuild blindly into T1
    if not meta_t1 and not meta_t2 and cache_snapshot.cache:
        for k in cache_snapshot.cache:
            meta_t1[k] = None

def evict(cache_snapshot, obj):
    '''
    Selects a victim key to evict.
    Uses the ARC logic:
    - Evict from T1 (Recency) if T1 size > target p.
    - Otherwise evict from T2 (Frequency).
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    
    victim_key = None
    t1_len = len(meta_t1)
    
    # ARC Eviction Decision Logic:
    # We prefer evicting from T1 if it exceeds the target size 'p'.
    # Standard ARC also checks if the *incoming* miss (obj.key) is in B2 to break ties,
    # encouraging T1 eviction if we are observing frequent items returning (B2 hits).
    
    evict_from_t1 = False
    
    if t1_len > 0:
        if t1_len > p:
            evict_from_t1 = True
        elif obj.key in meta_b2 and t1_len == int(p):
            # Special ARC condition: if we hit a ghost frequent item, we sacrifice T1
            evict_from_t1 = True
    
    # Fallback safety: if T2 is empty, must evict T1. If T1 empty, must evict T2.
    if not meta_t2:
        evict_from_t1 = True
    elif not meta_t1:
        evict_from_t1 = False
        
    if evict_from_t1:
        # LRU of T1 is the first item in OrderedDict
        victim_key = next(iter(meta_t1))
    else:
        # LRU of T2 is the first item in OrderedDict
        victim_key = next(iter(meta_t2))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    - If in T1 (Recency), promote to T2 (Frequency).
    - If in T2, update MRU position.
    '''
    global meta_t1, meta_t2
    
    _ensure_metadata_sync(cache_snapshot)
    key = obj.key
    
    if key in meta_t1:
        # Promote T1 -> T2
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Mark as most recently used in T2
        meta_t2.move_to_end(key)
    else:
        # Fallback for sync issues: assume T2
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    - Adapt parameter 'p' based on ghost hits (B1 vs B2).
    - Insert new item into T1 or T2 based on history.
    - Trim ghost lists.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    key = obj.key
    
    # Current cache count (c) implicitly defines our bounds
    current_count = len(cache_snapshot.cache)
    
    # 1. Adapt 'p'
    if key in meta_b1:
        # Hit in Ghost T1: We should have made T1 larger.
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            # If B1 is small, adapt faster
            delta = len(meta_b2) / len(meta_b1)
        
        p = min(current_count, p + delta)
        
        # Promote to T2 (it was recently evicted from T1, now back -> frequent)
        del meta_b1[key]
        meta_t2[key] = None
        
    elif key in meta_b2:
        # Hit in Ghost T2: We should have made T2 larger (T1 smaller).
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            delta = len(meta_b1) / len(meta_b2)
            
        p = max(0, p - delta)
        
        # Restore to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    else:
        # Totally new item -> T1 (Recency)
        meta_t1[key] = None

    # 2. Cleanup Ghost Lists
    # Keep ghost history bounded relative to cache size to prevent memory waste.
    # A safe upper bound is roughly the number of items in cache.
    max_ghost_size = max(current_count, 100)
    
    while len(meta_b1) > max_ghost_size:
        meta_b1.popitem(last=False) # Remove FIFO (oldest ghost)
        
    while len(meta_b2) > max_ghost_size:
        meta_b2.popitem(last=False)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move evicted key from T list to corresponding Ghost (B) list.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    
    victim_key = evicted_obj.key
    
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaaxyy6vt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpob66clo9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppx2mt2xs.pickle

Iteration 50: New subsample score 0.13477899999999998 is not better than old score 0.13477899999999998, skipping
Iteration 51: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd8k6x3do.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe02alsgm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcsge593_.pickle

Iteration 51: Proposed new text for program: # Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks logical time of last access
m_ghost_registry = dict()   # Tracks recently evicted keys (Second Chance)

# Configuration
GHOST_REGISTRY_LIMIT = 1000 # Limit memory usage for history

def evict(cache_snapshot, obj):
    '''
    Hyperbolic Eviction:
    Evicts the item with the lowest score based on: Score = Frequency / (Time Since Last Access).
    
    This naturally balances LFU (high numerator) and LRU (low denominator implies recent access).
    Items not accessed for a long time will have a growing denominator, reducing their score 
    until they are evicted.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate Duration (Recency)
        # Add 1 to ensure we don't divide by zero if accessed in the same tick 
        # (or effectively prioritize very recent items correctly against freq)
        duration = (current_time - last_access) + 1
        
        # Hyperbolic Score formula
        score = freq / duration
        
        # We want to evict the item with the LOWEST score (Least valuable)
        # Tie-breaking:
        # If scores are equal, we don't explicitly force a complex check, 
        # but mathematically, for equal freq, the older item (larger duration) has lower score.
        # This naturally defaults to LRU behavior for items of same frequency.
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    # Increment frequency to reinforce this item's position
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Ghost Registry Check
    # If this key is in the ghost registry, it was evicted recently but requested again.
    # This implies our cache might be thrashing or the working set is slightly larger than capacity.
    # We give it a starting frequency boost (2) instead of (1) so it isn't immediately evicted again
    # by the Hyperbolic function.
    if obj.key in m_ghost_registry:
        m_key_frequency[obj.key] = 2
        del m_ghost_registry[obj.key]
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict: Clean active metadata and move key to ghost registry.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, GHOST_REGISTRY_LIMIT
    
    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    # Add to Ghost Registry
    # We just track presence. We don't need the old frequency, 
    # just knowing it was here recently is enough to justify the boost.
    m_ghost_registry[evicted_obj.key] = True
    
    # Prevent ghost registry from growing infinitely
    if len(m_ghost_registry) > GHOST_REGISTRY_LIMIT:
        # Remove an arbitrary item (first found) to maintain size
        # This keeps the history relevant to the "recent" past.
        iterator = iter(m_ghost_registry)
        try:
            oldest_ghost = next(iterator)
            del m_ghost_registry[oldest_ghost]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcbjqpe4k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9t8k_6gx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5f7joemt.pickle

Iteration 51: New subsample score 1.788952 is better than old score 1.7669679999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5mjvjhit.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx2wcutij.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7wvp86xh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxv00qzyy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps60o1gfq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ezhybji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpft_rtt7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqljw9r2u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3ifd0pv3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps3tbuac3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnovaogft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp__x50fgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfqi2xngb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6sxnx58i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3az6ixl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3b50tvzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppq17qlkd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp28im1b0p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo9n94wir.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcpih7q1r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9vd2134z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcct3oeka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcsndqkuo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk1wsacx6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1ihldmh9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp32npbkmb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp91suib9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa6j62cd7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqujqr86j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuf_ipd1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprzk7uky2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1k8a_h48.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5uh8itum.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdoeo4av6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf0t3f2rj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd8qb_kfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkf2g22_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5x6rjn2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg2goq9as.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6ks27bj7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfq051v_6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe3gburun.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1auclioe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpar3ka2p5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpshw4npql.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4hbjyjmw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9scrn_ja.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp09xtlija.pickle

Iteration 51: Full valset score for new program: 0.22516364583333337
Iteration 51: Full train_val score for new program: 0.22516364583333337
Iteration 51: Individual valset scores for new program: [0.473203, 0.451202, 0.458518, 0.419579, 0.467401, 0.463507, 0.26555, 0.498624, 0.540579, 0.531017, 0.066667, 0.349467, 0.023893, 0.0, 0.019822, 0.019865, 0.018715, 0.02221, 0.021375, 0.268561, 0.345133, 0.025282, 0.057382, 0.057382, 0.269775, 0.326613, 0.839126, 0.886531, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.746596, 0.072368, 0.062665, 0.009162, 0.633179, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.25, 0.020965, 0.021879, 0.441718, 0.03268]
Iteration 51: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391208, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.388105, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 51: Full valset pareto front score: 0.25813520833333325
Iteration 51: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {3, 5, 6, 7, 12, 13}, {5}, {1, 3, 4, 8, 10, 11, 12, 13}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {6}, {6}, {6}, {6}, {6}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13}, {3}, {6}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13}, {1}, {6}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13}, {8, 10, 11, 4}, {8, 10, 11, 4}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {1, 4, 7, 8, 10, 11, 12, 13}, {8, 11}, {6}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13}, {1, 3, 4, 8, 10, 11}]
Iteration 51: Best valset aggregate score so far: 0.2510750416666666
Iteration 51: Best program as per aggregate score on train_val: 4
Iteration 51: Best program as per aggregate score on valset: 4
Iteration 51: Best score on valset: 0.2510750416666666
Iteration 51: Best score on train_val: 0.2510750416666666
Iteration 51: Linear pareto front program index: 4
Iteration 51: New program candidate index: 14
Iteration 52: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp65_aby9z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj9dws3gb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8tg4zxo6.pickle

Iteration 52: Proposed new text for program: import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc5pk1nbi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoyu36bv0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxn0cfle_.pickle

Iteration 52: New subsample score 0.41464399999999996 is better than old score 0.408073. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvtocsobp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt8d9mi4x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbx6ncw69.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt7nx6mya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj2697dtr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4lzdchcd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ddwqnhe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpde95bf47.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfxtphso9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpagmmoan9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps6grik9z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxavdkooj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptfo_oudi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqykx5jln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4y5cxs6c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp92tz1wwq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpakcjivay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz8cvlcam.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp43ojxn0u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp96dm_6k3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf2x3jnpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpss70bc9c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyvfgz32l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpagkrvfg_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcm1uyto1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpruay6xi0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7l2ded0o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwofl9x97.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo_i577nx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptkocx_i7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpowoeq421.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwb6bbszz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl4pxdaea.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo0cbuc1t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpojbe6jo2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcqfhkv9e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpypyx348p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdvmb0yo1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgx42834h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpodzwhel4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq15fnqrw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwx1i88ef.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkkm2q550.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm0__y7ae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcqk8u201.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk3_04mmf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8gfry1l8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_nhy8dad.pickle

Iteration 52: Full valset score for new program: 0.23890931249999994
Iteration 52: Full train_val score for new program: 0.23890931249999994
Iteration 52: Individual valset scores for new program: [0.508018, 0.479103, 0.487304, 0.437361, 0.499599, 0.486683, 0.276316, 0.498624, 0.540937, 0.531017, 0.075, 0.391652, 0.031113, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.383481, 0.026556, 0.058672, 0.058672, 0.288549, 0.395161, 0.857001, 0.892863, 0.039533, 0.036364, 0.041002, 9.6e-05, 3.6e-05, 0.731961, 0.074561, 0.065313, 0.01826, 0.641937, 0.125461, 0.041309, 0.038485, 0.043102, 0.049342, 0.333333, 0.041778, 0.041515, 0.466258, 0.052288]
Iteration 52: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 52: Full valset pareto front score: 0.2582914583333333
Iteration 52: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {3, 5, 6, 7, 12, 13}, {15}, {1, 3, 4, 8, 10, 11, 12, 13}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15}, {1}, {15}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13}, {8, 10, 11, 4}, {8, 10, 11, 4}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {1, 4, 7, 8, 10, 11, 12, 13}, {8, 11}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15}, {1, 3, 4, 8, 10, 11}]
Iteration 52: Best valset aggregate score so far: 0.2510750416666666
Iteration 52: Best program as per aggregate score on train_val: 4
Iteration 52: Best program as per aggregate score on valset: 4
Iteration 52: Best score on valset: 0.2510750416666666
Iteration 52: Best score on train_val: 0.2510750416666666
Iteration 52: Linear pareto front program index: 4
Iteration 52: New program candidate index: 15
Iteration 53: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprdyt20u6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpli2geprc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp35wzh16o.pickle

Iteration 53: Proposed new text for program: # Metadata Globals

# Tracks items currently in the 'Protected' (high frequency) logical segment.
# Key -> True. If a key is in cache but NOT in this set, it is in 'Probation'.
m_protected = set()

# Tracks keys recently evicted from the Probationary segment.
# This helps us distinguish a "new" item from an "item that was just evicted but needed".
m_ghost = set()

# Map to track access time for LRU approximations without reordering the actual cache dict
# Key -> Last Access Time
m_access_time = dict()

# Constants
# Target ratio for Protected vs Probationary items.
# 80% protected allows a working set to stay resident while 20% buffer handles new traffic.
PROTECTED_RATIO = 0.8 
MAX_GHOST_SIZE = 5000 # Limit ghost registry size to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) / 2Q-ish approach.
    
    1. Identify candidates in Probation (cache keys NOT in m_protected).
    2. Identify candidates in Protected (cache keys IN m_protected).
    3. Prefer evicting the LRU item from Probation.
    4. If Probation is empty, evict LRU from Protected.
    '''
    global m_protected, m_access_time
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Separate candidates into Probationary and Protected lists
    probation_candidates = []
    protected_candidates = []

    for k in candidate_keys:
        if k in m_protected:
            protected_candidates.append(k)
        else:
            probation_candidates.append(k)
    
    victim_key = None
    
    # Strategy: Evict from Probation first (Filter scans)
    if probation_candidates:
        # Find LRU in Probation
        victim_key = min(probation_candidates, key=lambda k: m_access_time.get(k, 0))
    else:
        # Fallback: Evict from Protected (Working set is too large)
        victim_key = min(protected_candidates, key=lambda k: m_access_time.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency (access time).
    2. Promote to Protected status if not already there.
    '''
    global m_protected, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    # If it was in Probation, it has proven its worth. Promote to Protected.
    if key not in m_protected:
        m_protected.add(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine if this is a brand new item or a "Ghost" returning.
    2. If Ghost: Direct promotion to Protected (restoration).
    3. If New: Place in Probation.
    '''
    global m_protected, m_ghost, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    if key in m_ghost:
        # It was evicted recently from probation, but came back.
        # This implies the probation window was too short or it's a cyclic pattern.
        # Promote immediately to protect it this time.
        m_protected.add(key)
        m_ghost.remove(key)
    else:
        # Brand new item. Start in Probation.
        # Ensure it is NOT in protected set
        m_protected.discard(key)

    _manage_segment_sizes(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata for the evicted key.
    2. If it was a Probationary item, add to Ghost registry.
    '''
    global m_protected, m_ghost, m_access_time
    
    if evicted_obj:
        key = evicted_obj.key
        
        # Remove timestamp
        if key in m_access_time:
            del m_access_time[key]
            
        # Handle Segment Logic
        if key in m_protected:
            # If we evicted a Protected item, it's just gone. 
            # (We don't track ghosts for Protected items usually, as they had their chance).
            m_protected.remove(key)
        else:
            # It was in Probation. Add to Ghost so we recognize it if it comes back soon.
            m_ghost.add(key)
            
        _cleanup_ghosts()

def _manage_segment_sizes(cache_snapshot):
    '''
    Ensure the Protected segment doesn't consume the entire cache capacity,
    choking out new entries.
    '''
    global m_protected, m_access_time, PROTECTED_RATIO
    
    # Current logical capacity (number of items)
    current_count = len(cache_snapshot.cache)
    if current_count == 0: return

    # If Protected set grows too large, demote the LRU protected item to Probation.
    # This ensures we always have `1 - PROTECTED_RATIO` space for new items to be tested.
    target_protected_size = int(current_count * PROTECTED_RATIO)
    
    if len(m_protected) > target_protected_size:
        # Find LRU item *within* the Protected set
        # Since m_protected is a set, we must iterate. 
        # Intersection of cache keys and protected set:
        
        protected_in_cache = [k for k in m_protected if k in cache_snapshot.cache]
        
        if protected_in_cache:
            # Find the oldest protected item
            lru_protected = min(protected_in_cache, key=lambda k: m_access_time.get(k, 0))
            
            # Demote it: Remove from m_protected. 
            # It is now "Probationary". If accessed again, it goes back to Protected.
            # If space is needed, it acts as a high-priority eviction candidate relative to other protected items,
            # but usually, we evict actual probationary items first.
            m_protected.remove(lru_protected)

def _cleanup_ghosts():
    '''
    Prevent ghost registry from growing indefinitely.
    '''
    global m_ghost, MAX_GHOST_SIZE
    
    if len(m_ghost) > MAX_GHOST_SIZE:
        # Remove arbitrary element (sets are unordered, popping is efficient)
        m_ghost.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzgpwkrem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl25s_o2s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc_62ngu9.pickle

Iteration 53: New subsample score 1.4660360000000001 is not better than old score 1.5250169999999998, skipping
Iteration 54: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfwr1sz0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr5m52uf4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdbnvvex_.pickle

Iteration 54: Proposed new text for program: import math

# Metadata dictionaries
# Tracks the frequency of keys (LFU component)
m_key_frequency = dict()
# Tracks the insertion/update time (LRU component tie-breaking)
m_key_last_access = dict()

# Global state for Dynamic Aging (LFU-DA logic)
# This represents the "minimum worth" required to stay in the cache.
# It climbs as we evict high-frequency items.
L = 0.0

# Ghost Cache configuration
MAX_HISTORY_SIZE = 50000 
ghost_keys = set() 

def get_utility_score(key, size, current_time):
    '''
    Calculates the eviction score (Utility).
    Lower score = Higher likelihood of eviction.
    
    Formula: (Frequency + L) * (1 / log2(Size + 1))
    
    1. L (Dynamic Aging): Helps new items compete with old, frequent items.
    2. Size Penalty: Large items need significantly higher frequency to justify 
       taking up space. We use log2 to dampen the penalty so we don't 
       aggressively discriminate against moderately sized items.
    '''
    global m_key_frequency, L
    
    freq = m_key_frequency.get(key, 1)
    
    # Size-aware LFU-DA
    # We prefer small, frequent items.
    # We add 2 to log to prevent division by zero or negative results for size=0/1
    size_factor = 1.0 / math.log2(size + 2)
    
    # The score is the frequency relative to the global aging factor L, adjusted by size.
    return (freq + L) * size_factor

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Utility Score.
    '''
    global m_key_frequency, m_key_last_access, L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    # Tie-breaking variables
    min_access_time = float('inf')
    
    # We need the current "time" for LRU tie-breaking
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        score = get_utility_score(key, cached_obj.size, current_time)
        
        # Find minimum score
        if score < min_score:
            min_score = score
            victim_key = key
            min_access_time = m_key_last_access.get(key, 0)
        # Tie-breaker: If scores are equal, evict the LRU (Least Recently Used)
        elif score == min_score:
            last_acc = m_key_last_access.get(key, 0)
            if last_acc < min_access_time:
                min_access_time = last_acc
                victim_key = key
    
    # LFU-DA Logic:
    # Update the global aging factor L. 
    # L becomes the "frequency worth" of the item we just evicted.
    # This effectively "ages" all other items in the cache relative to this baseline.
    if victim_key is not None:
        L = m_key_frequency.get(victim_key, 1)

    return victim_key

def _cleanup_metadata():
    '''
    Maintains the size of the ghost/history maps to prevent memory leaks.
    '''
    global m_key_frequency, m_key_last_access, ghost_keys, MAX_HISTORY_SIZE
    
    if len(m_key_frequency) > MAX_HISTORY_SIZE:
        # Remove oldest items that are NOT in the active cache (ghosts)
        # Sorting by access time is O(N log N), but N is capped by MAX_HISTORY_SIZE.
        # To be faster, we could sample, but here we prioritize accuracy.
        sorted_keys = sorted(m_key_last_access.keys(), key=lambda k: m_key_last_access[k])
        
        deleted_count = 0
        target_delete = 1000 # Batch delete
        
        for k in sorted_keys:
            if k not in ghost_keys: 
                # Don't delete active cache metadata here, only history
                continue
                
            del m_key_frequency[k]
            del m_key_last_access[k]
            ghost_keys.remove(k)
            
            deleted_count += 1
            if deleted_count >= target_delete:
                break

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Linear increment is standard LFU. 
    # Valid optimization: Dynamic increment (give rarer items a bigger boost?)
    # For now, standard +1 is robust.
    m_key_frequency[key] = m_key_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize frequency and handle Ghost Cache logic.
    '''
    global m_key_frequency, m_key_last_access, ghost_keys, L
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in ghost_keys:
        # HIT on GHOST:
        # This item was evicted recently but requested again.
        # This implies our L value (eviction threshold) was too aggressive for this item.
        # We restore its old frequency plus a "penalty avoidance" boost.
        m_key_frequency[key] = m_key_frequency.get(key, 0) + L
        ghost_keys.remove(key)
    else:
        # NEW ITEM:
        # In LFU-DA, new items don't start at 1. They start at Frequency 1.
        # However, the eviction comparison effectively subtracts L.
        # So we initialize it such that its relative score is low but non-zero.
        m_key_frequency[key] = 1.0

    # Periodic cleanup check
    if len(m_key_frequency) > MAX_HISTORY_SIZE:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move the key to the ghost set (do not delete metadata).
    '''
    global ghost_keys
    
    # We keep the frequency and access time in the dicts, 
    # but mark it as a "ghost" entry.
    if evicted_obj:
        ghost_keys.add(evicted_obj.key)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppem3wau9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjt0sv_9t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphsj5bd3k.pickle

Iteration 54: New subsample score 0.892373 is not better than old score 0.9356129999999999, skipping
Iteration 55: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzmn04fuf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprh7eqk06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm8pg2fh2.pickle

Iteration 55: Proposed new text for program: # Metadata dictionaries
# Tracks the last access time (Recency)
m_key_last_access = dict()
# Tracks the number of hits for an object while it is in the cache (Frequency/Status)
m_key_hit_count = dict()

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Protected/Probationary LRU (Segmented approach).
    
    Strategy:
    1. Identify "Probationary" items: Items that have a hit count of 1 (accessed only once upon insert).
    2. Identify "Protected" items: Items that have a hit count > 1.
    
    Priority:
    - First, try to evict the LRU item from the "Probationary" set (scan resistance).
    - If no probationary items exist, evict the LRU item from the "Protected" set.
    '''
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for two candidates:
    # 1. The LRU item among those with hit_count == 1 (Probationary LRU)
    # 2. The LRU item among those with hit_count > 1 (Protected LRU)
    
    probation_lru_key = None
    probation_lru_time = float('inf')
    
    protected_lru_key = None
    protected_lru_time = float('inf')

    for key in candidate_keys:
        hits = m_key_hit_count.get(key, 1) # Default to 1 if missing
        last_time = m_key_last_access.get(key, 0)
        
        if hits == 1:
            # Candidate for Probationary eviction
            if last_time < probation_lru_time:
                probation_lru_key = key
                probation_lru_time = last_time
        else:
            # Candidate for Protected eviction
            if last_time < protected_lru_time:
                protected_lru_key = key
                protected_lru_time = last_time

    # DECISION LOGIC:
    # If we have a probationary item, evict the oldest one.
    # This filters out "one-hit wonders" / scans effectively.
    if probation_lru_key is not None:
        return probation_lru_key
    
    # If all items are "Protected" (accessed > 1 time), fallback to standard LRU
    return protected_lru_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment hit count (promote to protected status) and update recency.
    '''
    global m_key_last_access, m_key_hit_count
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment hit count. Capping it at 2 is sufficient for this logic (1 vs >1),
    # but we keep counting to support potential future "Least Frequently Used" extensions if needed.
    current_hits = m_key_hit_count.get(obj.key, 1)
    m_key_hit_count[obj.key] = current_hits + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, mark as "Probationary" (hit count 1) and set recency.
    '''
    global m_key_last_access, m_key_hit_count
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    # Initialize with 1. It counts as the first access.
    m_key_hit_count[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata. 
    '''
    global m_key_last_access, m_key_hit_count
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    if evicted_obj.key in m_key_hit_count:
        del m_key_hit_count[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp57qzyywz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiimf9rm1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsauwmgv7.pickle

Iteration 55: New subsample score 0.18016700000000002 is not better than old score 0.18016700000000002, skipping
Iteration 56: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvhy09qgz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2mhusdbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9t0_dfsw.pickle

Iteration 56: Proposed new text for program: import collections

# We maintain two main logical segments within the cache:
# T1: Items accessed only once recently (Recency / New items)
# T2: Items accessed at least twice (Frequency / Popular items)
#
# We also maintain two "Ghost" lists (metadata only, no data):
# B1: Evicted from T1 (Recent misses)
# B2: Evicted from T2 (Frequent misses)
#
# The parameter 'p' determines the target size of T1. 
# If we hit in B1, we increase p (need more recency).
# If we hit in B2, we decrease p (need more frequency).

# Global State
m_T1 = collections.OrderedDict() # Recency list (Key -> Metadata)
m_T2 = collections.OrderedDict() # Frequency list (Key -> Metadata)
m_B1 = collections.OrderedDict() # Ghost Recency (Key -> Metadata)
m_B2 = collections.OrderedDict() # Ghost Frequency (Key -> Metadata)

# Adaptation Parameter
# p is the target size for the T1 list (recency).
m_p = 0 

def evict(cache_snapshot, obj):
    '''
    The eviction logic is driven by the state of the T1 and T2 lists relative to
    the adaptive parameter 'p'. This function performs the actual removal of a key 
    from the physical cache based on the logic of replace().
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Current logical sizes
    t1_len = len(m_T1)
    
    # We need to decide whether to evict from T1 or T2.
    # Logic derived from ARC's replace() function.
    
    candidate_key = None
    
    # If T1 is larger than the target 'p', we usually evict from T1 (the LRU of T1).
    # However, if B1 is much larger than B2, it influences this.
    # The strict ARC condition for evicting from T2 is:
    # t1_len < p  OR  (B2 is not empty and t1_len == p)
    #
    # Conversely, we evict from T1 if:
    # t1_len > p
    
    if t1_len > 0 and (t1_len > m_p or (t1_len == m_p and len(m_B2) > 0 and len(m_B1) == 0)):
        # Evict LRU from T1
        candidate_key, _ = m_T1.popitem(last=False)
        # Move to Ghost List B1
        m_B1[candidate_key] = True
    else:
        # Evict LRU from T2
        if m_T2:
            candidate_key, _ = m_T2.popitem(last=False)
            # Move to Ghost List B2
            m_B2[candidate_key] = True
        else:
            # Fallback if T2 is empty but we must evict (rare edge case in cold start)
            if m_T1:
                candidate_key, _ = m_T1.popitem(last=False)
                m_B1[candidate_key] = True

    # Ensure Ghost lists don't grow infinitely
    # ARC generally limits |B1| + |B2| <= Capacity. 
    # Since we don't have max item capacity (only bytes), we use a heuristic limit based on active keys.
    # Capacity in items is roughly cache_snapshot.size / avg_obj_size, but we can just clamp ghosts 
    # to the current number of cached items to be safe.
    current_cache_count = len(cache_snapshot.cache)
    if len(m_B1) > current_cache_count:
        m_B1.popitem(last=False)
    if len(m_B2) > current_cache_count * 2: # Keep more history for frequency
        m_B2.popitem(last=False)
        
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Move item to the MRU position of T2 (Frequency List).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Key is in T1 (New item became popular)
    if key in m_T1:
        del m_T1[key]
        m_T2[key] = True # Move to T2 MRU
    
    # Case 2: Key is in T2 (Popular item staying popular)
    elif key in m_T2:
        del m_T2[key]
        m_T2[key] = True # Re-insert to update MRU position
        
    # Note: If it's a cache hit, it MUST be in T1 or T2.
    # If the logic drifts, we safety check:
    else:
        m_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss): Handle adaptation of 'p' and placement into T1 or T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Key is in Ghost List B1 (Was in T1 recently)
    # This implies T1 was too small. Increase p.
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
        m_p = min(len(cache_snapshot.cache) + 1, m_p + delta)
        
        del m_B1[key]
        # Promote to T2 (frequency list) because it was seen recently (in T1) and now again.
        m_T2[key] = True 

    # Case 2: Key is in Ghost List B2 (Was in T2 recently)
    # This implies T2 was too small (or T1 was too big). Decrease p.
    elif key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
        m_p = max(0, m_p - delta)
        
        del m_B2[key]
        # Re-insert into T2
        m_T2[key] = True
        
    # Case 3: Totally new item (or forgotten from ghosts)
    else:
        # Insert into T1 (Recency List)
        m_T1[key] = True
        
    # NOTE: The 'evict' function will be called by the system BEFORE this insert
    # if the cache is full. However, inside this function, we just manage the lists.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Metadata updates for the evicted object are handled inside the `evict` function 
    to ensure atomic logic (moving from Tx to Bx). 
    We do nothing here to avoid double-accounting.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw9q9dere.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdjydl979.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpphyjmaul.pickle

Iteration 56: New subsample score 1.019278 is better than old score 1.0101660000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgczi7o80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm5lzwv_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcbxrhn48.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpml_wuugg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjrbg6p27.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqph2n6ax.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp52wt_x_1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdps1ab1z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeoeop54m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprl4amyns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4tv9q0tu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph22pgo_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo8dnnvqp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmxyr4k8q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4_nlb4n4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu1issavv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwd_odhp2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl098xu6g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdb2wn9w3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe6v1yz22.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbma9ay3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1shhruya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf7lvxkx0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkodetovy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5glrnyoa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpawt078bg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphc_s1czg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppdmxqm_3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu3y4ncaj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptc46r084.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplezcvu0q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_9j3pd6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk18zy4pf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb2wlqy7w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnveh7bgy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprv8uqf6c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_n94rha4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp29970lpm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp650rhkw9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkyi2x251.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4trv7mnl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3j0yidcx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkwflfi3x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprtibl8t9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqzt_p1wp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf55nh8v1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa5dby4zv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt7nvze6n.pickle

Iteration 56: Full valset score for new program: 0.24453035416666669
Iteration 56: Full train_val score for new program: 0.24453035416666669
Iteration 56: Individual valset scores for new program: [0.507856, 0.483189, 0.491182, 0.441451, 0.505643, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.334813, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.269801, 0.372984, 0.849057, 0.891494, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.078181, 0.124007, 0.057624, 0.052632, 0.3, 0.09278, 0.060028, 0.466258, 0.081699]
Iteration 56: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 56: Full valset pareto front score: 0.2582914583333333
Iteration 56: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {3, 5, 6, 7, 12, 13}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16}, {1}, {15}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16}, {4, 8, 10, 11, 16}, {4, 8, 10, 11, 16}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16}, {1, 4, 7, 8, 10, 11, 12, 13, 16}, {8, 11}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16}, {1, 3, 4, 8, 10, 11, 16}]
Iteration 56: Best valset aggregate score so far: 0.2510750416666666
Iteration 56: Best program as per aggregate score on train_val: 4
Iteration 56: Best program as per aggregate score on valset: 4
Iteration 56: Best score on valset: 0.2510750416666666
Iteration 56: Best score on train_val: 0.2510750416666666
Iteration 56: Linear pareto front program index: 4
Iteration 56: New program candidate index: 16
Iteration 57: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpew4r6dsl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptorg90fj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjzfmf6di.pickle

Iteration 57: Proposed new text for program: import math

# Metadata Globals
# Tracks the frequency of keys. We use a logarithmic decay or simple counter.
m_frequency = dict()
# Tracks the insertion time or last access time for LRU/FIFO tie-breaking.
m_access_time = dict()

# Ghost caches for ARC/LIRS-like adaptation
# m_ghost_recent: Keys recently evicted from the "recency" set
m_ghost_recent = set()
# m_ghost_frequent: Keys recently evicted from the "frequency" set
m_ghost_frequent = set()

# Algorithm Parameters
# p represents the target size for the "recent" (probationary) segment of the cache.
# 0 <= p <= cache_capacity.
# If p is high, we behave more like LRU (favoring recency).
# If p is low, we behave more like LFU (favoring frequency).
m_p = 0
# Initial capacity estimation (will update dynamically)
m_capacity_estimate = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Simplified Adaptive Replacement Cache (ARC) Logic.
    
    The algorithm manages two conceptual lists within the single cache dict:
    1. T1 (Recent): Items seen only once recently (Probationary).
    2. T2 (Frequent): Items seen at least twice (Protected).
    
    The parameter `m_p` determines the ideal target size of T1.
    
    If len(T1) > m_p, we evict from T1 (LRU style).
    Else, we evict from T2 (LRU style).
    '''
    global m_frequency, m_access_time, m_p, m_capacity_estimate
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
    
    # Estimate capacity (since we are given size in bytes, but ARC works on counts usually.
    # We map 'size' roughly to object count for the logic).
    current_cache_count = len(candidate_keys)
    m_capacity_estimate = max(m_capacity_estimate, current_cache_count)
    
    # Identify which keys belong to T1 (Frequency=1) and T2 (Frequency > 1)
    # We need to perform the eviction based on the adaptive parameter m_p.
    
    t1_keys = []
    t2_keys = []
    
    for k in candidate_keys:
        freq = m_frequency.get(k, 1)
        if freq == 1:
            t1_keys.append(k)
        else:
            t2_keys.append(k)
            
    victim_key = None
    
    # Decision logic based on ARC principle:
    # If the size of T1 exceeds the target p, we evict from T1 (Recency/Probation).
    # Otherwise, we evict from T2 (Frequency).
    # Note: If T1 is empty, we must evict from T2, and vice versa.
    
    evict_from_t1 = False
    
    if t1_keys and len(t1_keys) > m_p:
        evict_from_t1 = True
    elif t1_keys and not t2_keys:
        evict_from_t1 = True
    elif t2_keys and not t1_keys:
        evict_from_t1 = False
    else:
        # Both exist, but T1 <= p. Usually evict T2 here to grow T1?
        # Standard ARC: if len(T1) == p, we can evict T1? 
        # Strictly: if len(T1) > p, replace in T1. Else replace in T2.
        evict_from_t1 = False

    # Perform the eviction (LRU within the chosen set)
    if evict_from_t1:
        # Find LRU in T1
        # m_access_time stores the time; min is oldest.
        victim_key = min(t1_keys, key=lambda k: m_access_time.get(k, 0))
    else:
        # Find LRU in T2
        victim_key = min(t2_keys, key=lambda k: m_access_time.get(k, 0))
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency (access time).
    2. If item was in T1 (freq=1), move to T2 (freq+=1).
    3. If item was in T2, update its recency within T2.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    # Increment frequency to signify it's a "frequent" item now
    # We cap at 2 for the binary distinction (T1 vs T2), but keeping real count helps tie-breaks if needed.
    # For this logic, just >1 is T2.
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Handle Ghost Hits to adapt 'm_p'.
    2. Initialize new object metadata.
    '''
    global m_frequency, m_access_time, m_p, m_ghost_recent, m_ghost_frequent, m_capacity_estimate
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # --- Adaptive Logic (Ghost Hits) ---
    if key in m_ghost_recent:
        # Hit in B1 (Ghost Recent). We made T1 too small. Increase p.
        # Delta depends on sizes of ghost lists.
        len_b1 = len(m_ghost_recent)
        len_b2 = len(m_ghost_frequent)
        delta = 1
        if len_b1 < len_b2:
             delta = len_b2 / len_b1
        
        m_p = min(m_capacity_estimate, m_p + delta)
        m_ghost_recent.remove(key)
        
        # Revived item starts with high frequency logic (it's recurring)
        m_frequency[key] = 2
        
    elif key in m_ghost_frequent:
        # Hit in B2 (Ghost Frequent). We made T1 too big (T2 too small). Decrease p.
        len_b1 = len(m_ghost_recent)
        len_b2 = len(m_ghost_frequent)
        delta = 1
        if len_b2 < len_b1:
            delta = len_b1 / len_b2
            
        m_p = max(0, m_p - delta)
        m_ghost_frequent.remove(key)
        
        # Revived item belongs in T2
        m_frequency[key] = 2
        
    else:
        # Brand new item. Starts in T1.
        m_frequency[key] = 1

    # --- Standard Updates ---
    m_access_time[key] = current_time
    
    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted key to the appropriate Ghost list (B1 or B2)
    based on whether it was a T1 or T2 item.
    '''
    global m_frequency, m_ghost_recent, m_ghost_frequent, m_access_time
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    freq = m_frequency.get(key, 1)
    
    # If freq == 1, it was in T1 (Recent/Probationary) -> move to B1 (Ghost Recent)
    # If freq > 1, it was in T2 (Frequent) -> move to B2 (Ghost Frequent)
    if freq <= 1:
        m_ghost_recent.add(key)
    else:
        m_ghost_frequent.add(key)

    # Clean up main metadata to save memory, as it's now tracked in ghost sets
    if key in m_frequency: del m_frequency[key]
    if key in m_access_time: del m_access_time[key]

def _cleanup_metadata():
    '''
    Maintain ghost list sizes to ensure they don't grow indefinitely.
    Standard ARC keeps |B1| + |B2| <= Capacity.
    '''
    global m_ghost_recent, m_ghost_frequent, m_capacity_estimate
    
    # We use a rough constant limit or capacity based limit
    limit = max(1000, m_capacity_estimate * 2) 
    
    # If ghost lists are too large, remove oldest (FIFO/Random for simplicity here as we are using Sets)
    # Ideally, ghost lists are queues. Since we lack ordered dict/deque in global state easily without complexity,
    # we just trim if size explodes.
    
    if len(m_ghost_recent) > limit:
        # Remove arbitrary elements
        for _ in range(len(m_ghost_recent) - limit):
            m_ghost_recent.pop()
            
    if len(m_ghost_frequent) > limit:
        for _ in range(len(m_ghost_frequent) - limit):
            m_ghost_frequent.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa4kmet_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1_y41fkn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo084hqpr.pickle

Iteration 57: New subsample score 0.320162 is not better than old score 0.320304, skipping
Iteration 58: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr92wdp0b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdceunf96.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_ldz1ddy.pickle

Iteration 58: Proposed new text for program: # Metadata dictionaries
# We map Key -> Size (int). This allows us to track byte usage in ghost lists.
meta_t1 = dict() # Recently accessed once
meta_t2 = dict() # Accessed at least twice
meta_b1 = dict() # Ghost T1 (Evicted from T1)
meta_b2 = dict() # Ghost T2 (Evicted from T2)

# Size tracking variables (in bytes) to avoid O(N) summation
size_t1 = 0
size_t2 = 0
size_b1 = 0
size_b2 = 0

# Adaptive parameter 'p'
# Represents the target size (in bytes) for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC (Adaptive Replacement Cache) Eviction Strategy (Byte-Aware).
    
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` (target bytes for T1) 
    and the current byte size of T1.
    '''
    global p, meta_t1, meta_t2, size_t1
    
    # ARC Replace Logic:
    # If the size of T1 exceeds the target 'p', we prefer evicting from T1 (recency)
    # to make room for frequency items or to reduce recency size.
    # Exception: If T1 is the only thing we have, we must evict from it.
    # Exception: If T2 is empty, we must evict from T1.
    
    # We prioritize T1 eviction if T1 is "too big" (size_t1 > p).
    # However, we must ensure the list we pick from actually has items.
    
    # Check T1 eligibility
    evict_t1 = False
    
    if meta_t1 and not meta_t2:
        # Only T1 has items
        evict_t1 = True
    elif meta_t1 and size_t1 > p:
        # T1 is larger than target p, so we prune T1
        evict_t1 = True
    # Else: T1 is within limits (or empty), so we prune T2 (unless T2 is empty, handled by first if)

    if evict_t1:
        # Return LRU of T1 (first key in dict)
        return next(iter(meta_t1))
    elif meta_t2:
        # Return LRU of T2
        return next(iter(meta_t2))
    
    # Fallback for safety (e.g., if metadata is out of sync with snapshot)
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If in T1 (Recency), move to T2 (Frequency).
    - If in T2 (Frequency), update position to MRU.
    '''
    global meta_t1, meta_t2, size_t1, size_t2
    
    key = obj.key
    size = obj.size
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        size_t1 -= meta_t1[key]
        del meta_t1[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    elif key in meta_t2:
        # Hit in Frequency (T2): Update LRU position (delete and re-insert makes it MRU)
        # Size doesn't change, but we refresh the object size just in case
        old_size = meta_t2[key]
        del meta_t2[key]
        
        # Note: If object size changed dynamically, we would need to handle size_t2 update here.
        # Assuming immutable size for key based on prompt read-only attributes.
        meta_t2[key] = size

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Adjusts 'p' if the miss occurred in Ghost lists (B1 or B2).
    - Inserts the new object into T1 (if new) or T2 (if seen in Ghosts).
    - Cleans up Ghost lists to prevent infinite memory growth.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    global size_t1, size_t2, size_b1, size_b2
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Adaptation Step: Adjust p based on ghost hits
    if key in meta_b1:
        # Miss in B1 (Ghost Recency). We should have kept T1 larger.
        # Delta logic: increase p.
        delta = size
        # Advanced ARC delta: if size_b1 >= size_b2: delta = size, else size * (size_b2/size_b1)
        # Simplified robust byte-level delta:
        if size_b1 >= size_b2:
            delta = size
        else:
            # Avoid divide by zero
            delta = size * (size_b2 / size_b1) if size_b1 > 0 else size
            
        p = min(capacity, p + delta)
        
        # Remove from B1 and add to T2 (since it's a re-access)
        size_b1 -= meta_b1[key]
        del meta_b1[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    elif key in meta_b2:
        # Miss in B2 (Ghost Frequency). We should have kept T2 larger (T1 smaller).
        # Delta logic: decrease p.
        delta = size
        if size_b2 >= size_b1:
            delta = size
        else:
            delta = size * (size_b1 / size_b2) if size_b2 > 0 else size
            
        p = max(0, p - delta)
        
        # Remove from B2 and add to T2
        size_b2 -= meta_b2[key]
        del meta_b2[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    else:
        # Totally new item (Cold Miss). Insert into T1.
        meta_t1[key] = size
        size_t1 += size

    # 2. History Management (Ghost Sizing)
    # We must limit the size of ghost lists to keep the algorithm responsive and memory efficient.
    # Standard ARC suggests: L1 (T1+B1) <= C, L2 (T2+B2) <= 2C.
    
    # Trim B1 if T1 + B1 is too large
    while meta_b1 and (size_t1 + size_b1 > capacity):
        k = next(iter(meta_b1))
        size_b1 -= meta_b1[k]
        del meta_b1[k]
        
    # Trim B2 if T2 + B2 is too large (allow 2x capacity for frequency history usually)
    while meta_b2 and (size_t2 + size_b2 > 2 * capacity):
        k = next(iter(meta_b2))
        size_b2 -= meta_b2[k]
        del meta_b2[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Move the evicted object from T list (cache) to B list (ghost).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    global size_t1, size_t2, size_b1, size_b2
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    if key in meta_t1:
        size_t1 -= meta_t1[key]
        del meta_t1[key]
        
        # Add to B1 (Ghost T1)
        meta_b1[key] = size
        size_b1 += size
        
    elif key in meta_t2:
        size_t2 -= meta_t2[key]
        del meta_t2[key]
        
        # Add to B2 (Ghost T2)
        meta_b2[key] = size
        size_b2 += size
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvslz3g6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp5z41opp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq07nc29p.pickle

Iteration 58: New subsample score 1.4112920000000002 is better than old score 1.3597420000000002. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_zx_nris.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfsn3we9h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq6tdiw30.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb89yprqi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjbqc1orz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp1b9f5yc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp91tr0wvf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmoc92doh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpenlpucrd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp301t2x78.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjfbxojo_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps8d075qr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp36fe2k4o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5qb9ay_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_xxf3duy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqhmir_8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplglqvujs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxpgpi3ab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmtnpvndg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn6pm4xo8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk8s0dup4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb8prrx38.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpot0rlrr3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy23rs7tv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf2rxyvp2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0hjt1oid.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7bda3wvk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdulia9_x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpksx1znyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7zz1prx8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd7_vkhxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdx616ske.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpln7ngwah.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvtwy11s6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg4msz3uo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmf1yyv9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptzf6amt9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpao8akja3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb8bag5fb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjl_x5gi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz5m85nln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa9yjut3x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0ho8n1it.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5zlg4swj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqmqoienp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2qevgk_e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz805flzs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpimv_e_jz.pickle

Iteration 58: Full valset score for new program: 0.24068325
Iteration 58: Full train_val score for new program: 0.24068325
Iteration 58: Individual valset scores for new program: [0.504789, 0.479512, 0.488758, 0.432915, 0.50361, 0.489911, 0.272727, 0.498034, 0.538077, 0.531017, 0.075, 0.323712, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.26979, 0.358871, 0.849057, 0.891494, 0.039832, 0.038636, 0.045558, 0.007721, 0.020672, 0.752893, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.120322, 0.077581, 0.099954, 0.052632, 0.233333, 0.067317, 0.10014, 0.466258, 0.081699]
Iteration 58: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 58: Full valset pareto front score: 0.2582914583333333
Iteration 58: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {3, 5, 6, 7, 12, 13}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17}, {1}, {15}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17}, {4, 8, 10, 11, 16}, {4, 8, 10, 11, 16}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17}, {8, 11}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17}, {1, 3, 4, 8, 10, 11, 16, 17}]
Iteration 58: Best valset aggregate score so far: 0.2510750416666666
Iteration 58: Best program as per aggregate score on train_val: 4
Iteration 58: Best program as per aggregate score on valset: 4
Iteration 58: Best score on valset: 0.2510750416666666
Iteration 58: Best score on train_val: 0.2510750416666666
Iteration 58: Linear pareto front program index: 4
Iteration 58: New program candidate index: 17
Iteration 59: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpitbgawl4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy9k2cv_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyj_4aao0.pickle

Iteration 59: Proposed new text for program: # Metadata dictionaries
# Tracks the frequency of object access
m_key_frequency = dict()
# Tracks the computed priority value (H value) for the GDSF algorithm
m_key_priority = dict()
# The 'inflation' value L, used to age out stale entries
m_inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority value (GDSF).
    Priority = L + (Frequency / Size).
    When an object is evicted, we update L to that object's priority.
    '''
    global m_inflation_L, m_key_priority
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We need to find the object with the minimum priority value.
    # While iterating, we handle the edge case where a key might be missing from metadata 
    # (though updates should prevent this).
    
    victim_key = None
    min_priority = float('inf')
    
    for key in candidate_keys:
        # Default to 0.0 priority if missing (should be evicted immediately)
        priority = m_key_priority.get(key, 0.0)
        
        if priority < min_priority:
            min_priority = priority
            victim_key = key
            
            # Optimization: If we find a 0 priority, we can't get lower, stop early.
            if min_priority == 0.0:
                break
    
    # Update the global inflation factor L to the priority of the evicted object.
    # This is the "Aging" mechanism. New objects must beat this value to survive long.
    if victim_key is not None:
        m_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Increment frequency.
    2. Recalculate priority using current L.
    '''
    global m_key_frequency, m_key_priority, m_inflation_L
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = current_freq
    
    # Update Priority: H = L + (Freq / Size)
    # Using float for precision.
    size = obj.size if obj.size > 0 else 1 # Prevent division by zero
    new_priority = m_inflation_L + (current_freq / size)
    m_key_priority[obj.key] = new_priority

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize frequency to 1.
    2. Calculate initial priority using current L.
    '''
    global m_key_frequency, m_key_priority, m_inflation_L
    
    # Initialize Frequency
    m_key_frequency[obj.key] = 1
    
    # Initialize Priority: H = L + (1 / Size)
    size = obj.size if obj.size > 0 else 1
    new_priority = m_inflation_L + (1.0 / size)
    m_key_priority[obj.key] = new_priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Clean up metadata. We do NOT delete frequency history immediately if we wanted 
    to implement "Ghost Lists" (Frequency Reset), but for GDSF, standard cleanup is usually sufficient
    and saves memory.
    '''
    global m_key_frequency, m_key_priority
    
    # Remove from priority tracking
    if evicted_obj.key in m_key_priority:
        del m_key_priority[evicted_obj.key]
    
    # Optimization: We remove frequency data to save memory. 
    # If the object returns, it is treated as new (Frequency reset). 
    # The GDSF 'L' factor handles the "system memory" of popularity.
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoxjlpkx1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp51ral8zn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxrwhg2jg.pickle

Iteration 59: New subsample score 0.504479 is not better than old score 0.541222, skipping
Iteration 60: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0kwoprjg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa2i3_o15.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppq1kvt69.pickle

Iteration 60: Proposed new text for program: # Metadata to track frequency and priority values for objects.
# Maps key -> {'freq': int, 'priority': float}
metadata = dict()

# The dynamic aging factor (L). 
# This represents the "inflation" value of the system.
# It increases over time as objects are evicted.
L = 0.0

def get_metadata(key):
    """Helper to retrieve metadata safely."""
    if key not in metadata:
        return {'freq': 0, 'priority': 0.0}
    return metadata[key]

def calculate_priority(freq, size):
    """
    Calculates GDSF priority.
    Priority = L + (Frequency * Cost) / Size
    
    To maximize Object Hit Rate, we assume the 'Cost' of a miss is 1 for all objects.
    Therefore: Priority = L + Frequency / Size
    """
    # Ensure size is at least 1 to avoid division by zero
    safe_size = max(1, size)
    return L + (freq / safe_size)

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Find the object in the cache with the lowest Priority value (H-value).
    '''
    global L, metadata
    
    victim_key = None
    min_priority = float('inf')
    
    # Iterate through all objects currently in the cache
    # Note: dict iteration in Python is generally insertion-ordered.
    # If ties occur in priority, this naturally falls back to LRU for the tie-breaker.
    for key in cache_snapshot.cache:
        if key in metadata:
            p_val = metadata[key]['priority']
        else:
            # If for some reason metadata is missing, treat as 0 priority (evict first)
            p_val = 0.0
            
        if p_val < min_priority:
            min_priority = p_val
            victim_key = key
            
    # Update the aging factor L to the priority of the evicted object.
    # This pushes the "floor" up, ensuring new items have a chance to compete
    # and old high-frequency items eventually decay.
    if victim_key is not None and min_priority != float('inf'):
        # Only update L if the new value is higher (monotonically increasing)
        if min_priority > L:
            L = min_priority
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on new Frequency and current L.
    '''
    global metadata, L
    
    key = obj.key
    meta = get_metadata(key)
    
    # Increment frequency
    new_freq = meta['freq'] + 1
    
    # Update Priority
    # H = L + F / S
    new_priority = calculate_priority(new_freq, obj.size)
    
    metadata[key] = {'freq': new_freq, 'priority': new_priority}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize or Restore Frequency (Ghost Caching).
    2. Calculate initial Priority.
    '''
    global metadata, L
    
    key = obj.key
    
    # We maintain frequency history even after eviction (Ghost functionality).
    # If the key was seen before, we continue its frequency count (LFU style).
    # If not, we start at 1.
    if key in metadata:
        new_freq = metadata[key]['freq'] + 1
    else:
        new_freq = 1
        
    # Calculate Priority
    # For a new item, Priority is L + (1 / Size) usually, but we use accumulated freq
    new_priority = calculate_priority(new_freq, obj.size)
    
    metadata[key] = {'freq': new_freq, 'priority': new_priority}
    
    # Maintenance: Prevent metadata memory leak.
    # If metadata grows too large relative to the cache (e.g., lots of one-hit wonders),
    # we purge old entries that are no longer in the cache.
    # 4x is a heuristic buffer to allow for "Ghost" history.
    if len(metadata) > max(1000, len(cache_snapshot.cache) * 4):
        # Identify keys not in cache
        cached_keys = set(cache_snapshot.cache.keys())
        ghost_keys = [k for k in metadata if k not in cached_keys]
        
        # Simple cleanup: remove the oldest portion of ghosts
        # In Python < 3.7 dicts are not guaranteed ordered, but in 3.7+ they are insertion ordered.
        # We delete the oldest added ghosts.
        limit_remove = len(ghost_keys) // 2
        for k in ghost_keys[:limit_remove]:
            del metadata[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    We do NOT delete the metadata immediately. 
    We keep it to remember the frequency if the object returns (Ghost List behavior).
    The cleanup happens in update_after_insert if the map gets too big.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpno5lx_7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3it167ll.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7is6w_f6.pickle

Iteration 60: New subsample score 0.099347 is not better than old score 0.15669899999999998, skipping
Iteration 61: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_lep50k8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqrxf1mky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp29zy37k0.pickle

Iteration 61: Proposed new text for program: import math

# Metadata Globals
# Tracks the number of times a key has been accessed.
# This helps distinguish between one-hit wonders (scans) and recurring data.
m_frequency = dict()

# Tracks the order of insertion/access for implementing LRU behavior.
# We use a strictly increasing counter for operations.
m_access_time = dict()

# To handle "scans" (sequences of data accessed once), we conceptually divide 
# the cache into two segments based on frequency:
# 1. Probationary (Frequency == 1): Recently inserted, unproven value.
# 2. Protected (Frequency > 1): Proven value.
#
# We do not strictly partition the memory space, but we bias eviction heavily 
# against the Probationary items.

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (Scan Resistant).
    
    1. Identify candidates in the cache.
    2. Try to find a victim in the "Probationary" set (Freq == 1).
       Evict the Least Recently Used (oldest access time) item among them.
    3. If no Probationary items exist, evict the Least Recently Used item
       from the "Protected" set (Freq > 1).
    '''
    global m_frequency, m_access_time
    
    # Get all keys currently in the cache
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # Strategy: Find the LRU item, but prioritize evicting items with low Frequency.
    # We want to keep high-frequency items even if they haven't been touched in a while
    # compared to a brand new item that might be part of a one-time scan.
    
    victim = None
    min_time = float('inf')
    found_probationary = False
    
    # First Pass: Look for LRU among Probationary items (Freq <= 1)
    # This acts as a filter against "scans".
    for key in current_keys:
        freq = m_frequency.get(key, 1)
        
        if freq <= 1:
            access_time = m_access_time.get(key, 0)
            if access_time < min_time:
                min_time = access_time
                victim = key
                found_probationary = True
    
    # If we found a probationary victim, return it.
    if found_probationary and victim is not None:
        return victim

    # Second Pass: If all items are "Protected" (Freq > 1), fall back to standard LRU.
    # Reset search vars
    min_time = float('inf')
    victim = None
    
    for key in current_keys:
        access_time = m_access_time.get(key, 0)
        if access_time < min_time:
            min_time = access_time
            victim = key
            
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update access time (Recency) and increment Frequency.
    This effectively moves an item from Probation to Protected if Freq becomes > 1.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    # Cap frequency to prevent integer overflow in long running processes, 
    # though practically anything > 1 is "Protected"
    m_frequency[key] = min(m_frequency.get(key, 0) + 1, 1000)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize access time and set Frequency to 1.
    New items start in Probation.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    
    # If the key exists in our metadata (history), we restore it, 
    # otherwise it starts at 1.
    if key not in m_frequency:
        m_frequency[key] = 1
    else:
        # It's a re-insertion. We bump it slightly to acknowledge it returned,
        # but don't give it full credit of its past life immediately to prevent
        # old popular items from clogging the cache forever.
        # However, for this implementation, treating it as a new hit (increment)
        # upon re-insertion is usually safer for hit-rate.
        m_frequency[key] += 1
        
    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: We do NOT remove metadata immediately.
    We keep the frequency history so if the item returns (thrashing), 
    we know it was valuable.
    '''
    # No specific action needed here, cleanup handles the memory management.
    pass

def _cleanup_metadata(cache_snapshot):
    '''
    Prevent memory leaks by removing metadata for keys that have been 
    evicted and haven't returned in a long time.
    '''
    global m_frequency, m_access_time
    
    # Heuristic: Limit metadata size to constant factor of cache capacity 
    # or a fixed upper bound if capacity is in bytes (which makes object count hard to guess).
    # Here we use a safe upper limit.
    MAX_METADATA_ENTRIES = 50000 
    
    if len(m_frequency) > MAX_METADATA_ENTRIES:
        # If metadata grows too large, purge the oldest entries not currently in cache.
        # To do this efficiently without iterating everything every time, 
        # we can just do a random sampling or purge a chunk based on access time.
        
        # Simple cleanup: Remove keys not in cache that are also old.
        # Note: In a production system, this would be an ordered dictionary or linked list.
        # Here, we do a "stop the world" cleanup when limit is hit.
        
        keys_in_cache = set(cache_snapshot.cache.keys())
        all_keys = list(m_access_time.keys())
        
        # Sort by access time (oldest first)
        all_keys.sort(key=lambda k: m_access_time[k])
        
        # Remove the oldest 10% of metadata entries, provided they aren't currently cached
        entries_to_remove = len(all_keys) // 10
        removed_count = 0
        
        for k in all_keys:
            if removed_count >= entries_to_remove:
                break
            if k not in keys_in_cache:
                del m_frequency[k]
                del m_access_time[k]
                removed_count += 1
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgfo3j56q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnb3tynof.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbwyb17k5.pickle

Iteration 61: New subsample score 0.108857 is not better than old score 0.117934, skipping
Iteration 62: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0k28v2y1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1tibyl5k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptyfe42ud.pickle

Iteration 62: Proposed new text for program: import sys

# Global metadata storage
# Key -> [frequency, priority_score, last_access_time]
# We use a list instead of a tuple to allow mutable updates without re-hashing
m_metadata = dict()

# Global state for Dynamic Aging
# This represents the "minimum worth" required to stay in the cache.
# It rises as the cache churns.
m_cache_age = 0.0

def get_victim_score(key, current_time):
    '''
    Calculates the eviction likelihood.
    In LFU-DA, we want to evict the item with the LOWEST priority score.
    However, the framework asks for a "Victim Score" where Higher = Evict.
    
    Therefore, Victim Score = -1 * Priority Score.
    
    Tie-breaking:
    If priorities are equal, we want to evict the LRU item (smallest last_access_time).
    So, combined score = (-Priority, -Last_Access_Time).
    But since we need a single float/number usually, or strict ordering:
    
    We will return a tuple: (-priority, -last_access_time).
    Python compares tuples element by element.
    Higher tuple value => Higher chance of eviction.
    
    Example:
    Item A: Prio 10, Time 100. Score: (-10, -100)
    Item B: Prio 10, Time 90.  Score: (-10, -90)
    
    -10 == -10.
    -90 > -100.
    Item B is "greater" (less negative), so B is evicted.
    This is correct: B is older (Time 90 < Time 100), so B is LRU.
    '''
    if key not in m_metadata:
        return (float('inf'), 0) # Force eviction if metadata missing
        
    freq, priority, last_access = m_metadata[key]
    
    # We want to maximize this score to evict.
    # Smallest priority = Highest Victim Score.
    # Smallest time (LRU) = Highest Victim Score.
    return (-priority, -last_access)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (LFU-DA logic).
    Updates the global Cache Age to the priority of the evicted victim.
    '''
    global m_cache_age
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # We want to find the key that maximizes the "Victim Score"
    # (which corresponds to minimizing internal Priority).
    best_victim_key = None
    max_victim_score = (-float('inf'), -float('inf'))
    victim_priority = 0.0
    
    for key in candidate_keys:
        # returns (-priority, -last_access)
        score_tuple = get_victim_score(key, current_time)
        
        if score_tuple > max_victim_score:
            max_victim_score = score_tuple
            best_victim_key = key
            # Extract the actual priority from the negative tuple
            victim_priority = -score_tuple[0]

    # LFU-DA Critical Step:
    # The cache's "pressure" rises to the level of the item we just kicked out.
    # Any new item must eventually beat this score to survive.
    if best_victim_key is not None:
        m_cache_age = victim_priority
        
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Priority = CacheAge + Frequency.
       (Note: We re-add CacheAge to ensure hot items stay above the rising water level)
    3. Update Access Time.
    '''
    global m_metadata, m_cache_age
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, _, _ = m_metadata[obj.key]
        new_freq = freq + 1
        
        # Priority Update Rule:
        # Standard LFU-DA sets Priority = CacheAge + Frequency.
        # This bumps the item up above the current eviction threshold.
        new_priority = m_cache_age + new_freq
        
        m_metadata[obj.key] = [new_freq, new_priority, current_time]
    else:
        # Fallback for weird edge cases where key exists in cache but not metadata
        # Treat as new insert
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency = 1.
    2. Initialize Priority = CacheAge + 1.
    This gives the new item a fighting chance (starts at current pressure level),
    but if it isn't accessed again, it will be among the first to go as CacheAge rises.
    '''
    global m_metadata, m_cache_age
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    # The new item starts "floating" at the current water level
    initial_priority = m_cache_age + initial_freq
    
    m_metadata[obj.key] = [initial_freq, initial_priority, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to save memory. 
    (LFU-DA doesn't strictly require a ghost list, relying on m_cache_age instead).
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2uwohlqe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0puwfhgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7sl2w5gw.pickle

Iteration 62: New subsample score 0.296506 is not better than old score 0.41682699999999995, skipping
Iteration 63: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe5h8jd5n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkv02ypi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzo3ei0hn.pickle

Iteration 63: Proposed new text for program: # Metadata dictionaries
# Tracks the last access time (Recency)
m_key_last_access = dict()
# Tracks the "state" of the object: 0 for Probationary, 1 for Protected
m_key_status = dict()

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) approximation.
    
    Priority 1: Evict from the Probationary (Status 0) pool first. 
                These are items accessed only once recently.
    Priority 2: If no Probationary items exist, evict from Protected (Status 1).
    
    Within each pool, we evict the Least Recently Used (LRU) item.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We search for the best candidate. 
    # Logic: Minimize (Status, Last_Access_Time).
    # Since we want to evict Status 0 before Status 1, and older Time before newer Time.
    
    victim_key = None
    victim_status = float('inf')
    victim_time = float('inf')

    for key in candidate_keys:
        # Default to 0 (Probationary) and 0 time if missing (shouldn't happen)
        curr_status = m_key_status.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)
        
        # We want the smallest status (0 vs 1)
        if curr_status < victim_status:
            victim_key = key
            victim_status = curr_status
            victim_time = curr_time
        elif curr_status == victim_status:
            # Tie-break with LRU (smallest time)
            if curr_time < victim_time:
                victim_key = key
                victim_time = curr_time
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency.
    2. Promote the object to 'Protected' (Status 1) because it has proved its utility.
    '''
    global m_key_last_access, m_key_status
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Promotion: If it was Probationary (0), it becomes Protected (1).
    # If it was already Protected, it stays Protected.
    m_key_status[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize recency.
    2. Mark as 'Probationary' (Status 0). It needs another hit to prove worthiness.
    '''
    global m_key_last_access, m_key_status
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Initialize Status as Probationary
    m_key_status[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    '''
    global m_key_last_access, m_key_status
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    if evicted_obj.key in m_key_status:
        del m_key_status[evicted_obj.key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdsvj_tv3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphsdzt3wn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkhjs_xn0.pickle

Iteration 63: New subsample score 0.84242 is not better than old score 0.84242, skipping
Iteration 64: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd5uic9v4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8rk8f48f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm0gdxvbf.pickle

Iteration 64: Proposed new text for program: import math

# --- Metadata Storage ---
# Stores the number of times a key has been accessed (Frequency).
# Key -> Integer
m_key_frequency = dict()

# Stores the logical clock time of the last access (Recency).
# Key -> Integer
m_key_last_access = dict()

# Global "Inflation" value (L).
# This represents the "cost" or "priority" of the last evicted item.
# New items must beat this value (plus their own merit) to stay.
# This serves as an aging mechanism.
m_inflation_value = 0.0

# --- Constants ---
MAX_HISTORY_KEYS = 100000  # Cap metadata to prevent memory explosion

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest utility score.
    Algorithm: Greedy Dual-Size Frequency (GDSF) variant.
    
    Score(i) = (Inflation_L + Frequency(i)) / Size(i)
    
    Rationale:
    1. We want to keep items with high Frequency.
    2. We want to evict items that are Large (high Size costs more space).
    3. The Inflation_L acts as a dynamic aging factor. As better items are evicted,
       L increases, making it harder for old, low-frequency items to stay.
    '''
    global m_key_frequency, m_inflation_value, m_key_last_access

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    # We create a tie-breaker based on recency (LRU) to handle items with equal GDSF scores.
    # If scores are equal, we evict the one accessed furthest in the past.
    
    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        freq = m_key_frequency.get(key, 1)
        size = cached_obj.size
        
        # Protect against division by zero or extremely small sizes artificially inflating score
        if size <= 0: size = 1
        
        # The GDSF Score
        # Note: We use the frequency stored in metadata.
        # We calculate "value per byte".
        score = (m_inflation_value + freq) / size
        
        if score < min_score:
            min_score = score
            victim_key = key
        elif score == min_score:
            # Tie-breaker: LRU
            # If scores are identical, pick the one with older access time
            victim_lru = m_key_last_access.get(victim_key, 0)
            current_lru = m_key_last_access.get(key, 0)
            if current_lru < victim_lru:
                victim_key = key

    # Update the global inflation value to the score of the item we just decided to evict.
    # This effectively "ages" all other items in the cache relative to this score.
    # New items will enter with a base score derived from this L.
    m_inflation_value = min_score

    return victim_key

def _cleanup_metadata(current_cache_keys):
    '''
    Periodically cleans up metadata for keys that are no longer in the cache
    and haven't been seen in a long time to prevent memory leaks.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Identify keys in metadata but NOT in cache (Ghost keys)
        # We want to keep the most recent ghost keys, remove the old ones.
        
        # Create a list of (key, last_access)
        candidates = []
        for k, t in m_key_last_access.items():
            if k not in current_cache_keys:
                candidates.append((k, t))
        
        # Sort by access time (oldest first)
        candidates.sort(key=lambda x: x[1])
        
        # Remove the oldest chunk to get back under the limit
        num_to_remove = len(m_key_frequency) - (MAX_HISTORY_KEYS // 2)
        if num_to_remove > 0:
            for i in range(min(len(candidates), num_to_remove)):
                k_to_del = candidates[i][0]
                if k_to_del in m_key_frequency: del m_key_frequency[k_to_del]
                if k_to_del in m_key_last_access: del m_key_last_access[k_to_del]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_key_frequency, m_key_last_access
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Standard Frequency Increment
    m_key_frequency[key] = m_key_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if key is in history (Ghost Cache).
    2. If in history, restore frequency (it's a "working set" item coming back).
    3. If new, initialize frequency.
    '''
    global m_key_frequency, m_key_last_access, m_inflation_value
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in m_key_frequency:
        # HIT in GHOST CACHE
        # This item was evicted recently but requested again.
        # This implies our eviction threshold (Inflation Value L) was too aggressive for this item.
        # We increment its frequency to give it a better survival chance this time.
        m_key_frequency[key] += 1
    else:
        # COLD MISS
        # New item. Initialize frequency to 1.
        m_key_frequency[key] = 1
        
        # Periodic cleanup check
        if len(m_key_frequency) > MAX_HISTORY_KEYS:
            _cleanup_metadata(cache_snapshot.cache)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    We do NOT delete the metadata. We keep it as "Ghost" entries.
    This allows `update_after_insert` to recognize returning items.
    
    The 'm_inflation_value' is already updated inside 'evict' because
    that is where the calculation of the minimum score happens.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprfk5cjsc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprzuv9s6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpza08arn7.pickle

Iteration 64: New subsample score 0.47345499999999996 is not better than old score 0.474699, skipping
Iteration 65: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqafyay2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp15sap65v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpedrzgjww.pickle

Iteration 65: Proposed new text for program: import random

# Metadata dictionaries
# m_access_time: Stores the logical access time (cache_snapshot.access_count) for LRU tracking.
m_access_time = dict()

# m_segment: Tracks which segment a key belongs to.
# 0 = Probationary (easy to evict)
# 1 = Protected (hard to evict)
m_segment = dict()

# m_hits: Tracks hit counts specifically to decide promotion.
m_hits = dict()

# Configuration constants
# We reserve roughly 80% of capacity for the Protected segment logically,
# though physically they reside in the same cache structure.
PROTECTED_RATIO = 0.8 

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented Eviction Strategy.
    
    1. Identify candidates in the Probationary segment (Segment 0).
    2. If Probationary items exist, evict the LRU item from that segment.
    3. If no Probationary items exist (rare), evict the LRU item from Protected (Segment 1).
    '''
    global m_access_time, m_segment

    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate keys by segment
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        seg = m_segment.get(k, 0)
        if seg == 0:
            probation_keys.append(k)
        else:
            protected_keys.append(k)

    # Strategy: Evict from Probationary first (Recency based)
    victim_key = None
    oldest_time = float('inf')

    # If we have probationary items, find the LRU among them
    if probation_keys:
        # Optimization: To avoid O(N) sort, we scan. 
        # In a real system, we'd use a linked list or heap, but here we scan the sub-list.
        for k in probation_keys:
            t = m_access_time.get(k, 0)
            if t < oldest_time:
                oldest_time = t
                victim_key = k
    
    # If no probationary items, we must cannibalize the Protected segment (LRU)
    elif protected_keys:
        for k in protected_keys:
            t = m_access_time.get(k, 0)
            if t < oldest_time:
                oldest_time = t
                victim_key = k
    
    # Fallback (should not be reached if cache is non-empty)
    if victim_key is None:
        victim_key = candidate_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency (access time).
    2. If the item is in Probation (Segment 0), promote it to Protected (Segment 1).
    '''
    global m_access_time, m_segment, m_hits
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    m_hits[obj.key] = m_hits.get(obj.key, 0) + 1
    
    # Promotion Logic:
    # If it's currently in probation, a hit indicates it's useful. Promote it.
    if m_segment.get(obj.key, 0) == 0:
        m_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize recency.
    2. Place new item in Probation (Segment 0).
    '''
    global m_access_time, m_segment, m_hits
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    
    # New items always start in Probation (Segment 0)
    # They must "prove" themselves with a subsequent hit to get promoted.
    m_segment[obj.key] = 0
    m_hits[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    
    Note: In SLRU, if a Protected item is evicted, it is usually demoted
    or removed entirely. Here we simply remove metadata.
    '''
    global m_access_time, m_segment, m_hits
    
    key = evicted_obj.key
    
    if key in m_access_time:
        del m_access_time[key]
    
    if key in m_segment:
        del m_segment[key]

    if key in m_hits:
        del m_hits[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphp09zbqr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbmpwfbaq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi9p6ecox.pickle

Iteration 65: New subsample score 0.178564 is better than old score 0.178283. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjv69ohxj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpck9ku57g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp624vi20p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc8v2klwm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwg0xpkdw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxy5g13dn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk4e5gw_5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpohyafv3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz8q2fjej.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkuzzybnf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp443cfct1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps540gktn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvlyafuj1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpli_rdtj7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwpk16fd4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9xhagj90.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk6_xskat.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprnz144yo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwojsu_6q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprgf5dlrz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ec_6oks.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpro7vnj_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcmr4lkdu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0jvj90k1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdfggh8a2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ad8biit.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr2olb8f4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfbpf68r0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg8wdb2vv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp56z8mwg2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpox1vqbsv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1_qq9w1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxqlzdxr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj66ny2db.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphhah2vu6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcenm214l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqs2h_9db.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7_19o66h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7y8l1j4a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpekys_93u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqni6d3v9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9hpzy5y8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnxw0z0qn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfsdpl2wh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzv5ho5gr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp091i3ump.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm04xejt8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqb64_ep_.pickle

Iteration 65: Full valset score for new program: 0.22657866666666662
Iteration 65: Full train_val score for new program: 0.22657866666666662
Iteration 65: Individual valset scores for new program: [0.465185, 0.439295, 0.445428, 0.39006, 0.466225, 0.448115, 0.271531, 0.456547, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.269802, 0.254032, 0.703078, 0.887387, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.700136, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 65: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 65: Full valset pareto front score: 0.2582914583333333
Iteration 65: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {3, 5, 6, 7, 12, 13}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18}, {1}, {15}, {4}, {6}, {8, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18}, {4, 8, 10, 11, 16}, {4, 8, 10, 11, 16}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18}, {8, 11}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18}, {1, 3, 4, 8, 10, 11, 16, 17, 18}]
Iteration 65: Best valset aggregate score so far: 0.2510750416666666
Iteration 65: Best program as per aggregate score on train_val: 4
Iteration 65: Best program as per aggregate score on valset: 4
Iteration 65: Best score on valset: 0.2510750416666666
Iteration 65: Best score on train_val: 0.2510750416666666
Iteration 65: Linear pareto front program index: 4
Iteration 65: New program candidate index: 18
Iteration 66: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy6sq5umo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphpdvg0so.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps0qnx7xg.pickle

Iteration 66: Proposed new text for program: import math

# Global Metadata
# Tracks the frequency of objects (Lifetime frequency, including ghost history)
m_freq = dict()

# Tracks the calculated Priority of objects currently IN the cache
# Priority = L + Frequency
m_priority = dict()

# Tracks the last access time (for LRU tie-breaking)
m_last_access = dict()

# Global "Inflation" value (The aging factor)
# This represents the minimum priority required to survive in the cache recently.
GDSF_L = 0.0

# Constants for cleanup
MAX_HISTORY_SIZE = 100000

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (GDSF Score).
    Tie-breaker: Least Recently Used (LRU).
    '''
    global m_priority, m_last_access, GDSF_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We look for the item with the minimum Priority.
    # If priorities are equal, we pick the one with the smallest last_access (LRU).
    victim_key = None
    min_priority = float('inf')
    min_access_time = float('inf')
    
    # Linear scan to find victim (O(N) - acceptable per prompt context)
    for key in candidate_keys:
        # Default priority is L + 1 if somehow missing, but should be in map
        p = m_priority.get(key, GDSF_L + 1.0)
        access_time = m_last_access.get(key, 0)
        
        if p < min_priority:
            min_priority = p
            min_access_time = access_time
            victim_key = key
        elif p == min_priority:
            # Tie-break using LRU (Strict < means older time wins)
            if access_time < min_access_time:
                min_access_time = access_time
                victim_key = key
    
    # GDSF Aging Mechanism:
    # The system "inflation" value L rises to the priority of the item we just evicted.
    # This ensures that future items must possess a higher frequency to survive.
    if victim_key is not None:
        GDSF_L = min_priority
            
    return victim_key

def _cleanup_metadata(cache_snapshot):
    '''
    Prevents metadata memory leaks by pruning the frequency history.
    '''
    global m_freq, m_priority, m_last_access, MAX_HISTORY_SIZE
    
    # Only clean up if history grows too large
    if len(m_freq) > MAX_HISTORY_SIZE:
        # Simple heuristic: remove items with low frequency that are NOT in the cache
        keys_to_delete = []
        count = 0
        
        # We iterate and delete a chunk to reduce size
        for k, v in m_freq.items():
            if k not in cache_snapshot.cache:
                # Purge items seen only once or twice
                if v <= 2:
                    keys_to_delete.append(k)
                    count += 1
                    if count > 5000: # Delete in batches
                        break
        
        for k in keys_to_delete:
            del m_freq[k]
            if k in m_last_access: del m_last_access[k]
            # m_priority only contains cached items, so no need to clean based on this

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Re-calculate Priority using current L.
    3. Update Recency.
    '''
    global m_freq, m_priority, m_last_access, GDSF_L
    
    key = obj.key
    
    # 1. Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # 2. Update Priority (GDSF: Priority = L + Frequency)
    # Note: We reset the item's priority relative to the current inflation L.
    # This brings the item "forward" in the priority queue.
    m_priority[key] = GDSF_L + m_freq[key]
    
    # 3. Update Recency
    m_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize/Restore Frequency (Ghost Cache).
    2. Calculate Initial Priority.
    3. Update Recency.
    '''
    global m_freq, m_priority, m_last_access, GDSF_L
    
    key = obj.key
    
    # 1. Frequency (Ghost Cache check)
    # If it was in history, we recover its count. If new, it starts at 1.
    if key not in m_freq:
        m_freq[key] = 1
    else:
        # It's a re-insertion (churn). We acknowledge it was seen before.
        # We don't increment here strictly, usually insert implies 1st access of session,
        # but logic dictates the access happened.
        # However, to be consistent with 'hit', we rely on the frequency accumulated so far.
        # We ensure it's at least 1.
        if m_freq[key] < 1: m_freq[key] = 1
            
    # 2. Calculate Priority
    # New items enter with Priority = Current L + Frequency.
    # If it's a scan (freq=1), P = L + 1. It will likely be evicted next if L is high.
    m_priority[key] = GDSF_L + m_freq[key]
    
    # 3. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Periodic cleanup
    if len(m_freq) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active priority map.
    2. Remove from recency map.
    3. KEEP in frequency map (Ghost Cache).
    '''
    global m_priority, m_last_access
    
    if evicted_obj and evicted_obj.key in m_priority:
        del m_priority[evicted_obj.key]
    
    if evicted_obj and evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    
    # Do NOT delete from m_freq. This allows us to recognize "working set" items
    # that return after being evicted.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpux6wv_4q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprjtofb5a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa88k04_b.pickle

Iteration 66: New subsample score 0.389044 is not better than old score 0.405394, skipping
Iteration 67: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnd9rquy8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc9s41vr6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv5f6ir6n.pickle

Iteration 67: Proposed new text for program: import math

# Global metadata to store eviction info
# Key -> (H_score, Frequency, Size)
m_metadata = dict()

# Global variable to track the "aging" factor (L)
# This represents the priority of the most recently evicted item.
m_inflation_L = 0.0

# Ghost registry to detect trends and give 'second chances'
# Key -> timestamp of eviction
m_ghost_registry = dict()

def calculate_priority(freq, size, current_L):
    '''
    Calculates the GDSF priority score.
    H = L + (Frequency / Size)
    
    We add a tiny constant to size to avoid division by zero if size is 0 
    (though typically size >= 1).
    We use a weighting factor for Frequency to tune how aggressive we are on LFU vs LRU.
    '''
    # Cost function: We want to keep items that provide the most hits per byte occupied.
    # Standard GDSF uses cost = 1. So priority = L + Freq * (1/Size).
    # We use a slight logarithm on size to prevent overly penalizing large items 
    # if the trace has massive size variance, but usually 1/size is optimal for hit rate.
    
    # Check for zero size to be safe
    safe_size = max(1, size)
    
    # Priority formula
    priority = current_L + (freq / safe_size)
    return priority

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the LOWEST H-value (Priority).
    In GDSF, the "victim" is the item with the smallest priority score.
    When we evict, we update the global inflation factor L to the victim's priority.
    '''
    global m_metadata, m_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    min_priority = float('inf')
    victim_key = None
    
    # Find the object with the minimum priority score
    # Tie-breaking: If priorities are equal, we can use LRU (recency) logic implicitly 
    # if we stored access time, but strictly following the math is usually sufficient.
    for key in candidate_keys:
        if key in m_metadata:
            h_score, _, _ = m_metadata[key]
            if h_score < min_priority:
                min_priority = h_score
                victim_key = key
        else:
            # Metadata missing? Should be evicted first.
            return key
            
    # Update the global inflation factor L
    # This acts as the "Aging" mechanism. All future items will need a priority > this to survive.
    if victim_key is not None:
        m_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Recalculate Priority (H-value) using current L.
    '''
    global m_metadata, m_inflation_L
    
    # Retrieve existing data. 
    # If not found (shouldn't happen on hit), default to freq=0
    if obj.key in m_metadata:
        _, freq, size = m_metadata[obj.key]
    else:
        freq = 0
        size = obj.size
        
    new_freq = freq + 1
    
    # Calculate new priority
    # Note: We use the current global L. This "restores" the object's youth.
    new_priority = calculate_priority(new_freq, size, m_inflation_L)
    
    m_metadata[obj.key] = (new_priority, new_freq, size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry for "Second Chance".
    2. specific Frequency.
    3. Calculate Priority.
    '''
    global m_metadata, m_inflation_L, m_ghost_registry
    
    freq = 1
    
    # Check if recently evicted
    if obj.key in m_ghost_registry:
        # It was a mistake to evict this.
        # We restore a higher frequency to give it a survival boost.
        # We don't restore the EXACT old frequency because that might be too sticky,
        # but we give it a boost to escape immediate eviction.
        freq = 2
        del m_ghost_registry[obj.key]
        
    # Calculate priority
    # New items get priority based on current L + their intrinsic value (freq/size)
    priority = calculate_priority(freq, obj.size, m_inflation_L)
    
    m_metadata[obj.key] = (priority, freq, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
        
    # Add to ghost registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prune ghost registry if it gets too large
    # A good rule of thumb is keeping it roughly the size of the cache capacity (in counts)
    # or a fixed limit to prevent memory leaks.
    if len(m_ghost_registry) > 2000: # Arbitrary safety limit or based on cache len
         # Remove a chunk of keys to keep it fast
         keys_to_remove = list(m_ghost_registry.keys())[:100]
         for k in keys_to_remove:
             del m_ghost_registry[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0dmkm6pa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppcev904h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8fm63_h4.pickle

Iteration 67: New subsample score 0.962279 is not better than old score 0.996621, skipping
Iteration 68: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4pe4h87j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe35rb1rf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfqc16ooo.pickle

Iteration 68: Proposed new text for program: import math

# --- Metadata Storage ---
# Stores tuples: (access_frequency, last_access_time, is_in_t2_list)
# is_in_t2_list is a boolean indicating if the item is considered part of the "Frequent" set.
m_metadata = dict()

# Ghost Registries (Store keys only to save memory)
# B1: Recently evicted from the "Recent/T1" list
m_ghost_b1 = set()
# B2: Recently evicted from the "Frequent/T2" list
m_ghost_b2 = set()

# Adaptive Parameter 'p'
# Target size for the T1 (Recent) list.
# 0 <= p <= capacity
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on the Adaptive Replacement Cache (ARC) logic.
    We compare the size of T1 (recent items) against the adaptive parameter 'p'.
    '''
    global m_metadata, m_p, m_ghost_b1, m_ghost_b2

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
    
    # Classify current cache content into T1 (Recent) and T2 (Frequent)
    t1_keys = []
    t2_keys = []
    
    for k in candidate_keys:
        if k in m_metadata:
            _, last_access, in_t2 = m_metadata[k]
            if in_t2:
                t2_keys.append((k, last_access))
            else:
                t1_keys.append((k, last_access))
        else:
            # Fallback for untracked items (treat as T1/Recent)
            t1_keys.append((k, 0))

    # Determine eviction candidate based on adaptive parameter 'p'
    # logic: if len(T1) > p, we evict the LRU of T1. Else, evict LRU of T2.
    # Note: We must ensure we don't try to evict from an empty list.
    
    victim_key = None
    
    if len(t1_keys) > 0 and (len(t1_keys) > m_p or len(t2_keys) == 0):
        # Evict LRU from T1
        # Sort by last_access ascending (oldest first)
        t1_keys.sort(key=lambda x: x[1])
        victim_key = t1_keys[0][0]
    else:
        # Evict LRU from T2
        t2_keys.sort(key=lambda x: x[1])
        victim_key = t2_keys[0][0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it was in T1 (Recent), move it to T2 (Frequent).
    If it was in T2, update its recency.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, _, in_t2 = m_metadata[obj.key]
        
        # Whether it was T1 or T2, a hit implies it's valuable.
        # It now belongs/stays in T2 (Frequent list).
        m_metadata[obj.key] = (freq + 1, current_time, True)
    else:
        # Should rarely happen on a hit, but safety fallback
        m_metadata[obj.key] = (1, current_time, False)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    1. Check Ghost lists (B1, B2) to adapt parameter 'p'.
    2. Insert new metadata.
    '''
    global m_metadata, m_p, m_ghost_b1, m_ghost_b2
    
    current_time = cache_snapshot.access_count
    capacity = len(cache_snapshot.cache) # Approximation of capacity in objects
    if capacity == 0: capacity = 1 # avoid div by zero
    
    # --- Adapt 'p' based on ghost hits ---
    if obj.key in m_ghost_b1:
        # Hit in B1 (Recency Ghost): We should have made T1 larger.
        # Increase p.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
             delta = len(m_ghost_b2) // len(m_ghost_b1)
        m_p = min(capacity, m_p + delta)
        m_ghost_b1.remove(obj.key)
        
        # It returns to cache as a T2 item (freq > 1 conceptually, though this is a simplification)
        # Standard ARC puts it in T2 because it's been seen twice (once evicted, once now)
        is_t2 = True 
        
    elif obj.key in m_ghost_b2:
        # Hit in B2 (Frequency Ghost): We should have made T2 larger (T1 smaller).
        # Decrease p.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
        m_p = max(0, m_p - delta)
        m_ghost_b2.remove(obj.key)
        
        # It returns to cache as T2
        is_t2 = True
        
    else:
        # Brand new item, never seen recently.
        # Starts in T1 (Recent list).
        is_t2 = False

    # Initialize metadata
    # We use 1 as initial freq if new, or 2 if recovered from ghost
    initial_freq = 2 if is_t2 else 1
    m_metadata[obj.key] = (initial_freq, current_time, is_t2)
    
    # Initialize p if this is the very first insertion ever
    if m_p == 0 and len(m_metadata) == 1:
        m_p = 0 # Start with preference for frequency, will adapt up

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Identify if victim was T1 or T2.
    2. Move key to corresponding Ghost List (B1 or B2).
    3. Clean metadata.
    '''
    global m_metadata, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_metadata:
        _, _, in_t2 = m_metadata[key]
        
        if in_t2:
            # Evicted from T2 -> Add to B2
            m_ghost_b2.add(key)
        else:
            # Evicted from T1 -> Add to B1
            m_ghost_b1.add(key)
            
        del m_metadata[key]
        
    # --- Maintenance: Bound Ghost Lists ---
    # To prevent memory leaks, keep ghost lists roughly size of cache capacity
    max_ghost_size = cache_snapshot.capacity # using capacity as proxy for max items logic
    # Since we can't easily check exact length vs capacity in bytes, we use a heuristic based on cache count
    target_len = len(cache_snapshot.cache) 
    
    # Simple FIFO pruning for sets is hard, but we can prevent unbounded growth
    # by clearing if they get absurdly large, or just relying on Python's memory management 
    # for string keys. For strictness, we can check size:
    if len(m_ghost_b1) > target_len * 2:
        m_ghost_b1.clear() # drastic but safe
    if len(m_ghost_b2) > target_len * 2:
        m_ghost_b2.clear()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphq8qnsxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzwjr64po.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_a4ob6dx.pickle

Iteration 68: New subsample score 0.8861539999999999 is not better than old score 0.904836, skipping
Iteration 69: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9k7ffdds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv6by0b9l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8u339ptx.pickle

Iteration 69: Proposed new text for program: import math

# --- ARC (Adaptive Replacement Cache) Global State ---

# m_meta stores metadata for items currently in the cache.
# Key -> {'type': 1 (T1/Recent) or 2 (T2/Frequent), 'time': access_count}
m_meta = dict()

# Ghost Registries (Simulated B1 and B2 from ARC).
# These track keys that were recently evicted.
# m_ghost_b1: Keys evicted from T1 (Recency ghost)
# m_ghost_b2: Keys evicted from T2 (Frequency ghost)
# We use Python dicts to maintain insertion order (effectively FIFO/LRU).
m_ghost_b1 = dict() 
m_ghost_b2 = dict()

# m_p is the adaptive parameter representing the target size of the T1 (Recent) list.
# 0 <= m_p <= Cache Capacity (in items)
m_p = 0.0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on ARC logic.
    We prefer evicting from T1 if it exceeds the target size 'p', 
    or from T2 otherwise, while handling specific recovery conditions.
    '''
    global m_meta, m_p, m_ghost_b2

    # Get all current cache keys
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into T1 (Recent) and T2 (Frequent)
    #    and find the LRU item for each segment in one pass.
    t1_candidates = []
    t2_candidates = []
    
    for k in candidate_keys:
        # Default to T1 if metadata is missing (failsafe)
        meta = m_meta.get(k, {'type': 1, 'time': 0})
        if meta['type'] == 1:
            t1_candidates.append((k, meta['time']))
        else:
            t2_candidates.append((k, meta['time']))
            
    # Identify LRU for T1 and T2 (item with min 'time')
    lru_t1 = min(t1_candidates, key=lambda x: x[1])[0] if t1_candidates else None
    lru_t2 = min(t2_candidates, key=lambda x: x[1])[0] if t2_candidates else None
    
    len_t1 = len(t1_candidates)
    
    # 2. Check if the *incoming* object (obj) is in the ghost of T2 (B2).
    #    This signals we are recovering a frequent item, affecting the decision.
    xt_in_b2 = obj.key in m_ghost_b2
    
    # 3. ARC Decision Logic
    #    Evict from T1 (Recent) if:
    #    a) T1 is larger than its target size m_p.
    #    b) OR, the incoming item is in B2 and T1 has reached exactly m_p.
    evict_from_t1 = False
    
    if len_t1 > 0:
        if len_t1 > m_p:
            evict_from_t1 = True
        elif xt_in_b2 and len_t1 >= int(m_p):
             # When recovering T2, we are more aggressive on T1
             evict_from_t1 = True
    
    # 4. Return the victim
    if evict_from_t1:
        if lru_t1 is not None:
            return lru_t1
        # Fallback: if we wanted to evict T1 but it's empty, evict T2
        return lru_t2
    else:
        if lru_t2 is not None:
            return lru_t2
        # Fallback: if we wanted to evict T2 but it's empty, evict T1
        return lru_t1

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    The item has proven its utility. Move it to T2 (Frequent) 
    and update its access time (MRU).
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    # ARC Rule: Any hit (in T1 or T2) moves the item to T2 (MRU position)
    m_meta[obj.key] = {'type': 2, 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registries (B1, B2) to adapt parameter 'p'.
    2. Initialize metadata for the new object.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2, m_p
    
    current_time = cache_snapshot.access_count
    
    # Use current item count as a proxy for capacity "C" in ARC formulas
    c = len(cache_snapshot.cache) 
    if c == 0: c = 1 
    
    in_b1 = obj.key in m_ghost_b1
    in_b2 = obj.key in m_ghost_b2
    
    if in_b1:
        # Hit in Ghost T1 (Recency):
        # We evicted a recent item too soon. We should increase T1's target size (p).
        len_b1 = len(m_ghost_b1)
        len_b2 = len(m_ghost_b2)
        
        delta = 1.0
        if len_b1 < len_b2:
            delta = float(len_b2) / len_b1
            
        m_p = min(float(c), m_p + delta)
        
        # This is a "phantom hit", so it enters as a Frequent item (T2)
        m_meta[obj.key] = {'type': 2, 'time': current_time}
        del m_ghost_b1[obj.key]
        
    elif in_b2:
        # Hit in Ghost T2 (Frequency):
        # We evicted a frequent item too soon. We should decrease T1's target size (p)
        # to make more room for T2.
        len_b1 = len(m_ghost_b1)
        len_b2 = len(m_ghost_b2)
        
        delta = 1.0
        if len_b2 < len_b1:
            delta = float(len_b1) / len_b2
            
        m_p = max(0.0, m_p - delta)
        
        # Enters as Frequent item (T2)
        m_meta[obj.key] = {'type': 2, 'time': current_time}
        del m_ghost_b2[obj.key]
        
    else:
        # Brand new item (never seen recently).
        # Enters as Recent item (T1).
        m_meta[obj.key] = {'type': 1, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move the evicted item's key to the corresponding Ghost Registry.
    2. Clean up metadata.
    3. Ensure Ghost Registries stay bounded.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2
    
    # Determine which list the victim came from
    v_type = 1
    if evicted_obj.key in m_meta:
        v_type = m_meta[evicted_obj.key]['type']
        del m_meta[evicted_obj.key] 
    
    # Add to Ghost Registry
    if v_type == 1:
        m_ghost_b1[evicted_obj.key] = True
    else:
        m_ghost_b2[evicted_obj.key] = True
        
    # Maintenance: Prevent unbounded growth of ghost registries.
    # We keep history roughly equal to cache size.
    target_ghost_len = max(len(cache_snapshot.cache), 10)
    
    # Prune oldest entries (Standard Python dict preserves insertion order)
    while len(m_ghost_b1) > target_ghost_len:
        k = next(iter(m_ghost_b1))
        del m_ghost_b1[k]
        
    while len(m_ghost_b2) > target_ghost_len:
        k = next(iter(m_ghost_b2))
        del m_ghost_b2[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpir0mfyvg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpka0tu7o_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7tf62ru7.pickle

Iteration 69: New subsample score 1.042313 is better than old score 1.023382. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr256x6ut.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvea3jemt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwc1cbh3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdaia049_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0vscbglh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpll35o5bx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpctf4uzz7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd1xt9owh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpunk1j1k8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo5n99dm7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwz5ag4sz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyb4qeiwp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxivvmsfj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv848wzuk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu99t4bdl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3edjnput.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc_qd5qf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprq4ue7ea.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe6xyggjq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpli8eo3mj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpabeuisnt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvslzs4xh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvxtyex7n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0u35jbqv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl82cqdjy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdiwnel3h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxnnk_z6l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbe4fgr9w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphbd17mjk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppoznlfw1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjo0s4d02.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl9bogoe9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq3buninw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg0qaip35.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg3mfzqr0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7pb3deu3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsghajn43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9izzmbuu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqvy1s3m8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphc_w11ui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuqgqkpqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkz_2xbto.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo2kb8zoe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9pls10gx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo2dsczu7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmao5ome2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpefe2697u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa4qbmjab.pickle

Iteration 69: Full valset score for new program: 0.24327527083333333
Iteration 69: Full train_val score for new program: 0.24327527083333333
Iteration 69: Individual valset scores for new program: [0.50791, 0.483248, 0.491122, 0.441273, 0.505643, 0.490488, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.334813, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.342183, 0.026164, 0.058672, 0.058672, 0.269817, 0.368952, 0.85005, 0.891494, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.076964, 0.640392, 0.125461, 0.070141, 0.092853, 0.056543, 0.052632, 0.283333, 0.093314, 0.061431, 0.466258, 0.081699]
Iteration 69: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 69: Full valset pareto front score: 0.2582914583333333
Iteration 69: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15}, {3}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {3, 5, 6, 7, 12, 13}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {1}, {15}, {4}, {6}, {8, 19, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {4, 8, 10, 11, 16, 19}, {4, 8, 10, 11, 16, 19}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {8, 11}, {6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {3, 5, 6, 7}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19}]
Iteration 69: Best valset aggregate score so far: 0.2510750416666666
Iteration 69: Best program as per aggregate score on train_val: 4
Iteration 69: Best program as per aggregate score on valset: 4
Iteration 69: Best score on valset: 0.2510750416666666
Iteration 69: Best score on train_val: 0.2510750416666666
Iteration 69: Linear pareto front program index: 4
Iteration 69: New program candidate index: 19
Iteration 70: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprcng4p0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg_milq0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptjawi97d.pickle

Iteration 70: Proposed new text for program: import math

# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency). 
# We will NOT clear this immediately on eviction to provide "Ghost" history.
m_key_frequency = dict()

# Tracks the last access time of an object (Recency).
m_key_last_access = dict()

# Tracks when the object was first inserted into the cache. 
# Helps identify "short-lived" objects vs "long-lived" objects.
m_key_insertion_time = dict()

# A counter to perform periodic cleanup of the frequency map so it doesn't grow infinitely
cleanup_counter = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Strategy: Weighted Hybrid of Recency and Frequency.
    
    We calculate a "victim score" for every candidate. The higher the score, the more likely
    it is to be evicted.
    
    Score = (1 / Frequency) * Time_Since_Last_Access
    
    Logic:
    1. Low Frequency increases the eviction score significantly.
    2. Large time since last access (LRU) increases the eviction score.
    3. We prefer to evict items with Frequency == 1 (scan resistance).
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    victim_key = None
    max_victim_score = -1.0

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate 'age' in terms of access count
        age = current_time - last_access
        
        # --- SCORING LOGIC ---
        
        # Base score is the age (LRU behavior)
        score = age
        
        # Penalty for low frequency (LFU behavior).
        # If freq is 1, the score remains high (good for eviction).
        # If freq is high, the score shrinks (protected).
        # We square the frequency to protect popular items more aggressively.
        score = score / (freq * freq)
        
        # Special Case: "One-hit wonder" protection.
        # If an item was inserted long ago but never hit again (Freq=1), 
        # it is essentially dead weight. Boost its eviction score.
        if freq == 1:
            score = score * 2.0

        if score > max_victim_score:
            max_victim_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize metadata.
    '''
    global m_key_frequency, m_key_last_access, m_key_insertion_time, cleanup_counter
    
    current_time = cache_snapshot.access_count
    
    # If the key exists in our "Ghost" frequency map (it was evicted recently),
    # we restore that frequency + 1. Otherwise, it starts at 1.
    # This gives a chance for recently evicted popular items to stick better this time.
    previous_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = previous_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = current_time
    m_key_insertion_time[obj.key] = current_time

    # Periodic Cleanup logic for the metadata map to prevent memory leak
    # (Since we are keeping data for evicted items).
    cleanup_counter += 1
    if cleanup_counter > 5000:
        cleanup_metadata(cache_snapshot)
        cleanup_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove Recency data, but KEEP Frequency data (Ghost Cache).
    This allows the algorithm to "remember" items if they return soon.
    '''
    global m_key_last_access, m_key_insertion_time
    
    # We delete recency/time data because if it comes back, it's a new "life".
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    if evicted_obj.key in m_key_insertion_time:
        del m_key_insertion_time[evicted_obj.key]
        
    # NOTE: We intentionally do NOT delete from m_key_frequency here.
    # This acts as our historical frequency tracker.

def cleanup_metadata(cache_snapshot):
    '''
    Helper to prevent m_key_frequency from growing infinitely.
    We remove keys from metadata that are not currently in the cache
    and haven't been seen in a long time.
    '''
    global m_key_frequency
    
    # If the map isn't too big relative to cache capacity, don't bother
    # (Assuming average object size of 1 for rough estimation, or just simple count)
    if len(m_key_frequency) < len(cache_snapshot.cache) * 3:
        return

    keys_to_delete = []
    current_in_cache = cache_snapshot.cache
    
    # A simple way to halve the frequency of ghost items to decay them over time
    # rather than hard deleting, or delete very low freq ghosts.
    for k, freq in m_key_frequency.items():
        if k not in current_in_cache:
            # Decay: If it's not in cache, reduce its "ghost" frequency.
            # If it drops to 0, remove it.
            new_freq = freq // 2
            if new_freq <= 0:
                keys_to_delete.append(k)
            else:
                m_key_frequency[k] = new_freq
                
    for k in keys_to_delete:
        del m_key_frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnmnwen3p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnw8bjgj0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8qtgrm6s.pickle

Iteration 70: New subsample score 1.282944 is better than old score 1.1441949999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9pnw7wyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpabriqppa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv5u59ise.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpulzaowqx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqcnkr7k3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0zqejlga.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnpkt55k7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp77_7txsq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzeqa2kk5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxjevx15t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp47wdz99d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppv5wnn7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwaz2tp17.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy93d5pog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_vs475b4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxqmytjky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcxgaesuu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxk3p1c_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi9kuu62y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg3f3zeat.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppjbro6e1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7f6utn_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr5zj73fs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9ulyfpd1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp27dwyeoi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0olg3bjz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptn_nj3lp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9giat6tm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp34j44oof.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp212o1o68.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkg6b9bmj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppsx3j81t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5xq2jt3n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp33f947ou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8efpnvfi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw0qg1g33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3fyfvfr1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4w1m9e_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp97ov8fqe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbq4bts2x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi87xj7ib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxsnxuw0z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe2tqq5rm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5jaou30a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaupjus1z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxpu9acfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi3fylask.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyjtqag7u.pickle

Iteration 70: Full valset score for new program: 0.23672849999999998
Iteration 70: Full train_val score for new program: 0.23672849999999998
Iteration 70: Individual valset scores for new program: [0.499031, 0.475251, 0.481425, 0.433982, 0.494839, 0.48455, 0.271531, 0.498624, 0.541294, 0.531017, 0.091667, 0.379663, 0.024488, 0.0, 0.021379, 0.020992, 0.020331, 0.023756, 0.022922, 0.272227, 0.371681, 0.02636, 0.057382, 0.057382, 0.288851, 0.383065, 0.838133, 0.892521, 0.020365, 0.038636, 0.04328, 0.009732, 0.00308, 0.742682, 0.074561, 0.066196, 0.050051, 0.641937, 0.125461, 0.022179, 0.02077, 0.023637, 0.044408, 0.366667, 0.033239, 0.023001, 0.466258, 0.042484]
Iteration 70: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 70: Full valset pareto front score: 0.2582914583333333
Iteration 70: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {3, 5, 6, 7, 12, 13, 20}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19}, {1}, {15}, {4}, {6}, {8, 19, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {4, 8, 10, 11, 16, 19}, {4, 8, 10, 11, 16, 19}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {8, 11}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19}, {3, 5, 6, 7, 20}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19}]
Iteration 70: Best valset aggregate score so far: 0.2510750416666666
Iteration 70: Best program as per aggregate score on train_val: 4
Iteration 70: Best program as per aggregate score on valset: 4
Iteration 70: Best score on valset: 0.2510750416666666
Iteration 70: Best score on train_val: 0.2510750416666666
Iteration 70: Linear pareto front program index: 4
Iteration 70: New program candidate index: 20
Iteration 71: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkzwhx_c9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpddqaw5pz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8vtbhiqm.pickle

Iteration 71: Proposed new text for program: from collections import deque

# Global Metadata for S3-FIFO Algorithm

# Queue S: Small/New items (FIFO)
q_small = deque()
# Queue M: Main/Frequent items (FIFO)
q_main = deque()

# Frequency tracking: Maps key -> integer frequency (capped usually at 3)
meta_freq = dict()

# Size tracking: Maps key -> integer size in bytes
meta_sizes = dict()

# Track the current byte size of the S queue to maintain the 10% ratio
current_small_bytes = 0

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Policy (Byte-aware).
    
    Strategy:
    1. Check if we should evict from the Small queue (S) or Main queue (M).
       - We evict from S if it exceeds 10% of total cache capacity (bytes) OR if M is empty.
    2. Iterate through the chosen queue to find a victim:
       - If the candidate has been accessed (freq > 0), give it a second chance:
         - If in S: Promote to M, reset freq to 0.
         - If in M: Reinsert at tail of M, decrement freq.
       - If the candidate has not been accessed (freq == 0), evict it.
    '''
    global q_small, q_main, meta_freq, meta_sizes, current_small_bytes
    
    capacity = cache_snapshot.capacity
    # Target size for S queue is 10% of total capacity
    target_small = capacity * 0.1
    
    # Safety fallback if cache appears empty
    if not q_small and not q_main:
        if cache_snapshot.cache:
            return list(cache_snapshot.cache.keys())[0]
        return None

    while True:
        # Decision: Evict from Small or Main?
        # Prefer evicting from S if it's over budget or if M is empty
        evict_from_small = False
        if len(q_small) > 0:
            if current_small_bytes >= target_small or len(q_main) == 0:
                evict_from_small = True
        
        if evict_from_small:
            victim_key = q_small[0]
            
            # Lazy cleanup: ensure key is actually in cache (handles sync drifts)
            if victim_key not in cache_snapshot.cache:
                q_small.popleft()
                if victim_key in meta_sizes:
                    current_small_bytes -= meta_sizes[victim_key]
                    del meta_sizes[victim_key]
                if victim_key in meta_freq:
                    del meta_freq[victim_key]
                continue
                
            # S3-FIFO Logic for S
            freq = meta_freq.get(victim_key, 0)
            if freq > 0:
                # Promotion: Move from S to M
                q_small.popleft()
                
                # Update S-queue byte tracking
                size = meta_sizes.get(victim_key, 0)
                current_small_bytes -= size
                
                # Reset freq for M residency (standard S3-FIFO behavior)
                meta_freq[victim_key] = 0
                q_main.append(victim_key)
            else:
                # Victim found: Item in S with no hits
                q_small.popleft()
                
                # Adjust tracking immediately so loop logic remains consistent
                if victim_key in meta_sizes:
                    current_small_bytes -= meta_sizes[victim_key]
                
                # Return the key to be evicted
                return victim_key
        
        else:
            # Evict from Main
            if not q_main:
                # Fallback
                return list(cache_snapshot.cache.keys())[0]
                
            victim_key = q_main[0]
            
            if victim_key not in cache_snapshot.cache:
                q_main.popleft()
                if victim_key in meta_sizes: del meta_sizes[victim_key]
                if victim_key in meta_freq: del meta_freq[victim_key]
                continue
                
            # S3-FIFO Logic for M
            freq = meta_freq.get(victim_key, 0)
            if freq > 0:
                # Reinsert: Give second chance in M
                q_main.popleft()
                meta_freq[victim_key] -= 1 # Decay frequency
                q_main.append(victim_key)
            else:
                # Victim found: Item in M with expired frequency
                q_main.popleft()
                return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit.
    Cap at 3 to distinguish "warm" vs "hot" without infinite growth.
    '''
    global meta_freq
    key = obj.key
    # Capped increment
    meta_freq[key] = min(meta_freq.get(key, 0) + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    Insert new objects into the Small queue (S).
    '''
    global q_small, meta_freq, meta_sizes, current_small_bytes
    
    key = obj.key
    # New items start with 0 frequency (need a subsequent hit to survive eviction)
    meta_freq[key] = 0
    meta_sizes[key] = obj.size
    
    q_small.append(key)
    current_small_bytes += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global meta_freq, meta_sizes
    
    key = evicted_obj.key
    
    if key in meta_freq:
        del meta_freq[key]
    
    if key in meta_sizes:
        del meta_sizes[key]
        # Note: current_small_bytes was already adjusted inside evict() 
        # if the item was chosen from the S queue.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp09t47b8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3hdx9mes.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt2tn14ma.pickle

Iteration 71: New subsample score 0.093097 is not better than old score 0.101535, skipping
Iteration 72: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp130lasto.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvoz81wfd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjzvqk3gn.pickle

Iteration 72: Proposed new text for program: import math

# Global Metadata
# Tracks the state of the ARC algorithm
# m_lists maps key -> location ('t1', 't2', 'b1', 'b2')
m_lists = dict()

# We need ordered structures to manage LRU behavior within the sets.
# Since we can't import complex structures, we use dictionaries as ordered maps 
# (standard in Python 3.7+), utilizing re-insertion to move to end.
m_t1 = dict() # Recent items (in cache)
m_t2 = dict() # Frequent items (in cache)
m_b1 = dict() # Ghosts of recent items (evicted history)
m_b2 = dict() # Ghosts of frequent items (evicted history)

# Adaptability parameter 'p' (target size for T1)
m_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic:
    Decide whether to evict from T1 or T2 based on the target size 'p'
    and the current population of T1.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    # Safeties to ensure we don't crash if empty
    if not m_t1 and not m_t2:
        return None
        
    # We need to decide which victim to pick.
    # In ARC, the logic is usually handled during the *insertion* phase (replace()),
    # but here the system asks us to pick a victim explicitly.
    
    # We approximate the standard ARC replacement logic here:
    # If len(T1) > p, we usually prefer evicting from T1 (Recency).
    # Otherwise, we evict from T2 (Frequency).
    
    # Note: dict keys are ordered by insertion. 
    # The first key in the dict is the LRU.
    
    candidate = None
    
    # Logic:
    # If we have "too many" recent items (len(m_t1) > p), we shed from T1.
    # However, we must ensure T2 is touched if T1 is empty.
    if m_t1 and (len(m_t1) > m_p or not m_t2):
        # Evict LRU from T1
        candidate = next(iter(m_t1))
    else:
        # Evict LRU from T2
        candidate = next(iter(m_t2))
        
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in T1, move to T2 (it has been accessed twice now).
    2. If in T2, move to MRU of T2 (it is still popular).
    '''
    global m_lists, m_t1, m_t2
    
    key = obj.key
    
    # Locate the item
    location = m_lists.get(key)
    
    if location == 't1':
        # Promotion: Recent -> Frequent
        del m_t1[key]
        m_t2[key] = True # Add to MRU of T2
        m_lists[key] = 't2'
    elif location == 't2':
        # Maintenance: Re-insert to update MRU position
        del m_t2[key]
        m_t2[key] = True
        # m_lists remains 't2'

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was in Ghost Lists (B1 or B2) to adjust 'p'.
    Add new item to T1.
    '''
    global m_lists, m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually works on count, here we treat 1 item = 1 unit.
    
    # 1. Adaptability Check (Ghost Hits)
    if key in m_b1:
        # Hit in Ghost Recent (B1).
        # We favored Frequency too much. Increase p (target size for T1).
        delta = 1
        if len(m_b1) < len(m_b2):
             delta = len(m_b2) / len(m_b1)
        m_p = min(capacity, m_p + delta)
        
        # Remove from ghost
        del m_b1[key]
        # Will be added to T2 (similar to a hit promotion) because it was known
        # But standard ARC adds to T2 if it was in B1/B2
        m_t2[key] = True
        m_lists[key] = 't2'
        
    elif key in m_b2:
        # Hit in Ghost Frequent (B2).
        # We favored Recency too much. Decrease p.
        delta = 1
        if len(m_b2) < len(m_b1):
            delta = len(m_b1) / len(m_b2)
        m_p = max(0, m_p - delta)
        
        # Remove from ghost
        del m_b2[key]
        # Restore to T2
        m_t2[key] = True
        m_lists[key] = 't2'
        
    else:
        # Total Miss (New item).
        # Add to MRU of T1
        m_t1[key] = True
        m_lists[key] = 't1'

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from T1/T2 to B1/B2 (Ghost lists).
    Maintain Ghost List sizes.
    '''
    global m_lists, m_t1, m_t2, m_b1, m_b2
    
    key = evicted_obj.key
    location = m_lists.get(key)
    capacity = cache_snapshot.capacity
    
    # 1. Clean up active lists
    if location == 't1':
        if key in m_t1: del m_t1[key]
        # Move to B1
        m_b1[key] = True
        m_lists[key] = 'b1'
    elif location == 't2':
        if key in m_t2: del m_t2[key]
        # Move to B2
        m_b2[key] = True
        m_lists[key] = 'b2'
    
    # 2. Maintain Ghost Registry Size
    # ARC typically enforces: len(T1)+len(B1) <= C and len(T2)+len(B2) <= 2C
    # Roughly, we just ensure ghosts don't grow infinitely.
    # A safe upper bound for ghosts is Capacity.
    
    while len(m_b1) > capacity:
        oldest = next(iter(m_b1))
        del m_b1[oldest]
        if m_lists.get(oldest) == 'b1':
            del m_lists[oldest]
            
    while len(m_b2) > capacity:
        oldest = next(iter(m_b2))
        del m_b2[oldest]
        if m_lists.get(oldest) == 'b2':
            del m_lists[oldest]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb2uytbq2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwv42tg76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpghi3k0zo.pickle

Iteration 72: New subsample score 0.649829 is better than old score 0.6434030000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpznp3x3yu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgoxko76w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp98o4ujq2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn3c1lc_a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw_tcaire.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvkbur_it.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3lfj57c8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnggu9imh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwl8x524g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr8i_rzoj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6_kv46vx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsajiamkr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp55qlxbjz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa68u4u_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp54qa2c_3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptwbcsr8s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw4yz0b2k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvyvxgox4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpelp5wcuw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvaxj9g1h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmztnks1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcypmind3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_l7jbqx5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3keo92_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7k16xex5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw891t_xr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpf42mmzhb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqv77ijhj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2muarzmo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb0rn2epj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw3fllhn9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_3zqn85t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz7qp79tr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp61wcwjw5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj3soo40n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2rm0xgdv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3zxgd3eo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmv58axkh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpni0cr494.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5u9qpjhi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpedo_93fg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2jp5iuub.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9t24zawm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpywe0n2ji.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpycrzaav5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcou6ntgv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjx8m4b60.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp84ux8jrm.pickle

Iteration 72: Full valset score for new program: 0.24445077083333333
Iteration 72: Full train_val score for new program: 0.24445077083333333
Iteration 72: Individual valset scores for new program: [0.507856, 0.483189, 0.491182, 0.441451, 0.505643, 0.49043, 0.272727, 0.498034, 0.539864, 0.531017, 0.075, 0.334813, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.3412, 0.026164, 0.058672, 0.058672, 0.269811, 0.372984, 0.849057, 0.891494, 0.071279, 0.038636, 0.045558, 0.026575, 0.028976, 0.752893, 0.083333, 0.067961, 0.08339, 0.640392, 0.125461, 0.077904, 0.124007, 0.054071, 0.052632, 0.3, 0.09278, 0.060028, 0.466258, 0.081699]
Iteration 72: New valset pareto front scores: [0.509255, 0.486516, 0.491667, 0.442963, 0.506177, 0.493024, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.074573, 0.038636, 0.045558, 0.026575, 0.028976, 0.753063, 0.083333, 0.067961, 0.096221, 0.641937, 0.125461, 0.14361, 0.126451, 0.162676, 0.052632, 0.366667, 0.16269, 0.140533, 0.466258, 0.081699]
Iteration 72: Full valset pareto front score: 0.2582914583333333
Iteration 72: Updated valset pareto front programs: [{4}, {4}, {4}, {4}, {4}, {4}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {3, 5, 6, 7, 12, 13, 20}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21}, {1}, {15}, {4}, {6}, {8, 19, 11}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21}, {4, 8, 10, 11, 16, 19, 21}, {4, 8, 10, 11, 16, 19, 21}, {8, 11}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21}, {8, 11}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {4}, {4}, {4}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21}, {3, 5, 6, 7, 20}, {4}, {4}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21}]
Iteration 72: Best valset aggregate score so far: 0.2510750416666666
Iteration 72: Best program as per aggregate score on train_val: 4
Iteration 72: Best program as per aggregate score on valset: 4
Iteration 72: Best score on valset: 0.2510750416666666
Iteration 72: Best score on train_val: 0.2510750416666666
Iteration 72: Linear pareto front program index: 4
Iteration 72: New program candidate index: 21
Iteration 73: Selected program 8 score: 0.2443585416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps1c65cz6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphv2z9k0w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv07973n1.pickle

Iteration 73: Proposed new text for program: # Metadata Global State
# T1: Recent Cache (LRU is 0, MRU is -1)
t1 = dict()
# T2: Frequent Cache (LRU is 0, MRU is -1)
t2 = dict()
# B1: Ghost Recent (Evicted from T1)
b1 = dict()
# B2: Ghost Frequent (Evicted from T2)
b2 = dict()
# p: Target size for T1 (Adaptation parameter)
p = 0

def evict(cache_snapshot, obj):
    '''
    Determines the victim to evict.
    Implements the ARC REPLACE(x) logic.
    We must look ahead at `obj.key` to see if it is in B1 or B2 
    to decide whether to prefer evicting from T1 or T2.
    '''
    global t1, t2, b1, b2, p

    incoming_key = obj.key
    
    # We use len(cache) as the working set capacity 'C' for ARC calculations.
    # In byte-based capacity scenarios, this adapts to the count of items currently fitting.
    c = len(cache_snapshot.cache) 
    if c == 0: 
        # Should not happen if cache is full, but safety first
        return None

    # Determine eviction target based on ARC logic
    # We must simulate the adaptation of 'p' to make the correct decision 
    # corresponding to the state we will be in after `update_after_insert` runs.
    
    # Logic derived from ARC "REPLACE" subroutine:
    evict_from_t1 = False
    
    if t1 and not t2:
        evict_from_t1 = True
    elif t2 and not t1:
        evict_from_t1 = False
    else:
        # Both lists have items, check ARC condition
        # If incoming key is in B2, we would decrement p (prefer T2), 
        # making the condition (len(t1) > p) more likely to be true.
        
        # Calculate adaptation delta locally for decision making
        if incoming_key in b2:
            delta = 1
            if len(b2) > 0 and len(b1) > 0:
                 delta = len(b1) / len(b2)
            # If hit in B2, target p decreases. 
            # The condition for evicting T1 is lenient:
            # REPLACE Logic: if (len(t1) > p) OR (key in B2 and len(t1) == p) -> evict T1
            # We use a temporary p for this check or explicit logic:
            if len(t1) > p or (len(t1) == int(p)): # int(p) handles float precision
                evict_from_t1 = True
        else:
            # If incoming key is in B1 (p increases) or New:
            # Standard check: if len(t1) > p -> evict T1
            if len(t1) > p:
                evict_from_t1 = True
    
    # Perform selection (keys are insertion ordered, iter(d) gives first/LRU)
    if evict_from_t1:
        # Return LRU of T1
        return next(iter(t1))
    else:
        # Return LRU of T2
        return next(iter(t2))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If in T1, move to T2 (promotion).
    - If in T2, move to MRU of T2.
    '''
    global t1, t2, b1, b2
    key = obj.key

    # If it is in T1, it has been accessed a second time -> Promote to T2
    if key in t1:
        del t1[key]
        t2[key] = None
    # If it is in T2, update LRU position
    elif key in t2:
        del t2[key]
        t2[key] = None
    # If it is in ghosts (rare race condition or data sync issue), revive to T2
    elif key in b1:
        del b1[key]
        t2[key] = None
    elif key in b2:
        del b2[key]
        t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Handle Ghost hits (B1/B2) to adapt p.
    - Insert new object into correct list (T1 if new, T2 if from Ghost).
    - Enforce bounds on Ghost lists.
    '''
    global t1, t2, b1, b2, p

    key = obj.key
    # Capacity in terms of count (approximate)
    c = len(cache_snapshot.cache) 

    # Case 1: Ghost Hit in B1 (Recency Miss) -> Increase p (Enlarge T1)
    if key in b1:
        delta = 1
        if len(b1) >= len(b2):
            delta = 1
        else:
            # Safety check for divide by zero
            delta = len(b2) / len(b1)
        
        p = min(float(c), p + delta)
        
        # Remove from Ghost and Add to Frequency (T2)
        del b1[key]
        t2[key] = None

    # Case 2: Ghost Hit in B2 (Frequency Miss) -> Decrease p (Shrink T1, Enlarge T2)
    elif key in b2:
        delta = 1
        if len(b2) >= len(b1):
            delta = 1
        else:
            delta = len(b1) / len(b2)
            
        p = max(0.0, p - delta)
        
        # Remove from Ghost and Add to Frequency (T2)
        del b2[key]
        t2[key] = None

    # Case 3: Totally New Item -> Add to Recency (T1)
    else:
        t1[key] = None

    # Maintenance: Trim Ghost Lists
    # ARC generally suggests keeping ghost history roughly equal to capacity
    # If ghosts grow too large, memory overhead increases without utility.
    if len(b1) > c:
        del b1[next(iter(b1))] # Remove LRU of B1
    
    if len(b2) > c:
        del b2[next(iter(b2))] # Remove LRU of B2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move evicted object key to the corresponding Ghost list.
    '''
    global t1, t2, b1, b2
    
    victim_key = evicted_obj.key

    # If evicted from T1, add to B1
    if victim_key in t1:
        del t1[victim_key]
        b1[victim_key] = None
    # If evicted from T2, add to B2
    elif victim_key in t2:
        del t2[victim_key]
        b2[victim_key] = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5iok7uq5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3buiv43q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5xbak9cz.pickle

Iteration 73: New subsample score 1.4867689999999998 is not better than old score 1.4867689999999998, skipping
Iteration 74: Selected program 1 score: 0.22787797916666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6qro6mzy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi21odh27.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5gekx2cw.pickle

Iteration 74: Proposed new text for program: import math

# Metadata dictionaries
# Tracks the accumulated frequency of an object
m_key_frequency = dict()
# Tracks the last access time (logical clock) of an object
m_key_last_access = dict()
# Tracks the time an object was inserted (to calculate retention duration)
m_key_insertion_time = dict()

def get_utility_score(key, current_time):
    '''
    Calculates a utility score based on Hyperbolic Caching principles.
    Score = (Frequency) / (Time Since Last Access + 1)
    
    This balances Recency and Frequency naturally.
    - Recently accessed items have a small denominator -> High Score.
    - Frequently accessed items have a large numerator -> High Score.
    - Old, infrequent items have small numerator, large denominator -> Low Score (Eviction targets).
    '''
    freq = m_key_frequency.get(key, 1)
    last_access = m_key_last_access.get(key, 0)
    
    # +1 prevents division by zero and normalizes immediate reuse
    duration_since_access = (current_time - last_access) + 1
    
    # We use a slight modification where frequency is somewhat dampened to prevent 
    # historical heavy hitters from staying forever if they stop being accessed.
    # Score = Freq / (Delta_Time)
    return freq / duration_since_access

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Utility Score.
    Algorithm: Hyperbolic Caching / Least Decayed Frequency
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to minimize O(N) scans, but given the constraints, we must scan the keys.
    # We look for the item with the MINIMUM score.
    
    current_time = cache_snapshot.access_count
    
    # Initialize best candidate (lowest score) with the first item
    victim_key = candidate_keys[0]
    min_score = get_utility_score(victim_key, current_time)

    # Optimization: Check a sample of keys if the cache is huge, 
    # but strictly following the prompt, we iterate to find the absolute best victim.
    for key in candidate_keys[1:]:
        score = get_utility_score(key, current_time)
        
        if score < min_score:
            min_score = score
            victim_key = key
        # Tie-breaker: If scores are identical, evict the one inserted earliest (FIFO fallback)
        elif score == min_score:
            if m_key_insertion_time.get(key, 0) < m_key_insertion_time.get(victim_key, 0):
                victim_key = key
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update recency.
    '''
    global m_key_frequency, m_key_last_access
    
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    If the object was seen recently (in metadata but not in cache), we restore some frequency.
    Otherwise, start fresh.
    '''
    global m_key_frequency, m_key_last_access, m_key_insertion_time
    
    # "Ghost" Cache Logic:
    # If the key exists in frequency dict but not in the cache (implied by this being an insert),
    # it means it was evicted previously. We treat it as a "returning hero".
    if obj.key in m_key_frequency:
        # We boost it, but not fully, to prevent stale items from bouncing back too hard.
        # However, for pure hit-rate maximization, resuming frequency is usually best.
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    m_key_insertion_time[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    CRITICAL CHANGE: We do NOT delete the metadata immediately.
    We keep the frequency data. This allows the algorithm to "remember" items 
    that are evicted but requested again shortly after (solving the "Scan" vs "Working Set" problem).
    
    To prevent memory leaks in a real system, we would cap the size of m_key_frequency.
    Here, to maximize hit rate within the logic constraints, we retain history.
    '''
    global m_key_last_access, m_key_insertion_time
    
    # We remove insertion time as it's only relevant for the current residence in cache
    if evicted_obj.key in m_key_insertion_time:
        del m_key_insertion_time[evicted_obj.key]
    
    # We remove last_access to ensure if it comes back, delta_time calculation is fresh
    if evicted_obj.key in m_key_last_access:
         del m_key_last_access[evicted_obj.key]

    # NOTE: We intentionally keep m_key_frequency[evicted_obj.key].
    # This acts as our "Ghost Cache".
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj26a8zl8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi7qiufha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpscau7j0m.pickle

Iteration 74: New subsample score 0.542021 is not better than old score 0.561498, skipping
Iteration 75: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyp24bd4_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1czksf8x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu02w930l.pickle

Iteration 75: Proposed new text for program: from collections import OrderedDict

# Metadata Globals
# We effectively maintain two logical queues using dictionaries to track state.
# 1. probation_set: Keys that have been accessed roughly once recently.
# 2. protected_set: Keys that have been accessed >= 2 times.
m_probation = set()
m_protected = set()

# We maintain explicit LRU tracking using OrderedDicts for O(1) operations.
# While the main cache holds the objects, these hold the order.
m_lru_probation = OrderedDict()
m_lru_protected = OrderedDict()

# Configuration
# This ratio determines the maximum size of the protected segment relative to the cache size.
# 80% protected / 20% probation is a standard starting point for scan resistance.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) equivalent.
    
    We prefer to evict from the Probationary segment (items seen once).
    If the Probationary segment is empty (unlikely), we evict from Protected.
    '''
    global m_lru_probation, m_lru_protected
    
    # 1. Try to evict from Probation (Scan resistance)
    # The first item in an OrderedDict is the LRU.
    if m_lru_probation:
        victim_key, _ = m_lru_probation.popitem(last=False)
        return victim_key
        
    # 2. If Probation is empty, evict from Protected (fallback)
    if m_lru_protected:
        victim_key, _ = m_lru_protected.popitem(last=False)
        return victim_key
        
    # Fallback if metadata is desynced (should not happen in normal flow)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in Probation, promote to Protected.
    2. If in Protected, update LRU position (move to end).
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected, PROTECTED_RATIO
    
    key = obj.key
    
    if key in m_probation:
        # Promotion: Probation -> Protected
        m_probation.remove(key)
        if key in m_lru_probation:
            del m_lru_probation[key]
            
        m_protected.add(key)
        m_lru_protected[key] = True # Add to MRU of protected
        
        # Enforce Protected Segment Size Limit
        # If Protected grows too big, demote the LRU of Protected back to Probation
        # This keeps the "elite" list exclusive.
        target_protected_size = int(cache_snapshot.capacity * PROTECTED_RATIO)
        if len(m_protected) > target_protected_size:
            demoted_key, _ = m_lru_protected.popitem(last=False) # Pop LRU
            m_protected.remove(demoted_key)
            m_probation.add(demoted_key)
            m_lru_probation[demoted_key] = True # Add to MRU of probation
            
    elif key in m_protected:
        # Hit on Protected: Just update recency
        m_lru_protected.move_to_end(key)
    else:
        # Edge case: Hit on item not in metadata (e.g., initial state or desync), treat as probation hit
        m_protected.add(key)
        m_lru_protected[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    New items always go to Probation.
    '''
    global m_probation, m_lru_probation
    
    key = obj.key
    
    # Add to Probationary segment
    m_probation.add(key)
    m_lru_probation[key] = True # Add to MRU
    
    _sync_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Cleanup metadata.
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected
    
    if evicted_obj:
        key = evicted_obj.key
        
        # Remove from whichever set it was in
        if key in m_probation:
            m_probation.remove(key)
        if key in m_protected:
            m_protected.remove(key)
            
        # Remove from LRU trackers if present (might have been removed in evict())
        if key in m_lru_probation:
            del m_lru_probation[key]
        if key in m_lru_protected:
            del m_lru_protected[key]

def _sync_metadata(cache_snapshot):
    '''
    Ensure our metadata doesn't hold keys that the cache dropped silently
    or grew stale.
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected
    
    # Simple check to ensure we don't leak memory indefinitely if the simulation
    # framework does something unexpected with the cache dictionary.
    if len(m_lru_probation) + len(m_lru_protected) > cache_snapshot.capacity + 10:
        
        # Rebuild based on actual cache content if we drifted significantly
        current_keys = set(cache_snapshot.cache.keys())
        
        # Prune Probation
        probation_keys = list(m_lru_probation.keys())
        for k in probation_keys:
            if k not in current_keys:
                del m_lru_probation[k]
                m_probation.discard(k)
                
        # Prune Protected
        protected_keys = list(m_lru_protected.keys())
        for k in protected_keys:
            if k not in current_keys:
                del m_lru_protected[k]
                m_protected.discard(k)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcm2hgmt3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmj6v46mi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaw9ordn0.pickle

Iteration 75: New subsample score 0.422463 is not better than old score 0.5350349999999999, skipping
Iteration 76: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptknx3bp7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8sgtprrt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyezmqoay.pickle

Iteration 76: Proposed new text for program: # Metadata Globals
# T1: Main Cache for recent items (seen once). Maps key -> object (though we only track keys here for logic)
# T2: Main Cache for frequent items (seen >1 times). Maps key -> object
# B1: Ghost Cache for T1 (recently evicted from T1). Maps key -> None
# B2: Ghost Cache for T2 (recently evicted from T2). Maps key -> None
# Note: In the actual `cache_snapshot.cache`, both T1 and T2 items reside mixed. 
# We track their logical state here.

m_T1 = set()
m_T2 = set()
m_B1 = set()
m_B2 = set()

# P: Target size for T1. 
# If P increases, we favor Recency. If P decreases, we favor Frequency.
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Replacement (similar to ARC).
    
    We decide whether to evict from the T1 (Recency) set or the T2 (Frequency) set 
    based on the current cache state relative to the target parameter `p`.
    
    Logic:
    1. If len(T1) > p, we usually evict from T1 (LRU behavior for recent items).
    2. Otherwise, we evict from T2 (LRU behavior for frequent items).
    
    Crucially, "evicting" here just means picking the victim. We must select the 
    LRU item from the chosen logical set (T1 or T2).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Need access times to determine LRU within the sets
    # We rely on the order in cache_snapshot.cache not being guaranteed, 
    # so we need to track access order or infer it. 
    # To keep it efficient without maintaining a separate sorted list, 
    # we will scan the cache once to find the LRU candidates for T1 and T2.
    # In a production ARC, T1 and T2 are Doubly Linked Lists. 
    # Here we approximate by iterating the cache keys and tracking access times.
    
    # Note: We need a way to track LRU order. Since we can't change the object class,
    # we will maintain a separate `m_access_time` dict updated on every hit/insert.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    # We need the global access time map to find LRU
    global m_access_time
    
    # Identify the LRU item in T1 and the LRU item in T2
    lru_t1_key = None
    lru_t1_time = float('inf')
    
    lru_t2_key = None
    lru_t2_time = float('inf')
    
    # Also track a global LRU just in case metadata is out of sync
    global_lru_key = None
    global_lru_time = float('inf')

    # Iterate once to find candidates
    for key in candidate_keys:
        acc_time = m_access_time.get(key, 0)
        
        if acc_time < global_lru_time:
            global_lru_time = acc_time
            global_lru_key = key
            
        if key in m_T1:
            if acc_time < lru_t1_time:
                lru_t1_time = acc_time
                lru_t1_key = key
        elif key in m_T2:
            if acc_time < lru_t2_time:
                lru_t2_time = acc_time
                lru_t2_key = key
        # If key is in neither (shouldn't happen often), it falls to global backup

    # ARC Replacement Logic Decision
    # If len(T1) exceeds the target p, we evict from T1 to make room.
    # Otherwise, we evict from T2.
    
    current_t1_size = len([k for k in m_T1 if k in cache_snapshot.cache])
    
    # Determine Victim
    if current_t1_size > 0 and (current_t1_size > m_p or (lru_t2_key is None and current_t1_size > 0)):
        # Evict LRU from T1
        victim = lru_t1_key
    else:
        # Evict LRU from T2
        victim = lru_t2_key

    # Fallback if logic fails (e.g. T1 empty but p tells us to evict T1, or sync issues)
    if victim is None:
        victim = global_lru_key
        
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - Update access time.
    - If item was in T1 (Recency), move it to T2 (Frequency).
    - If item was in T2, update it to MRU position (by access time).
    '''
    global m_T1, m_T2, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    if key in m_T1:
        m_T1.remove(key)
        m_T2.add(key)
    # If in T2, it stays in T2, just gets new time

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Determine if this is a Ghost Hit (B1 or B2).
    - Adapt parameter `p`.
    - Insert into T2 if it was in B2 (it's a returning frequent item).
    - Insert into T1 otherwise (it's a new item).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: In object count usually, but here sizes are bytes.
    # We will assume capacity roughly correlates to count for logic adaptation.
    # Using a fixed approximation for capacity count helps the math.
    approx_count_capacity = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100
    
    m_access_time[key] = current_time

    # Case 1: Key is in B1 (Ghost Recency) -> We should have made T1 larger
    if key in m_B1:
        delta = 1
        if len(m_B1) >= len(m_B2) and len(m_B2) > 0:
             delta = 1
        elif len(m_B2) > 0:
             delta = len(m_B1) / len(m_B2)
             
        m_p = min(approx_count_capacity, m_p + delta)
        
        # Move from Ghost to Real Frequency list
        m_B1.remove(key)
        m_T2.add(key) # Promoted to frequent because it came back

    # Case 2: Key is in B2 (Ghost Frequency) -> We should have kept T2 larger (T1 smaller)
    elif key in m_B2:
        delta = 1
        if len(m_B2) >= len(m_B1) and len(m_B1) > 0:
             delta = 1
        elif len(m_B1) > 0:
             delta = len(m_B2) / len(m_B1)
             
        m_p = max(0, m_p - delta)
        
        # Move from Ghost to Real Frequency list
        m_B2.remove(key)
        m_T2.add(key) 

    # Case 3: Totally new item
    else:
        # Add to T1 (Recency list)
        m_T1.add(key)
    
    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Move from T1 -> B1 (Ghost Recency)
    - Or T2 -> B2 (Ghost Frequency)
    - Remove from Access Time map
    '''
    global m_T1, m_T2, m_B1, m_B2, m_access_time
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    
    # Remove from access time
    if key in m_access_time:
        del m_access_time[key]
        
    if key in m_T1:
        m_T1.remove(key)
        m_B1.add(key)
    elif key in m_T2:
        m_T2.remove(key)
        m_B2.add(key)
    
    # If for some reason it's in neither (shouldn't happen), we just ignore

# Helper global for access times (needed for LRU approximation within sets)
m_access_time = dict()

def _cleanup_metadata():
    '''
    Keep ghost lists bounded to prevent memory leaks.
    ARC typically suggests |L1| + |L2| <= 2 * c.
    '''
    global m_B1, m_B2
    
    # We use a loose upper bound to keep memory predictable
    MAX_GHOST = 5000 
    
    if len(m_B1) > MAX_GHOST:
        # Remove arbitrary element (FIFO ideal, but set doesn't support, we just need to bound size)
        # Using pop on set is random but O(1)
        for _ in range(100):
            if m_B1: m_B1.pop()
            
    if len(m_B2) > MAX_GHOST:
        for _ in range(100):
            if m_B2: m_B2.pop()
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4tu1sav_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbsg8497b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8pbb2cj2.pickle

Iteration 76: New subsample score 1.157703 is not better than old score 1.242111, skipping
Iteration 77: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmoq25a5f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsw1b2s6a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2u81xkgo.pickle

Iteration 77: Proposed new text for program: import math

# Metadata dictionary
# Maps key -> [frequency, last_access_time]
# We use a list so it's mutable in place if needed, though tuple replacement is fine.
m_access_info = dict()

# Ghost registry (Shadow Cache)
# Maps key -> removal_time
# Used to give a boost to items that were recently evicted but requested again (handling 'churn' or 'thrashing').
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculates a victim score based on Recency and Frequency.
    Higher Score = Better candidate for eviction (Less valuable).
    
    Formula: Generalized LRFU (Least Recently/Frequently Used) approach.
    Value = Frequency * (1 / (TimeSinceAccess + Epsilon))
    
    Victim Score (Inverse of Value) = (TimeSinceAccess) / Frequency
    
    This effectively means:
    - An item accessed 1000 ticks ago with freq 1 has score 1000.
    - An item accessed 1000 ticks ago with freq 10 has score 100. (Harder to evict).
    '''
    if key not in m_access_info:
        return float('inf')
        
    freq, last_access = m_access_info[key]
    
    staleness = current_time - last_access
    
    # We apply a logarithmic dampener to the frequency. 
    # This prevents an item with 10,000 hits from becoming virtually un-evictable 
    # compared to an item with 5,000 hits. We care more about 1 vs 2 than 100 vs 101.
    # We add 1 to freq to avoid division by zero if logic somehow sets freq to 0.
    weighted_freq = math.log2(freq + 1) + 1
    
    # Victim Score = How stale it is / How popular it is.
    # High Staleness + Low Frequency = High Victim Score = Evict.
    return staleness / weighted_freq

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score (Oldest and Least Frequent).
    '''
    # Optimization: If the cache is very large, iterating all keys is slow.
    # However, given the constraints of the standard definition where we don't control the 
    # cache data structure (it's a dict), we must scan.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    max_score = -1.0
    best_victim_key = None
    
    # We sample a subset if the cache is massive to keep eviction O(1)-ish, 
    # but strictly following the prompt to minimize miss rate, we scan all 
    # to find the absolute best victim.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with aging/capping).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    freq, _ = m_access_info.get(obj.key, (0, current_time))
    
    # Simple aging mechanism:
    # If frequency gets too high, it becomes "cache pollution".
    # We cap it. A cap of 20-50 allows distinct "tiers" of heat without infinite locking.
    new_freq = min(freq + 1, 50)
    
    m_access_info[obj.key] = [new_freq, current_time]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Adaptive Component:
    # If this item was recently evicted (it's in the ghost registry), it implies our 
    # cache was too small or we evicted it prematurely. 
    # We grant it a "Frequency Boost" so it survives longer this time.
    if obj.key in m_ghost_registry:
        # Boost frequency to simulate that it has been "remembered"
        # We give it a head start of 5 (arbitrary heuristic based on trace analysis)
        initial_freq = 5
        del m_ghost_registry[obj.key]
    
    m_access_info[obj.key] = [initial_freq, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
    
    # 2. Add to Ghost Registry
    # This marks that we saw this item recently.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # If the ghost registry grows too large, it consumes memory.
    # We keep it roughly the size of the cache capacity to balance memory vs adaptability.
    # Since Python 3.7+, dicts maintain insertion order. We can treat it as a FIFO queue easily.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest inserted item (the first key in the dict)
        oldest_key = next(iter(m_ghost_registry))
        del m_ghost_registry[oldest_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo181y_1a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnkyfp1xe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpis0wrof3.pickle

Iteration 77: New subsample score 0.057494 is not better than old score 0.111411, skipping
Iteration 78: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptno212eu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjf1izchu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptnwhr27f.pickle

Iteration 78: Proposed new text for program: import math

# Metadata management
# We need to maintain the state of the ARC (Adaptive Replacement Cache) algorithm.
# Since we cannot create a class instance to hold state between calls, we use global dictionaries.

# ARC divides the cache into two lists:
# T1: Recently accessed items (Recency) - currently in cache
# T2: Frequently accessed items (Frequency) - currently in cache
# B1: Ghost list for T1 (Evicted Recency history) - keys only
# B2: Ghost list for T2 (Evicted Frequency history) - keys only

# Globals to store state
meta_lists = {
    'T1': set(), # Keys in T1
    'T2': set(), # Keys in T2
    'B1': set(), # Keys in B1
    'B2': set()  # Keys in B2
}

# LRU tracking: We need to know the order of elements in T1, T2, B1, B2.
# Since Python dicts preserve insertion order (3.7+), we can use dicts mapping key -> None/Time
# to simulate an ordered set or LRU queue.
lru_T1 = dict()
lru_T2 = dict()
lru_B1 = dict()
lru_B2 = dict()

# Target size for T1 list.
# 0 <= p <= capacity
# This adapts: if we hit B1, p increases (favor recency). If we hit B2, p decreases (favor frequency).
p = 0.0 

def access_lru(lru_dict, key):
    '''Helper to refresh a key in an LRU dict (move to end).'''
    if key in lru_dict:
        del lru_dict[key]
    lru_dict[key] = None

def pop_lru(lru_dict):
    '''Helper to pop the Least Recently Used item (first item).'''
    if not lru_dict:
        return None
    key = next(iter(lru_dict))
    del lru_dict[key]
    return key

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic.
    Decides which item to evict from the cache (T1 or T2) to make room.
    The decision relies on the adaptive parameter `p`.
    '''
    global p, lru_T1, lru_T2, meta_lists

    # IMPORTANT: The "obj" passed here is the new object we are trying to insert.
    # However, evict() is called when the physical cache is full.
    # We must pick a victim from the keys present in cache_snapshot.cache.
    
    # In ARC, T1 U T2 constitutes the cache content.
    # Rule: Replace from T1 if |T1| > p, else replace from T2.
    
    t1_size = len(lru_T1)
    
    # We also check if the incoming item `obj` is in B2.
    # If the new item is in B2, the replacement logic is slightly stricter on T1.
    is_in_B2 = obj.key in meta_lists['B2']
    
    candidate = None
    
    # Logic derived from ARC "REPLACE" subroutine
    if (t1_size > p) or (is_in_B2 and t1_size == p):
        # Evict LRU from T1
        if lru_T1:
            candidate = next(iter(lru_T1))
        else:
            # Fallback if T1 is empty (shouldn't happen given conditions, but safety first)
            candidate = next(iter(lru_T2))
    else:
        # Evict LRU from T2
        if lru_T2:
            candidate = next(iter(lru_T2))
        else:
            # Fallback
            candidate = next(iter(lru_T1))
            
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1 or T2, it's a hit.
    Move it to MRU of T2 (since it has been referenced again, it has frequency).
    '''
    global lru_T1, lru_T2, meta_lists
    
    key = obj.key
    
    # Case 1: Hit in T1.
    # Move x from T1 to MRU of T2.
    if key in meta_lists['T1']:
        meta_lists['T1'].remove(key)
        if key in lru_T1: del lru_T1[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 2: Hit in T2.
    # Move x to MRU of T2.
    elif key in meta_lists['T2']:
        access_lru(lru_T2, key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    This happens when we insert a new object.
    It might be a "real" miss (new key) or a "ghost" hit (key in B1 or B2).
    '''
    global p, lru_T1, lru_T2, lru_B1, lru_B2, meta_lists

    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually defined by count, here we approximate with capacity count if possible, but problem says capacity is Bytes. 
    # Provided "cache is full when total number of cached objects reaches its capacity" implies capacity is count-based in this specific challenge context or we treat it as such.
    # If capacity is bytes, we can't implement pure ARC easily. However, looking at the context: "The cache is full when the total number of cached objects reaches its capacity."
    # This implies Capacity is NUMBER OF OBJECTS.
    c = capacity 
    
    # Case 1: x in B1 (Ghost Hit on Recency)
    # We missed in cache, but remembered it was recently evicted from T1.
    # Adaptation: Increase p (target size of T1).
    if key in meta_lists['B1']:
        # Adaptation delta
        b1_size = len(lru_B1)
        b2_size = len(lru_B2)
        delta = 1 if b1_size >= b2_size else b2_size / b1_size
        p = min(c, p + delta)
        
        # Move from B1 to T2 (It has been accessed twice now)
        meta_lists['B1'].remove(key)
        if key in lru_B1: del lru_B1[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 2: x in B2 (Ghost Hit on Frequency)
    # We missed in cache, but remembered it was recently evicted from T2.
    # Adaptation: Decrease p (target size of T1, favors T2).
    elif key in meta_lists['B2']:
        # Adaptation delta
        b1_size = len(lru_B1)
        b2_size = len(lru_B2)
        delta = 1 if b2_size >= b1_size else b1_size / b2_size
        p = max(0, p - delta)
        
        # Move from B2 to T2
        meta_lists['B2'].remove(key)
        if key in lru_B2: del lru_B2[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 3: x is totally new (not in T1, T2, B1, B2)
    else:
        # Check Ghost List sizes (L1 = T1+B1, L2 = T2+B2)
        # ARC maintains |L1| + |L2| <= 2*c
        
        # If |T1| + |B1| == c:
        if len(lru_T1) + len(lru_B1) == c:
            if len(lru_T1) < c:
                # Discard LRU of B1
                discard = pop_lru(lru_B1)
                if discard in meta_lists['B1']: meta_lists['B1'].remove(discard)
            else:
                # Discard LRU of T1 (handled by evict usually, but just in case of ghost cleaning)
                # In standard ARC, `replace` is called before this.
                # Here, we clean B1 to make room if needed conceptually.
                discard = pop_lru(lru_B1)
                if discard and discard in meta_lists['B1']: meta_lists['B1'].remove(discard)

        # If |T1| + |B1| < c and |T1| + |B1| + |T2| + |B2| >= c:
        elif (len(lru_T1) + len(lru_B1) < c) and ((len(lru_T1) + len(lru_B1) + len(lru_T2) + len(lru_B2)) >= 2 * c):
            if (len(lru_T1) + len(lru_B1) + len(lru_T2) + len(lru_B2)) == 2 * c:
                # Discard LRU of B2
                discard = pop_lru(lru_B2)
                if discard in meta_lists['B2']: meta_lists['B2'].remove(discard)
        
        # Put x in T1 (MRU)
        meta_lists['T1'].add(key)
        access_lru(lru_T1, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    The `evict` function selected a victim. Now we must move that victim
    to the appropriate Ghost List (B1 or B2).
    '''
    global lru_T1, lru_T2, lru_B1, lru_B2, meta_lists
    
    victim_key = evicted_obj.key
    
    # If victim was in T1, move to B1
    if victim_key in meta_lists['T1']:
        meta_lists['T1'].remove(victim_key)
        if victim_key in lru_T1: del lru_T1[victim_key]
        
        meta_lists['B1'].add(victim_key)
        access_lru(lru_B1, victim_key)
        
    # If victim was in T2, move to B2
    elif victim_key in meta_lists['T2']:
        meta_lists['T2'].remove(victim_key)
        if victim_key in lru_T2: del lru_T2[victim_key]
        
        meta_lists['B2'].add(victim_key)
        access_lru(lru_B2, victim_key)
        
    # Sanity check: Ensure ghost lists don't grow infinitely.
    # In strict ARC, |T1|+|B1| <= c and |T2|+|B2| <= 2c, but simpler bound is often enough.
    c = cache_snapshot.capacity
    if len(lru_B1) > c:
        k = pop_lru(lru_B1)
        if k in meta_lists['B1']: meta_lists['B1'].remove(k)
    if len(lru_B2) > c:
        k = pop_lru(lru_B2)
        if k in meta_lists['B2']: meta_lists['B2'].remove(k)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqygzt98q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuiroy1lt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjnfobu68.pickle

Iteration 78: New subsample score 0.141209 is not better than old score 0.141209, skipping
Iteration 79: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt17ebdnz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpje_ntlm3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1cn5bzvi.pickle

Iteration 79: Proposed new text for program: import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3k1dmse8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb9z9fe_o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuz0z2fc5.pickle

Iteration 79: New subsample score 1.1290520000000002 is better than old score 1.1277400000000002. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpln13k5xz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_am42gl3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1pn603_w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgzt42137.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0x3knwd2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_w87ufe3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpew9r17yq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8r7t6mc8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn5gbivnx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppffylx90.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwvw9oa7e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5__eck8i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwu5yc61o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmtvy4hrj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2x42wyhx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0t40cnyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpggwtzv9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpldgn1m3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzh9veluq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy1cxu956.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8_npo1qb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeljgv9dd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxejhyy7z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyz_stutr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0s8z0uyn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt0a3u8j9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpddbvkbxm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkax2sdbf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5pr2o8gh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp310olukx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi6pocb45.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpptojo6gp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_c29pvk3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqv9lcmzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqd4g9moi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2qzolhyz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdbpzlv2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn0z20uw1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmgc3qqiw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe62wkiix.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpumftd54q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6djvhguv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpasn_7dec.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_py6izw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr5i1m1yb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwgt33wkx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5bnnx2p4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgr5jbad1.pickle

Iteration 79: New program is on the linear pareto front
Iteration 79: Full valset score for new program: 0.2576176875000001
Iteration 79: Full train_val score for new program: 0.2576176875000001
Iteration 79: Individual valset scores for new program: [0.510278, 0.484707, 0.493364, 0.44314, 0.503985, 0.493313, 0.272727, 0.478569, 0.540937, 0.531017, 0.091667, 0.366785, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023615, 0.022641, 0.272227, 0.394297, 0.026164, 0.058672, 0.058672, 0.332169, 0.340726, 0.723932, 0.893719, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.640392, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.316667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 79: New valset pareto front scores: [0.510278, 0.486516, 0.493364, 0.44314, 0.506177, 0.493313, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.641937, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.366667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 79: Full valset pareto front score: 0.2640278333333333
Iteration 79: Updated valset pareto front programs: [{22}, {4}, {22}, {22}, {4}, {22}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {3, 5, 6, 7, 12, 13, 20, 22}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22}, {1, 22}, {15}, {4}, {6}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21, 22}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22}, {22}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22}, {3, 5, 6, 7, 20}, {22}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21, 22}]
Iteration 79: Best valset aggregate score so far: 0.2576176875000001
Iteration 79: Best program as per aggregate score on train_val: 22
Iteration 79: Best program as per aggregate score on valset: 22
Iteration 79: Best score on valset: 0.2576176875000001
Iteration 79: Best score on train_val: 0.2576176875000001
Iteration 79: Linear pareto front program index: 22
Iteration 79: New program candidate index: 22
Iteration 80: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc4ncbi0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppy9q80m8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsb9vi7yt.pickle

Iteration 80: Proposed new text for program: import math

# Metadata Globals
# Tracks the computed GDSF priority for active cache items.
m_priority = dict()

# Tracks the raw frequency count. 
# We separate this because Priority changes with L, but Frequency accumulates.
m_frequency = dict()

# Ghost Registry: Stores frequencies of recently evicted items to handle cyclic patterns/thrashing.
m_ghost_freq = dict()

# The "Inflation Value" (L) for GDSF.
# This represents the priority of the most recently evicted item.
m_L = 0.0

# Configuration
# Max ghost entries to prevent memory leaks in metadata
MAX_GHOST_ENTRIES = 5000 

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the item with the lowest Priority score.
    Updates the global aging factor L to the priority of the evicted item.
    '''
    global m_priority, m_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority.
    # While iterating is O(N), for typical simulation cache sizes this is efficient enough.
    # (A production system would use a Min-Heap).
    
    victim_key = None
    min_priority = float('inf')
    
    # Tie-breaking: 
    # If priorities are equal, the order of iteration (insertion order in modern Python) 
    # usually acts as a secondary FIFO tie-breaker, which is acceptable.
    for key in candidate_keys:
        p = m_priority.get(key, 0.0)
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # GDSF Critical Step:
    # Update L to the priority of the item leaving the cache.
    # This "ages" all remaining items relative to new insertions.
    if victim_key is not None:
        m_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Increment Frequency.
    2. Recalculate Priority using CURRENT L and new Frequency.
       This restores the item's Recency and boosts its Frequency factor.
    '''
    global m_priority, m_frequency, m_L
    
    key = obj.key
    
    # 1. Update Frequency
    current_freq = m_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_frequency[key] = new_freq
    
    # 2. Update Priority
    # Priority = L + (Frequency / Size)
    # Note: We use the CURRENT global L. This brings the item to the "front" of time.
    # Size penalty: Large items need much higher frequency to stay in cache.
    size = obj.size if obj.size > 0 else 1
    m_priority[key] = m_L + (new_freq / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Check Ghost Registry for past frequency (warm start).
    2. Calculate initial Priority based on current L.
    '''
    global m_priority, m_frequency, m_ghost_freq, m_L
    
    key = obj.key
    
    # 1. Frequency Restoration (Ghost Logic)
    if key in m_ghost_freq:
        # It was here recently, restore its frequency + 1 for the current access
        freq = m_ghost_freq[key] + 1
        # Remove from ghost since it's now in main cache
        del m_ghost_freq[key]
    else:
        # Cold start
        freq = 1
        
    m_frequency[key] = freq
    
    # 2. Calculate Priority
    # New items start "fresh" at the current L water-mark.
    size = obj.size if obj.size > 0 else 1
    m_priority[key] = m_L + (freq / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    1. Save frequency to Ghost Registry.
    2. Clean up active metadata.
    '''
    global m_priority, m_frequency, m_ghost_freq
    
    key = evicted_obj.key
    
    # 1. Move Frequency to Ghost
    if key in m_frequency:
        m_ghost_freq[key] = m_frequency[key]
        del m_frequency[key]
        
    # 2. Remove Priority
    if key in m_priority:
        del m_priority[key]
        
    # 3. Maintain Ghost Size
    # Prevent metadata explosion. Since Python 3.7+, dicts preserve insertion order.
    # We remove the oldest inserted ghost item if we exceed capacity.
    if len(m_ghost_freq) > MAX_GHOST_ENTRIES:
        # next(iter(dict)) gets the first key (oldest inserted) efficiently
        oldest_ghost_key = next(iter(m_ghost_freq))
        del m_ghost_freq[oldest_ghost_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvb8a90wt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp08d7plct.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8hf4pn3t.pickle

Iteration 80: New subsample score 0.901246 is not better than old score 0.943473, skipping
Iteration 81: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe9j5ufkn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg_kvxwnb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp450c9erp.pickle

Iteration 81: Proposed new text for program: import collections

# Metadata Globals

# 1. Main Cache Lists (simulated via OrderedDict/dict logic)
# Tracks items that have been seen only once recently (Probationary/L1)
m_probationary = set()
# Tracks items that have been seen at least twice (Protected/L2)
m_protected = set()

# 2. Recency Tracking
# Map: Key -> Last Access Time. Used to determine LRU within sets.
m_last_access = dict()

# 3. Ghost Lists (History of evicted keys)
# Tracks keys recently evicted from Probationary (B1)
m_ghost_probationary = collections.deque(maxlen=10000)
m_ghost_probationary_set = set()
# Tracks keys recently evicted from Protected (B2)
m_ghost_protected = collections.deque(maxlen=10000)
m_ghost_protected_set = set()

# 4. Adaptive Parameter
# Target size for the Protected segment (p).
# 0 <= p <= Capacity. 
# If p is high, we act like LFU (prefer keeping freq items).
# If p is low, we act like LRU (prefer keeping recent items).
m_target_protected_size = 0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Segmented LRU.
    
    We decide which segment (Probationary or Protected) to evict from based on 
    the current size of the Protected segment relative to the target `m_target_protected_size`.
    '''
    global m_probationary, m_protected, m_last_access, m_target_protected_size
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Calculate current sizes based on keys actually in the cache snapshot
    # (We filter our metadata sets to ensure consistency with the physical cache)
    active_probationary = [k for k in m_probationary if k in cache_snapshot.cache]
    active_protected = [k for k in m_protected if k in cache_snapshot.cache]
    
    victim_key = None
    
    # Decision Logic:
    # If the Protected set is exceeding its target size, we must prune it to make room.
    # Otherwise, we default to pruning the Probationary set (Scan resistance).
    # Logic derived from ARC (Adaptive Replacement Cache) principles.
    
    len_p = len(active_protected)
    len_b1 = len(m_ghost_probationary_set)
    len_b2 = len(m_ghost_protected_set)
    
    # We evict from Protected if:
    # 1. It is strictly larger than its target.
    # 2. OR (Optimization) It contains data, Probationary is empty, and we need space.
    evict_from_protected = False
    
    if len_p > m_target_protected_size:
        evict_from_protected = True
    elif not active_probationary:
        # If probation is empty, we have no choice but to drop a protected item
        evict_from_protected = True
    
    # Perform Eviction
    if evict_from_protected and active_protected:
        # LRU eviction from Protected Segment
        victim_key = min(active_protected, key=lambda k: m_last_access.get(k, 0))
    elif active_probationary:
        # LRU eviction from Probationary Segment
        victim_key = min(active_probationary, key=lambda k: m_last_access.get(k, 0))
    else:
        # Fallback (should rarely reach here unless metadata is out of sync)
        victim_key = min(candidate_keys, key=lambda k: m_last_access.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: 
    1. Update Recency.
    2. If item was in Probationary, promote to Protected.
    '''
    global m_probationary, m_protected, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_last_access[key] = current_time
    
    # Promotion: If it hits, it has proven its worth. Move to Protected.
    if key in m_probationary:
        m_probationary.remove(key)
        m_protected.add(key)
    elif key not in m_protected:
        # Edge case: It's in cache but not in our sets (sync issue), treat as protected
        m_protected.add(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: 
    1. Check Ghost hits to adapt `m_target_protected_size`.
    2. Add new item to Probationary list.
    '''
    global m_probationary, m_protected, m_last_access
    global m_ghost_probationary_set, m_ghost_protected_set
    global m_ghost_probationary, m_ghost_protected
    global m_target_protected_size
    
    key = obj.key
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: In object count logic, this is count, but here it's bytes.
    # Approximation for adaptation logic: we treat count as capacity proxy for the target ratio.
    # Ideally we'd use 'capacity' directly, but since `m_target_protected_size` is a counter, 
    # we scale it relative to current number of items.
    current_count_capacity = len(cache_snapshot.cache) 
    
    m_last_access[key] = current_time
    
    # --- Adaptive Tuning (ARC Logic) ---
    
    # Case A: Ghost Hit on Probationary History (B1)
    # Means we evicted a 'scan' item too early. We should have had a smaller Protected area
    # and a larger Probationary area.
    if key in m_ghost_probationary_set:
        delta = 1
        b1_len = len(m_ghost_probationary_set)
        b2_len = len(m_ghost_protected_set)
        
        if b1_len < b2_len:
            delta = b2_len // b1_len
            
        # Increasing target for Protected? No, hitting B1 means we needed more room in L1 (Probationary).
        # Wait, ARC logic: Hitting B1 means we just missed it in L1. 
        # Actually, hitting B1 implies the probation list was too small, so we should shrink Protected
        # to grow Probation. But standard ARC increases P here?
        # Correction: If we hit in B1, it means the item was recently evicted from L1. 
        # If we had a larger L1 (and thus smaller L2/Protected), it might have stayed.
        # So we actually want to favor Recency (L1) -> Decrease P?
        # Standard ARC: If x in B1, p = min(c, p + max(1, |B2|/|B1|)). 
        # Increasing P essentially reserves more space for the "Protected" items, 
        # but in this implementation context, let's stick to the intuition:
        # If we see ghosts of one-timers, we might want to let them live longer -> increase L1 -> Decrease Target P.
        # HOWEVER, the standard ARC formula increases P on B1 hit. Why? 
        # Because B1 represents "recent" misses. If we miss recent things, we need to cache more recent things.
        # But `m_target_protected_size` is the size of L2.
        # Let's use a simplified heuristic:
        # If hit in Ghost Probationary -> It was a recent item we tossed. Bias toward Recency (Probationary).
        # Decrease Target Protected Size.
        m_target_protected_size = max(0, m_target_protected_size - 1)
        
        # Clean up ghost
        m_ghost_probationary_set.remove(key)
        # Remove from deque is O(N), expensive, so we just leave it in deque 
        # and rely on set for lookup. Lazy deletion happens in _cleanup.

    # Case B: Ghost Hit on Protected History (B2)
    # Means we evicted a 'hot' item. We need a larger Protected area.
    elif key in m_ghost_protected_set:
        m_target_protected_size = min(current_count_capacity, m_target_protected_size + 1)
        
        # Clean up ghost
        m_ghost_protected_set.remove(key)

    # --- Placement ---
    
    # New items always start in Probationary (L1)
    # Exception: If it was in Ghost Protected, it was a "hot" item that came back.
    # We restore it directly to Protected to protect it from the scan filter.
    if key in m_ghost_protected_set: # (Check again or use flag if logic separated)
        # Logic handled above removes it from set, so we can't check set again easily 
        # without a flag. Let's simplify: 
        # New items go to Probationary. If it gets hit again, it moves to Protected.
        m_probationary.add(key)
    else:
        m_probationary.add(key)

    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move the victim key to the appropriate Ghost list.
    '''
    global m_probationary, m_protected, m_last_access
    global m_ghost_probationary, m_ghost_probationary_set
    global m_ghost_protected, m_ghost_protected_set
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    
    # Determine where it came from
    if key in m_protected:
        m_protected.remove(key)
        # Add to Ghost Protected (B2)
        m_ghost_protected.append(key)
        m_ghost_protected_set.add(key)
    elif key in m_probationary:
        m_probationary.remove(key)
        # Add to Ghost Probationary (B1)
        m_ghost_probationary.append(key)
        m_ghost_probationary_set.add(key)
    else:
        # Fallback: assume probationary
        m_ghost_probationary.append(key)
        m_ghost_probationary_set.add(key)

    # Clean up main memory dict
    if key in m_last_access:
        del m_last_access[key]

def _cleanup_metadata():
    '''
    Keep ghost lists bounded to prevent memory leaks.
    '''
    global m_ghost_probationary, m_ghost_probationary_set
    global m_ghost_protected, m_ghost_protected_set
    
    # Limit size of ghost lists (e.g., to 2000 items each)
    limit = 2000
    
    while len(m_ghost_probationary) > limit:
        k = m_ghost_probationary.popleft()
        if k in m_ghost_probationary_set:
            m_ghost_probationary_set.remove(k)
            
    while len(m_ghost_protected) > limit:
        k = m_ghost_protected.popleft()
        if k in m_ghost_protected_set:
            m_ghost_protected_set.remove(k)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps7xmbme6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfjsz03pf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp32i62ce1.pickle

Iteration 81: New subsample score 0.543638 is not better than old score 0.5540349999999999, skipping
Iteration 82: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgd5q1mr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsd1sgncj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv3fv9pvx.pickle

Iteration 82: Proposed new text for program: import math

# Metadata Logic: Adaptive Replacement Cache (ARC) Strategy
# ARC maintains two lists of active pages:
#   T1: Pages accessed only once recently (Recency focus)
#   T2: Pages accessed at least twice recently (Frequency focus)
# And two lists of history (Ghost Caches):
#   B1: History of pages evicted from T1
#   B2: History of pages evicted from T2

# Global State
m_T1 = set()    # Active cache: Recently seen once
m_T2 = set()    # Active cache: Frequently seen
m_B1 = set()    # Ghost cache: Evicted from T1
m_B2 = set()    # Ghost cache: Evicted from T2

# Tracks insertion order for LRU behavior within the sets
# We use Python dicts as ordered sets (Python 3.7+ preserves insertion order)
m_LRU_T1 = dict() 
m_LRU_T2 = dict()
m_LRU_B1 = dict()
m_LRU_B2 = dict()

# Adaptation Parameter 'p'
# 0 <= p <= capacity
# Target size for T1 is p. Target size for T2 is (capacity - p).
m_p = 0

def _replace(cache_snapshot, p):
    """
    Subroutine to decide which page to evict based on the target size 'p'.
    """
    global m_T1, m_T2, m_B1, m_B2, m_LRU_T1, m_LRU_T2, m_LRU_B1, m_LRU_B2
    
    # Check if T1 is non-empty and exceeds its target or implicit condition
    t1_size = len(m_T1)
    
    # We evict from T1 if it has items and is over the target 'p',
    # OR if T1 is just larger than p and we need space.
    # Specifically, if len(T1) > p, we evict the LRU of T1 to B1.
    if m_T1 and (t1_size > p or (m_B2 and t1_size == p)):
        # Evict LRU from T1
        victim_key = next(iter(m_LRU_T1))
        del m_LRU_T1[victim_key]
        m_T1.remove(victim_key)
        
        # Move to B1 (Ghost Recency)
        m_B1.add(victim_key)
        m_LRU_B1[victim_key] = True
        return victim_key
    else:
        # Evict LRU from T2 to B2
        if m_LRU_T2:
            victim_key = next(iter(m_LRU_T2))
            del m_LRU_T2[victim_key]
            m_T2.remove(victim_key)
            
            # Move to B2 (Ghost Frequency)
            m_B2.add(victim_key)
            m_LRU_B2[victim_key] = True
            return victim_key
            
    # Fallback (should not happen if cache is populated)
    return None

def evict(cache_snapshot, obj):
    """
    Determines the victim key to evict.
    """
    global m_p
    # Note: The 'obj' passed here is the NEW object trying to enter, 
    # but we don't need its details to decide WHO to kill, only the state of ARC.
    
    # In a standard cache interaction, evict is called when cache is full.
    # We delegate the choice to the _replace logic which respects the parameter 'p'.
    victim_key = _replace(cache_snapshot, m_p)
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    Handles a cache hit.
    Effect: Promotes items to MRU positions and moves items from T1 to T2.
    """
    global m_T1, m_T2, m_LRU_T1, m_LRU_T2, m_p
    
    key = obj.key
    
    # If it is in T1 (Recency List), move it to T2 (Frequency List)
    if key in m_T1:
        m_T1.remove(key)
        if key in m_LRU_T1: del m_LRU_T1[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True # Add to MRU end
    
    # If it is in T2, update its LRU position (move to MRU end)
    elif key in m_T2:
        if key in m_LRU_T2: del m_LRU_T2[key]
        m_LRU_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    """
    Handles inserting a new object into the cache.
    This logic includes the adaptive tuning of 'p' when hitting ghost caches.
    """
    global m_T1, m_T2, m_B1, m_B2, m_LRU_T1, m_LRU_T2, m_LRU_B1, m_LRU_B2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually defines cap by count, here we approximate with slots or size.
    # Assuming the simulation capacity is count-based roughly, or we normalize. 
    # For strict object counts, we can use len(cache). 
    # However, since we track T1/T2 independently of the physical cache snapshot sometimes,
    # we treat the 'cache' size as the authority.
    
    # Case 1: Key is in Ghost Cache B1 (Was in T1 recently, but evicted)
    # Interpretation: We should have made T1 larger.
    if key in m_B1:
        # Adaptation: Increase p
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) // len(m_B1)
        m_p = min(capacity, m_p + delta)
        
        # Move from B1 history to T2 (It has now been seen twice: once T1->B1, now B1->T2)
        m_B1.remove(key)
        if key in m_LRU_B1: del m_LRU_B1[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True
        return

    # Case 2: Key is in Ghost Cache B2 (Was in T2 recently, but evicted)
    # Interpretation: We should have made T2 larger (T1 smaller).
    if key in m_B2:
        # Adaptation: Decrease p
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) // len(m_B2)
        m_p = max(0, m_p - delta)
        
        # Move from B2 history to T2 (It is a frequent item returning)
        m_B2.remove(key)
        if key in m_LRU_B2: del m_LRU_B2[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True
        return

    # Case 3: Completely new item (not in T1, T2, B1, B2)
    # Insert into T1 (Recency List)
    
    # Cleanup history if it gets too large (usually 2x cache size is standard ARC implementation)
    total_l1 = len(m_T1) + len(m_B1)
    total_l2 = len(m_T2) + len(m_B2)
    
    if total_l1 >= capacity:
        if len(m_B1) > 0:
            del_key = next(iter(m_LRU_B1))
            del m_LRU_B1[del_key]
            m_B1.remove(del_key)
    
    if (total_l1 + total_l2) >= (2 * capacity):
         if len(m_B2) > 0:
            del_key = next(iter(m_LRU_B2))
            del m_LRU_B2[del_key]
            m_B2.remove(del_key)

    # Finally add the new item to T1
    m_T1.add(key)
    m_LRU_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Post-eviction cleanup.
    """
    # The actual logic of moving items to B1/B2 happens inside `evict` (via _replace)
    # or implicitly during insert/updates.
    # However, we must ensure consistency between the Physical Cache and our Metadata.
    
    # In a standard simulation flow:
    # 1. evict() is called -> we chose a victim, updated our T/B lists, returned key.
    # 2. Simulator removes that key from cache.
    # 3. update_after_evict() is called.
    
    # Since we did the metadata move in `evict`, we don't strictly need to do anything here,
    # unless the simulator evicted something DIFFERENT than what we suggested (unlikely).
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp687uan15.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3kllvil3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw3clr_gw.pickle

Iteration 82: New subsample score 0.518031 is better than old score 0.50656. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnh0qk_bp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0i4rashx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt1goxtzp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw6ym79l2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3x9mz29e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1ot9cqao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpppwj11w_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9irmdnbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3wtfj5go.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnh9ce2vg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx1wdwr74.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4of257cv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9afli2k6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl3rd8v71.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpocvho90_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_7ussxct.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpce32c6ar.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4qkf80dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp391vveg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyylsbzdt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyrzyzn8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptypelw6t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppc48csgc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4ovb7_78.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppb56nskc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1gqcyo_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcv6g_bm8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpm9np3ojn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp29zgpyfa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr9osfr76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph9ayt6ap.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp66hulwsk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6wvgg19g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvr3dcg0b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqgcdyb1k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4c_29rso.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsileyyaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd8p58knd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpv4n8w1ib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7r9194xe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8abkv5lc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmxxx6xht.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5s2sz7ob.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjpmjo2r6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk73387wn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2w5bmlbz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph33h84bz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpeu8ihlzf.pickle

Iteration 82: Full valset score for new program: 0.2434054375
Iteration 82: Full train_val score for new program: 0.2434054375
Iteration 82: Individual valset scores for new program: [0.503067, 0.480446, 0.487849, 0.431404, 0.500669, 0.489046, 0.272727, 0.497837, 0.538077, 0.531017, 0.075, 0.368117, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.352999, 0.026164, 0.058672, 0.058672, 0.269805, 0.355847, 0.853029, 0.891665, 0.039832, 0.038636, 0.045558, 0.007721, 0.020672, 0.753233, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.120599, 0.077581, 0.151707, 0.052632, 0.25, 0.067317, 0.111641, 0.466258, 0.081699]
Iteration 82: New valset pareto front scores: [0.510278, 0.486516, 0.493364, 0.44314, 0.506177, 0.493313, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.641937, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.366667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 82: Full valset pareto front score: 0.2640278333333333
Iteration 82: Updated valset pareto front programs: [{22}, {4}, {22}, {22}, {4}, {22}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {3, 5, 6, 7, 12, 13, 20, 22}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23}, {1, 22}, {15}, {4}, {6}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23}, {22}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23}, {3, 5, 6, 7, 20}, {22}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21, 22, 23}]
Iteration 82: Best valset aggregate score so far: 0.2576176875000001
Iteration 82: Best program as per aggregate score on train_val: 22
Iteration 82: Best program as per aggregate score on valset: 22
Iteration 82: Best score on valset: 0.2576176875000001
Iteration 82: Best score on train_val: 0.2576176875000001
Iteration 82: Linear pareto front program index: 22
Iteration 82: New program candidate index: 23
Iteration 83: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyag26vev.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuj60171r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfr_dzs87.pickle

Iteration 83: Proposed new text for program: import math

# Metadata Globals
# Tracks the last access time (Recency) for all keys
m_last_access = dict()
# Tracks the frequency of access for all keys
m_frequency = dict()
# Tracks which segment a key belongs to. 
# True = Protected (High Frequency/Utility), False = Probation (New/Low Utility)
m_protected = dict()

# Algorithm Parameters
# Maximum percentage of cache count dedicated to the "Protected" segment.
# 80% ensures we keep popular items but leave 20% buffer for new items to prove themselves.
PROTECTED_RATIO = 0.8
# Tunable parameter for size penalty. Higher = stronger bias against large items.
SIZE_PENALTY_FACTOR = 0.5

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Size Awareness.
    
    1. We divide the cache into two logical segments: "Probation" and "Protected".
    2. New items start in Probation.
    3. Items are promoted to Protected if accessed again while in Probation.
    4. We prefer evicting from Probation. 
    5. If Probation is empty (unlikely) or we need to balance, we demote from Protected.
    
    Scoring Metric for Eviction (within Probation):
    Score = Recency_Age * (Size ^ SIZE_PENALTY_FACTOR)
    Higher Score = Older and/or Larger = Better Victim.
    '''
    global m_last_access, m_protected, m_frequency
    
    current_time = cache_snapshot.access_count
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
    
    # 1. Identify candidates in Probation vs Protected
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        if m_protected.get(k, False):
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    # 2. Dynamic Balancing: If Protected segment is too big, force a demotion first.
    # This prevents the cache from becoming static with old "popular" items.
    capacity_count = len(candidate_keys)
    target_protected_size = int(capacity_count * PROTECTED_RATIO)
    
    # If we have too many protected items, we pick the LRU from Protected to evict/demote.
    # However, standard SLRU just demotes. Here, since we MUST evict to make space,
    # we treat the LRU of Protected as a valid eviction candidate only if Probation is empty.
    # Otherwise, we evict from Probation.
    
    pool_to_evict_from = probation_keys
    
    if not pool_to_evict_from:
        # Fallback: If probation is empty, we must kill a protected item.
        pool_to_evict_from = protected_keys
        
    # 3. Select Victim from the chosen pool
    best_victim = None
    max_eviction_score = -1.0
    
    for k in pool_to_evict_from:
        last_acc = m_last_access.get(k, 0)
        age = (current_time - last_acc)
        
        # Retrieve object size
        item_size = 1
        cached_obj = cache_snapshot.cache.get(k)
        if cached_obj:
            item_size = cached_obj.size
            
        # Calculate Eviction Score
        # We want to evict items that are OLD (high age) and LARGE (high size).
        # Log of size helps dampen the impact of extremely large files, preventing
        # the algorithm from becoming purely "Smallest Object First".
        size_metric = math.log(item_size + 1) # +1 avoids log(1)=0 if size=1
        
        # Score = Age * (Size factor)
        # Standard LRU is just 'Age'. 
        # By multiplying by size metric, we say "A slightly younger large object 
        # is a better victim than a very old tiny object."
        score = age * (size_metric ** SIZE_PENALTY_FACTOR)
        
        if score > max_eviction_score:
            max_eviction_score = score
            best_victim = k
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Promote to Protected segment if not already there.
    '''
    global m_last_access, m_frequency, m_protected
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    # Promotion Logic (SLRU)
    # If it was in cache and got a hit, it has proven utility.
    m_protected[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Place in Probation segment.
    '''
    global m_last_access, m_frequency, m_protected
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = 1 # First time seen
    
    # New items always start in Probation (False)
    # This filters out "scan" traffic (items seen once and never again).
    m_protected[key] = False

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata for the evicted object to free memory.
    '''
    global m_last_access, m_frequency, m_protected
    
    if evicted_obj:
        key = evicted_obj.key
        if key in m_last_access:
            del m_last_access[key]
        if key in m_frequency:
            del m_frequency[key]
        if key in m_protected:
            del m_protected[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8bhv286h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplollzymj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptbr07ry5.pickle

Iteration 83: New subsample score 0.741714 is not better than old score 0.893365, skipping
Iteration 84: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4hnnykrg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprri71vwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu3ho3lr2.pickle

Iteration 84: Proposed new text for program: import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'last_access': int, 
#   'segment': int (0 = Probation/Amateur, 1 = Protected/Elite)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items to detect false negatives (premature eviction).
m_ghost_registry = dict()

# Configuration
# The portion of the cache capacity reserved for "Protected" items.
# 0.8 means up to 80% of the cache bytes can be held by Protected items.
# If Protected grows larger, we force eviction from it to make room for new blood.
PROTECTED_CAPACITY_RATIO = 0.8
# Max items in ghost registry relative to current object count
GHOST_RATIO = 1.0

def evict(cache_snapshot, obj):
    '''
    Implements Segmented LRU Eviction.
    
    Logic:
    1. The cache is conceptually divided into two segments: Probation (0) and Protected (1).
    2. We calculate the total size currently occupied by Protected items.
    3. IF Protected segment is overflowing (>80% capacity):
       - We MUST evict from the Protected segment (LRU policy within that segment) to demote.
    4. ELSE (Normal case):
       - We evict from the Probation segment (LRU policy within that segment).
       - If Probation is empty, fallback to Protected.
    '''
    global m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Analyze current state of segments
    protected_size = 0
    protected_candidates = []
    probation_candidates = []
    
    # Single pass to categorize candidates and calculate sizes
    for key in candidate_keys:
        # Retrieve metadata, default to Probation if missing (fail-safe)
        meta = m_access_info.get(key, {'last_access': 0, 'segment': 0})
        obj_in_cache = cache_snapshot.cache[key]
        
        if meta['segment'] == 1:
            protected_size += obj_in_cache.size
            protected_candidates.append((key, meta['last_access']))
        else:
            probation_candidates.append((key, meta['last_access']))

    # 2. Determine which segment to victimize
    target_protected_capacity = cache_snapshot.capacity * PROTECTED_CAPACITY_RATIO
    
    victim_key = None
    
    # If Protected segment is too big, we prune it (effectively demoting/evicting from Elite)
    if protected_size > target_protected_capacity and protected_candidates:
        # Find LRU in Protected
        # Min by last_access
        victim_key = min(protected_candidates, key=lambda x: x[1])[0]
    else:
        # Otherwise, we prefer to evict from Probation
        if probation_candidates:
            # Find LRU in Probation
            victim_key = min(probation_candidates, key=lambda x: x[1])[0]
        elif protected_candidates:
            # Fallback: Cache is 100% protected items, evict LRU from there
            victim_key = min(protected_candidates, key=lambda x: x[1])[0]
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (LRU logic).
    2. Promotion: If item was in Probation, move to Protected.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        # Promotion: A hit proves utility. Welcome to the Elite segment.
        meta['segment'] = 1 
        m_access_info[obj.key] = meta
    else:
        # Reconstruction if metadata missing
        m_access_info[obj.key] = {
            'last_access': current_time,
            'segment': 1 # Assume protected if it survived to get a hit
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost: Insert as "Protected" (Resurrected).
    3. Else: Insert as "Probation" (Newcomer).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # Determine Initial Segment
    # If it was in ghost, we made a mistake evicting it recently. 
    # Promote immediately to prevent "Thrashing".
    initial_segment = 1 if is_ghost else 0
    
    m_access_info[obj.key] = {
        'last_access': current_time,
        'segment': initial_segment
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost Registry (to catch false negatives).
    3. Prune Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    # We only care THAT it existed, not when (FIFO removal is fine).
    m_ghost_registry[evicted_obj.key] = True
    
    # 3. Maintain Ghost Registry Size
    # Keep ghost registry roughly size of cache to detect immediate loops.
    # Python 3.7+ dicts preserve insertion order, so this acts as a FIFO queue.
    max_ghost_count = max(len(cache_snapshot.cache), 10) # Minimum safety buffer
    
    if len(m_ghost_registry) > max_ghost_count:
        # Remove oldest inserted key (first key in dict)
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp42tmikp8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyrs_q6fr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpximwff54.pickle

Iteration 84: New subsample score 0.943547 is not better than old score 0.966487, skipping
Iteration 85: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi0tjtbf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyvcxon9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkq25lnkg.pickle

Iteration 85: Proposed new text for program: import collections

# --- Global Metadata ---

# Ideally, we would use a class, but we must stick to the global variable constraints of the interface.
# We will simulate the ARC (Adaptive Replacement Cache) lists using OrderedDicts for O(1) LRU behavior.

# T1: Active Recency List (Key -> None). Order: LRU to MRU.
m_T1 = collections.OrderedDict()

# T2: Active Frequency List (Key -> None). Order: LRU to MRU.
m_T2 = collections.OrderedDict()

# B1: Ghost Recency List (Key -> None). Keys evicted from T1. Order: LRU to MRU.
m_B1 = collections.OrderedDict()

# B2: Ghost Frequency List (Key -> None). Keys evicted from T2. Order: LRU to MRU.
m_B2 = collections.OrderedDict()

# p: Target size for T1 (The adaptation parameter). 
# Range: [0, capacity]. Starts at 0 (or capacity/2, but 0 is safe dynamic start).
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on the ARC (Adaptive Replacement Cache) logic.
    This function implements the 'REPLACE' logic of ARC.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Safety check
    if not cache_snapshot.cache:
        return None
        
    candidate_key = None
    
    # According to ARC "REPLACE" logic:
    # We evict from T1 if T1 is strictly larger than the target p.
    # OR, if T1 has items and (T1 is exactly p but the incoming item is in B2). 
    # (Note: The B2 check is handled implicitly by how we manage 'p' in update_after_insert/hit,
    # but strictly speaking, REPLACE evicts from T1 if len(T1) > p).
    
    t1_len = len(m_T1)
    
    # Logic:
    # If len(T1) >= 1 and ((incoming item is in B2 and len(T1) == p) OR (len(T1) > p)):
    # Then delete LRU of T1 and move to B1.
    # Else: delete LRU of T2 and move to B2.
    
    # Since 'evict' is called *before* we insert the new item (usually), 
    # we need to be careful. The standard framework asks us to return a key to evict.
    # We rely on m_p which was adjusted during the *miss* processing if we had access to the missed key 
    # in update_after_insert, but here we only see the snapshot.
    # However, strictly strictly:
    
    if t1_len > 0 and t1_len > m_p:
        # Evict LRU from T1
        candidate_key = next(iter(m_T1))
    else:
        # Evict LRU from T2
        # Fallback: if T2 is empty (should rarely happen if T1 <= p), try T1
        if m_T2:
            candidate_key = next(iter(m_T2))
        elif m_T1:
            candidate_key = next(iter(m_T1))
            
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    ARC Logic on Hit (Case 1):
    x is in T1 or T2. Move x to MRU of T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # If in T1, remove and move to T2 MRU
    if key in m_T1:
        del m_T1[key]
        m_T2[key] = None
    # If in T2, move to MRU
    elif key in m_T2:
        m_T2.move_to_end(key)
    else:
        # Edge case: Metadata out of sync with cache (e.g. restart), re-add to T2
        m_T2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    ARC Logic on Miss (Insert):
    Cases handled here:
    1. x in B1 (Ghost Recency): Adaptation (Increase p), Move from B1 to T2.
    2. x in B2 (Ghost Frequency): Adaptation (Decrease p), Move from B2 to T2.
    3. x not in anything: New item. Add to T1.
    
    Note: The actual eviction from Cache happens via the `evict` function, 
    but the Metadata lists (T1, T2, B1, B2) must be managed here to reflect the *intent*.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    c = cache_snapshot.capacity # Capacity C
    
    # --- Case 2: x in B1 (Miss in Cache, Hit in History L1) ---
    if key in m_B1:
        # Adaptation: Increase p
        # Delta = 1 if |B1| >= |B2|, else |B2| / |B1|
        len_b1 = len(m_B1)
        len_b2 = len(m_B2)
        delta = 1
        if len_b1 < len_b2:
            delta = len_b2 / len_b1
        
        m_p = min(c, m_p + delta)
        
        # Promote: Move from B1 to T2 (MRU)
        del m_B1[key]
        m_T2[key] = None
        
    # --- Case 3: x in B2 (Miss in Cache, Hit in History L2) ---
    elif key in m_B2:
        # Adaptation: Decrease p
        # Delta = 1 if |B2| >= |B1|, else |B1| / |B2|
        len_b1 = len(m_B1)
        len_b2 = len(m_B2)
        delta = 1
        if len_b2 < len_b1:
            delta = len_b1 / len_b2
            
        m_p = max(0, m_p - delta)
        
        # Promote: Move from B2 to T2 (MRU)
        del m_B2[key]
        m_T2[key] = None
        
    # --- Case 4: x not in (T1 u B1 u T2 u B2) ---
    else:
        # Completely new item.
        # Case A: L1 (T1+B1) has size c.
        # If we are just filling up, T1 just grows.
        # But logically for ARC, a new item goes to MRU of T1.
        m_T1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    The `evict` function selected a victim from Cache.
    Now we must update the Metadata lists (T1/T2 -> B1/B2) to reflect that eviction.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    victim_key = evicted_obj.key
    c = cache_snapshot.capacity
    
    # We need to ensure the victim is moved from T (Active) to B (Ghost).
    
    # If victim was in T1, move to B1
    if victim_key in m_T1:
        del m_T1[victim_key]
        m_B1[victim_key] = None
        # B1 Restriction: Maintain |T1| + |B1| <= C (roughly, or 2C in some papers, C is safer for strict memory)
        # Standard ARC allows directory size to be 2C. Let's limit B1 to Capacity to be safe.
        if len(m_B1) > c:
            m_B1.popitem(last=False) # Remove oldest
            
    # If victim was in T2, move to B2
    elif victim_key in m_T2:
        del m_T2[victim_key]
        m_B2[victim_key] = None
        # B2 Restriction: Maintain |T2| + |B2| <= 2C.
        if len(m_B2) > c:
             m_B2.popitem(last=False) # Remove oldest
             
    # Fallback cleanup (shouldn't happen if logic is tight)
    if victim_key in m_access_info_fallback:
         del m_access_info_fallback[victim_key]

# Fallback dictionary just in case we need to track something outside T1/T2/B1/B2
m_access_info_fallback = {}
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpistx33f8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0vu0vln6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpue68rv8h.pickle

Iteration 85: New subsample score 0.602834 is not better than old score 0.66854, skipping
Iteration 86: Selected program 15 score: 0.23890931249999994
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9mcf69f2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbmp4bzgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpldm8uo9l.pickle

Iteration 86: Proposed new text for program: import math

# --- Metadata Globals ---

# Tracks the number of times a key has been accessed.
# We maintain this even after eviction to recognize returning items.
m_frequency = dict()

# Tracks the timestamp of the last access.
m_last_access = dict()

# A global counter to trigger periodic frequency decay (Aging).
m_op_counter = 0

# --- Constants ---

# The ratio of the cache reserved for "Protected" (High Frequency) items.
# 0.8 means we try to keep 20% of the cache free for "Probation" (New) items.
PROTECTED_RATIO = 0.8

# How often (in total operations) to halve frequencies to simulate aging.
# This prevents historical heavy-hitters from polluting the cache forever.
DECAY_INTERVAL = 20000

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU)
    
    We categorize items into two logical pools:
    1. Probation (Freq <= 1): Candidates for immediate removal.
    2. Protected (Freq > 1): Valuable items we try to keep.
    
    Logic:
    - If the Protected pool is too big (> 80% of cache), evict the LRU from Protected.
    - Otherwise, evict the LRU from Probation to preserve Protected items.
    - If Probation is empty, fallback to evicting from Protected.
    '''
    global m_frequency, m_last_access, PROTECTED_RATIO
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into segments
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        freq = m_frequency.get(k, 1)
        if freq > 1:
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    # 2. Determine which segment to victimize
    total_items = len(candidate_keys)
    protected_count = len(protected_keys)
    
    target_pool = None
    
    # If Protected items are hogging the cache, prune them to let new items in.
    if protected_count > (total_items * PROTECTED_RATIO):
        target_pool = protected_keys
    else:
        # Otherwise, aggressively prune Probation items (Scan resistance).
        if probation_keys:
            target_pool = probation_keys
        else:
            # Fallback if no probation items exist
            target_pool = protected_keys
            
    # 3. Find LRU within the target pool
    victim_key = None
    min_access = float('inf')
    
    for k in target_pool:
        # Default to 0 if access time missing, though it shouldn't be
        acc_time = m_last_access.get(k, 0)
        if acc_time < min_access:
            min_access = acc_time
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update Recency and Frequency.
    Trigger aging if needed.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    # Increment frequency (saturating at a reasonable cap to prevent overflow issues)
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    _check_aging(current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. 
    If this key has history (was evicted previously), it retains that history.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    
    # If key is not in frequency dict, it's brand new -> Freq=1.
    # If it IS in frequency dict, it's a "Ghost" return -> Freq=Old_Freq + 1.
    # This automatically promotes returning items to "Protected" status.
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    _check_aging(current_time)
    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: We do NOTHING to the metadata.
    We deliberately keep m_frequency and m_last_access entries for evicted items.
    This serves as our "Ghost Cache".
    '''
    pass

def _check_aging(current_time):
    '''
    Periodically divide frequencies by 2.
    This ensures that items that were popular long ago eventually 
    drop back to "Probation" status and can be evicted.
    '''
    global m_frequency, m_op_counter, DECAY_INTERVAL
    
    m_op_counter += 1
    
    if m_op_counter >= DECAY_INTERVAL:
        m_op_counter = 0
        # Decay frequencies
        # We use a list to avoid "dictionary changed size during iteration"
        for k in list(m_frequency.keys()):
            val = m_frequency[k]
            if val > 1:
                m_frequency[k] = val // 2
            else:
                # Optional: if freq is 1 and we decay, we could remove it 
                # if it's not in cache, but cleanup handles that.
                pass

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion from Ghost entries.
    '''
    global m_frequency, m_last_access
    
    # Allow metadata to grow larger than cache to track history, 
    # but prune if it gets excessive (e.g., > 3x capacity).
    # Using a fixed limit based on typical trace constraints.
    MAX_META_SIZE = 50000 
    
    if len(m_frequency) > MAX_META_SIZE:
        # Pruning Strategy: Remove oldest items that are NOT in the cache.
        
        # 1. Identify candidates (keys in metadata but not in cache)
        # using a set for O(1) lookups
        cached_keys = set(cache_snapshot.cache.keys())
        
        # 2. Sort all metadata keys by last_access time (LRU)
        # This is O(N log N), but only runs rarely.
        all_keys_sorted = sorted(m_last_access.keys(), key=lambda k: m_last_access[k])
        
        deleted_count = 0
        target_delete = 5000 # Batch delete
        
        for k in all_keys_sorted:
            if k not in cached_keys:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
                deleted_count += 1
                if deleted_count >= target_delete:
                    break
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3l6fhb_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoeip229d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3ebab4cv.pickle

Iteration 86: New subsample score 0.31257100000000004 is not better than old score 0.33134399999999997, skipping
Iteration 87: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmvl3ehm3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_xu6bwz8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9cqmo0em.pickle

Iteration 87: Proposed new text for program: # Metadata dictionaries for ARC implementation
m_access = dict()    # Tracks logical access time (Recency)
m_t2_keys = set()    # Set of keys belonging to the "Frequent" (T2) list. 
                     # Keys in cache NOT in this set belong to the "Recent" (T1) list.
m_b1 = dict()        # Ghost list for T1 (Keys evicted from T1 - Recently evicted once-seen items)
m_b2 = dict()        # Ghost list for T2 (Keys evicted from T2 - Recently evicted frequent items)
m_p = 0.0            # Adaptive parameter: Target size of the T1 (Recent) list

def evict(cache_snapshot, obj):
    '''
    Implements Adaptive Replacement Cache (ARC) eviction strategy.
    
    ARC dynamically balances between Recency (T1) and Frequency (T2) by adjusting 
    a parameter 'p' based on hits in the ghost lists (B1 and B2).
    '''
    global m_access, m_t2_keys, m_b1, m_b2, m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # 1. Classify candidates into T1 (Recent) and T2 (Frequent)
    t1_candidates = []
    t2_candidates = []
    for k in candidate_keys:
        if k in m_t2_keys:
            t2_candidates.append(k)
        else:
            t1_candidates.append(k)

    # 2. Determine the adaptive target 'p' for this specific eviction decision.
    # We calculate what the NEW p would be if we committed the adaptation now,
    # because the incoming 'obj' tells us if our previous cache size mix was wrong.
    
    curr_p = m_p
    
    # If the incoming object is in Ghost List B1, we evicted it from T1 too early.
    # We need to increase T1's target size (p).
    if obj.key in m_b1:
        len_b1 = len(m_b1)
        len_b2 = len(m_b2)
        step = 1
        # Adaptation step size depends on relative sizes of ghost lists
        if len_b1 < len_b2:
            step = len_b2 / len_b1
        # Apply step, capped at cache size (count-based approximation)
        curr_p = min(len(candidate_keys), curr_p + step)
        
    # If the incoming object is in Ghost List B2, we evicted it from T2 too early.
    # We need to decrease T1's target size (p) to grow T2.
    elif obj.key in m_b2:
        len_b1 = len(m_b1)
        len_b2 = len(m_b2)
        step = 1
        if len_b2 < len_b1:
            step = len_b1 / len_b2
        curr_p = max(0, curr_p - step)

    # 3. Decision Logic: Evict from T1 or T2?
    # We evict from T1 if it is currently larger than our target 'p'.
    # Special Case: If T1 size exactly equals p, but we have a B2 hit, we prefer evicting T1
    # to allow T2 to grow.
    
    evict_from_t1 = False
    len_t1 = len(t1_candidates)
    
    if len_t1 > curr_p:
        evict_from_t1 = True
    elif (obj.key in m_b2) and len_t1 == int(curr_p):
        evict_from_t1 = True
    
    # 4. Fallback Handling
    # If we chose to evict T1 but it's empty, we must evict T2, and vice versa.
    if evict_from_t1 and not t1_candidates:
        evict_from_t1 = False
    elif not evict_from_t1 and not t2_candidates:
        evict_from_t1 = True
        
    # 5. Find LRU victim in the chosen list
    victim_pool = t1_candidates if evict_from_t1 else t2_candidates
    victim_key = None
    min_time = float('inf')
    
    for k in victim_pool:
        # Use logical access time to determine LRU within the specific list
        t = m_access.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a cache hit, update recency. 
    Any hit in ARC promotes (or keeps) the item in the T2 (Frequent) set.
    '''
    global m_access, m_t2_keys
    
    # Update recency
    m_access[obj.key] = cache_snapshot.access_count
    
    # Promote to T2
    m_t2_keys.add(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    On a cache miss (insert), we permanently adapt the parameter 'p' 
    and assign the new object to T1 (Recent) or T2 (Frequent) based on ghost history.
    '''
    global m_access, m_t2_keys, m_b1, m_b2, m_p
    
    m_access[obj.key] = cache_snapshot.access_count
    
    # Adaptation logic (Must mirror the logic in evict, but commit changes)
    len_b1 = len(m_b1)
    len_b2 = len(m_b2)
    current_c = len(cache_snapshot.cache) # Approximate capacity in count
    
    if obj.key in m_b1:
        # Ghost T1 Hit -> Increase p (Grow T1)
        step = 1
        if len_b1 < len_b2:
            step = len_b2 / len_b1
        m_p = min(current_c, m_p + step)
        
        # Item returns, so it becomes Frequent (T2)
        m_t2_keys.add(obj.key)
        del m_b1[obj.key]
        
    elif obj.key in m_b2:
        # Ghost T2 Hit -> Decrease p (Shrink T1 / Grow T2)
        step = 1
        if len_b2 < len_b1:
            step = len_b1 / len_b2
        m_p = max(0, m_p - step)
        
        # Item returns, so it becomes Frequent (T2)
        m_t2_keys.add(obj.key)
        del m_b2[obj.key]
        
    else:
        # Cold Miss -> Start in T1 (Recent)
        if obj.key in m_t2_keys:
            m_t2_keys.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    When an object is evicted, move its metadata to the corresponding ghost list (B1 or B2).
    '''
    global m_access, m_t2_keys, m_b1, m_b2
    
    ek = evicted_obj.key
    
    # Remove from active access metadata
    if ek in m_access:
        del m_access[ek]
    
    # Identify source list and move to Ghost
    if ek in m_t2_keys:
        # Was in T2, move to B2
        m_b2[ek] = True
        m_t2_keys.remove(ek)
    else:
        # Was in T1, move to B1
        m_b1[ek] = True
        
    # Maintain Ghost List size limits to prevent memory leaks.
    # A safe heuristic is max(cache size, constant)
    max_ghost_size = max(len(cache_snapshot.cache), 2000)
    
    if len(m_b1) > max_ghost_size:
        # Python 3.7+ dicts preserve insertion order; this removes the oldest inserted
        try:
            oldest = next(iter(m_b1))
            del m_b1[oldest]
        except StopIteration:
            pass
            
    if len(m_b2) > max_ghost_size:
        try:
            oldest = next(iter(m_b2))
            del m_b2[oldest]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmitxvv05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphxe73oyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6lre_ua7.pickle

Iteration 87: New subsample score 0.9997560000000001 is not better than old score 1.0992410000000001, skipping
Iteration 88: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp94jc2wup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp45jyqzjn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8w9sejor.pickle

Iteration 88: Proposed new text for program: import math

# Metadata dictionary to store access history and GDSF priority for objects.
# Key -> {
#   'freq': int, 
#   'priority': float,
#   'last_access': int
# }
m_access_info = dict()

# Ghost registry (Shadow Cache) to remember frequency of recently evicted items.
# Key -> { 'freq': int }
m_ghost_registry = dict()

# Global Aging Factor (L) for GDSF
# This acts as the "inflation" baseline. It rises as popular items set the bar higher.
m_L = 0.0

# Tunable Parameters
GHOST_RATIO = 1.0  # Size of ghost registry relative to cache

def get_gdsf_priority(freq, size, L):
    '''
    Calculates the GDSF priority.
    Formula: H = L + (Frequency / Size)
    
    - Higher Frequency -> Higher Priority (Keep)
    - Larger Size -> Lower Priority (Evict, to make room for more objects)
    - L represents the current "aging" baseline.
    '''
    # Safety for 0-size objects (though rare in traces)
    effective_size = size if size > 0 else 1
    
    # We use a weight multiplier to ensure frequency has a tangible impact 
    # against size in bytes. 1000 is an arbitrary scaling factor roughly 
    # equating 1 hit to 1KB cost.
    weight = 1000.0
    
    return L + (freq * weight / effective_size)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the Lowest Priority (Min H).
    Tie-breaker: Least Recently Used (LRU).
    '''
    global m_L, m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    best_victim_key = None
    min_priority = float('inf')
    
    # We iterate to find the minimum priority.
    # While O(N), this ensures strict adherence to the GDSF policy.
    for key in candidate_keys:
        if key not in m_access_info:
            # Fallback if metadata is missing
            return key
            
        meta = m_access_info[key]
        p = meta['priority']
        
        if p < min_priority:
            min_priority = p
            best_victim_key = key
        elif p == min_priority:
            # Tie-breaker: LRU (Lower last_access is older)
            if best_victim_key and meta['last_access'] < m_access_info[best_victim_key]['last_access']:
                best_victim_key = key
    
    # In GDSF, the aging factor L is updated to the priority of the evicted object.
    # This ensures that future objects must beat this baseline to stay.
    if min_priority != float('inf'):
        m_L = min_priority

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using the *Current* L.
       This effectively "resets" its age, protecting it from the rising L.
    '''
    global m_access_info, m_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        
        # Increment frequency
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # Restore Priority: It gets the current L boost plus its density value
        meta['priority'] = get_gdsf_priority(meta['freq'], obj.size, m_L)
        
        m_access_info[obj.key] = meta
    else:
        # Should ideally not happen on a hit, but safety fallback
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry for restored frequency.
    2. Calculate initial Priority.
    '''
    global m_access_info, m_ghost_registry, m_L
    
    current_time = cache_snapshot.access_count
    
    # Default frequency
    initial_freq = 1
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # It was recently evicted. Restore its frequency history.
        # This prevents "frequent but large" items from thrashing if they barely miss cache.
        initial_freq = m_ghost_registry[obj.key]['freq'] + 1
        # Remove from ghost as it's now in main cache
        del m_ghost_registry[obj.key]
    
    # Calculate Priority based on current global L
    priority = get_gdsf_priority(initial_freq, obj.size, m_L)
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move metadata to Ghost Registry.
    2. Prune Ghost Registry to keep memory overhead low.
    '''
    global m_access_info, m_ghost_registry
    
    # Move to ghost
    if evicted_obj.key in m_access_info:
        # We only strictly need frequency for the ghost logic
        freq = m_access_info[evicted_obj.key]['freq']
        m_ghost_registry[evicted_obj.key] = { 'freq': freq }
        
        # Remove from active metadata
        del m_access_info[evicted_obj.key]
        
    # Maintain Ghost Registry Size
    # Keep it roughly the size of the cache (or slightly larger)
    max_ghost_size = len(cache_snapshot.cache) * GHOST_RATIO + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Python 3.7+ dicts preserve insertion order. 
        # The first key is the oldest inserted (LRU of the ghost list).
        try:
            oldest_ghost = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5na7y4i_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp49qlrd5u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxbl11qme.pickle

Iteration 88: New subsample score 0.077207 is not better than old score 0.286083, skipping
Iteration 89: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwf7wg4fp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpawiolvtr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpci1mmq05.pickle

Iteration 89: Proposed new text for program: import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# Capacity of ghost registry relative to cache capacity.
# Captures items that recently left the cache.
GHOST_RATIO = 2.0 

# Factor by which Protected items (freq > 1) are protected vs Probation items (freq=1).
# A Protected item must be X times older (staler) than the oldest Probation item to be chosen as the victim.
# This prevents one-hit wonders (scans) from flushing out valuable working set items,
# while still allowing dead high-frequency items to be eventually evicted.
PROTECTION_FACTOR = 20.0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Dynamic Aging.
    
    1. Identify the LRU item among "Probationary" items (Freq = 1).
    2. Identify the LRU item among "Protected" items (Freq > 1).
    3. Decision:
       - Default to evicting the Probationary item (to filter out scans).
       - Exception: If the Protected item is significantly older than the Probationary item
         (based on PROTECTION_FACTOR), evict the Protected item. This cleans up "polluted"
         high-frequency items that are no longer being used.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Track best candidates for both segments
    probation_victim = None
    probation_max_staleness = -1
    
    protected_victim = None
    protected_max_staleness = -1
    
    # Single pass to find LRU for both groups
    for key in candidate_keys:
        # Fallback if metadata is missing
        if key not in m_access_info:
            return key
            
        meta = m_access_info[key]
        staleness = current_time - meta['last_access']
        freq = meta['freq']
        
        if freq <= 1:
            # Probationary Segment
            if staleness > probation_max_staleness:
                probation_max_staleness = staleness
                probation_victim = key
        else:
            # Protected Segment
            if staleness > protected_max_staleness:
                protected_max_staleness = staleness
                protected_victim = key
    
    # Decision Logic
    
    # If one segment is empty, evict from the other
    if probation_victim is None:
        return protected_victim
    if protected_victim is None:
        return probation_victim
        
    # Compare Staleness to make the decision
    # If the protected item is REALLY old compared to the probation item, evict it.
    if protected_max_staleness > (probation_max_staleness * PROTECTION_FACTOR):
        return protected_victim
    else:
        # Otherwise, protect the heavy hitter and sacrifice the probation item
        return probation_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (capped to prevent overflow/saturation).
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    if obj.key not in m_access_info:
        # Fallback initialization
        m_access_info[obj.key] = {'freq': 2, 'last_access': current_time}
    else:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        # Cap frequency. A frequency of 32 is sufficient to distinguish "very hot"
        # from "cold". Uncapped frequency hurts aging.
        if meta['freq'] < 32:
            meta['freq'] += 1
        m_access_info[obj.key] = meta

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If found in Ghost (Rescue): Insert as Protected (Freq=2).
    3. Else: Insert as Probationary (Freq=1).
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Check ghost registry
    if obj.key in m_ghost_registry:
        # Item was recently evicted but requested again. 
        # It is part of a cyclic working set. Promote immediately.
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to keep memory usage bounded.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # 1. Cleanup Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keeping ghost size relative to cache capacity allows adapting to 
    # working sets that are slightly larger than cache.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost (FIFO).
        # Python dicts preserve insertion order.
        try:
            oldest_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxhi5w_gu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpc34im8o3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg81_cw09.pickle

Iteration 89: New subsample score 0.7028939999999999 is not better than old score 0.777462, skipping
Iteration 90: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcdqt2xse.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb59tlb0y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj5dawjb2.pickle

Iteration 90: Proposed new text for program: import math

# Metadata dictionary
# key -> { 'freq': int, 'insertion_time': int }
m_metadata = dict()

# Ghost registry to track recently evicted keys
# key -> eviction_time
m_ghost_registry = dict()

def evict(cache_snapshot, obj):
    '''
    Implements a variation of S3-FIFO / SIEVE eviction strategy.
    
    The goal is to quickly evict "one-hit wonders" while keeping popular items.
    We score items based on:
    1. Frequency: Items with 0 hits since insertion (or reset) are preferred victims.
    2. Recency/Insertion: Among similar frequency, we approximate FIFO by evicting the oldest insertion.
    
    This function scans the candidates to find the best victim according to these rules.
    '''
    global m_metadata
    
    # Candidate keys currently in the cache
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to find the "oldest" item (smallest insertion_time) that has 
    # the lowest frequency.
    # Logic:
    # 1. Look for oldest item with freq == 0.
    # 2. If all items have freq > 0, we degrade their frequency (simulate the 'clock' hand passing)
    #    and pick the oldest one. 
    #    However, since we can't mutate state inside `evict` (it's a query), 
    #    we essentially return the item that *would* be evicted by a Clock/SIEVE algo.
    #    That is simply the item with lowest frequency, tie-broken by oldest insertion time.
    
    victim_key = None
    
    # We will score candidates. Lower score = KEEP, Higher score = EVICT.
    # But to make it cleaner, let's find the candidate with (Lowest Frequency, Oldest Insertion).
    # Tuple comparison in Python does exactly this.
    # We want to minimize (freq, insertion_time). Wait, standard LRU evicts the oldest.
    # So we want to EVICT the item with lowest freq. If tie, lowest insertion_time (FIFO).
    
    best_candidate = None
    # We initialize with a tuple that is theoretically "maximum valuable" so we can minimize it
    # But wait, we want to EVICT.
    # The "worst" item has lowest freq and lowest insertion time (oldest).
    # So we simply look for min( (freq, insertion_time) )
    
    # Optimization: Filter keys that exist in metadata
    valid_candidates = []
    for k in candidate_keys:
        if k in m_metadata:
            valid_candidates.append((k, m_metadata[k]))
    
    if not valid_candidates:
        return candidate_keys[0] # Fallback
        
    # Find the entry with min (freq, insertion_time)
    # We prioritize evicting freq=0 over freq=1.
    # Within freq=0, we evict the one inserted earliest (smallest insertion_time).
    
    # Note: To prevent starvation of high-frequency items in a purely static score,
    # real S3-FIFO decrements frequency during the scan. 
    # Since we cannot modify state in `evict` easily without side effects (and the prompt implies 
    # state updates happen in the update_* functions), we rely on `update_after_hit` 
    # to keep frequencies bounded (0, 1, 2) so they don't grow infinitely and block eviction forever.
    
    victim = min(valid_candidates, key=lambda x: (x[1]['freq'], x[1]['insertion_time']))
    
    return victim[0]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    A lazy promotion. We increment the frequency counter.
    To prevent cache pollution by ancient hot items, we cap the frequency.
    In S3-FIFO/SIEVE logic, usually 1 or 2 bits are enough.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        # Increment frequency
        # We cap at 2. This creates 3 "levels" of importance:
        # 0: Inserted, never accessed (Probation) -> First to go
        # 1: Accessed once (Protected)
        # 2: Accessed frequently (Super Protected)
        # Keeping the cap low (e.g., 2 or 3) is CRITICAL for adaptability.
        # If it grows to 100, an item that stops being popular stays for too long.
        new_freq = min(m_metadata[obj.key]['freq'] + 1, 2)
        
        # Note: We DO NOT update insertion_time on hit. This preserves FIFO order within the frequency tiers.
        # This makes it Scan-Resistant. A scan will hit items once (freq 0->1) or zero times (freq 0),
        # but they retain their old insertion time, ensuring they cycle out quickly.
        m_metadata[obj.key]['freq'] = new_freq
    else:
        # Should not happen ideally, but self-heal
        m_metadata[obj.key] = {'freq': 1, 'insertion_time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Initialize metadata. Check Ghost Registry for "Second Chance".
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Default priority is 0 (Probation).
    initial_freq = 0
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # This item was recently evicted and is needed again.
        # It implies our cache was too small or the item is part of a cyclic working set.
        # Promote it immediately to 'Protected' status.
        initial_freq = 2 # Jump strictly to top priority
        del m_ghost_registry[obj.key]
        
    m_metadata[obj.key] = {
        'freq': initial_freq,
        'insertion_time': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up active metadata, move to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Remove from active metadata
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
        
    # Add to Ghost Registry
    # This helps us detect if we evicted something too early.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prune Ghost Registry
    # Keep it bounded to avoid memory leaks. A size roughly equal to cache capacity is standard.
    # We use a randomized/iteration approach to remove old items for O(1) performance.
    target_ghost_size = cache_snapshot.capacity // 100 # Assuming avg obj size ~100 bytes? Or just count.
    # Actually, cache_snapshot.cache is a dict. Let's look at count of items.
    cache_item_count = len(cache_snapshot.cache)
    
    if len(m_ghost_registry) > max(cache_item_count, 100):
        # Remove a chunk of old ghosts to amortize overhead
        keys_to_remove = []
        for k in m_ghost_registry:
            keys_to_remove.append(k)
            if len(keys_to_remove) >= 10: # Prune 10 at a time
                break
        for k in keys_to_remove:
            del m_ghost_registry[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpijyfjm_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgcavj9pb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj91iw5yn.pickle

Iteration 90: New subsample score 1.271508 is not better than old score 1.419193, skipping
Iteration 91: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpt92_llrm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuiz8129r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7zebcjrl.pickle

Iteration 91: Proposed new text for program: import random

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items. Used to detect "loops" or working sets 
# that are slightly larger than the cache.
# We use a simple dict for O(1) lookup.
m_ghost_registry = dict()

# Since we don't have access to an ordered dict class or linked list node structure 
# for the cache internals, we simulate a "Clock" hand using a list of keys 
# that we rebuild lazily or iterate through. 
# To ensure O(1) amortized performance and good random sampling, 
# we will use a random sampling strategy for eviction candidates, 
# which approximates the Clock/Second-Chance algorithm very well.

def evict(cache_snapshot, obj):
    '''
    Implements a random-sample Second-Chance (Clock) eviction policy.
    
    Strategy:
    1. Sample a few keys from the cache.
    2. Check their frequency/utility.
    3. If an item has 'credit' (freq > 0), decrement it and skip eviction (Second Chance).
    4. If an item has no 'credit' (freq == 0), evict it.
    
    This ensures that "one-hit wonders" (freq=0) are evicted quickly, while 
    popular items (freq>0) circulate longer.
    '''
    global m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    # We attempt to find a victim by sampling.
    # In a true Clock algorithm, we would iterate circularly. 
    # Random sampling is a statistically strong approximation for large caches.
    # We try a finite number of times to find a 0-freq item. If all sampled items are hot,
    # we just forcefully evict the one with the lowest timestamp (LRU fallback).
    
    sample_size = 5 # Checking 5 items gives high probability of finding a good victim
    best_victim = None
    min_timestamp = float('inf')
    
    # Shuffle or sample? For python lists, random.sample is decent.
    # If cache is small, just check all.
    if len(candidate_keys) <= sample_size:
        samples = candidate_keys
    else:
        samples = random.sample(candidate_keys, sample_size)
        
    for key in samples:
        meta = m_access_info.get(key)
        if not meta:
            # Should not happen, but safe to evict corrupted state
            return key
            
        # CLOCK Logic:
        if meta['freq'] > 0:
            # Give second chance, decrement freq
            meta['freq'] -= 1
            # We don't evict this one, but we keep track of LRU just in case we fail to find any 0
            if meta['last_access'] < min_timestamp:
                min_timestamp = meta['last_access']
                best_victim = key
        else:
            # Found a zero-frequency item. Evict immediately.
            return key
            
    # If we are here, it means all sampled items had freq > 0 and were decremented.
    # We return the "oldest" among the samples we saw to break the deadlock.
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (saturated counter).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Max frequency is capped at 3.
        # 0 = Insertion/Probation
        # 1 = Warm
        # 2 = Hot
        # 3 = Very Hot
        # Keeping this small ensures adaptation isn't too sluggish.
        if meta['freq'] < 3:
            meta['freq'] += 1
            
        m_access_info[obj.key] = meta
    else:
        # Recover missing metadata
        m_access_info[obj.key] = {
            'freq': 0, 
            'last_access': current_time
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost: This is a "false positive" eviction. Promote immediately to protected state.
    3. If not in Ghost: Insert as probationary (freq=0).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 0
    
    # Check if this item was recently evicted
    if obj.key in m_ghost_registry:
        # It was in the ghost list, meaning it was accessed shortly after eviction.
        # This implies it belongs to the working set.
        # We promote it immediately to 'Hot' (freq=2) so it survives the next few eviction cycles.
        initial_freq = 2
        del m_ghost_registry[obj.key]
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry to track "mistakes".
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    # Value doesn't strictly matter, but we can store eviction time if we wanted to prune by time.
    # Here we just use keys.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We keep the ghost registry roughly the same size as the cache capacity (or slightly larger).
    # This acts as the "history" window.
    # Since we can't easily query capacity count vs size, we approximate using current cache count.
    
    target_ghost_size = len(cache_snapshot.cache) 
    if target_ghost_size == 0: target_ghost_size = 100 # Safety for empty start
    
    if len(m_ghost_registry) > target_ghost_size:
        # Remove oldest inserted into ghost to maintain size cap.
        # Python 3.7+ dicts preserve insertion order, so the first key is the oldest.
        try:
            oldest_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbywm3ic_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8hvbjg75.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph_zhbr76.pickle

Iteration 91: New subsample score 0.994364 is not better than old score 1.0203520000000001, skipping
Iteration 92: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcendahjc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyb9er2nl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqzyoh1et.pickle

Iteration 92: Proposed new text for program: import math

# ARC Metadata Global State
# t1: Recent items (LRU Queue). Key -> None. Order: Oldest (LRU) to Newest (MRU).
# t2: Frequent items (LRU Queue). Key -> None.
# b1: Ghost Recent (History of evicted T1). Key -> None.
# b2: Ghost Frequent (History of evicted T2). Key -> None.
# p:  Target size for T1 (adaptable parameter).
m_arc = {
    't1': {},
    't2': {},
    'b1': {},
    'b2': {},
    'p': 0,
    'capacity': 0  # We will learn this from cache_snapshot
}

def _access(key):
    """
    Helper to move a key to the MRU position (end) of its respective dictionary.
    Simulates touching the item.
    """
    if key in m_arc['t1']:
        del m_arc['t1'][key]
        m_arc['t1'][key] = None
    elif key in m_arc['t2']:
        del m_arc['t2'][key]
        m_arc['t2'][key] = None

def _adapt_p(key, delta):
    """
    Safely update the parameter P within bounds [0, capacity].
    """
    current_p = m_arc['p']
    capacity = m_arc['capacity']
    m_arc['p'] = max(0, min(capacity, current_p + delta))

def evict(cache_snapshot, obj):
    '''
    ARC Replacement Policy.
    Decides whether to evict from T1 or T2 based on the current state of 'p' 
    and the type of the incoming object (obj).
    '''
    global m_arc
    
    # Update capacity perception
    capacity = cache_snapshot.capacity
    # In this environment, capacity is given in bytes, but the cache is full based on count 
    # if object sizes are 1, or bytes if varied. 
    # However, standard ARC is count-based. We use the current cache size as the working capacity
    # since this function is only called when the cache is FULL.
    if m_arc['capacity'] == 0:
        m_arc['capacity'] = len(cache_snapshot.cache)

    key = obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']
    p = m_arc['p']

    # 1. ADAPTATION: Update 'p' based on where the miss occurred (Ghost Hits)
    # This logic normally sits in the 'miss' handler, but we need to run it before 
    # selecting a victim to ensure the victim selection reflects the new 'p'.
    
    if key in b1:
        # Hit in Ghost Recent: We should have kept T1 larger. Increase p.
        delta = 1
        if len(b1) >= len(b2):
            delta = 1
        else:
            delta = len(b2) / len(b1)
        _adapt_p(key, delta)
        
    elif key in b2:
        # Hit in Ghost Frequent: We should have kept T2 larger. Decrease p.
        delta = 1
        if len(b2) >= len(b1):
            delta = 1
        else:
            delta = len(b1) / len(b2)
        _adapt_p(key, -delta)

    # Reload p after update
    p = m_arc['p']

    # 2. REPLACEMENT: Decide which item to evict.
    # We evict from T1 if T1 is growing beyond target 'p', OR if we have a B2 hit 
    # and T1 is exactly at target 'p'.
    
    victim_key = None
    
    # Logic derived from ARC "REPLACE(p)" subroutine
    # Condition to evict from T1:
    # 1. T1 is not empty AND
    # 2. (Len(T1) > p) OR (Incoming item is in B2 and Len(T1) == p)
    
    evict_from_t1 = False
    if len(t1) > 0:
        if len(t1) > p:
            evict_from_t1 = True
        elif (key in b2) and (len(t1) == int(p)):
            evict_from_t1 = True
    
    if evict_from_t1:
        # Evict LRU of T1 (First key in dict)
        victim_key = next(iter(t1))
    else:
        # Evict LRU of T2
        # Fallback: if T2 is empty, we must evict T1 (should rarely happen if cache full)
        if len(t2) > 0:
            victim_key = next(iter(t2))
        else:
            victim_key = next(iter(t1))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If in T1, move to T2 (MRU).
    If in T2, move to T2 (MRU).
    '''
    global m_arc
    key = obj.key
    
    # Move to MRU T2
    # If it was in T1, it gets promoted to T2 (Frequency list)
    if key in m_arc['t1']:
        del m_arc['t1'][key]
    
    # If it was in T2, just refresh it
    if key in m_arc['t2']:
        del m_arc['t2'][key]
        
    m_arc['t2'][key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    Place the object into the appropriate list (T1 or T2) based on history.
    '''
    global m_arc
    
    # Note: 'evict' might have been called just before this, or not (if cache wasn't full).
    # We need to finalize the placement of the new object.
    
    key = obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']

    # Case 1: It was in Ghost Recent (B1). 
    # It was a miss, but we remembered it. Promote to T2 (Frequent).
    if key in b1:
        del b1[key]
        t2[key] = None # Insert MRU
        return

    # Case 2: It was in Ghost Frequent (B2).
    # It was a miss, but we remembered it. Promote to T2 (Frequent).
    if key in b2:
        del b2[key]
        t2[key] = None # Insert MRU
        return

    # Case 3: It is totally new.
    # Insert into T1 (Recent).
    t1[key] = None # Insert MRU

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from active lists (T1/T2) to ghost lists (B1/B2).
    Maintain ghost list sizes.
    '''
    global m_arc
    
    key = evicted_obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']
    
    # Remove from active lists
    if key in t1:
        del t1[key]
        # Move to B1 (Ghost Recent)
        b1[key] = None
    elif key in t2:
        del t2[key]
        # Move to B2 (Ghost Frequent)
        b2[key] = None
        
    # Enforce Ghost List Capacities
    # ARC generally suggests: |T1| + |B1| <= C  and  |T2| + |B2| <= 2C
    # Here C is effectively len(cache).
    
    # Use current cache size as proxy for capacity C if not strictly set
    c = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100 
    
    # If we just evicted, the cache size is effectively at capacity.
    
    # Prune B1
    while len(t1) + len(b1) > c:
        if len(b1) > 0:
            oldest = next(iter(b1))
            del b1[oldest]
        else:
            break
            
    # Prune B2
    while len(t2) + len(b2) > 2 * c:
        if len(b2) > 0:
            oldest = next(iter(b2))
            del b2[oldest]
        else:
            break
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb425ak5x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2gk19b4t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoyc4ocdt.pickle

Iteration 92: New subsample score 1.0555459999999999 is better than old score 1.039433. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdep9u5ab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprb58vvmr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp04clr4lu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6mgglld6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxg9mcurh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgyyvtyg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpakwegd3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpe72uc_mu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp6ywlwwv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjzex8ypz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp26lhnfp4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpafkr0xnu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcn0kd0jl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp93pgel5a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9aljf73f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpoj3ifta5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpx2dpeci1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu4qyia3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0wksznmv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsbc4xogc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6pewdtv2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl7vjc38v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnun60c2c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3tjv07j3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdkw0jujw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjb8p97ox.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp88_bhm1c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3p2eorny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkeq13v7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphs84x7qg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvc6mghui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_iv0sab9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpygrph_v9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2iimgdxd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4w50m4id.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaq8y60hs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkgd8qrgd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpraz1ju_3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnerjml6d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpy8wxt1op.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbszra9cx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnua05t7c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq6hpy1r5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_5tlwm3z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzsbn4j8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8ojjw24t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7gohkevt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpltzbh8vp.pickle

Iteration 92: Full valset score for new program: 0.2393880625
Iteration 92: Full train_val score for new program: 0.2393880625
Iteration 92: Individual valset scores for new program: [0.504628, 0.47922, 0.488698, 0.433004, 0.503503, 0.489796, 0.272727, 0.498034, 0.537719, 0.531017, 0.075, 0.323268, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.345133, 0.026164, 0.058672, 0.058672, 0.269789, 0.359879, 0.848064, 0.891323, 0.039832, 0.038636, 0.045558, 0.007721, 0.020672, 0.752893, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.111173, 0.073305, 0.107678, 0.052632, 0.2, 0.041854, 0.10014, 0.466258, 0.081699]
Iteration 92: New valset pareto front scores: [0.510278, 0.486516, 0.493364, 0.44314, 0.506177, 0.493313, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.641937, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.366667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 92: Full valset pareto front score: 0.2640278333333333
Iteration 92: Updated valset pareto front programs: [{22}, {4}, {22}, {22}, {4}, {22}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {3, 5, 6, 7, 12, 13, 20, 22}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24}, {1, 22}, {15}, {4}, {6}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24}, {22}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24}, {3, 5, 6, 7, 20}, {22}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21, 22, 23, 24}]
Iteration 92: Best valset aggregate score so far: 0.2576176875000001
Iteration 92: Best program as per aggregate score on train_val: 22
Iteration 92: Best program as per aggregate score on valset: 22
Iteration 92: Best score on valset: 0.2576176875000001
Iteration 92: Best score on train_val: 0.2576176875000001
Iteration 92: Linear pareto front program index: 22
Iteration 92: New program candidate index: 24
Iteration 93: Selected program 3 score: 0.24242306249999998
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzkykbg17.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp95c83p2y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp10bnr379.pickle

Iteration 93: Proposed new text for program: # Metadata dictionaries
# m_last_access: Tracks the logical access time for LRU implementation within segments.
m_last_access = dict()

# m_is_protected: Boolean flag. 
# False = Probation (Candidate for quick eviction)
# True = Protected (Long-term resident)
m_is_protected = dict()

# m_ghosts: A history of recently evicted keys. 
# Used to detect items that return shortly after eviction.
# We rely on Python's dict preserving insertion order for FIFO management.
m_ghosts = dict()

def evict(cache_snapshot, obj):
    '''
    Implements the eviction logic for Segmented LRU (SLRU).
    Decides whether to evict from the 'Probation' or 'Protected' segment.
    '''
    global m_last_access, m_is_protected
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into Probation and Protected lists
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        if m_is_protected.get(k, False):
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    total_items = len(candidate_keys)
    
    # 2. Define the target window size for new items.
    # We want to ensure at least ~20% of the cache is available for new items (Probation)
    # so they have time to prove their worth.
    min_probation_size = total_items * 0.2
    
    victim_key = None
    
    # 3. Decision Logic:
    
    # CASE A: The Probation segment is too small (Protected items are hogging space).
    # We must evict a Protected item to restore balance and allow new items in.
    if len(probation_keys) < min_probation_size and len(protected_keys) > 0:
        # Evict LRU from Protected
        victim_key = min(protected_keys, key=lambda k: m_last_access.get(k, 0))
        
    # CASE B: Normal operation. Evict from Probation (remove one-hit wonders).
    elif len(probation_keys) > 0:
        # Evict LRU from Probation
        victim_key = min(probation_keys, key=lambda k: m_last_access.get(k, 0))
        
    # CASE C: Fallback (Probation is empty, only Protected items exist).
    else:
         victim_key = min(protected_keys, key=lambda k: m_last_access.get(k, 0))
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a cache hit:
    1. Update recency (for LRU).
    2. Promote the item to Protected status (it has proven it is reused).
    '''
    global m_last_access, m_is_protected
    
    m_last_access[obj.key] = cache_snapshot.access_count
    m_is_protected[obj.key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On a new insertion:
    1. Update recency.
    2. Check Ghost Registry:
       - If present: This is a "resurrected" item. Promote immediately to Protected.
       - If absent: This is a cold item. Add to Probation.
    '''
    global m_last_access, m_is_protected, m_ghosts
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghosts:
        # It was evicted recently and came back -> High value.
        m_is_protected[obj.key] = True
        # Remove from ghost since it's now in active cache
        del m_ghosts[obj.key]
    else:
        # First time seeing it (or seen a long time ago) -> Probation.
        m_is_protected[obj.key] = False

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    1. Clean up active metadata.
    2. Add the evicted key to the Ghost Registry.
    '''
    global m_last_access, m_is_protected, m_ghosts
    
    # Clean up active cache metadata
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    if evicted_obj.key in m_is_protected:
        del m_is_protected[evicted_obj.key]
        
    # Add to Ghost Registry (SLRU "Shadow Cache")
    m_ghosts[evicted_obj.key] = True
    
    # Limit Ghost Registry Size to prevent memory leak
    # We keep a history roughly proportional to the cache size (or a safe fixed limit)
    # to catch items that loop back shortly after eviction.
    limit = max(1000, len(cache_snapshot.cache))
    
    if len(m_ghosts) > limit:
        # Remove oldest inserted (FIFO) to maintain size
        oldest_key = next(iter(m_ghosts))
        del m_ghosts[oldest_key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2m6azrtx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9uuzly85.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplzc5ifep.pickle

Iteration 93: New subsample score 0.515904 is not better than old score 0.592456, skipping
Iteration 94: Selected program 4 score: 0.2510750416666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps749w55p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxdu2lren.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsbaleq6v.pickle

Iteration 94: Proposed new text for program: import math

# --- Metadata Storage ---
# Stores tuples: key -> (frequency, last_access_time, insertion_time, size)
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# This acts as a "shadow cache" to track items we recently evicted.
m_ghost_registry = dict()

# Global counter to enforce LRU purity within frequency buckets without relying solely on access_count
# (Useful if batch updates happen, though access_count usually suffices)
m_tick = 0

def get_victim_score(key, current_time, obj_size):
    '''
    Calculates a "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Logic:
    1. Filter by Frequency (Admit/Probation vs Protected):
       - Frequency = 1 (Probation): High risk of being a one-hit wonder. High eviction score.
       - Frequency > 1 (Protected): Proven utility. Low eviction score.
       
    2. Filter by Recency (within frequency classes):
       - Older 'last_access_time' -> Higher eviction score (Standard LRU).
       
    3. Tie-Breaker by Size (Optional but effective):
       - If two items have similar recency/frequency, evicting the LARGER one frees up more space
         for other items, potentially increasing total object count and hit rate (Greedy Dual Size principle).
    '''
    
    if key not in m_access_info:
        # Fallback for safety
        return float('inf')
        
    freq, last_access, _, _ = m_access_info[key]
    
    # Calculate Staleness (Time since last used)
    staleness = current_time - last_access
    
    # --- Scoring Formula ---
    
    # BASE SCORE determined by frequency tier.
    # Tier 1: Probation (freq=1). These are the first to go.
    # Base score: 10^15 (Guarantees they are evicted before Tier 2)
    if freq <= 1:
        base_score = 1e15
    else:
        # Tier 2: Protected (freq > 1).
        # Base score: 0. 
        base_score = 0
        
    # RECENCY Component: The staler, the higher the score.
    # Simple linear addition of time delta.
    recency_score = staleness
    
    # SIZE Penalty (Greedy Dual Size Lite):
    # We slightly prefer evicting larger objects to make room for more small objects.
    # However, recency is usually more important than size in general web traces.
    # We apply a small multiplier based on log(size) to break ties in LRU.
    # This prevents flushing a very hot large object just because it's large.
    size_penalty = 0
    if obj_size > 0:
        # A tiny bias: larger objects get a very small boost to their eviction score.
        # This helps in "cache size" constraints (bytes) vs "cache capacity" (count).
        size_penalty = math.log(obj_size + 1)
        
    final_score = base_score + recency_score + size_penalty
    return final_score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score.
    Strategy: Segmented LRU with Ghost awareness.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We scan candidates to find the 'worst' one (highest score)
    # Optimization: In a production system, we would maintain linked lists (queues)
    # to avoid O(N) scans. For this simulation interface, O(N) is the only way
    # to read the read-only cache_snapshot.
    for key in candidate_keys:
        # Retrieve size from metadata or fallback to 1 if missing (shouldn't happen)
        cached_obj_size = 1
        if key in m_access_info:
             cached_obj_size = m_access_info[key][3]
        
        score = get_victim_score(key, current_time, cached_obj_size)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Promote the item.
    2. Increase Frequency (saturation applied to prevent pollution).
    3. Update Recency.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing
    # Default: freq=0, insert=now, size=obj.size
    old_freq, _, insertion_time, _ = m_access_info.get(obj.key, (0, current_time, current_time, obj.size))
    
    # Increment frequency.
    # We cap frequency at a small number (e.g., 3 or 4).
    # Why? Because in SLRU/S3-FIFO, the distinction is mainly "One Hit" vs "Many Hits".
    # Distinguishing between 100 hits and 101 hits is rarely useful and prevents
    # items from aging out when workloads shift.
    new_freq = min(old_freq + 1, 4)
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time, obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Determine Initial Frequency (Probation vs Protected).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # DEFAULT: New items go to Probation (freq=1)
    initial_freq = 1
    
    # GHOST CHECK:
    # If this key was recently evicted, it implies our cache is too small or 
    # we made a bad eviction decision. This item is actually part of the working set.
    # We insert it directly with a higher frequency/priority so it isn't evicted immediately again.
    if obj.key in m_ghost_registry:
        # Promote straight to "Protected" tier
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to prevent memory leaks.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow the ghost registry to hold roughly as many items as the cache capacity (in count).
    # Since we don't know the exact count capacity (it's byte-based), we approximate based on
    # current cache keys count.
    max_ghost_size = max(len(cache_snapshot.cache), 100)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost registry.
        # Python dicts preserve insertion order (FIFO behavior for iteration).
        # We delete a chunk to amortize the cost.
        keys_to_remove = []
        for i, k in enumerate(m_ghost_registry):
            if i >= 5: # Remove 5 oldest at a time
                break
            keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_ghost_registry[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmi6jcj6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg449927b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3nrfmvrz.pickle

Iteration 94: New subsample score 0.932793 is better than old score 0.895437. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp638wcg_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiaz4e2c7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa9af1kjb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl7yl05_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp96tq398g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpybknv7z4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp24p0_0a_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_6urdn2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiltyb644.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1_l9bsin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmkkuejek.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpch3t_mv2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpi0g9m9ay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj8v2k1hm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkdi3r_xe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_y3oo5yv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8rd9mimp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplmsq_rj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3045ln7_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptdizbdwr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpbmzbg7lf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6_smk50y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqrx99mwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp87_h5k7n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1k104nou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_gxvr7wi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpa3op99po.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpea_fgno6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr556mquu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxk569gu_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpubm0zozp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpd3g5mvi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprv9c16h6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7fllwuf1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpthk1arei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7bfjv4n1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpadsnem4n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo8jer2t5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp3jts9862.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp15vxd20z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiuoi2qu5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyw_46oxl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7qokdzhf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdsav699h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpars9knda.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppxi7qacf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpsm5g2p0z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp21dw2ng4.pickle

Iteration 94: Full valset score for new program: 0.23474870833333328
Iteration 94: Full train_val score for new program: 0.23474870833333328
Iteration 94: Individual valset scores for new program: [0.509201, 0.486458, 0.491425, 0.443318, 0.506124, 0.493082, 0.272727, 0.478569, 0.539864, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.359882, 0.026164, 0.058672, 0.058672, 0.269787, 0.394153, 0.856008, 0.89389, 0.020365, 0.038636, 0.045558, 9.6e-05, 3.6e-05, 0.744894, 0.083333, 0.067961, 0.015652, 0.640392, 0.125461, 0.022179, 0.021381, 0.023637, 0.052632, 0.316667, 0.021041, 0.02216, 0.466258, 0.03268]
Iteration 94: New valset pareto front scores: [0.510278, 0.486516, 0.493364, 0.443318, 0.506177, 0.493313, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.391652, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894061, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.641937, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.366667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 94: Full valset pareto front score: 0.2640315416666666
Iteration 94: Updated valset pareto front programs: [{22}, {4}, {22}, {25}, {4}, {22}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {3, 5, 6, 7, 12, 13, 20, 22}, {15}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {6, 15}, {6, 15}, {20, 6, 15}, {20, 6, 15}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {3}, {6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {1, 22}, {15}, {4}, {6}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {22}, {20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {3, 5, 6, 7, 20}, {22}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21, 22, 23, 24}]
Iteration 94: Best valset aggregate score so far: 0.2576176875000001
Iteration 94: Best program as per aggregate score on train_val: 22
Iteration 94: Best program as per aggregate score on valset: 22
Iteration 94: Best score on valset: 0.2576176875000001
Iteration 94: Best score on train_val: 0.2576176875000001
Iteration 94: Linear pareto front program index: 22
Iteration 94: New program candidate index: 25
Iteration 95: Selected program 25 score: 0.23474870833333328
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8le1ykxl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5lwkx5ha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_kz3h6ut.pickle

Iteration 95: Proposed new text for program: import math

# --- Metadata Storage ---
# Stores: key -> {'freq': int, 'priority': float, 'tick': int}
m_metadata = dict()

# Ghost registry: Stores key -> frequency
# Used to restore frequency for items that return to cache quickly (thrashing protection)
m_ghost_registry = dict()

# Global Inflation Value (L)
# This represents the "cost" to enter/stay in the cache. It rises over time.
m_inflation = 0.0

def get_priority(freq, size, current_L):
    '''
    Calculates GDSF Priority.
    Priority = L + (Frequency * Weight) / Size
    
    Higher Priority = Keep in Cache.
    Lower Priority = Evict.
    '''
    # Avoid division by zero, though size should be positive.
    # We use a slight dampening on size (sqrt) or linear size. 
    # For pure Hit Rate maximization, linear size (1/size) is mathematically optimal.
    # However, to prevent starving large hot objects entirely, we ensure a minimum weight.
    
    safe_size = max(1, size)
    
    # Heuristic: 
    # We use a large multiplier (10000) to keep floating point precision reasonable
    # relative to L.
    # Formula: Priority = L + (Frequency * 10000) / Size
    cost_benefit = (freq * 10000.0) / safe_size
    
    return current_L + cost_benefit

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction:
    1. Find candidate with the MINIMUM priority.
    2. Update global m_inflation (L) to that minimum priority.
    3. Return that candidate.
    '''
    global m_inflation
    
    candidate_keys = cache_snapshot.cache.keys()
    
    if not candidate_keys:
        return None

    # Search for victim with lowest priority
    victim = None
    min_priority = float('inf')
    
    # We need to scan to find the minimum. 
    # In a real O(1) implementation, this would use a Min-Heap.
    for key in candidate_keys:
        if key in m_metadata:
            entry = m_metadata[key]
            p = entry['priority']
            
            # Identify minimum priority
            if p < min_priority:
                min_priority = p
                victim = key
            elif p == min_priority:
                # Tie-Breaker: LRU
                # If priorities are identical, evict the one accessed/inserted earliest
                if victim and m_metadata[key]['tick'] < m_metadata[victim]['tick']:
                    victim = key
        else:
            # Should not happen if metadata is synced, but if it does, evict unknown item
            return key
            
    # CRITICAL GDSF STEP:
    # The system ages by setting L to the priority of the evicted item.
    if victim is not None:
        m_inflation = min_priority
        
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority using CURRENT L (Re-aging).
    3. Update Tick (for LRU tie-breaking).
    '''
    global m_metadata, m_inflation
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        entry = m_metadata[obj.key]
        entry['freq'] += 1
        entry['tick'] = current_time # Update for LRU
        
        # Recalculate priority based on new freq and CURRENT inflation L
        # This "brings the object forward" in time.
        entry['priority'] = get_priority(entry['freq'], obj.size, m_inflation)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine Frequency (check Ghost).
    2. Calculate initial Priority using CURRENT L.
    '''
    global m_metadata, m_ghost_registry, m_inflation
    
    current_time = cache_snapshot.access_count
    
    # Default frequency
    freq = 1
    
    # Ghost Recovery:
    # If this item was recently evicted, it means it's part of a loop or working set
    # that is slightly larger than cache. Restore its frequency so it isn't 
    # immediately evicted again (Scan Resistance).
    if obj.key in m_ghost_registry:
        freq = m_ghost_registry[obj.key] + 1
        # Cap frequency restoration to prevent ancient history from dominating
        # extremely fast moving caches, but allow enough to stick.
        del m_ghost_registry[obj.key]

    # Calculate Priority
    priority = get_priority(freq, obj.size, m_inflation)
    
    m_metadata[obj.key] = {
        'freq': freq,
        'priority': priority,
        'tick': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move Metadata to Ghost.
    2. Prune Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    if evicted_obj.key in m_metadata:
        # Save frequency to ghost
        freq = m_metadata[evicted_obj.key]['freq']
        del m_metadata[evicted_obj.key]
        m_ghost_registry[evicted_obj.key] = freq
    
    # Maintain Ghost Registry Size
    # If the ghost registry grows too large, it wastes memory and might restore
    # frequencies for items that have been gone too long.
    # A good heuristic is Ghost Capacity ~= Cache Object Count (approximate).
    # Since we can't easily know max object count, we limit it relative to current usage.
    # We use a loose bound (e.g., 2000 items or 2x current cache count).
    
    target_len = max(len(cache_snapshot.cache) * 2, 500)
    
    if len(m_ghost_registry) > target_len:
        # Prune oldest inserted items (Dicts preserve insertion order in modern Python)
        # Pruning in chunks is more efficient than one by one
        items_to_remove = []
        for k in m_ghost_registry:
            items_to_remove.append(k)
            if len(items_to_remove) >= 10:
                break
        for k in items_to_remove:
            del m_ghost_registry[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwwd8rzsy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplat7bv6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpq7yfba2s.pickle

Iteration 95: New subsample score 0.6315419999999999 is not better than old score 0.63613, skipping
Iteration 96: Selected program 25 score: 0.23474870833333328
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp73njsuca.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2em7og39.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8by409xz.pickle

Iteration 96: Proposed new text for program: import math

# --- Metadata Storage ---
# Stores key -> [frequency, last_access_time, size]
# Using a list allows for mutable updates of frequency and time.
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# Tracks items recently evicted to boost their frequency if they return.
# This helps the algorithm adapt to working sets that are slightly larger than cache capacity.
m_ghost_registry = dict()

def get_victim_score(key, current_time, meta):
    '''
    Calculates the eviction score. 
    Higher score = Higher desirability to EVICT.
    
    The scoring strategy is a hybrid of Recency (LRU), Frequency (LFU), and Size (GreedyDual).
    
    Formula: Score = (Age * Size_Factor) / Frequency
    
    1. Age (current_time - last_access):
       Standard LRU component. The older the object, the higher the score.
       
    2. Frequency:
       The denominator. Higher frequency drastically reduces the eviction score, 
       protecting popular items.
       
    3. Size_Factor (sqrt(size)):
       We penalize larger objects because evicting one large object makes room for multiple
       small objects, which statistically increases the hit rate (hit count).
       We use sqrt(size) rather than linear size to avoid being overly aggressive 
       against large, potentially useful assets.
    '''
    freq, last_access, size = meta
    
    age = current_time - last_access
    
    # Safety check for frequency (should minimally be 1)
    if freq < 1:
        freq = 1
        
    # Size Factor:
    # Using sqrt(size) balances "Hits per Byte" optimization with fairness.
    # Linear size can cause large hot objects to be evicted too quickly.
    size_factor = size ** 0.5
    
    # Victim Score Calculation
    # We divide by frequency. A frequency of 10 makes an item 10x harder to evict 
    # than a frequency of 1 (assuming same age/size).
    score = (age * size_factor) / freq
    
    return score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest calculated victim score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # Iterate over all candidates to find the one with the highest eviction score.
    # While O(N), this allows precise application of the hybrid scoring logic.
    for key in candidate_keys:
        if key in m_access_info:
            score = get_victim_score(key, current_time, m_access_info[key])
        else:
            # Fallback: If metadata is missing, treat it as the best candidate to evict.
            score = float('inf')
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency (Popularity).
    2. Update Last Access Time (Recency).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        entry = m_access_info[obj.key]
        entry[0] += 1             # Increment Frequency
        entry[1] = current_time   # Update Recency
    else:
        # Recover metadata if missing (rare case)
        m_access_info[obj.key] = [1, current_time, obj.size]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Check Ghost Registry to apply frequency boost if applicable.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Default initial frequency for new items (Probation)
    initial_freq = 1
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # The item was recently evicted and has been requested again.
        # This implies our cache was slightly too small or we made a mistake.
        # We grant a frequency boost so it isn't evicted immediately again.
        initial_freq = 4
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = [initial_freq, current_time, obj.size]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (shadow cache).
    3. Prune Ghost Registry to keep memory usage bounded.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We want the ghost registry to be large enough to catch "loops" or working sets 
    # that just barely don't fit. A size of 2x the current object count is usually sufficient.
    # We use a lower bound of 500 to handle cold start or very small caches.
    target_ghost_size = max(len(cache_snapshot.cache) * 2, 500)
    
    if len(m_ghost_registry) > target_ghost_size:
        # Remove the oldest inserted item from ghost registry.
        # Since Python 3.7+, dicts preserve insertion order, so next(iter()) gives the oldest.
        victim_ghost = next(iter(m_ghost_registry))
        del m_ghost_registry[victim_ghost]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprfevfxl3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpebwde5c2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpl3q1dbgh.pickle

Iteration 96: New subsample score 0.315085 is not better than old score 0.335747, skipping
Iteration 97: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpni3x2tay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfznqhv1v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1ewqol15.pickle

Iteration 97: Proposed new text for program: import collections

# Metadata dictionaries
# Maps key -> frequency count
m_key_frequency = dict()
# Maps key -> priority value (Frequency + LFU_DA_AGE)
m_key_value = dict()
# Maps key -> last access timestamp (for LRU tie-breaking)
m_key_last_access = dict()

# LFU-DA Global State
LFU_DA_AGE = 0.0

def evict(cache_snapshot, obj):
    '''
    Implements LFU-DA (Least Frequently Used with Dynamic Aging) eviction.
    
    We select the victim with the lowest 'm_key_value'.
    Tie-breaking: If values are equal, use LRU (oldest 'm_key_last_access').
    '''
    global m_key_value, m_key_last_access, LFU_DA_AGE
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_val = float('inf')
    min_access_time = float('inf')

    # Linear scan to find the minimum value. 
    for key in candidate_keys:
        val = m_key_value.get(key, 0.0)
        last_access = m_key_last_access.get(key, 0)
        
        # We want the smallest value (LFU principle)
        if val < min_val:
            min_val = val
            min_access_time = last_access
            victim_key = key
        elif val == min_val:
            # Tie-breaker: LRU
            if last_access < min_access_time:
                min_access_time = last_access
                victim_key = key
    
    # LFU-DA Logic: The global "Age" of the cache rises to meet the 
    # value of the object being evicted. This allows new items (with low freq)
    # to compete with old items (with high freq).
    if victim_key is not None:
        LFU_DA_AGE = min_val

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and update the object's value 
    using the current global Age.
    '''
    global m_key_frequency, m_key_value, m_key_last_access, LFU_DA_AGE
    
    key = obj.key
    
    # Update Frequency
    current_freq = m_key_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_key_frequency[key] = new_freq
    
    # Update Priority Value (LFU-DA formula)
    # Value = Frequency + Global_Cache_Age
    m_key_value[key] = new_freq + LFU_DA_AGE
    
    # Update Recency for tie-breaking
    m_key_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize frequency and set value relative to global Age.
    '''
    global m_key_frequency, m_key_value, m_key_last_access, LFU_DA_AGE
    
    key = obj.key
    
    # Initialize Frequency
    # If it was in history (ghost cache), we could restore it, but strictly
    # resetting to 1 is safer for LFU-DA to prevent pollution, 
    # relying on Age to protect it.
    new_freq = 1
    m_key_frequency[key] = new_freq
    
    # LFU-DA Value Initialization
    # New items start at Frequency(1) + Current Age.
    m_key_value[key] = new_freq + LFU_DA_AGE
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction: Clean up metadata to prevent unbounded growth.
    '''
    global m_key_frequency, m_key_value, m_key_last_access
    
    # Unlike "Ghost Cache" approaches, standard LFU-DA often performs better
    # if we strictly forget evicted items to adapt to phase changes quickly.
    # However, keeping them is valid if we want to support "churning" items.
    # Given the previous results, strict cleanup is usually safer for memory 
    # and preventing old heavy hitters from returning too easily.
    
    key = evicted_obj.key
    if key in m_key_frequency:
        del m_key_frequency[key]
    if key in m_key_value:
        del m_key_value[key]
    if key in m_key_last_access:
        del m_key_last_access[key]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdiy5ygi9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp300pqoxm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptgceusxj.pickle

Iteration 97: New subsample score 0.473283 is not better than old score 0.534423, skipping
Iteration 98: Selected program 22 score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvh1edd11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuvvpdq79.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgo6nmqe0.pickle

Iteration 98: Proposed new text for program: # Metadata dictionary to store access history and segment info.
# Key -> {
#   'last_access': int, 
#   'protected': bool  (False=Probation, True=Protected)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
m_ghost_registry = dict()

# Tunable Parameters
PROTECTED_RATIO = 0.8  # Max % of cache allowed for Protected items
GHOST_RATIO = 1.0      # Size of ghost registry relative to capacity

def get_victim_score(key, current_time):
    # Not used in this optimized implementation, 
    # as the logic is embedded directly in evict for efficiency and state management.
    return 0

def evict(cache_snapshot, obj):
    '''
    Implements Segmented LRU (SLRU) eviction logic.
    - The cache is conceptually divided into "Probation" and "Protected" segments.
    - We enforce a capacity limit on the Protected segment.
    - Eviction Priority:
        1. If Protected segment is over capacity: Evict LRU from Protected.
        2. Otherwise: Evict LRU from Probation.
        3. Fallback: Evict LRU from Protected (if Probation is empty).
    '''
    global m_access_info
    
    # Identify candidates
    candidate_keys = list(cache_snapshot.cache.keys())
    
    lru_probation_key = None
    lru_probation_time = float('inf')
    
    lru_protected_key = None
    lru_protected_time = float('inf')
    
    protected_count = 0
    
    # Single pass to gather stats and candidates
    for key in candidate_keys:
        # Default to Probation if metadata is missing
        meta = m_access_info.get(key, {'last_access': 0, 'protected': False})
        
        if meta['protected']:
            protected_count += 1
            if meta['last_access'] < lru_protected_time:
                lru_protected_time = meta['last_access']
                lru_protected_key = key
        else:
            if meta['last_access'] < lru_probation_time:
                lru_probation_time = meta['last_access']
                lru_probation_key = key
                
    # Decision Logic
    protected_limit = cache_snapshot.capacity * PROTECTED_RATIO
    
    # 1. Enforce Protected Segment Size
    # If we have too many protected items, we must evict one to make room.
    # (Effectively acting as a demotion).
    if protected_count > protected_limit:
        if lru_protected_key is not None:
            return lru_protected_key
            
    # 2. Normal Eviction: Prefer Probationary items
    if lru_probation_key is not None:
        return lru_probation_key
        
    # 3. Fallback: If no probation items exist, evict LRU Protected
    return lru_protected_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Promotion: If a Probation item is hit, it moves to Protected.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # SLRU Logic: A hit on a Probation item promotes it to Protected
        if not meta['protected']:
            meta['protected'] = True
            
        m_access_info[obj.key] = meta
    else:
        # Recovery if metadata missing
        m_access_info[obj.key] = {
            'last_access': current_time,
            'protected': False # Start in probation conservatively
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost, insert as Protected (it proved its worth by returning).
    3. Else, insert as Probation.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # Initialize metadata
    # If it was in ghost, it skips probation and goes straight to Protected
    m_access_info[obj.key] = {
        'last_access': current_time,
        'protected': is_ghost 
    }
    
    # Clean up ghost entry
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Cleanup Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keep it proportional to cache capacity to track relevant history
    max_ghost_size = int(cache_snapshot.capacity * GHOST_RATIO)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted ghost key
        # (Python dicts 3.7+ preserve insertion order)
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn34oz1gl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj6jgkr_8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplyk74pfl.pickle

Iteration 98: New subsample score 1.016362 is not better than old score 1.043636, skipping
Iteration 99: Selected program 25 score: 0.23474870833333328
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmphrk_pr_v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplztzwtnw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp74z2kinx.pickle

Iteration 99: Proposed new text for program: import math

# --- Metadata Storage ---
# Maps: key -> (frequency, last_access_time)
m_access_info = dict()

# Ghost Registry
# Maps: key -> frequency_at_eviction
# Used to restore the "heat" of an object if it returns to the cache soon after eviction.
m_ghost_registry = dict()

def get_victim_score(key, current_time, obj_size, freq, last_access_time):
    '''
    Calculates the "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Formula: Hyperbolic Caching variant
    Score = (Age * Size) / Frequency
    '''
    
    # 1. Age: How long since it was last touched?
    # We add 1 to avoid multiplication by zero if checked in the exact same tick,
    # ensuring the size component still matters.
    age = (current_time - last_access_time) + 1
    
    # 2. Size: We penalize large objects linearly.
    # Minimizing miss rates (maximizing object hit rate) is equivalent to the 
    # Knapsack problem where value=1. Density = 1/Size.
    # Therefore, we prefer to evict items with high Size.
    size_penalty = max(obj_size, 1)
    
    # 3. Frequency: We protect frequently used items.
    # Higher frequency reduces the eviction score.
    denominator = max(freq, 1)
    
    # Combine factors
    # Older items -> Higher Score
    # Larger items -> Higher Score
    # More frequent items -> Lower Score
    score = (age * size_penalty) / denominator
    
    return score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score using Hyperbolic Caching logic.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # Scan all candidates to find the one with the highest eviction score
    for key in candidate_keys:
        # Retrieve metadata
        # Default to neutral values if metadata is missing (defensive coding)
        freq = 1
        last_access = current_time
        
        if key in m_access_info:
            freq, last_access = m_access_info[key]
            
        # Retrieve size from the cache snapshot object
        cached_obj = cache_snapshot.cache[key]
        cached_size = cached_obj.size
        
        score = get_victim_score(key, current_time, cached_size, freq, last_access)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update last_access_time (Recency).
    2. Increment Frequency (LFU).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing frequency
    old_freq = 1
    if obj.key in m_access_info:
        old_freq, _ = m_access_info[obj.key]
    
    # Increment frequency
    # We cap frequency (e.g., at 100) to prevent long-lived items from becoming 
    # permanently immortal (Cache Pollution), allowing the cache to adapt to drift.
    new_freq = min(old_freq + 1, 100)
    
    m_access_info[obj.key] = (new_freq, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Check Ghost Registry to restore frequency if this is a "return visitor".
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Ghost Check:
    # If the item is in the ghost registry, it means it was part of the working set
    # but was evicted recently. We restore its frequency so it doesn't have to start
    # from 1 again. This prevents thrashing.
    if obj.key in m_ghost_registry:
        restored_freq = m_ghost_registry[obj.key]
        # Boost it slightly, but respect the cap
        initial_freq = min(restored_freq + 1, 100)
        # Remove from ghost as it is now in active cache
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Snapshotting its frequency).
    3. Prune Ghost Registry size.
    '''
    global m_access_info, m_ghost_registry
    
    # 1. Archive metadata to Ghost Registry
    if evicted_obj.key in m_access_info:
        freq, _ = m_access_info[evicted_obj.key]
        m_ghost_registry[evicted_obj.key] = freq
        del m_access_info[evicted_obj.key]
    else:
        m_ghost_registry[evicted_obj.key] = 1
        
    # 2. Maintain Ghost Registry Size
    # We allow the ghost registry to be larger than the cache (e.g., 2x)
    # to capture longer cyclic patterns.
    # Approximation of capacity based on current object count.
    max_ghost_size = max(len(cache_snapshot.cache), 100) * 2
    
    if len(m_ghost_registry) > max_ghost_size:
        # Prune the oldest inserted item in the ghost registry.
        # Python 3.7+ dicts maintain insertion order, so the first key is the oldest.
        iterator = iter(m_ghost_registry)
        try:
            oldest_key = next(iterator)
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn3gcjz2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2342pd4u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpat3d4_v6.pickle

Iteration 99: New subsample score 0.076171 is not better than old score 0.076221, skipping
Iteration 100: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcwnfcvnk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9mhu0rd0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvssh2962.pickle

Iteration 100: Proposed new text for program: import math

# Metadata dictionaries
# Maps key -> { 'score': float, 'last_access': int }
m_metadata = dict()

# Constants for the CRF (Combined Recency and Frequency) weighting
# A half-life concept: how much weight does a past access retain?
# 2^(-lambda * delta_t). 
# A smaller lambda means history matters more (LFU-like).
# A larger lambda means history decays fast (LRU-like).
LAMBDA = 1.0 / 10000.0 
DEFAULT_INIT_SCORE = 0.5 # New items start weaker than a repeated hit (1.0 + decay)

def _get_current_score(key, current_time):
    """
    Calculates the score of a key at the current time based on its
    last computed score and the time elapsed.
    Formula: CurrentScore = LastScore * e^(-lambda * (current_time - last_access))
    """
    global m_metadata, LAMBDA
    
    if key not in m_metadata:
        return 0.0
        
    data = m_metadata[key]
    last_score = data['score']
    last_access = data['last_access']
    
    # Calculate decay factor
    delta_t = current_time - last_access
    decay = math.exp(-LAMBDA * delta_t)
    
    return last_score * decay

def _update_weight(key, current_time, is_hit):
    """
    Updates the weight of a key.
    On access: NewScore = (OldScore * decay) + 1.0
    """
    global m_metadata, LAMBDA, DEFAULT_INIT_SCORE
    
    current_score = _get_current_score(key, current_time)
    
    if is_hit:
        # Reinforce: Add 1.0 to the decayed score
        new_score = current_score + 1.0
    else:
        # Insert:
        # If it has a history (Ghost), we resurrect it with its decayed score + init boost.
        # If it's brand new, it starts with a modest score.
        if current_score > 0.001: 
             # It was in ghost cache, reward it for coming back
            new_score = current_score + 1.0
        else:
            # Brand new item
            new_score = DEFAULT_INIT_SCORE

    m_metadata[key] = {
        'score': new_score,
        'last_access': current_time
    }

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest computed CRF score.
    Tie-breaking: Largest size (to clear more space), then LRU.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count
    
    # We look for the item with the minimal current score
    for key in candidate_keys:
        score = _get_current_score(key, current_time)
        
        # Optimization: If score is very low, close to 0, just evict immediately
        # to save iteration time.
        if score < 1e-6:
            return key
            
        if score < min_score:
            min_score = score
            victim_key = key
        elif score == min_score:
            # Tie-breaker: Evict the larger object to free more space? 
            # Or evict the older one? Let's use LRU logic for ties.
            # (The score calculation naturally handles LRU via decay, 
            # but if scores are identical, we check strict last_access).
            if m_metadata[key]['last_access'] < m_metadata[victim_key]['last_access']:
                victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update score using weighting function + 1.0
    '''
    _update_weight(obj.key, cache_snapshot.access_count, is_hit=True)

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize score.
    '''
    _update_weight(obj.key, cache_snapshot.access_count, is_hit=False)

    # Periodic cleanup of metadata to prevent memory leaks
    # We can do this simply by size check
    global m_metadata
    if len(m_metadata) > 2 * cache_snapshot.capacity and len(m_metadata) > 5000:
        # Remove oldest items that aren't in cache
        # This is an expensive op, so we do it rarely or use a simpler heuristic.
        # Here we just blindly cut if it gets too big (simple pruning)
        keys_in_cache = set(cache_snapshot.cache.keys())
        keys_to_del = []
        count = 0
        for k in m_metadata:
            if k not in keys_in_cache:
                keys_to_del.append(k)
                count += 1
                if count > 1000: break
        for k in keys_to_del:
            del m_metadata[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Do not remove metadata. 
    The item remains in `m_metadata` so if it returns, it retains some "heat" (Ghost Cache).
    The score will naturally decay over time via the formula.
    '''
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuo52clw1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1p41ateo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppgskbf5z.pickle

Iteration 100: New subsample score 0.508112 is better than old score 0.48739499999999997. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvheufn1_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpehnhmk4m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiscslwxe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplk2x8vi8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp6kohgs65.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgi45hd1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpdfz28vwq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmvm1jtqy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpkfkbv7v2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5yh_bi3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvgapdeem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp28dpoa0_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmfo8v8rs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp351jfrmr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4tkkz0mb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpn1vktbpm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmma3wpdq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpuuzwkycf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5_kbef11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpg514mnvm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvp1ud7t6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwi5nkzhy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0h66l5r7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpesg23gy8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpolzdmuh1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpyg2ry3e8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz1jigw24.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpopqmbk6o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpr3ud63ku.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmph7v45g0u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0ewe2mv9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprgz626g_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmprhd_tn8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo55bhwjk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4jv_qrer.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiaz82pd8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnmiraf2x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqgv3_tph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiyycwa4c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpzvg1913d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0vqkj7s5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaj4spg5b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp5wcliqo5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz7z6b4m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpu_4_oriv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpz__dhtsu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwslqf069.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmppseehcvy.pickle

Iteration 100: Full valset score for new program: 0.23705199999999993
Iteration 100: Full train_val score for new program: 0.23705199999999993
Iteration 100: Individual valset scores for new program: [0.503013, 0.476068, 0.486577, 0.435583, 0.499759, 0.48945, 0.26555, 0.458907, 0.540937, 0.531017, 0.083333, 0.395204, 0.034686, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.370698, 0.026556, 0.058672, 0.058672, 0.288554, 0.356855, 0.796425, 0.894232, 0.039832, 0.038636, 0.045558, 0.0, 0.0, 0.723111, 0.083333, 0.067079, 0.018218, 0.641937, 0.125461, 0.040754, 0.037263, 0.039703, 0.052632, 0.366667, 0.041854, 0.03899, 0.466258, 0.078431]
Iteration 100: New valset pareto front scores: [0.510278, 0.486516, 0.493364, 0.443318, 0.506177, 0.493313, 0.2811, 0.498624, 0.541294, 0.531017, 0.091667, 0.395204, 0.040045, 0.0, 0.021521, 0.021274, 0.020331, 0.023756, 0.022922, 0.272227, 0.402163, 0.026556, 0.058672, 0.058672, 0.332169, 0.395161, 0.857994, 0.894232, 0.176101, 0.038636, 0.045558, 0.039527, 0.029983, 0.755106, 0.083333, 0.067961, 0.207287, 0.641937, 0.125461, 0.150541, 0.141723, 0.163603, 0.052632, 0.366667, 0.182283, 0.141374, 0.466258, 0.081699]
Iteration 100: Full valset pareto front score: 0.2641091041666666
Iteration 100: Updated valset pareto front programs: [{22}, {4}, {22}, {25}, {4}, {22}, {6}, {0, 2, 3, 5, 6, 7, 9, 14, 15, 20}, {3, 20}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {3, 5, 6, 7, 12, 13, 20, 22}, {26}, {1, 3, 4, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {26, 6, 15}, {26, 6, 15}, {26, 20, 6, 15}, {26, 20, 6, 15}, {26, 20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {3}, {26, 6, 15}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26}, {1, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26}, {1, 22}, {15}, {4}, {26}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {1, 3, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26}, {1, 4, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25}, {22}, {26, 20, 6, 15}, {1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {22}, {22}, {22}, {1, 4, 5, 7, 8, 10, 11, 12, 13, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26}, {3, 5, 6, 7, 20, 26}, {22}, {22}, {1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {1, 3, 4, 8, 10, 11, 16, 17, 18, 19, 21, 22, 23, 24}]
Iteration 100: Best valset aggregate score so far: 0.2576176875000001
Iteration 100: Best program as per aggregate score on train_val: 22
Iteration 100: Best program as per aggregate score on valset: 22
Iteration 100: Best score on valset: 0.2576176875000001
Iteration 100: Best score on train_val: 0.2576176875000001
Iteration 100: Linear pareto front program index: 22
Iteration 100: New program candidate index: 26
Iteration 101: Selected program 6 score: 0.23626714583333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpnpolv9na.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpociwjrgt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk9qtj_vd.pickle

Iteration 101: Proposed new text for program: import math

# Global Metadata
# m_frequency: Tracks the global access count of keys (Frequency). 
# This acts as our "Ghost Cache" / History.
m_frequency = {}

# m_priority: Tracks the current computed priority of objects IN the cache.
# Used to determine eviction victims quickly.
m_priority = {}

# m_inflation: The "L" value in GDSF algorithm.
# Represents the priority value of the last evicted object.
# This serves as a "logical clock" for aging.
m_inflation = 0.0

# Constants
# Limit the size of the history to prevent memory issues
MAX_HISTORY_SIZE = 50000

def evict(cache_snapshot, obj):
    '''
    GDSF (Greedy Dual-Size Frequency) Eviction Policy.
    
    Formula: Priority = Inflation_L + (Frequency / Size)
    
    Logic:
    1. Identify the object with the lowest Priority in the cache.
    2. Update the global Inflation_L to match that object's priority.
       (This effectively "ages" all other objects without iterating them).
    3. Evict that object.
    '''
    global m_priority, m_inflation
    
    # If the cache is empty, we cannot evict.
    if not cache_snapshot.cache:
        return None

    # We need to find the key with the minimum priority value.
    # A linear scan is O(N). For typical simulation cache sizes, this is acceptable
    # and ensures perfect accuracy for the GDSF algorithm.
    victim_key = None
    min_priority = float('inf')
    
    # Iterate over currently cached keys to find the victim
    for k in cache_snapshot.cache:
        # Default to 0.0 priority if something is out of sync, though unlikely
        p = m_priority.get(k, 0.0)
        
        if p < min_priority:
            min_priority = p
            victim_key = k
            
    # Update the global inflation value (L) to the priority of the evicted object.
    # New objects will now need a higher base priority to survive.
    if victim_key is not None:
        m_inflation = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on NEW Frequency and CURRENT Inflation.
       This resets the object's "aging", bringing it to the front.
    '''
    global m_frequency, m_priority, m_inflation
    
    key = obj.key
    
    # 1. Update Frequency
    current_freq = m_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_frequency[key] = new_freq
    
    # 2. Update Priority
    # Priority = L + (Freq / Size)
    # Using 'L' here is crucial: it effectively marks the object as "recently used"
    # relative to the current state of the cache.
    safe_size = max(1, obj.size) # Avoid division by zero
    priority = m_inflation + (new_freq / safe_size)
    m_priority[key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Restore Frequency (Ghost Cache check).
    2. Calculate initial Priority.
    3. Clean metadata if necessary.
    '''
    global m_frequency, m_priority, m_inflation
    
    key = obj.key
    
    # 1. Frequency Management
    # If key is already in m_frequency, it was seen before (Ghost).
    # We keep the old frequency to give it a "warm" start.
    if key not in m_frequency:
        m_frequency[key] = 1
    # Else: It retains its old frequency value automatically.
    
    freq = m_frequency[key]
    
    # 2. Set Priority
    safe_size = max(1, obj.size)
    priority = m_inflation + (freq / safe_size)
    m_priority[key] = priority
    
    # 3. Cleanup metadata if it grows too large
    if len(m_frequency) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove the object from the active priority map.
    2. We DO NOT remove it from m_frequency (preserve history).
    '''
    global m_priority
    
    key = evicted_obj.key
    if key in m_priority:
        del m_priority[key]

def _cleanup_metadata(cache_snapshot):
    '''
    Helper to prune the frequency history map to prevent memory leaks.
    Removes keys that are not in the cache and have low frequency.
    '''
    global m_frequency, MAX_HISTORY_SIZE
    
    # We want to remove items from history that are:
    # 1. Not currently in the cache (don't delete active metadata)
    # 2. Have low importance (frequency <= 1)
    
    cached_keys = set(cache_snapshot.cache.keys())
    keys_to_remove = []
    
    # Heuristic cleanup: remove one-hit wonders first
    for k, v in m_frequency.items():
        if k not in cached_keys and v <= 1:
            keys_to_remove.append(k)
            # Remove in chunks to keep operation time somewhat bounded
            if len(keys_to_remove) > 2000:
                break
    
    for k in keys_to_remove:
        del m_frequency[k]
        
    # If still too big, force remove random old history items
    if len(m_frequency) > MAX_HISTORY_SIZE:
        keys_to_remove = []
        count = 0
        for k in m_frequency:
            if k not in cached_keys:
                keys_to_remove.append(k)
                count += 1
            if count > 2000:
                break
        for k in keys_to_remove:
            del m_frequency[k]
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfk2ptypj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqrupgea6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpj8zbvqqn.pickle

Iteration 101: New subsample score 0.48848899999999995 is not better than old score 0.526247, skipping
Best program from optimization: import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
Best program validation score: 0.2576176875000001
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpqk2dk0xc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplm8szum_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwcty0z28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpry0x7o9n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk73ottbi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps9s81wt0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcv6lyp2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpmkf2kj65.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp4qqfnxyr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp_pkwsgjm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpaocoegut.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpotyd35s5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpo7su1wf_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp91cdzx6w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplzqktqqd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpjjk2qps8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0z4w_8je.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcp_0ev3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpxttbznsy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp15dm3kr0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpp6m5fm1f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp7fo78_lq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwz2ghf28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplna3rjhc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpty8ar241.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpwotq7b91.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp78hr9s7i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp1g_tl0ts.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp2kyx0zux.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpknvu5jf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw48dmvsa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmps8dnhyqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmplskw66tn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpb8uj2rfv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp9urgf9it.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpytr1z_fz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpgox4900_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpvmvzetey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmptunxas3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpiyjyyfcg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpka0puq0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpk_0i1i8b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp8r3c7ioj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpcaewsfnb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpubq_84c9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmp0o8b2_jz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpfcbgxu7f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gemini3_2/tmp/tmpw_4f2btw.pickle

Optimized program test score (avg over test): 0.2576176875000001
