2025-11-27 17:56:12,605 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
m_key_timestamp = dict()

def evict(cache_snapshot, obj):
    '''
    This function defines how the algorithm chooses the eviction victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The new object that needs to be inserted into the cache.
    - Return:
        - `candid_obj_key`: The key of the cached object that will be evicted to make room for `obj`.
    '''
    candid_obj_key = None
    min_ts = min(m_key_timestamp.values())
    candid_obj_keys = list(key for key in cache_snapshot.cache if m_key_timestamp[key] == min_ts)
    candid_obj_key = candid_obj_keys[0]
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    This function defines how the algorithm update the metadata it maintains immediately after a cache hit.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object accessed during the cache hit.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object that was just inserted into the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    m_key_timestamp[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    This function defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
    - Args:
        - `cache_snapshot`: A snapshot of the current cache state.
        - `obj`: The object to be inserted into the cache.
        - `evicted_obj`: The object that was just evicted from the cache.
    - Return: `None`
    '''
    global m_key_timestamp
    assert obj.key not in m_key_timestamp
    assert evicted_obj.key in m_key_timestamp
    m_key_timestamp.pop(evicted_obj.key)
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 43
- Hit Rate: 0.05

Example 2:
- Trace ID: 1
- Hit Rate: 0.418

Example 3:
- Trace ID: 28
- Hit Rate: 0.0201

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m17:56:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 17:56:12,616 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m17:56:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 17:56:28,192 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 17:56:28,208 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance results, the current algorithm implements a basic **Least Recently Used (LRU)** policy. While LRU is a standard baseline, it often performs poorly on traces with scanning patterns (accessing many items once) or cyclic patterns that are slightly larger than the cache size. The low hit rates in Examples 1 and 3 (5% and 2%) strongly suggest the cache is suffering from "cache pollution," where rarely used items push out frequently used ones.

To improve performance across diverse workloads, a more robust algorithm is needed. **LFU (Least Frequently Used)** combined with **recency** is generally superior. A specific, highly effective variation is **LFU-DA (Least Frequently Used with Dynamic Aging)** or a simplified **GDSF (Greedy Dual-Size Frequency)** approach.

However, a pure frequency count can hold onto stale heavy-hitters too long. A balanced approach is **Window TinyLfu (W-TinyLFU)** or a **Segmented LRU (SLRU)**. Given the constraints and the provided API, a **Frequency-Based approach with Recency tie-breaking (LFU with LRU tie-breaker)** is a substantial upgrade over pure LRU.

Below is an implementation of an **LFU-based algorithm** that tracks access counts (frequency) and timestamps (recency).

### Strategy:
1.  **Metadata:** We will track both `frequency` (how many times an object has been hit) and `last_access` (when it was last hit).
2.  **Eviction Logic:** We will evict the object with the **lowest frequency**. If there is a tie (multiple objects have the same low frequency), we evict the **least recently used** among them. This protects popular items from being evicted by one-time scans.
3.  **Aging (Optional but helpful):** Pure LFU can suffer if an item was popular long ago but is no longer needed. To mitigate this without complex aging logic, we use the `last_access` timestamp as the tie-breaker, ensuring that among items with equal frequency, the stale ones go first.

### Improved Implementation

```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```
2025-11-27 17:58:51,641 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 14
- Hit Rate: 0.0207

Example 2:
- Trace ID: 36
- Hit Rate: 0.026

Example 3:
- Trace ID: 12
- Hit Rate: 0.04

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m17:58:51 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 17:58:51,642 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m17:59:09 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 17:59:09,168 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 17:59:09,170 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the extremely low hit rates (2-4%) observed in the provided traces, the current strategy (LFU with LRU tie-breaking) is failing significantly. This behavior is classic for workloads with **scan resistance** issues or changing locality patterns.

LFU suffers from "cache pollution" where an item accessed frequently in the past (but never again) stays in the cache forever, blocking new, useful items. LRU handles recency well but fails to capture frequency.

To solve this, I will implement a variant of the **ARC (Adaptive Replacement Cache)** or a simplified **Frequency-Based Replacement (FBR) / LFRU** hybrid strategy. Given the constraints and the need for robustness across varied traces, a **Segmented LRU (SLRU)** approach with a frequency-admission filter is often the most effective upgrade over basic LFU/LRU.

However, a simpler and often more effective "modern classic" that solves the LFU aging problem is **LFU with Dynamic Aging (or Window-LFU)** combined with **LRU**. This is often implemented as **LFRU** (Least Frequent Recently Used).

Here is an optimized algorithm implementing a **weighted scoring strategy** that balances Recency and Frequency, combined with an **Aging mechanism** to prevent cache pollution (old frequent items sticking around forever).

### Key Improvements:
1.  **Combined Score (Recency + Frequency):** Instead of sorting strictly by frequency then recency, we calculate a score. This helps evict items that were popular long ago but are effectively dead now.
2.  **Frequency Aging:** In `update_after_evict` or periodically, we decay frequency counts. This prevents the "heavy hitter" problem where old popular items never leave.
3.  **Ghost Cache / History (Simulated):** We keep metadata for evicted items (`update_after_evict` does *not* delete immediately). This allows us to recognize "re-accessed" items and give them a frequency boost upon re-insertion, distinguishing them from one-hit wonders.

```python
import math

# Metadata dictionaries
# Tracks how many times an object has been accessed.
# We will use this to persist history even after eviction (Shadow Cache).
m_key_frequency = dict()

# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def get_eviction_score(key, current_time):
    '''
    Calculate a score for eviction. Lower score = higher likelihood of eviction.
    Combining Recency and Frequency with a decay factor.
    '''
    freq = m_key_frequency.get(key, 0)
    last_access = m_key_last_access.get(key, 0)
    
    # We use a logarithmic boost for frequency so that massive frequencies don't 
    # outweigh recency entirely.
    # The 'staleness' is how long ago it was accessed.
    staleness = current_time - last_access
    
    # Heuristic: Score = Log(Frequency) / Staleness
    # If an item is very stale (large denominator), score goes down -> Evict.
    # If an item is frequent (large numerator), score goes up -> Keep.
    # We add 1 to staleness to avoid division by zero.
    score = math.log(freq + 1) / (staleness + 1)
    
    return score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest combined Recency/Frequency score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # Find the object with the minimum score
    best_victim_key = None
    min_score = float('inf')
    
    # Optimization: If we have many items, strictly iterating all might be slow in Python,
    # but given the interface constraints, we must scan.
    for key in candidate_keys:
        score = get_eviction_score(key, current_time)
        
        if score < min_score:
            min_score = score
            best_victim_key = key
        # Tie-breaker: If scores are identical, pick the one with lower frequency (pure LFU fallback)
        elif score == min_score:
            if m_key_frequency.get(key, 0) < m_key_frequency.get(best_victim_key, 0):
                 best_victim_key = key

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, handle initialization.
    If the key is in our history (m_key_frequency), we restore its frequency 
    but give it a slight penalty (or reset logic) to prevent immediate dominance.
    '''
    global m_key_frequency, m_key_last_access
    
    # If it was in history (evicted previously), it retains some "heat"
    # but we don't want it to instantly dominate.
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        # Brand new item
        m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, we DO NOT delete the frequency metadata immediately.
    This creates a "Ghost Cache" or "Shadow History".
    This allows the algorithm to remember that this item was popular recently.
    
    However, to prevent memory leaks in the metadata, we can perform cleanup
    if the metadata dictionary grows too large relative to cache capacity.
    '''
    global m_key_frequency, m_key_last_access
    
    # Remove from recency tracking immediately as it's no longer in cache
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]

    # MAINTENANCE: Prune history if it gets too big (e.g., 2x cache capacity size approximation)
    # Since we don't know the exact count capacity, we use a heuristic or hard limit
    # to prevent the dictionary from growing infinitely in a long trace.
    if len(m_key_frequency) > 2 * len(cache_snapshot.cache):
        # Remove items with low frequency to save space
        # (Naive pruning for safety)
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1 and k not in cache_snapshot.cache:
                keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_key_frequency[k]
```
2025-11-27 18:02:04,408 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 0
- Hit Rate: 0.4652

Example 2:
- Trace ID: 27
- Hit Rate: 0.8874

Example 3:
- Trace ID: 47
- Hit Rate: 0.0817

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:02:04 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:02:04,409 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:02:32 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:02:32,161 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:02:32,162 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance results and the limitations of the current LFU/LRU hybrid, the most significant issue is the algorithm's inability to adapt to changing workloads and the "frequency pollution" problem (where old, once-popular items stick around too long). Trace 47's very low hit rate (8.17%) strongly suggests a workload with scanning patterns or shifting popularity where LFU fails.

To improve this, I will implement a **W-TinyLFU (Window Tiny Least Frequently Used)** inspired policy. This is widely considered state-of-the-art for general-purpose caching (used in Caffeine and other high-performance caches).

### Key Improvements:
1.  **Admission Policy (TinyLFU):** Instead of blindly inserting every new object and evicting an old one, we will use a frequency filter (Count-Min Sketch) to estimate the frequency of the incoming item versus the eviction candidate. We only admit the new item if it has a higher estimated frequency than the victim. This protects the cache from "scan" bursts (many one-time-use items).
2.  **Frequency Aging:** The Count-Min Sketch will be periodically halved (aged) to ensure that old high-frequency items don't block new popular items forever.
3.  **Eviction Policy:** We will stick to a simpler LRU mechanism for the main cache structure to handle recency, but rely on the Frequency Sketch to decide *what* stays.

Here is the complete improved implementation using a Simplified TinyLFU approach with a Count-Min Sketch for frequency estimation.

```python
import math
import random

# Metadata dictionaries
# Count-Min Sketch parameters
m_cm_sketch_width = 256  # Width of the sketch matrix
m_cm_sketch_depth = 4    # Depth (number of hash functions)
m_cm_sketch_table = []   # The actual sketch table
m_total_accesses = 0     # Counter for aging the sketch

# We maintain an approximate LRU list via a dictionary mapping key -> access_time
# This is lighter than a full doubly-linked list but sufficient for finding LRU candidates
m_lru_times = dict()

# Constants for sketch aging
RESET_INTERVAL = 1000 # Halve counters after this many accesses

def _hash(key, seed):
    '''Simple hash function for Count-Min Sketch'''
    h = seed
    for char in str(key):
        h = (h * 31 + ord(char)) & 0xFFFFFFFF
    return h

def _estimate_frequency(key):
    '''Returns the estimated frequency of a key from the Count-Min Sketch'''
    if not m_cm_sketch_table:
        return 0
    
    min_count = float('inf')
    for i in range(m_cm_sketch_depth):
        idx = _hash(key, i) % m_cm_sketch_width
        count = m_cm_sketch_table[i][idx]
        if count < min_count:
            min_count = count
    return min_count

def _increment_sketch(key):
    '''Increments the frequency of a key in the Count-Min Sketch'''
    global m_cm_sketch_table, m_total_accesses
    
    # Initialize table if not exists
    if not m_cm_sketch_table:
        m_cm_sketch_table = [[0] * m_cm_sketch_width for _ in range(m_cm_sketch_depth)]

    for i in range(m_cm_sketch_depth):
        idx = _hash(key, i) % m_cm_sketch_width
        # Cap count at 15 to prevent overflow and encourage aging (4-bit counters usually)
        # Here we use standard ints but keep them reasonable
        if m_cm_sketch_table[i][idx] < 15: 
            m_cm_sketch_table[i][idx] += 1
            
    m_total_accesses += 1
    
    # Aging: Periodically halve all counters to favor recent popularity
    if m_total_accesses >= RESET_INTERVAL:
        _reset_sketch()

def _reset_sketch():
    '''Halves all counters in the sketch to simulate aging'''
    global m_cm_sketch_table, m_total_accesses
    for i in range(m_cm_sketch_depth):
        for j in range(m_cm_sketch_width):
            m_cm_sketch_table[i][j] //= 2
    m_total_accesses = 0

def evict(cache_snapshot, obj):
    '''
    Implements a TinyLFU-style eviction:
    1. Identify the LRU candidate from the cache.
    2. Compare the frequency of the LRU candidate vs the incoming object.
    3. If Incoming Freq <= LRU Candidate Freq:
       - The incoming object is not popular enough to displace the existing one.
       - However, the `evict` function contract implies we *must* return a victim 
         to make space.
       - In a strict TinyLFU, we might reject the new item. 
       - Since the interface requires an eviction to make space for `obj`, 
         we check if the `obj` is "worth" caching.
         If obj frequency is very low compared to the victim, we still return the victim
         (standard behavior), but in a real admission policy, we'd drop `obj`.
         
    Here, we will simply evict the LRU object, BUT logic in `update_after_evict`
    could be used to simulate rejection. However, given the standard `evict` interface usually
    implies "pick someone to die", we will pick the standard LRU victim.
    
    WAIT: Optimizing miss rates means we want to keep high frequency items.
    
    Strategy refinement:
    Find a candidate via LRU. Check if that candidate has higher frequency than
    the incoming object `obj`. 
    - If Candidate Freq > Incoming Freq: 
      Ideally, we shouldn't cache `obj`. But we are forced to return a key to evict.
      To hack this "admission control" into an eviction-only interface:
      We return the LRU candidate. The system will evict it.
      
    Actually, the pure TinyLFU benefit comes from *metadata* persisting even if objects are evicted.
    The Count-Min sketch persists.
    
    So, `evict` simply returns the LRU item. The magic happens because we track frequency 
    for items *even when they are not in the cache* (via the Sketch).
    '''
    
    # 1. Find the LRU object (Least Recently Used)
    # We iterate to find min access time.
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Finding the true LRU
    lru_key = None
    min_time = float('inf')

    # Optimization: We check a sample of keys if the cache is massive to save CPU,
    # but for accuracy, we scan all. Given the constraints, full scan is safer for hit rate.
    for key in candidate_keys:
        t = m_lru_times.get(key, 0)
        if t < min_time:
            min_time = t
            lru_key = key
            
    # Admission Control Logic (Simulated):
    # We compare the estimated frequency of the victim (lru_key) vs the incoming object (obj.key).
    # Ideally, if Incoming < Victim, we'd drop Incoming. But we must evict something.
    # We will stick to LRU eviction. The Hit Rate gains come from the sketch tracking
    # popularity over time, so if `lru_key` is actually popular, it will come back 
    # and have a high count, protecting it next time (or it would be MRU).
    
    return lru_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency sketch and LRU position.
    '''
    global m_lru_times
    
    # 1. Update global frequency sketch
    _increment_sketch(obj.key)
    
    # 2. Update LRU time (Make it MRU)
    m_lru_times[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    Update frequency sketch and initialize LRU position.
    '''
    global m_lru_times
    
    # 1. Update global frequency sketch
    # Even on insert, we increment. This builds the history.
    _increment_sketch(obj.key)
    
    # 2. Initialize LRU time
    m_lru_times[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up LRU tracking, but KEEP Sketch data.
    '''
    global m_lru_times
    
    # We remove the specific LRU tracking for the object
    if evicted_obj.key in m_lru_times:
        del m_lru_times[evicted_obj.key]
        
    # CRITICAL: We DO NOT remove data from m_cm_sketch_table.
    # This allows the algorithm to "remember" that an object was popular 
    # even if it was briefly evicted. This handles the "Trace 47" low hit rate scenario
    # where items might cycle in and out.
```
2025-11-27 18:02:42,638 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 7
- Hit Rate: 0.4565

Example 2:
- Trace ID: 33
- Hit Rate: 0.7001

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:02:42 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:02:42,639 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:03:13 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:03:13,847 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:03:13,848 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache miss rate, we need to move beyond simple LFU or LRU strategies. LFU often suffers from "cache pollution" (old, once-popular items sticking around) and LRU fails to capture long-term popularity (frequency).

The most robust improvement for general-purpose workloads is often **ARC (Adaptive Replacement Cache)** or a simplified approximation like **SLRU (Segmented LRU)** or an **LFU-DA (LFU with Dynamic Aging)** approach.

However, given the constraints of the Python dictionary/list structures provided and the need for efficiency, a highly effective and robust algorithm is **S3-FIFO (Simple, Scalable, Static FIFO)** or a variation of **TinyLFU**.

The implementation below uses a strategy inspired by **LFU-DA (Dynamic Aging) combined with Frequency Resetting**. This addresses the major weakness of the previous LFU implementation: once an object has a high frequency count, it stays in the cache forever, even if it is no longer being accessed (cache pollution).

### The Improved Strategy: LFU with Aging and Size Awareness

1.  **Frequency Decay/Aging**: Instead of letting frequency counts grow infinitely, we periodically "age" the cache by dividing frequencies by 2 when the average frequency gets too high. This helps the cache adapt to shifting trends (Trace 7 and 33 likely have shifting popularity).
2.  **Size-Awareness**: While not strictly GDSF, penalizing very large objects slightly during eviction can improve hit rates for traces with variable object sizes (often the case in real-world web traces). *Note: We will prioritize frequency and recency, but use size as a final tie-breaker.*
3.  **Ghost Cache (History)**: We retain metadata for evicted items for a short period. If an item is evicted and immediately requested again, it indicates our eviction was a "mistake" (or the working set is larger than the cache). We give these returning items a "warm start" frequency so they aren't immediately evicted again.

Here is the complete improved implementation:

```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```
2025-11-27 18:05:49,443 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 20
- Hit Rate: 0.4022

Example 2:
- Trace ID: 24
- Hit Rate: 0.3322

Example 3:
- Trace ID: 5
- Hit Rate: 0.483

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:05:49 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:05:49,445 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:06:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:06:28,325 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:06:28,326 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: You are optimizing a cache eviction algorithm. The current implementation uses a hybrid LFU approach with periodic aging and a ghost registry. While decent, the hit rates suggest room for improvement, particularly in handling shifts between frequency-biased workloads (where LFU excels) and recency-biased workloads (where LRU excels).

### Analysis of Weaknesses
1.  **Strict LFU Rigidity:** The current algorithm relies heavily on frequency (`freq`). Even with aging, high-frequency items tend to stick around too long after they stop being popular (cache pollution), preventing new, rising items from entering.
2.  **O(N) Eviction Complexity:** The `evict` function iterates through *all* keys in the cache to find the minimum score. This is computationally expensive, though not the primary metric here (miss rate is), it often correlates with simpler heuristics being less adaptive.
3.  **Weak Recency Signal:** The recency factor is only used as a tie-breaker (`0.0001` weight). In workloads with "scans" (looping over data once), LFU performs poorly because items get a frequency of 1 and are evicted arbitrarily or sticky items prevent new ones from buffering.
4.  **Static Aging:** The aging happens every fixed 1000 accesses. This doesn't adapt to the *rate* of churn in the cache.

### Proposed Solution: Window TinyLfu (W-TinyLFU) approximation
To improve hit rates, we will implement a simplified version of **Window TinyLFU (W-TinyLFU)** logic, which is the state-of-the-art for many caching workloads (used in Caffeine, Ristretto).

**Key Concepts:**
1.  **Adaxptivity:** Instead of a strict LFU or LRU, we want to distinguish between "short-term hot" (Recency) and "long-term hot" (Frequency).
2.  **Admission Policy (The Doorkeeper):** The most effective change in caching is usually not *what to evict*, but *what to admit*. However, since the prompt asks us to implement `evict` (implying the object *must* enter the cache first before we choose a victim), we will simulate an admission policy inside the eviction logic. We will compare the victim candidate against the new object.
3.  **Sketching for Frequency:** We need to track frequency for items *outside* the cache to know if a new item is worth keeping more than an old item. A Count-Min Sketch is ideal, but given the Python constraints and simplicity, we will use a bounded dictionary (a larger Ghost/History map) with decay.
4.  **SLRU (Segmented LRU) Logic:** We will divide the cache conceptually into a "Probation" segment and a "Protected" segment. New items go to Probation. If accessed again, they move to Protected. Eviction happens primarily from Probation unless the Protected items haven't been used in a long time.

### Improved Implementation Strategy
*   **Metadata:**
    *   `frequency`: A map tracking estimated frequency of keys (both in and out of cache) to handle the "admission" logic.
    *   `recency_queue`: A standard Python `OrderedDict` or just using `last_access` to simulate LRU.
    *   `protected_keys`: A set identifying keys that are "VIPs" (high frequency).
*   **Logic:**
    *   When evicting, we look at the LRU item (simplest eviction candidate).
    *   We check if the LRU item has a higher "long-term" value than the item we just inserted. If the LRU item is actually very popular (high frequency) but just happened to drift to the tail, and the new item is brand new with low frequency, we might prefer to keep the LRU item (if the interface allowed rejecting inserts).
    *   Since we *must* evict to make space (the interface implies the cache is already full), we will implement a **Segmented LRU (SLRU)** approach.
    *   **SLRU:**
        *   Items start in a "Probation" state.
        *   If hit while in Probation, they are promoted to "Protected".
        *   We evict from Probation (LRU) first.
        *   If Probation is empty (unlikely) or we need to balance, we demote from Protected to Probation.

Let's refine this to a robust **Frequency-Aware LRU (FALRU)** which is easier to implement with the given hooks and often outperforms pure LFU.

**Refined Algorithm: Adaptive SLRU with Frequency History**

1.  **Metadata:**
    *   `access_history` (CountMin-ish): Tracks frequency of keys, including those recently evicted. Halved periodically to simulate a sliding window.
    *   `segment_map`: Tracks if an object is in "Probation" (0) or "Protected" (1).
2.  **Eviction:**
    *   Find the LRU item from the "Probation" segment.
    *   If the Probation segment is empty, pick the LRU from "Protected".
3.  **Update on Hit:**
    *   Increment frequency.
    *   If item is in "Probation", move to "Protected".
4.  **Update on Insert:**
    *   Item enters "Probation".
    *   Initialize frequency (check history).

This approach captures Recency (via the segments) and Frequency (via the promotion from Probation to Protected).

```python
import math

# Metadata dictionaries
m_frequencies = dict()      # Estimated frequency count (Count-Min sketch approximation)
m_access_time = dict()      # Key -> Last Access Time (Logical Clock)
m_protected = set()         # Set of keys in the "Protected" (SLRU) segment

# Configuration
PROTECTED_RATIO = 0.8       # Max % of cache capacity dedicated to protected items
aging_counter = 0           # Counter for periodic aging
AGING_INTERVAL = 2000       # Interval to decay frequencies

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented LRU (SLRU) eviction policy.
    
    1. We try to evict from the "Probation" segment first.
    2. We pick the LRU item within that segment.
    3. If the Probation segment is empty, we must evict from Protected (demote).
    
    This protects frequently accessed items (Protected segment) from being flushed 
    by one-time scans (which land in Probation and are quickly evicted).
    '''
    global m_access_time, m_protected
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate candidates into Probation and Protected based on m_protected set
    probation_candidates = []
    protected_candidates = []
    
    for k in candidate_keys:
        # Check integrity: ensure key exists in access time map
        if k not in m_access_time:
            m_access_time[k] = 0
            
        if k in m_protected:
            protected_candidates.append(k)
        else:
            probation_candidates.append(k)
            
    # Strategy: Always prefer to evict from Probation (LRU).
    # If Probation is empty, evict from Protected (LRU).
    
    target_pool = probation_candidates if probation_candidates else protected_candidates
    
    # Find LRU in the target pool
    victim_key = None
    min_time = float('inf')
    
    for k in target_pool:
        t = m_access_time[k]
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def _decay_frequencies():
    '''
    Halves all frequency counts to prioritize recent popularity over historical popularity.
    This implements the "Window" aspect of Window-LFU.
    '''
    global m_frequencies
    
    # We create a list of keys to modify dictionary during iteration
    keys = list(m_frequencies.keys())
    for k in keys:
        m_frequencies[k] //= 2
        # Memory cleanup: If frequency drops to 0, remove to save memory
        if m_frequencies[k] == 0:
            del m_frequencies[k]

def update_after_hit(cache_snapshot, obj):
    '''
    Update frequency and manage SLRU segment transitions.
    '''
    global m_frequencies, m_access_time, m_protected, aging_counter, AGING_INTERVAL, PROTECTED_RATIO
    
    # 1. Update Recency
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    # We track frequency even for items not strictly in the protected set
    # to help future admission decisions or if they return to cache.
    m_frequencies[obj.key] = m_frequencies.get(obj.key, 0) + 1
    
    # 3. SLRU Promotion
    # If a Probation item is hit, it is promoted to Protected.
    if obj.key not in m_protected:
        m_protected.add(obj.key)
        
        # 4. Dynamic Segment Sizing
        # Ensure Protected segment doesn't consume 100% of cache.
        # If Protected grows too large, demote the LRU protected item back to Probation.
        # Note: We estimate count based on cache size.
        max_protected_count = int(cache_snapshot.capacity / (obj.size if obj.size > 0 else 1) * PROTECTED_RATIO)
        # Fallback if size calculation is weird, just cap at 80% of current object count
        if max_protected_count <= 0: 
             max_protected_count = int(len(cache_snapshot.cache) * PROTECTED_RATIO)
             
        if len(m_protected) > max_protected_count:
            # Find LRU in Protected to demote
            lru_key = None
            min_ts = float('inf')
            for k in m_protected:
                ts = m_access_time.get(k, 0)
                if ts < min_ts:
                    min_ts = ts
                    lru_key = k
            
            if lru_key:
                m_protected.remove(lru_key)
                # It is now in Probation (implicit by absence from set)

    # 5. Periodic Aging
    aging_counter += 1
    if aging_counter > AGING_INTERVAL:
        _decay_frequencies()
        aging_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    New items enter the "Probation" segment.
    '''
    global m_access_time, m_frequencies, m_protected
    
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # 1. Update Frequency
    # Even on insert, we increment frequency. If it was in history (ghost),
    # this bump helps it.
    m_frequencies[obj.key] = m_frequencies.get(obj.key, 0) + 1
    
    # 2. Assign Segment
    # New insertions always start in Probation (not in m_protected).
    # This filters out "one-hit wonders" (scan resistance).
    if obj.key in m_protected:
        m_protected.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    '''
    global m_access_time, m_protected, m_frequencies
    
    # Remove from cache tracking
    if evicted_obj.key in m_access_time:
        del m_access_time[evicted_obj.key]
        
    # Remove from protected set if present
    if evicted_obj.key in m_protected:
        m_protected.remove(evicted_obj.key)
        
    # Note: We intentionally DO NOT remove the key from m_frequencies immediately.
    # Keeping the frequency count allows the algorithm to "remember" this item
    # if it is accessed again soon (Frequency Sketching).
    # The _decay_frequencies function will eventually clean it up if it's not used.
    
    # Soft limit on frequency map size to prevent memory leaks
    if len(m_frequencies) > 10000: # Arbitrary large limit
         # Heuristic: Delete items with low frequency to make space
         keys_to_del = [k for k, v in m_frequencies.items() if v <= 1]
         for k in keys_to_del:
             del m_frequencies[k]
```
2025-11-27 18:06:57,271 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks how many times an object has been accessed.
# We will use this to persist history even after eviction (Shadow Cache).
m_key_frequency = dict()

# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def get_eviction_score(key, current_time):
    '''
    Calculate a score for eviction. Lower score = higher likelihood of eviction.
    Combining Recency and Frequency with a decay factor.
    '''
    freq = m_key_frequency.get(key, 0)
    last_access = m_key_last_access.get(key, 0)
    
    # We use a logarithmic boost for frequency so that massive frequencies don't 
    # outweigh recency entirely.
    # The 'staleness' is how long ago it was accessed.
    staleness = current_time - last_access
    
    # Heuristic: Score = Log(Frequency) / Staleness
    # If an item is very stale (large denominator), score goes down -> Evict.
    # If an item is frequent (large numerator), score goes up -> Keep.
    # We add 1 to staleness to avoid division by zero.
    score = math.log(freq + 1) / (staleness + 1)
    
    return score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest combined Recency/Frequency score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # Find the object with the minimum score
    best_victim_key = None
    min_score = float('inf')
    
    # Optimization: If we have many items, strictly iterating all might be slow in Python,
    # but given the interface constraints, we must scan.
    for key in candidate_keys:
        score = get_eviction_score(key, current_time)
        
        if score < min_score:
            min_score = score
            best_victim_key = key
        # Tie-breaker: If scores are identical, pick the one with lower frequency (pure LFU fallback)
        elif score == min_score:
            if m_key_frequency.get(key, 0) < m_key_frequency.get(best_victim_key, 0):
                 best_victim_key = key

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, handle initialization.
    If the key is in our history (m_key_frequency), we restore its frequency 
    but give it a slight penalty (or reset logic) to prevent immediate dominance.
    '''
    global m_key_frequency, m_key_last_access
    
    # If it was in history (evicted previously), it retains some "heat"
    # but we don't want it to instantly dominate.
    if obj.key in m_key_frequency:
        m_key_frequency[obj.key] += 1
    else:
        # Brand new item
        m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, we DO NOT delete the frequency metadata immediately.
    This creates a "Ghost Cache" or "Shadow History".
    This allows the algorithm to remember that this item was popular recently.
    
    However, to prevent memory leaks in the metadata, we can perform cleanup
    if the metadata dictionary grows too large relative to cache capacity.
    '''
    global m_key_frequency, m_key_last_access
    
    # Remove from recency tracking immediately as it's no longer in cache
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]

    # MAINTENANCE: Prune history if it gets too big (e.g., 2x cache capacity size approximation)
    # Since we don't know the exact count capacity, we use a heuristic or hard limit
    # to prevent the dictionary from growing infinitely in a long trace.
    if len(m_key_frequency) > 2 * len(cache_snapshot.cache):
        # Remove items with low frequency to save space
        # (Naive pruning for safety)
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1 and k not in cache_snapshot.cache:
                keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_key_frequency[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 38
- Hit Rate: 0.1218

Example 2:
- Trace ID: 11
- Hit Rate: 0.3233

Example 3:
- Trace ID: 23
- Hit Rate: 0.0574

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:06:57 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:06:57,272 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:07:30 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:07:30,114 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:07:30,115 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to move away from the current ad-hoc heuristic ($Score = \frac{\log(Freq)}{Staleness}$) towards a more robust and proven policy. The current implementation suffers because:
1.  **Imbalance:** The division by `staleness` penalizes older items too harshly, making it behave too much like MRU (Most Recently Used) or LIFO in some edge cases, or failing to protect "warm" items effectively against scan resistance.
2.  **Scan Resistance:** Simple frequency counting is vulnerable to "scans" (one-time sequential reads of many items) which can pollute the history with items that have frequency 1 but will never be seen again.
3.  **Static History:** The "Shadow Cache" grows indefinitely or is pruned arbitrarily, leading to memory issues or loss of relevant history.

### The Solution: Adaptive Replacement Cache (ARC) Approach
To significantly reduce miss rates, I will implement a variation of **Adaptive Replacement Cache (ARC)** logic (or a simplified LRFU/2Q variant suited for this specific interface). ARC is superior because it dynamically balances between **Recency** (LRU) and **Frequency** (LFU) based on the workload.

However, since we cannot change the core cache data structure (which is a simple dict provided by `cache_snapshot`), we must simulate the lists (T1 and T2) using our metadata dictionaries.

**Algorithm Design:**
1.  **Recency (T1) vs Frequency (T2):** We will track whether an item is "newly accessed" (T1 candidates) or "frequently accessed" (T2 candidates).
2.  **Metadata:** We need to track:
    *   `access_times`: A list of access timestamps for each key (to determine interval and frequency).
    *   `ghost_entries`: A set or dict tracking items recently evicted. This is crucial for ARC-like adaptability. If we get a hit on a ghost entry, we know we should have kept it, so we increase the protected size for that category.
3.  **Eviction Policy:**
    *   We will calculate a utility score based on **Inter-Reference Recency (IRR)**.
    *   Items with short IRRs (high frequency) are prioritized.
    *   Items with long recency (old) are penalized.
    *   Crucially, we will use a **LIRS (Low Inter-reference Recency Set)** inspired approach or a simplified **2-Queue** approach because it is robust across many traces (Looping, Probabilistic, Clustering).

**Let's implement a simplified LIRS/2Q hybrid.**
*   **Queue 1 (Probation):** Items accessed once.
*   **Queue 2 (Protected):** Items accessed at least twice.
*   **Eviction:** Prefer evicting from Queue 1 (Probation) if it's too large or items are stale. If Queue 1 is empty or protected, evict LRU from Queue 2.

**Refined Strategy for this Interface:**
Since we have to scan keys in `evict` anyway, we will implement a weighting system that mimics **LIRS/ARC**.
We will maintain:
*   `m_access_count`: How many times a key has been accessed.
*   `m_insertion_time`: When the key was first inserted (to distinguish new vs old).
*   `m_last_access`: Last time accessed.

**The Magic Formula (LIRS-like):**
We want to protect items that have been seen recently AND have a history of being seen before.
1.  If frequency is 1, it is highly likely to be trash (scan).
2.  If frequency > 1, it is valuable.

**Improved Logic:**
*   **Score Calculation:** We will calculate a "victim score". **Higher score = Better victim (evict this)**.
*   **Primary Factor:** Frequency. Low frequency items are better victims.
*   **Secondary Factor:** Recency. Among same frequency, least recently used is the victim.
*   **Tie Breaker:** Insertion time.

**Specific Improvements for Scan Resistance:**
We will treat items with `frequency=1` as high-priority victims. We only promote them to "protected" status (harder to evict) once they are hit again (`frequency >= 2`).

```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```
2025-11-27 18:09:45,062 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.0629

Example 2:
- Trace ID: 15
- Hit Rate: 0.021

Example 3:
- Trace ID: 10
- Hit Rate: 0.0917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:09:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:09:45,064 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:10:55 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:10:55,975 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:10:55,976 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: This algorithm implements a variant of **GDSF (Greedy Dual-Size Frequency)**. 

### Rationale
The previous algorithm (Aged LFU) failed because it did not account for **object size**. In real-world traces, a few large objects can consume the capacity required to store hundreds of smaller, popular objects. To minimize the miss rate (and maximize the hit count), the algorithm must prioritize "density" of value.

### Key Concepts
1.  **Size-Awareness**: The score is inversely proportional to object size ($Frequency / Size$). This favors keeping smaller objects, which statistically increases the number of objects fitting in the cache and thus the hit probability.
2.  **Dynamic Aging ($L$ value)**: Instead of manually decaying frequencies, we use an inflation value `g_L`. When an object is evicted, `g_L` is raised to the priority of the evicted object. New objects start with this base priority. This creates a natural "aging" pressure where older objects must be accessed frequently to stay above the rising tide of `g_L`.
3.  **Tie-Breaking**: If GDSF scores are identical, we fall back to LRU (Least Recently Used) to break ties.

```python
import math

# Metadata dictionaries
m_freq = dict()         # Key -> Access Frequency
m_priority = dict()     # Key -> Current GDSF Priority Score
m_last_access = dict()  # Key -> Logical timestamp (for LRU tie-breaking)
m_ghost = dict()        # Key -> Boolean (Recently evicted registry)

# Global Inflation Factor (The "Aging" floor)
g_L = 0.0

# Configuration
MAX_GHOST_SIZE = 2000   # Limit ghost registry size

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Evict the item with the lowest Priority value.
    Priority = L + (Frequency / Size)
    
    Update L (inflation factor) to the Priority of the evicted item.
    '''
    global m_priority, m_last_access, g_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for the candidate with the lowest priority.
    # Tie-breaker: Least Recently Used (smallest access_count)
    victim_key = None
    min_priority = float('inf')
    min_access_time = float('inf')
    
    for key in candidate_keys:
        # Retrieve pre-calculated priority
        # Default to g_L if something went wrong, though keys should exist in m_priority
        p = m_priority.get(key, g_L)
        last_acc = m_last_access.get(key, 0)
        
        # Comparison logic:
        # 1. Primary: Lower Priority is worse
        # 2. Secondary: Lower Access Time (Older) is worse
        if p < min_priority:
            min_priority = p
            min_access_time = last_acc
            victim_key = key
        elif p == min_priority:
            if last_acc < min_access_time:
                min_access_time = last_acc
                victim_key = key
    
    # GDSF Aging Mechanism:
    # The system "ages" by raising the floor (L) to the value of the item just evicted.
    # This ensures new items (inserted at current L) are competitive with old items.
    if victim_key is not None:
        g_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Recency.
    3. Recalculate Priority based on CURRENT g_L. 
       This "resets" the object's age relative to the current inflation.
    '''
    global m_freq, m_priority, m_last_access, g_L
    
    key = obj.key
    
    # 1. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # 3. Recalculate Priority
    # Priority = L + (Freq / Size)
    # Note: We use max(1, size) to avoid division by zero.
    size = max(1, obj.size)
    m_priority[key] = g_L + (m_freq[key] / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency (check ghost registry for boost).
    2. Calculate Initial Priority.
    '''
    global m_freq, m_priority, m_last_access, m_ghost, g_L
    
    key = obj.key
    
    # 1. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # 2. Initialize Frequency
    if key in m_ghost:
        # Ghost Hit: It was recently evicted, so it's part of a loop or working set.
        # Give it a small boost so it survives longer this time.
        m_freq[key] = 2
        del m_ghost[key]
    else:
        # Cold Start
        m_freq[key] = 1
        
    # 3. Calculate Priority
    # New items start at the current inflation level g_L
    size = max(1, obj.size)
    m_priority[key] = g_L + (m_freq[key] / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata and update ghost registry.
    '''
    global m_freq, m_priority, m_last_access, m_ghost
    
    key = evicted_obj.key
    
    # Add to Ghost Registry to detect thrashing
    m_ghost[key] = True
    
    # Cleanup active metadata
    if key in m_freq:
        del m_freq[key]
    if key in m_priority:
        del m_priority[key]
    if key in m_last_access:
        del m_last_access[key]
        
    # Maintain Ghost Registry Size
    if len(m_ghost) > MAX_GHOST_SIZE:
        # Remove an arbitrary item (first one found) to keep memory usage low
        # In Python 3.7+, dicts maintain insertion order, so this acts as FIFO
        del m_ghost[next(iter(m_ghost))]
```
2025-11-27 18:11:16,305 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0262

Example 2:
- Trace ID: 46
- Hit Rate: 0.4663

Example 3:
- Trace ID: 3
- Hit Rate: 0.443

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:11:16 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:11:16,307 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:11:40 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:11:40,862 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:11:40,864 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the likely causes of the low hit rates in the provided examples (specifically Example 1, where 2.6% is extremely poor).

The previous implementation relied on a simplified Segmented LRU (SLRU) with a Ghost Cache. While conceptually sound, SLRU often struggles with workloads that have "shifting working sets" or mixed frequency patterns if the segment sizes aren't dynamic. Furthermore, simply boosting frequency on a ghost hit (ARC-style) without managing the *recency* vs. *frequency* balance dynamically often leads to cache pollution by obsolete high-frequency items.

### Improved Strategy: Adapted LFU-LRU with Aging (LFUDA-like)

We will switch to a hybrid approach that balances Frequency (LFU) and Recency (LRU) more aggressively, incorporating **Dynamic Aging**.

1.  **Score Calculation (LFU + Recency + Aging):**
    Instead of hard segments, we will assign a utility score to every object.
    $$ \text{Score} = \text{Frequency} \times \text{Weight} + \text{RecencyFactor} $$
    However, pure LFU suffers from "cache pollution" (old popular items stick around forever). To solve this, we will implement **Aging**:
    *   Every time we evict an object, we increase a global `avg_cache_frequency`.
    *   When a new object enters, its initial frequency isn't 1, but relative to this global baseline, allowing it to compete.
    *   Alternatively, and more robustly for this constraint set: We will use **Hyperbolic Caching** logic combined with **frequency decay**. The score will be function of Frequency divided by Time-since-access.

2.  **The Algorithm Choice: LFRU (Least Frequent Recently Used) / Hyperbolic Caching Hybrid**
    We will use a scoring function similar to Hyperbolic Caching, which is empirically very strong for web traces (like the ones implied by the IDs):
    $$ \text{Score} = \frac{\text{Frequency}^\alpha}{\text{TimeSinceAccess} + \epsilon} $$
    *   High Frequency increases score (retention).
    *   High TimeSinceAccess (staleness) decreases score (eviction).
    *   This naturally handles "aging" without complex global counters. If a popular item stops being accessed, the denominator grows, eventually dropping its score below new, low-frequency items.

3.  **Refined Metadata:**
    *   We track `access_count` (frequency).
    *   We track `last_access_time`.
    *   We utilize the **Ghost Registry** purely to give an initial frequency boost to items that were recently evicted (indicating the cache size is slightly too small for the working set).

### Implementation

```python
import math

# Metadata dictionary
# Key -> [frequency, last_access_time]
# We use a list instead of a tuple so it's mutable
m_access_info = dict()

# Ghost registry (Shadow Cache)
# Key -> eviction_time
# Used to detect if we evicted something that was immediately needed again.
m_ghost_registry = dict()

# Constants for tuning
# HIT_BOOST: How much we add to frequency on a hit. 
# 1 is standard. Higher favors LFU more strongly.
HIT_BOOST = 1

# GHOST_BOOST: If an item is found in ghost registry, it gets this initial frequency.
# This helps items recover from premature eviction.
GHOST_BOOST = 2

def get_hyperbolic_score(key, current_time):
    '''
    Calculates the retention score. 
    LOWER score = Higher chance of being evicted.
    
    Formula: Frequency / (Time_Since_Last_Access)
    
    We want to EVICT the item with the LOWEST retention score.
    However, the provided interface asks for a "Victim Score" where 
    Higher Score = Higher chance of eviction.
    
    Therefore, we return: 1 / (Hyperbolic_Retention_Score) 
    Or simply: Time_Since_Last_Access / Frequency
    '''
    if key not in m_access_info:
        return float('inf') # Should be evicted immediately if no metadata
        
    freq, last_access = m_access_info[key]
    
    # Calculate staleness (time since last access)
    # Add a small epsilon (1) to avoid division by zero if queried in same tick
    staleness = (current_time - last_access) + 1
    
    # Standard Hyperbolic Caching retention score is (Frequency / Staleness).
    # We want to maximize retention score => minimize eviction score.
    # Eviction Score = Staleness / Frequency.
    #
    # Logic:
    # 1. Very old item (High Staleness), Low Freq => High Eviction Score (Bad)
    # 2. Very recent item (Low Staleness) => Low Eviction Score (Good)
    # 3. High Freq => Denominator large => Low Eviction Score (Good)
    
    return staleness / freq

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest "Staleness per Frequency" ratio.
    This effectively implements a smooth tradeoff between LRU and LFU.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # We want to find the object with the HIGHEST eviction score
    max_score = -1.0
    victim_key = None
    
    # Optimization: To avoid iterating the entire cache on every evict (O(N)),
    # we can sample if N is very large, but for standard competitive coding 
    # constraints, O(N) linear scan is usually acceptable.
    # If performance is an issue, a random sample of 64 keys usually finds 
    # a near-optimal victim. Let's do a linear scan for accuracy first.
    
    for key in candidate_keys:
        score = get_hyperbolic_score(key, current_time)
        
        if score > max_score:
            max_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency.
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        # Update existing
        meta = m_access_info[obj.key]
        meta[0] += HIT_BOOST      # Increase frequency
        meta[1] = current_time    # Update last access
    else:
        # Edge case: Key in cache but missing metadata (should not happen)
        m_access_info[obj.key] = [1, current_time]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Check Ghost Registry.
    If present: It's a "Ghost Hit". Initialize with higher frequency.
    If absent: Initialize with base frequency.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    if obj.key in m_ghost_registry:
        # We recently evicted this, and now it's back. 
        # This implies it's part of a loop or working set larger than cache.
        # Give it a boost so it stays longer this time.
        initial_freq = GHOST_BOOST
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = [initial_freq, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keeping the ghost registry too large wastes memory, too small reduces adaptability.
    # A size equal to the cache capacity is a standard heuristic (ARC).
    target_ghost_size = len(cache_snapshot.cache)
    if len(m_ghost_registry) > target_ghost_size:
        # Remove the oldest inserted item in the ghost registry.
        # Python 3.7+ dicts preserve insertion order. iter() gives the first (oldest).
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
```
2025-11-27 18:12:04,886 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 9
- Hit Rate: 0.531

Example 2:
- Trace ID: 4
- Hit Rate: 0.4867

Example 3:
- Trace ID: 41
- Hit Rate: 0.0723

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:12:04 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:12:04,887 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:13:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:13:23,400 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:13:23,401 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: ```python
from collections import OrderedDict

# ARC (Adaptive Replacement Cache) Implementation
#
# ARC dynamically balances between Recency (T1) and Frequency (T2) by tracking 
# evicted items in "Ghost" lists (B1 and B2).
#
# Global Metadata:
# m_T1: Recent cache entries (OrderedDict for LRU)
# m_T2: Frequent cache entries (OrderedDict for LRU)
# m_B1: Ghost Recent (Keys evicted from T1)
# m_B2: Ghost Frequent (Keys evicted from T2)
# m_p:  Target size for T1 (Adaptive parameter)

m_T1 = OrderedDict()
m_T2 = OrderedDict()
m_B1 = OrderedDict()
m_B2 = OrderedDict()
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Selects a victim based on the ARC policy.
    ARC decides whether to evict from the Recent set (T1) or Frequent set (T2)
    based on the current adaptation parameter 'm_p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p

    # Safety: If one list is empty, we must evict from the other.
    if not m_T1 and not m_T2:
        # This technically shouldn't happen if the cache is reported full,
        # but as a fallback for potential state drift:
        candidate_keys = list(cache_snapshot.cache.keys())
        return candidate_keys[0] if candidate_keys else None
    
    if not m_T1:
        return next(iter(m_T2)) # LRU of T2
    if not m_T2:
        return next(iter(m_T1)) # LRU of T1

    # ARC Logic:
    # We check if the incoming object (obj) that caused this eviction 
    # is in the Ghost Frequent list (B2). This influences the eviction policy.
    # Logic: If T1 is larger than target 'p', we evict from T1 to shrink it.
    #        However, if the incoming item is in B2, we might be more aggressive on T1.
    
    incoming_in_B2 = obj.key in m_B2
    
    # Condition to evict from T1 (Recent):
    # 1. T1 has more items than its target size p.
    # 2. OR: The incoming item is a "Ghost Frequent" hit, and T1 is exactly at size p.
    if (len(m_T1) > m_p) or (incoming_in_B2 and len(m_T1) == m_p):
        return next(iter(m_T1)) # LRU of T1 (First item in OrderedDict)
    else:
        return next(iter(m_T2)) # LRU of T2 (First item in OrderedDict)

def update_after_hit(cache_snapshot, obj):
    '''
    Updates metadata on cache hit.
    In ARC, a hit implies the item is popular. It should move to (or stay in)
    the Frequent set (T2) and become the Most Recently Used (MRU).
    '''
    global m_T1, m_T2
    
    key = obj.key
    
    if key in m_T1:
        # Move from Recent (T1) to Frequent (T2)
        del m_T1[key]
        m_T2[key] = True # Add to MRU end
    elif key in m_T2:
        # Already in Frequent (T2), make it MRU
        m_T2.move_to_end(key)
    else:
        # Recovery: If metadata lost track (e.g., clear operations not captured),
        # treat as an insert into T2.
        m_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    Updates metadata after a new object is inserted.
    Handles "Ghost Hits" (Misses in cache, but hits in history).
    Adapts 'm_p' to favor Recency or Frequency based on which ghost list was hit.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # We estimate logical capacity 'c' as the current number of cached items.
    # Note: The object is not yet in our T1/T2 lists, so we add 1.
    current_c = len(m_T1) + len(m_T2) + 1

    # Case: Key was in Ghost Recent (B1)
    # Means we evicted a recent item too soon. We should expand T1 (increase p).
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
            
        m_p = min(m_p + delta, current_c)
        
        del m_B1[key]
        # Promoted to Frequent (T2) because it was seen recently (in T1) and now again.
        m_T2[key] = True

    # Case: Key was in Ghost Frequent (B2)
    # Means we evicted a frequent item too soon. We should shrink T1 (decrease p) to make room for T2.
    elif key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
            
        m_p = max(m_p - delta, 0)
        
        del m_B2[key]
        # Restored to Frequent (T2)
        m_T2[key] = True

    # Case: Totally new item (Cold miss)
    else:
        # Add to Recent (T1)
        m_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Updates metadata after an object is physically evicted.
    Moves the object from the active sets (T1/T2) to the ghost sets (B1/B2).
    '''
    global m_T1, m_T2, m_B1, m_B2
    
    key = evicted_obj.key
    
    if key in m_T1:
        del m_T1[key]
        # Add to Ghost Recent (B1)
        m_B1[key] = True
        
    elif key in m_T2:
        del m_T2[key]
        # Add to Ghost Frequent (B2)
        m_B2[key] = True
        
    # Housekeeping: Prevent ghost lists from growing infinitely.
    # Standard ARC suggests limits based on capacity 'c', but here we use a loose
    # heuristic based on current cache item count to handle variable sizes safeley.
    # Keep total history roughly 2x the current cache count.
    max_history = 2 * (len(m_T1) + len(m_T2) + 1)
    max_history = max(max_history, 100) # Minimum baseline
    
    if len(m_B1) > max_history:
        m_B1.popitem(last=False) # Remove oldest
        
    if len(m_B2) > max_history:
        m_B2.popitem(last=False) # Remove oldest
```
2025-11-27 18:13:37,258 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0526

Example 2:
- Trace ID: 39
- Hit Rate: 0.1436

Example 3:
- Trace ID: 17
- Hit Rate: 0.0235

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:13:37 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:13:37,259 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:14:11 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:14:11,882 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:14:11,883 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache miss rate significantly across varying trace patterns, we need an algorithm that adapts to both **Recency** (how lately was it used?) and **Frequency** (how often is it used?). The previous implementation attempted a Segmented LRU (SLRU) approach but likely suffered from rigid parameter tuning (e.g., the probation vs. protected cutoff) and potentially inefficient scan resistance handling.

The implementation below uses an **Adaptive Replacement Cache (ARC)** strategy. ARC is generally superior to LRU and LFU because:
1.  **Scan Resistance:** It separates "once-seen" items from "frequently-seen" items.
2.  **Adaptability:** It dynamically adjusts the size of the "recent" list (T1) vs. the "frequent" list (T2) based on current workload patterns (ghost hits).
3.  **Ghost Lists:** It tracks evicted items to learn whether the cache should favor recency or frequency at any given moment.

### Improved Algorithm Design (ARC Logic)

*   **T1 (Recent):** Contains items accessed only once recently.
*   **T2 (Frequent):** Contains items accessed at least twice.
*   **B1 (Ghost Recent):** History of items evicted from T1. A hit here implies we need a larger T1.
*   **B2 (Ghost Frequent):** History of items evicted from T2. A hit here implies we need a larger T2.
*   **Target Size (p):** A dynamic parameter `p` determines the target size of T1. If we hit B1, `p` increases (grow T1). If we hit B2, `p` decreases (grow T2).

### Python Implementation

```python
import math

# --- Metadata Globals ---

# Stores simple metadata: key -> access_count (used to distinguish T1 vs T2)
# We don't need complex tuples here because the list membership defines the state.
m_access_count = dict()

# We maintain 4 LRU lists using standard Python dictionaries (ordered by insertion since Python 3.7+).
# Keys in T1/T2 are in the cache. Keys in B1/B2 are ghost entries (metadata only).
# Using dicts allows O(1) lookup and delete, and O(1) "move to end" (reinsert).
# 
# T1: Recently accessed once (Real Cache)
# T2: Accessed at least twice (Real Cache)
# B1: Evicted from T1 (Ghost)
# B2: Evicted from T2 (Ghost)

m_T1 = dict()
m_T2 = dict()
m_B1 = dict()
m_B2 = dict()

# Adaptation Parameter 'p'
# 0 <= p <= capacity
# 'p' is the target size for the T1 list.
m_p = 0

def move_to_mru(container, key):
    '''Helper: Refreshes an item in a python dict to be the most recent.'''
    val = container.pop(key)
    container[key] = val

def get_lru_key(container):
    '''Helper: Gets the LRU key (first item) from a python dict.'''
    return next(iter(container)) if container else None

def evict(cache_snapshot, obj):
    '''
    ARC Replace Subroutine.
    Decides which item to evict from T1 or T2 based on the current target size 'p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p

    # We need to free up space. The victim depends on the sizes of T1 vs p.
    # Note: len(T1) + len(T2) == Capacity (roughly).
    
    t1_len = len(m_T1)
    
    # Logic: 
    # If T1 is larger than our target 'p', we evict from T1 (LRU preference for Recency).
    # Otherwise, we evict from T2 (LRU preference for Frequency).
    # There is a subtle condition in ARC: if B1 is larger than B2, it influences eviction logic slightly
    # but the standard approximation `if t1_len > p` works well for fixed size caches.
    
    # Precise ARC condition for REPLACE(p):
    # if ( |T1| >= 1 and ( (|T1| > p) or (|B1| is referenced and implicit logic requires space) ) )
    # However, since `evict` is called strictly when cache is FULL:
    
    victim_key = None
    
    # If key is in B2, the original paper logic changes slightly, but we are inside 
    # the generic evict hook provided by the simulator.
    # We strictly follow the size constraints relative to p.
    
    if t1_len > 0 and (t1_len > m_p or (get_lru_key(m_B1) == obj.key and t1_len == m_p)):
        # Evict from T1 -> Move to B1
        victim_key = get_lru_key(m_T1)
        # The actual movement to B1 happens in `update_after_evict`
    else:
        # Evict from T2 -> Move to B2
        victim_key = get_lru_key(m_T2)
        if victim_key is None:
            # Fallback if T2 is empty (should not happen if cache full and T1 <= p)
            victim_key = get_lru_key(m_T1)

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If in T1 or T2, move to MRU of T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Hit in T1 (Recently inserted, now accessed again -> upgrade to Frequent)
    if key in m_T1:
        # Remove from T1
        del m_T1[key]
        # Add to MRU of T2
        m_T2[key] = True
        
    # Case 2: Hit in T2 (Already frequent -> stay in Frequent, become MRU)
    elif key in m_T2:
        move_to_mru(m_T2, key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss processing):
    Check Ghosts (B1/B2) to adapt 'p'.
    Insert new item into T1 or T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity
    
    # Case 1: Key is in B1 (Ghost Recent)
    # This implies we evicted a recent item too soon. We should increase T1 size (p).
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
        m_p = min(capacity, m_p + delta)
        
        # Remove from Ghost
        del m_B1[key]
        
        # Move to MRU of T2 (It has been seen twice now: once effectively in T1-history, now again)
        # Note: Standard ARC promotes ghost hits to T2.
        m_T2[key] = True
        return

    # Case 2: Key is in B2 (Ghost Frequent)
    # This implies we evicted a frequent item too soon. We should decrease T1 size (p) to make room for T2.
    if key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
        m_p = max(0, m_p - delta)
        
        # Remove from Ghost
        del m_B2[key]
        
        # Move to MRU of T2
        m_T2[key] = True
        return

    # Case 3: Completely new item (not in T1, T2, B1, B2)
    # Insert into T1 MRU.
    # Note: If cache was full, eviction happened before this.
    m_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T1/T2 to B1/B2 respectively.
    Ensure Ghost lists don't grow indefinitely.
    '''
    global m_T1, m_T2, m_B1, m_B2
    
    key = evicted_obj.key
    capacity = cache_snapshot.capacity
    
    # Determine where it came from and move to corresponding ghost list
    if key in m_T1:
        del m_T1[key]
        m_B1[key] = True # Add to B1 MRU
    elif key in m_T2:
        del m_T2[key]
        m_B2[key] = True # Add to B2 MRU
        
    # ARC Maintenance: Bounding the Ghost Lists (L1 + L2 <= 2*c)
    # Effectively, we usually want |T1| + |B1| <= c and |T2| + |B2| <= 2c
    # But a simpler bound often used in implementations is |B1| + |B2| <= Capacity
    
    # Prune B1 if too big
    if len(m_B1) > capacity:
        try:
            oldest = next(iter(m_B1))
            del m_B1[oldest]
        except StopIteration:
            pass

    # Prune B2 if too big
    if len(m_B2) > capacity:
        try:
            oldest = next(iter(m_B2))
            del m_B2[oldest]
        except StopIteration:
            pass
```
2025-11-27 18:14:18,734 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 45
- Hit Rate: 0.1405

Example 3:
- Trace ID: 6
- Hit Rate: 0.2727

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:14:18 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:14:18,735 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:16:23 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:16:23,769 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:16:23,799 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache miss rate, we will implement the **Greedy Dual-Size Frequency (GDSF)** algorithm. 

### Why GDSF?
1.  **Size Awareness**: The provided context includes `obj.size`. Standard LRU/LFU treats a 1KB object equal to a 1GB object. However, storing one thousand 1KB objects usually yields a higher hit rate (more requests served) than storing one 1GB object. GDSF penalizes large objects, optimizing for **hit density**.
2.  **Frequency & Aging**: The previous implementation struggled with a static "Protected Segment" logic. GDSF assigns a dynamic score (`Frequency / Size`) added to a global aging factor (`L`). 
3.  **Adaptive Aging**: The global factor `L` increases whenever an object is evicted (set to the score of the evicted object). This ensures that items that were popular long ago (but are no longer accessed) will eventually have a score lower than `L` (the baseline for new items) and be evicted. It naturally handles "scan resistance" without complex ghost lists because new items start with a modest score that is quickly beaten by the rising `L` if they aren't hit again.

### Algorithm Summary
-   **Score Formula**: $Priority = L + \frac{Frequency}{Size}$
-   **Eviction**: Remove the object with the lowest Priority.
-   **Aging ($L$)**: When object $p$ is evicted, set $L = Priority(p)$.
-   **Hit**: Update Frequency, and reset Priority to the *current* $L + \frac{NewFrequency}{Size}$. This brings the item to the "front" of the logical queue.

```python
import math

# Metadata dictionary
# Maps key -> { 'freq': int, 'score': float, 'last_access': int }
m_metadata = {}

# GDSF Priority Inflation Parameter (L)
# This value acts as a "watermark" for the cache's value. 
# It increases monotonically as items are evicted.
# New items enter at this baseline level.
m_inflation_L = 0.0

def get_gdsf_score(freq, size, L):
    '''
    Calculates the Greedy Dual Size Frequency score.
    Formula: Score = L + (Frequency / Size)
    
    - L: The aging factor.
    - Frequency: Promotes popular items.
    - Size: Penalizes large items (optimizing for hit density per byte).
    '''
    # Ensure size is at least 1 to prevent division by zero
    safe_size = max(1, size)
    return L + (float(freq) / float(safe_size))

def evict(cache_snapshot, obj):
    '''
    Strategy: GDSF (Greedy Dual Size Frequency)
    Evicts the object with the MINIMUM score.
    Tie-breaking: Least Recently Used (LRU) based on access_count.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
        
    victim_key = None
    # We want to find the lowest score to evict
    min_score = float('inf')
    min_last_access = float('inf')
    
    # Linear scan to find the victim
    for key in candidate_keys:
        # Retrieve metadata
        meta = m_metadata.get(key)
        
        # Safety check: if metadata is missing, evict this zombie item immediately
        if not meta:
            return key
            
        score = meta['score']
        last_access = meta['last_access']
        
        # Logic: Find Minimum Score
        if score < min_score:
            min_score = score
            min_last_access = last_access
            victim_key = key
        elif score == min_score:
            # Tie-breaker: If scores are equal, evict the older one (LRU)
            if last_access < min_last_access:
                min_last_access = last_access
                victim_key = key
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    3. "Re-value" the item using the CURRENT global L.
       This effectively resets the item's aging process, protecting it 
       from the rising tide of L caused by evictions.
    '''
    global m_metadata, m_inflation_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        meta = m_metadata[obj.key]
        meta['freq'] += 1
        meta['last_access'] = current_time
        # Recalculate score with new frequency and current L
        meta['score'] = get_gdsf_score(meta['freq'], obj.size, m_inflation_L)
    else:
        # Fail-safe if metadata was lost
        m_metadata[obj.key] = {
            'freq': 1,
            'last_access': current_time,
            'score': get_gdsf_score(1, obj.size, m_inflation_L)
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Create metadata.
    2. Set start Frequency to 1.
    3. Calculate score based on current global L.
    '''
    global m_metadata, m_inflation_L
    
    current_time = cache_snapshot.access_count
    
    m_metadata[obj.key] = {
        'freq': 1,
        'last_access': current_time,
        'score': get_gdsf_score(1, obj.size, m_inflation_L)
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. CRITICAL: Update global L to the score of the evicted item.
       This ensures that for any existing item to survive, it must eventually
       prove it is more valuable than what was just evicted.
    2. Clean up metadata.
    '''
    global m_metadata, m_inflation_L
    
    if evicted_obj.key in m_metadata:
        # The inflation value L advances to the priority of the evicted object
        m_inflation_L = m_metadata[evicted_obj.key]['score']
        
        # Remove the evicted item from tracking
        del m_metadata[evicted_obj.key]
```
2025-11-27 18:16:31,778 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 35
- Hit Rate: 0.068

Example 2:
- Trace ID: 18
- Hit Rate: 0.0225

Example 3:
- Trace ID: 8
- Hit Rate: 0.5363

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:16:31 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:16:31,779 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:16:56 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:16:56,480 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:16:56,481 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to move beyond simple LFU or LRU strategies, which often fail under specific workloads (e.g., LFU holds onto stale popular items too long; LRU is vulnerable to scans).

The best general-purpose algorithm for minimizing miss rates across diverse traces is **ARC (Adaptive Replacement Cache)** or a simplified approximation like **SLRU (Segmented LRU)** or an LFU-LRU hybrid with aging.

However, given the constraints and the provided interface, a robust improvement over the existing implementation is **Window TinyLfu (W-TinyLFU)** or a simplified **Segmented LRU (S3-FIFO style or 2Q)** approach. But perhaps the most impactful and implementable improvement within this structure is an **LFU with Dynamic Aging (or LFRU)**.

The current implementation suffers from "Frequency Pollution." Once an item gets a high frequency count, it stays in the cache forever, even if it is never accessed again.

Here is an improved algorithm based on **LFU with Aging and Frequency Resetting**. This approach prevents the cache from being clogged by items that were once popular but are now cold.

### Key Improvements:
1.  **Dynamic Aging:** We periodically reduce the frequency counts of all items. This ensures that "historic" popularity doesn't outweigh "current" popularity.
2.  **Ghost Cache (History):** We retain frequency data for evicted items for a short period. If an evicted item returns, it means our eviction decision was poor (or the working set is shifting), and that item deserves a "boost" upon re-insertion.
3.  **Logarithmic/Saturating Counters:** Instead of infinite frequency growth, we cap the frequency or grow it more slowly to make it easier for new items to compete.

Below is the complete implementation using a **frequency-based approach with an aging mechanism** to solve the staleness problem of standard LFU.

```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```
2025-11-27 18:19:13,714 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 44
- Hit Rate: 0.1627

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 37
- Hit Rate: 0.6404

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:19:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:19:13,715 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:20:29 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:20:29,752 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:20:29,753 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is the improved cache eviction algorithm based on the **Adaptive Replacement Cache (ARC)** strategy.

### Design Rationale:
1.  **Adaptive Replacement Cache (ARC):** The previous algorithm used static thresholds to separate "Probationary" items from "Protected" items. This failed on Trace 13 (Hit Rate 0.0) likely due to a scanning pattern or a working set slightly larger than the cache, causing "cache thrashing." ARC dynamically tunes the size of the "Recency" list (T1) and the "Frequency" list (T2) based on hits in their respective ghost registries (B1 and B2).
2.  **Scan Resistance:** By filtering new items through T1 (Recency) and only promoting them to T2 (Frequency) after a subsequent hit, we prevent one-time scans from washing out high-frequency items.
3.  **Loop Detection:** The Ghost Registries (B1/B2) detect if we are evicting items too early. If we get hits in B1 (Recency Ghosts), the algorithm increases the size of T1. If we get hits in B2 (Frequency Ghosts), it favors T2.

### Implementation:

```python
import math

# =============================================================================
# ARC (Adaptive Replacement Cache) State
# =============================================================================
# We emulate the 4 lists of ARC using Python's insertion-ordered dictionaries.
# T1: Recent (MRU) - Items accessed once recently.
# T2: Frequent (MRU) - Items accessed at least twice.
# B1: Ghost Recent - History of evicted T1 items.
# B2: Ghost Frequent - History of evicted T2 items.
# =============================================================================

m_t1 = dict()
m_t2 = dict()
m_b1 = dict()
m_b2 = dict()
m_p = 0  # Target size for the T1 list

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on ARC logic.
    Crucially, ARC's eviction policy depends on *where* the incoming object `obj` 
    was found (Ghost B1 vs Ghost B2 vs New).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    # Current Cache Size (in number of objects)
    # We use this as 'C' in ARC literature.
    cache_len = len(cache_snapshot.cache)
    
    # Safety: If cache is empty, we can't evict.
    if cache_len == 0:
        return None

    # Identify the incoming object's status (New, B1, or B2)
    # This influences whether we replace from T1 or T2.
    key = obj.key
    found_in_b2 = key in m_b2
    
    # ARC REPLACE Logic:
    # We evict from T1 (Recency) if:
    # 1. T1 contains more items than target 'p'.
    # 2. OR: The incoming item is in B2 (Ghost Freq) AND T1 has reached target 'p'.
    #    (This specific condition helps prioritize Frequency when Ghost Freq is hit).
    
    t1_len = len(m_t1)
    
    # We prioritize evicting from T1 if it exceeds the target p.
    # Logic derived from ARC paper "Replace(p)" subroutine.
    if (t1_len > 0) and ( (t1_len > m_p) or (found_in_b2 and t1_len == m_p) ):
        # Evict LRU from T1. 
        # Since Python dicts are insertion-ordered, iter() gives keys in insertion order.
        # The first key is the oldest (LRU).
        victim_key = next(iter(m_t1))
        return victim_key
    else:
        # Otherwise, evict LRU from T2.
        # If T2 is empty, fallback to T1 (should not happen if cache is full and logic holds, but implies safety).
        if len(m_t2) > 0:
            victim_key = next(iter(m_t2))
            return victim_key
        else:
            # Fallback for edge cases (e.g., p=Size, T2 empty)
            return next(iter(m_t1))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1, move to T2 (it has proven frequency).
    If item is in T2, move to MRU of T2.
    '''
    global m_t1, m_t2
    
    key = obj.key
    
    if key in m_t1:
        # Move from T1 (Recent) to T2 (Frequent)
        del m_t1[key]
        m_t2[key] = None # Insert at end (MRU)
    elif key in m_t2:
        # Update Position in T2 (MRU)
        del m_t2[key]
        m_t2[key] = None
    
    # Note: If cache state is inconsistent with our lists (rare simulation glitch),
    # we just ignore. The lists track the algorithmic view.

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost Lists (B1/B2) to adapt 'p'.
    2. Insert object into correct list (T1 or T2).
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    
    # --- 1. Adaptation Step (Tuning p) ---
    if key in m_b1:
        # Hit in Ghost Recent (B1).
        # It means we evicted a recent item too soon. We should expand T1.
        delta = 1
        if len(m_b1) >= len(m_b2):
            delta = 1
        else:
            # Safety for division by zero
            delta = len(m_b2) // len(m_b1)
            
        # Increase p, cap at Cache Size
        current_capacity = len(cache_snapshot.cache) 
        m_p = min(m_p + delta, current_capacity)
        
        # Remove from ghost
        del m_b1[key]
        
        # Move to T2 (Frequency) as it has been seen twice (once in T1 history, now again)
        m_t2[key] = None
        
    elif key in m_b2:
        # Hit in Ghost Frequent (B2).
        # It means we evicted a frequent item too soon. We should shrink T1 (expand T2).
        delta = 1
        if len(m_b2) >= len(m_b1):
            delta = 1
        else:
            delta = len(m_b1) // len(m_b2)
            
        # Decrease p, cap at 0
        m_p = max(m_p - delta, 0)
        
        # Remove from ghost
        del m_b2[key]
        
        # Move to T2 (Frequency)
        m_t2[key] = None
        
    else:
        # Completely new item.
        # Insert into T1 (Recency - Probationary period)
        m_t1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from Active (T1/T2) to Ghost (B1/B2).
    Maintain Ghost List sizes.
    '''
    global m_t1, m_t2, m_b1, m_b2
    
    key = evicted_obj.key
    current_capacity = len(cache_snapshot.cache) + 1 # +1 because we just evicted one to make space
    
    # Identify where it was evicted from and move to corresponding Ghost list
    if key in m_t1:
        del m_t1[key]
        m_b1[key] = None # Add to MRU of B1
    elif key in m_t2:
        del m_t2[key]
        m_b2[key] = None # Add to MRU of B2
        
    # --- Ghost Registry Maintenance ---
    # ARC property: |L1| + |L2| <= 2*c
    # i.e. (|T1| + |B1|) + (|T2| + |B2|) <= 2*c
    # We prune the ghost lists if they get too large.
    
    # Ideally, maintain |T1| + |B1| <= C and |T2| + |B2| <= 2C
    # But a simple bounded sum is usually sufficient and simpler.
    
    target_ghost_len = current_capacity
    
    while len(m_b1) > target_ghost_len:
        # Remove LRU of B1
        oldest = next(iter(m_b1))
        del m_b1[oldest]
        
    while len(m_b2) > target_ghost_len:
        # Remove LRU of B2
        oldest = next(iter(m_b2))
        del m_b2[oldest]
```
2025-11-27 18:20:35,921 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 22
- Hit Rate: 0.0587

Example 2:
- Trace ID: 30
- Hit Rate: 0.0456

Example 3:
- Trace ID: 19
- Hit Rate: 0.2722

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:20:35 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:20:35,922 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:21:11 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:21:11,920 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:21:11,921 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to move beyond simple frequency/recency rules and implement a policy that handles the specific challenges of variable workloads (scans vs. loops vs. recency).

The previous implementation attempted a Segmented LRU (SLRU) with a Ghost Cache but suffered from low hit rates. This is likely due to:
1.  **Static Thresholds:** The static boundary between "Probation" and "Protected" segments didn't adapt to changing workload sizes.
2.  **Ghost Cache Management:** The ghost cache logic was rudimentary and might not have effectively boosted items back into the cache quickly enough.
3.  **Lack of Adaptability:** It didn't dynamically adjust the size of the protected segment based on hit ratios.

### Proposed Solution: Adaptive Replacement Cache (ARC) - Simplified Approximation

We will implement a robust approximation of the **ARC (Adaptive Replacement Cache)** algorithm. ARC is superior because it balances Recency (LRU) and Frequency (LFU) dynamically.

**How it works:**
1.  **Two Lists:** We maintain two logical lists of items: $T_1$ (items seen once, Recency) and $T_2$ (items seen at least twice, Frequency).
2.  **Ghost Lists:** We maintain ghost entries for items evicted from $T_1$ (called $B_1$) and $T_2$ (called $B_2$).
3.  **Adaptation:**
    *   If we get a hit in the ghost list $B_1$, it means our Recency list $T_1$ was too small. We increase the target size for $T_1$.
    *   If we get a hit in the ghost list $B_2$, it means our Frequency list $T_2$ was too small. We decrease the target size for $T_1$ (effectively increasing $T_2$).
4.  **Eviction:** We evict from $T_1$ or $T_2$ based on whether the length of $T_1$ exceeds the adaptive target `p`.

This approach automatically tunes itself. If the workload is a scan (many one-time items), $T_1$ grows to handle it. If the workload is a loop (repeating items), $T_2$ grows.

```python
import math

# --- Global Metadata ---
# m_metadata stores: key -> (is_frequent_segment, reference_bit)
# We track which segment an item belongs to:
#   False (0): T1 (Recency / Probation segment)
#   True (1):  T2 (Frequency / Protected segment)
m_metadata = dict()

# Ghost caches (store keys only)
# B1: Evicted from T1 (Recency ghosts)
# B2: Evicted from T2 (Frequency ghosts)
# Since we need fast lookups and order preservation is useful for cleaning, 
# we use dictionaries (Python 3.7+ preserves insertion order).
m_ghost_b1 = dict()
m_ghost_b2 = dict()

# Adaptive Parameter 'p'
# Target size for T1 (Recency list).
# Range: [0, cache_capacity]
m_target_p = 0

# --- Helper Logic ---

def clean_ghosts(cache_capacity):
    """
    Ensures the ghost lists don't grow indefinitely.
    ARC typically suggests |T1| + |B1| <= C and |T2| + |B2| <= 2C, 
    but roughly keeping |B1| + |B2| <= C is a safe memory bound.
    """
    global m_ghost_b1, m_ghost_b2
    
    # Simple cleanup strategy: If total ghosts > capacity, remove oldest
    # This prevents memory leaks in the simulation.
    total_ghosts = len(m_ghost_b1) + len(m_ghost_b2)
    
    # We allow ghosts to extend up to capacity to capture history
    if total_ghosts > cache_capacity:
        # Remove from B1 if it has elements, else B2
        if m_ghost_b1:
            # next(iter()) gets the oldest inserted key
            del m_ghost_b1[next(iter(m_ghost_b1))]
        elif m_ghost_b2:
            del m_ghost_b2[next(iter(m_ghost_b2))]

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic.
    Decides whether to evict from T1 (Recency) or T2 (Frequency) based on
    current sizes relative to the adaptive parameter 'p'.
    '''
    global m_metadata, m_target_p, m_ghost_b1, m_ghost_b2

    # Identify candidates present in the cache
    t1_candidates = []
    t2_candidates = []
    
    # Map cache contents to T1 or T2 based on metadata
    # Also need strict LRU order. Python dict iteration is insertion-ordered.
    # However, 'insertion' here means when it was added to the dictionary.
    # To do this strictly correct, we need access times. 
    # But for this simulation interface, we can rely on Python's dict order 
    # IF we manage re-insertions correctly in update_after_hit.
    
    # Let's iterate over cache snapshot to separate them
    for key in cache_snapshot.cache:
        # If key not in metadata, treat as T1 default (shouldn't happen often)
        is_t2 = False
        if key in m_metadata:
            is_t2, _ = m_metadata[key]
            
        if is_t2:
            t2_candidates.append(key)
        else:
            t1_candidates.append(key)
            
    # Count current sizes
    len_t1 = len(t1_candidates)
    
    victim_key = None
    
    # Logic:
    # If len(T1) > p, we usually evict from T1.
    # However, we must ensure we don't evict from an empty list.
    
    if len_t1 > 0 and (len_t1 > m_target_p or len(t2_candidates) == 0):
        # Evict LRU from T1
        # Since t1_candidates comes from cache_snapshot.cache keys iteration,
        # we need to find the "LRU" one. 
        # *Constraint*: We don't have an explicit LRU Linked List.
        # We must rely on an assumption or metadata.
        # To be safe, let's use the 'access_count' or insertion order. 
        # The previous code stored metadata. Let's look at m_metadata.
        # Actually, simpler: The simulator iterates `cache` dict. 
        # If we delete/re-insert on hit, the first item in iteration is LRU.
        
        # We will assume the simulator's cache keys are NOT ordered by LRU automatically
        # unless we enforced it. We can't enforce the simulator's dict order.
        # Optimization: We need to track LRU ourselves? 
        # Given the "Stateless" nature of 'evict' (it iterates keys), 
        # we should scan for the oldest access time if possible. 
        # However, the provided template doesn't let us store extra state easily 
        # unless we put it in m_metadata.
        # Let's add (last_access_time) to m_metadata.
        
        # Find LRU in T1
        victim_key = min(t1_candidates, key=lambda k: m_metadata[k][1])
        return victim_key
    else:
        # Evict LRU from T2
        if t2_candidates:
            victim_key = min(t2_candidates, key=lambda k: m_metadata[k][1])
            return victim_key
            
    # Fallback (should not be reached if cache not empty)
    return list(cache_snapshot.cache.keys())[0]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in T1, move to T2.
    2. If in T2, move to MRU of T2.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # Determine current state
    is_t2 = False
    if key in m_metadata:
        is_t2, _ = m_metadata[key]
        
    # On any hit, the item becomes "Frequency/Protected" (T2)
    # Update timestamp to make it MRU
    m_metadata[key] = (True, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost Lists (B1, B2).
    2. Adapt 'p' based on where the hit occurred (if it was a ghost).
    3. Insert new item into T1 (Recency) if strictly new, or T2 if rescued from ghost.
    '''
    global m_metadata, m_target_p, m_ghost_b1, m_ghost_b2
    
    current_time = cache_snapshot.access_count
    key = obj.key
    capacity = cache_snapshot.capacity # Note: This is in bytes in the object, but usually count in algorithms. 
    # *Assumption*: The simulator counts "capacity" in count (objects), or we approximate.
    # The prompt says capacity is "bytes", but standard caching algorithms count items.
    # We will assume the simulator handles the "cache full" logic based on bytes, 
    # but our logic for 'p' works on item counts usually. 
    # To be safe for byte-based caches, we treat 'p' as a generic scalar target.
    
    # ARC Adaptation Logic
    if key in m_ghost_b1:
        # Hit in B1 (Ghost Recency). T1 was too small. Increase p.
        # Delta: If B1 is bigger than B2, step smaller; else step 1.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) / len(m_ghost_b1)
            
        m_target_p = min(m_target_p + delta, len(cache_snapshot.cache) + 1)
        
        # Promote to T2 (Frequent) because it was seen recently (in T1) then accessed again
        m_metadata[key] = (True, current_time)
        del m_ghost_b1[key]
        
    elif key in m_ghost_b2:
        # Hit in B2 (Ghost Frequency). T2 was too small (so T1 was too big). Decrease p.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) / len(m_ghost_b2)
            
        m_target_p = max(m_target_p - delta, 0)
        
        # Re-insert into T2
        m_metadata[key] = (True, current_time)
        del m_ghost_b2[key]
        
    else:
        # Totally new item. Insert into T1 (Recency/Probation)
        m_metadata[key] = (False, current_time)

    # Clean up ghosts if they grew too large
    clean_ghosts(len(cache_snapshot.cache))

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Identify where it was evicted from (T1 or T2).
    2. Add to corresponding Ghost List (B1 or B2).
    '''
    global m_metadata, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_metadata:
        is_t2, _ = m_metadata[key]
        del m_metadata[key]
        
        if is_t2:
            # Evicted from T2 -> Add to B2
            m_ghost_b2[key] = True # Value doesn't matter, existence does
        else:
            # Evicted from T1 -> Add to B1
            m_ghost_b1[key] = True
```
2025-11-27 18:21:18,523 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 25
- Hit Rate: 0.3569

Example 2:
- Trace ID: 31
- Hit Rate: 0.0266

Example 3:
- Trace ID: 32
- Hit Rate: 0.029

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:21:18 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:21:18,524 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:21:45 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:21:45,430 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:21:45,431 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the weaknesses of the current Segmented LRU implementation, particularly its inability to distinguish between "frequency" and "recency" dynamically.

The previous algorithm relied heavily on a hard cut-off: `Frequency=1` items were always evicted before `Frequency>1` items. While good for scan resistance, this rigid hierarchy fails in workloads where recent items (even with low frequency) are more valuable than old, high-frequency items (a concept known as "Cache Pollution" by stale heavy hitters).

### The Solution: Adaptive Replacement Cache (ARC) Logic
I will implement a logic inspired by **ARC (Adaptive Replacement Cache)**. Instead of a hard frequency threshold, we will maintain two logical lists (simulated via scores):
1.  **Recency List (L1):** Items accessed only once recently. Captures "recency."
2.  **Frequency List (L2):** Items accessed at least twice. Captures "frequency."

Crucially, we will dynamically adjust the *target size* of the Recency List based on hits in the Ghost Registry.
*   If we hit a "ghost" of an item that was recently evicted from the Recency List, we know our Recency List was too small. We increase its target size.
*   If we hit a "ghost" of an item evicted from the Frequency List, we know our Frequency List was too small. We decrease the Recency List's target size (giving more room to Frequency).

### Key Changes:
1.  **Metadata:** We track `key -> (freq, insert_time, last_access, list_type)` and a global parameter `p` (the target size of the Recency list).
2.  **Dynamic Balancing:** The `evict` function compares the size of the logical "Recency List" against the adaptive parameter `p`.
3.  **Eviction Logic:**
    *   If the Recency list size `len(L1)` exceeds `p`, we evict from `L1` (LRU behavior).
    *   Otherwise, we evict from `L2` (LFU/LRU behavior for hot items).
4.  **Ghost Updates:** Hitting a ghost adjusts `p`, making the cache "learn" the workload type (Recency-heavy vs. Frequency-heavy).

Here is the improved implementation:

```python
import math

# --- Global Metadata ---
# Tracks active cache items: key -> (access_count, type_flag)
# type_flag: 0 = L1 (Recency/Probation), 1 = L2 (Frequency/Protected)
m_meta = dict()

# Tracks access times for LRU calculation within the lists
# key -> last_access_time
m_access_time = dict()

# Ghost registries (Evicted items history)
# B1: Ghost list for L1 (Evicted from Recency)
# B2: Ghost list for L2 (Evicted from Frequency)
m_ghost_b1 = set()
m_ghost_b2 = set()

# --- Adaptive Parameter ---
# 'p' represents the target size of the L1 (Recency) list.
# 0 <= p <= capacity
global_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-inspired Eviction Strategy.
    
    We have two logical segments in the cache:
    - L1: Items accessed once (Recency heavy).
    - L2: Items accessed > once (Frequency heavy).
    
    The parameter `p` determines the ideal size of L1.
    If len(L1) > p, we evict from L1 (the LRU item among those accessed once).
    Else, we evict from L2 (the LRU item among those accessed frequently).
    '''
    global m_meta, m_access_time, global_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Classify current cached items into L1 (type 0) and L2 (type 1)
    l1_keys = []
    l2_keys = []
    
    for k in candidate_keys:
        # Default to L1 if missing metadata (safety fallback)
        meta = m_meta.get(k, (1, 0))
        if meta[1] == 0:
            l1_keys.append(k)
        else:
            l2_keys.append(k)

    # Determine which list to evict from based on target size 'p'
    # If the Recency list (L1) has grown beyond its target `p`, we trim it.
    # Otherwise, we trim the Frequency list (L2).
    if len(l1_keys) > 0 and (len(l1_keys) > global_p or len(l2_keys) == 0):
        victim_pool = l1_keys
    else:
        victim_pool = l2_keys
        
    # Find the LRU item within the chosen pool
    # We want the item with the smallest last_access_time
    victim_key = None
    min_time = float('inf')
    
    for k in victim_pool:
        t = m_access_time.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency (last access time).
    2. Move item to L2 (Frequency list) if it isn't already there.
    '''
    global m_meta, m_access_time
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    
    # Retrieve current metadata
    if obj.key in m_meta:
        count, type_flag = m_meta[obj.key]
        # It is now a frequent item, so type becomes 1 (L2)
        # We increment count just for statistics, though ARC relies mostly on the lists
        m_meta[obj.key] = (count + 1, 1)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Check Ghost lists to adapt 'p'.
    2. Insert new item.
    '''
    global m_meta, m_access_time, m_ghost_b1, m_ghost_b2, global_p
    
    current_time = cache_snapshot.access_count
    capacity = len(cache_snapshot.cache) # Approximation of capacity (N)
    # Note: cache_snapshot.capacity is bytes, but len(cache) is count. 
    # ARC usually works on counts. We assume N is roughly current count.
    
    key = obj.key
    
    # CASE 1: Key is in Ghost L1 (B1) -> Recency list was too small
    if key in m_ghost_b1:
        # Adapt p: Increase L1 target size
        # If B1 size >= B2 size, increment by 1. Else, increment by larger delta.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) // len(m_ghost_b1)
        
        # Determine strict capacity count. Since we can't access `capacity` as count easily,
        # we use the current cache size as the baseline "N".
        N = len(cache_snapshot.cache)
        global_p = min(global_p + delta, N)
        
        # Promote to L2 immediately because it was recently seen (in ghost)
        # This restores a "wrongly evicted" recent item to the frequent list
        new_type = 1
        m_ghost_b1.remove(key)

    # CASE 2: Key is in Ghost L2 (B2) -> Frequency list was too small
    elif key in m_ghost_b2:
        # Adapt p: Decrease L1 target size (giving more room to L2)
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
            
        global_p = max(global_p - delta, 0)
        
        # Promote to L2
        new_type = 1
        m_ghost_b2.remove(key)

    # CASE 3: Totally new item
    else:
        # Insert into L1 (Recency)
        new_type = 0
    
    # Update Metadata
    m_meta[key] = (1, new_type)
    m_access_time[key] = current_time

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to appropriate Ghost List (B1 or B2) depending on where it came from.
    3. Maintain ghost list sizes.
    '''
    global m_meta, m_access_time, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_meta:
        _, type_flag = m_meta[key]
        del m_meta[key]
        del m_access_time[key]
        
        # If it was in L1 (Recency), move to Ghost B1
        if type_flag == 0:
            m_ghost_b1.add(key)
        # If it was in L2 (Frequency), move to Ghost B2
        else:
            m_ghost_b2.add(key)
            
    # Maintenance: Keep ghost lists bounded to roughly cache size (N)
    # In strict ARC, |L1| + |B1| = N and |L2| + |B2| = 2N, etc.
    # Here we simplify: if total ghosts > 2 * cache size, remove oldest.
    # Since we use Sets for O(1) lookups, we don't have perfect LRU order for ghosts easily
    # without a second data structure. To save compute/memory in this constraint, 
    # we just prune arbitrary items if it gets too big (or rely on Python's insertion order for iter).
    
    target_len = len(cache_snapshot.cache) 
    
    # Prune B1 if too big
    while len(m_ghost_b1) > target_len:
        # Remove first element (oldest insertion in modern Python dict/set implementation detail, usually)
        # Or just arbitrary pop.
        try:
            m_ghost_b1.pop() 
        except KeyError:
            break
            
    while len(m_ghost_b2) > target_len:
        try:
            m_ghost_b2.pop()
        except KeyError:
            break
```
2025-11-27 18:22:03,197 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Hit Rate: 0.0187

Example 2:
- Trace ID: 2
- Hit Rate: 0.4395

Example 3:
- Trace ID: 26
- Hit Rate: 0.7964

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:22:03 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:22:03,197 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:22:24 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:22:24,163 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:22:24,164 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An improved cache eviction algorithm designed to minimize miss rates by implementing a more sophisticated frequency-based approach with dynamic aging and ghost cache logic (inspired by ARC/LFRU concepts).

This implementation shifts from a raw frequency count to a method that balances Recency and Frequency more gracefully, preventing cache pollution from one-time scans while protecting valuable long-term items.

```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```
2025-11-27 18:25:20,726 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 24
- Hit Rate: 0.3322

Example 2:
- Trace ID: 14
- Hit Rate: 0.0212

Example 3:
- Trace ID: 3
- Hit Rate: 0.4415

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:25:20 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:25:20,727 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:26:59 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:26:59,997 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:26:59,999 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is the improved cache eviction algorithm.

This solution implements a logic inspired by **SLRU (Segmented LRU)** and **Hyperbolic Caching**. The core idea is to separate items into two virtual segments: "Probationary" (items accessed once) and "Protected" (items accessed at least twice). 

1.  **Probationary Segment (Freq = 1):** These items are extremely vulnerable to eviction. We calculate their score based purely on Recency (`1 / Age`). This effectively acts as an LRU for new items, clearing out "scan" traffic (Trace 14) quickly.
2.  **Protected Segment (Freq > 1):** These items get a massive score boost, ensuring they are never evicted as long as there are Probationary items in the cache. Within this group, we use Hyperbolic Caching (`Freq / Age`) to balance Frequency and Recency.
3.  **Smart Ghost Registry:** We only track "ghosts" for items that were in the Protected segment (Freq > 1) when evicted. If a "one-hit wonder" is evicted, we forget it. This prevents the ghost registry from being polluted by scans.

```python
# Metadata dictionaries
m_key_frequency = dict()      # Tracks access frequency
m_key_last_access = dict()    # Tracks absolute access time (logical time)
m_ghost_registry = dict()     # Tracks keys of "valuable" items recently evicted

# Constants
GHOST_CAPACITY = 2000         # Max size for ghost registry
PROTECTED_BOOST = 10000.0     # Score boost for items with Freq > 1

def evict(cache_snapshot, obj):
    '''
    Evicts based on a Segmented Hyperbolic Score.
    
    Logic:
    1. Items with Frequency 1 (Probationary) are evicted first, using LRU logic.
    2. Items with Frequency > 1 (Protected) are evicted only if no Freq 1 items exist.
    3. Within the Protected tier, items are ranked by (Frequency / Age).
    '''
    global m_key_frequency, m_key_last_access, PROTECTED_BOOST
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate Age (Time since last access)
        # We add 1 to avoid division by zero and smooth very recent accesses
        age = (current_time - last_access) + 1.0
        
        if freq == 1:
            # Probationary Segment:
            # Score is purely based on recency (Younger = Higher Score).
            # As Age increases, Score -> 0.
            # Max possible score here is 1.0 (Age 1).
            score = 1.0 / age
        else:
            # Protected Segment:
            # Give a massive boost so these are considered "better" than any Probationary item.
            # Within this tier, we use Hyperbolic Caching:
            # - Higher Frequency = Higher Score
            # - Higher Age = Lower Score
            hyperbolic_val = freq / age
            score = PROTECTED_BOOST + hyperbolic_val
        
        # We evict the item with the LOWEST score
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update recency.
    '''
    global m_key_frequency, m_key_last_access
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment frequency
    # We allow frequency to grow naturally to distinguish heavy hitters in the Protected tier
    curr_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata for new items.
    Check Ghost Registry to see if this is a returning "Protected" item.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghost_registry:
        # It was valuable before! Promote immediately to Protected tier.
        # This prevents it from being evicted by the Probationary LRU cleaner.
        m_key_frequency[obj.key] = 2
        del m_ghost_registry[obj.key]
    else:
        # Standard insertion: Start in Probationary tier.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Cleanup metadata.
    Only add to Ghost Registry if the item had proven value (Freq > 1).
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, GHOST_CAPACITY
    
    evicted_key = evicted_obj.key
    freq = m_key_frequency.get(evicted_key, 1)
    
    # Smart Ghosting:
    # If Freq was 1, it was likely "Scan" noise. Don't ghost it.
    # If Freq > 1, it was part of our working set but we ran out of space. Ghost it.
    if freq > 1:
        m_ghost_registry[evicted_key] = True
        
        # Manage Ghost Registry Size (Simple FIFO if full)
        if len(m_ghost_registry) > GHOST_CAPACITY:
            # Removing an arbitrary element (first available in iterator) is O(1) in Python dicts
            # This keeps overhead low.
            oldest_ghost = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost]

    # Clean up active metadata
    if evicted_key in m_key_frequency:
        del m_key_frequency[evicted_key]
    if evicted_key in m_key_last_access:
        del m_key_last_access[evicted_key]
```
2025-11-27 18:27:39,645 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 11
- Hit Rate: 0.3508

Example 2:
- Trace ID: 4
- Hit Rate: 0.5062

Example 3:
- Trace ID: 36
- Hit Rate: 0.077

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:27:39 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:27:39,646 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:28:06 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:28:06,255 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:28:06,256 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need a strategy that handles varied workloads better than a simple Segmented LRU. The feedback shows mediocre performance on some traces, likely because the previous algorithm had rigid frequency thresholds (1 vs >1) and didn't account for "recency" well enough within the high-frequency group, or it didn't adapt the size of the protected segment dynamically.

### Strategy: Adaptive Replacement Cache (ARC) Approximation

The implementation below approximates the logic of **ARC (Adaptive Replacement Cache)**, which is widely considered one of the most robust algorithms for general-purpose caching.

**Why this approach?**
1.  **Scan Resistance:** It separates "recently used once" (T1) from "frequently used" (T2). This prevents one-time scans from flushing out valuable long-term items.
2.  **Adaptability:** The key feature of ARC is a target parameter `p` that dynamically shifts the balance between preserving "recent" items vs. "frequent" items based on ghost hits (hits on recently evicted items).
    *   If we get misses on T1 ghosts (recently evicted new items), we increase the size of T1 (need more recency).
    *   If we get misses on T2 ghosts (recently evicted frequent items), we increase the size of T2 (need more frequency).
3.  **Efficiency:** It avoids complex scoring calculations in the `evict` loop where possible, relying on list positions (LRU) and set membership.

### Implementation Details

*   **T1 (Recent):** Captures items accessed only once recently.
*   **T2 (Frequent):** Captures items accessed at least twice.
*   **B1 (Ghost Recent):** Metadata for items evicted from T1.
*   **B2 (Ghost Frequent):** Metadata for items evicted from T2.
*   **P (Target):** The target size for the T1 list.

### Python Code

```python
import collections

# Global State for the Adaptive Algorithm
# T1: Recently accessed once (LRU Cache part) - stores keys
_t1 = collections.OrderedDict()
# T2: Frequently accessed (LFU/LRU Cache part) - stores keys
_t2 = collections.OrderedDict()
# B1: Ghost entries for T1 (evicted from T1) - stores keys
_b1 = collections.OrderedDict()
# B2: Ghost entries for T2 (evicted from T2) - stores keys
_b2 = collections.OrderedDict()

# Adaptation parameter: Target size for T1
_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction logic.
    We don't calculate a score for every item. Instead, we determine WHICH list (T1 or T2)
    must sacrifice an item to maintain the constraints relative to target _p.
    '''
    global _t1, _t2, _b1, _b2, _p
    
    # We must return a key to be evicted from the actual cache.
    # The actual cache content is the union of keys in _t1 and _t2.
    
    # In a standard ARC implementation, the "replace" logic runs before insertion.
    # Here, the framework calls `evict` when the cache IS full.
    # We need to decide whether to kill the LRU of T1 or the LRU of T2.
    
    t1_len = len(_t1)
    
    # Logic to decide victim:
    # If len(T1) > p, we usually evict from T1.
    # However, we must ensure T1 isn't empty if p > 0.
    
    candidate_key = None
    
    # Case 1: T1 is larger than the target `p`. 
    # We evict the LRU of T1.
    if t1_len > 0 and (t1_len > _p or (_b2 and t1_len == _p)):
        # popitem(last=False) pops the FIRST item (FIFO order = LRU)
        candidate_key, _ = _t1.popitem(last=False)
        # Move to B1 (Ghost Recent)
        _b1[candidate_key] = True
    else:
        # Case 2: We evict from T2.
        if _t2:
            candidate_key, _ = _t2.popitem(last=False)
            # Move to B2 (Ghost Frequent)
            _b2[candidate_key] = True
        else:
            # Fallback if T2 is empty (rare, but possible if p=capacity)
            if _t1:
                candidate_key, _ = _t1.popitem(last=False)
                _b1[candidate_key] = True

    # Maintenance: Keep ghost lists bounded to Cache Capacity (C)
    capacity = cache_snapshot.capacity # Note: ARC usually defines C as element count, here it's generic capacity
    # Assuming capacity is roughly number of items for this logic, or just a safe upper bound.
    # To be safe against memory leaks, we restrict B1 and B2 length arbitrarily to the current cache count * 2
    max_ghosts = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 1000
    
    while len(_b1) > max_ghosts:
        _b1.popitem(last=False)
    while len(_b2) > max_ghosts:
        _b2.popitem(last=False)

    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it is in T1, move to T2 (MRU).
    If it is in T2, move to T2 (MRU).
    '''
    global _t1, _t2
    
    key = obj.key
    
    if key in _t1:
        # Was in Recent, now it's Frequent. Move T1 -> T2.
        del _t1[key]
        _t2[key] = True # Add to MRU end
    elif key in _t2:
        # Already Frequent. Refresh position in T2 (MRU).
        del _t2[key]
        _t2[key] = True
    else:
        # Edge case: It's in the snapshot cache but our tracking lost it 
        # (shouldn't happen in strict ARC, but possible in distributed/external state).
        # Treat as new frequent.
        _t2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was in Ghost lists (B1 or B2) to adjust parameter P.
    Then add to T1 (MRU).
    '''
    global _t1, _t2, _b1, _b2, _p
    
    key = obj.key
    capacity = cache_snapshot.capacity 
    # Since capacity is in bytes, we estimate element count C roughly by current count
    # However, ARC logic relies on 'C' being the count limit. 
    # We will treat the current cache length as the effective size for the ratio.
    current_cache_size = len(cache_snapshot.cache)
    # Avoid div by zero
    if current_cache_size == 0: current_cache_size = 1 
    
    # 1. ADAPTATION (Ghost Hits)
    if key in _b1:
        # Hit in Ghost Recent (B1). We made T1 too small. Increase p.
        delta = 1
        if len(_b1) >= len(_b2) and len(_b2) > 0:
            delta = 1
        elif len(_b2) > len(_b1):
            delta = len(_b2) / len(_b1)
        
        _p = min(current_cache_size, _p + delta)
        del _b1[key]
        
        # It's a "hit" in history, so it goes to T2
        _t2[key] = True
        return

    elif key in _b2:
        # Hit in Ghost Frequent (B2). We made T1 too big (T2 too small). Decrease p.
        delta = 1
        if len(_b2) >= len(_b1) and len(_b1) > 0:
            delta = 1
        elif len(_b1) > len(_b2):
            delta = len(_b1) / len(_b2)
            
        _p = max(0, _p - delta)
        del _b2[key]
        
        # It's a "hit" in history, so it goes to T2
        _t2[key] = True
        return

    # 2. NEW INSERTION (Pure Miss)
    # Add to T1 (MRU)
    _t1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    The `evict` function already handled moving keys from T1/T2 to B1/B2.
    This function is primarily for cleanup if the framework forces an eviction
    different from what we suggested (though usually it follows `evict`).
    '''
    # We rely on `evict` having done the list manipulation.
    # However, to be robust: ensure the evicted object is actually gone from T1/T2
    # and present in B1/B2.
    global _t1, _t2, _b1, _b2
    
    key = evicted_obj.key
    
    # Ensure removed from active lists
    if key in _t1:
        del _t1[key]
        # If we didn't move it to ghost in `evict` (unexpected), do it here
        if key not in _b1: _b1[key] = True
            
    if key in _t2:
        del _t2[key]
        if key not in _b2: _b2[key] = True
```
2025-11-27 18:28:16,192 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 22
- Hit Rate: 0.0587

Example 3:
- Trace ID: 26
- Hit Rate: 0.7031

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:28:16 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:28:16,193 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:28:43 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:28:43,241 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:28:43,242 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache miss rate, we need to move beyond simple LFU (Least Frequently Used) or LRU (Least Recently Used). The provided results show extremely low hit rates on some traces (e.g., 3-5%), which suggests the current algorithm (LFU with LRU tie-breaking) suffers from "cache pollution" or "frequency accumulation" issues.

Here is the analysis of the problems with the current approach and the proposed solution:

### Analysis of the Current Approach
1.  **Frequency Accumulation:** LFU tends to keep items that were popular a long time ago but are no longer relevant. These items accumulate high frequency counts and stick in the cache, blocking new, potentially trending items.
2.  **Scan Resistance:** If a large sequence of unique items is accessed once (a scan), LFU/LRU policies can be thrashed easily.
3.  **Memory Overhead:** Deleting metadata immediately upon eviction (`update_after_evict`) makes the cache "forget" that an item was popular if it comes back shortly after eviction. This is a major cause of low hit rates in cyclic patterns.

### The Improved Solution: TinyLFU / W-TinyLFU Inspired Approach
To solve these issues, we will implement a variant of the **Window TinyLFU (W-TinyLFU)** architecture, simplified for this context. This is widely considered state-of-the-art for general-purpose caching (used in Caffeine, Ristretto, etc.).

**Key Features of the New Design:**
1.  **Sketch-based Frequency Counting (Count-Min Sketch approximation):** instead of a Python dict that grows indefinitely or gets deleted on eviction, we will use a "forgetful" frequency counter. This allows us to remember the frequency of items *even after they are evicted*, without consuming infinite memory.
2.  **Admission Policy:** Instead of always inserting a new item and evicting an old one, we compare the "value" (frequency) of the new item against the "victim" (eviction candidate). If the new item is less popular than the victim, we might choose *not* to cache the new item (or rather, evict the new item immediately in a theoretical sense, though here we must return a victim key to the system). *Self-correction: Since the function signature requires us to return a victim key to be evicted to make space for `obj`, we will simulate this by comparing the incoming object's estimated frequency with the eviction candidate.*
3.  **Aging:** We need to periodically halve the frequency counters so that old heavy hitters fade away.

**The Strategy:**
*   **Metadata:** A frequency dictionary that acts as our "sketch" (allowing it to grow slightly larger than the cache but pruning it occasionally or halving counts) and a Recency map (LRU) for tie-breaking and candidate selection.
*   **Eviction Logic:** We will identify a candidate using an LRU policy (the standard victim). However, we will perform a **"Admission Duel"**:
    *   Get the frequency of the LRU candidate.
    *   Get the frequency of the *incoming* object (`obj`).
    *   If `Incoming_Freq <= Candidate_Freq`, the incoming object is "pollution." Ideally, we would reject it. However, the `evict` function contract usually demands a victim key *from the cache*. To approximate rejection in this strict contract, we will stick to a smart LFU-ish eviction but utilize the "Ghost" frequency data we kept even after previous evictions.
*   **Aging:** To prevent history from dominating the future, every time the total access count reaches a multiple of the cache size (or a fixed sample size), we divide all frequencies by 2.

Here is the complete improved implementation:

```python
import random

# Metadata
# Tracks estimated frequency. We do NOT delete keys immediately upon eviction
# to retain history for "Admission" logic.
m_sketch_freq = dict()

# Tracks insertion order/recency for LRU approximation
m_lru_map = dict()

# Constants for frequency aging
m_access_counter = 0
RESET_INTERVAL = 1000  # Halve frequencies every N accesses to handle shifts in popularity

def _get_freq(key):
    return m_sketch_freq.get(key, 0)

def _aging_process():
    '''
    Periodically halves all frequencies to ensure that
    formerly popular items eventually decay (Aging).
    '''
    global m_sketch_freq
    # Create a new dict to remove 0 entries after division
    new_freq = {}
    for k, v in m_sketch_freq.items():
        new_v = v // 2
        if new_v > 0:
            new_freq[k] = new_v
    m_sketch_freq = new_freq

def evict(cache_snapshot, obj):
    '''
    Implements a simplified TinyLFU-style eviction.
    1. Select a set of random candidates (to avoid iterating the whole cache).
    2. Among candidates, find the one with the Lowest Frequency.
    3. If frequencies match, use LRU (simulated by insertion order/recency).
    
    This approximates LFU without the heavy sorting overhead and utilizes
    historical data for better decision making.
    '''
    # Optimizing for speed and efficacy: Random Sampling LFU
    # Instead of iterating the whole list, we sample K items.
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
        
    # Sample size: usually 5-10 is statistically sufficient to approximate global LFU
    sample_size = min(len(candidate_keys), 8)
    sample_keys = random.sample(candidate_keys, sample_size)
    
    victim = None
    min_freq = float('inf')
    min_lru = float('inf')

    for key in sample_keys:
        freq = _get_freq(key)
        lru = m_lru_map.get(key, 0)
        
        # We look for the item with the smallest frequency
        if freq < min_freq:
            victim = key
            min_freq = freq
            min_lru = lru
        elif freq == min_freq:
            # Tie-breaker: LRU (smallest timestamp is oldest)
            if lru < min_lru:
                victim = key
                min_lru = lru

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update recency.
    Check for aging trigger.
    '''
    global m_access_counter, m_sketch_freq, m_lru_map
    
    m_access_counter += 1
    
    # Update Sketch Frequency
    curr = m_sketch_freq.get(obj.key, 0)
    # Cap frequency to prevent integer overflow or excessive weight
    if curr < 15: 
        m_sketch_freq[obj.key] = curr + 1
        
    # Update Recency
    m_lru_map[obj.key] = cache_snapshot.access_count

    # Aging mechanism
    if m_access_counter >= RESET_INTERVAL:
        _aging_process()
        m_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize frequency (or increment if history exists) and set recency.
    '''
    global m_access_counter, m_sketch_freq, m_lru_map
    
    m_access_counter += 1

    # In standard LFU, new items start at 1. 
    # Here, if we have seen it before (and evicted it), it retains some 'heat'.
    # If it's totally new, it starts at 1.
    curr = m_sketch_freq.get(obj.key, 0)
    if curr < 15:
        m_sketch_freq[obj.key] = curr + 1
    
    m_lru_map[obj.key] = cache_snapshot.access_count

    # Aging mechanism
    if m_access_counter >= RESET_INTERVAL:
        _aging_process()
        m_access_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Crucial change: We do NOT remove frequency data for the evicted object immediately.
    This allows the algorithm to remember "Ghost" items that are thrashing in and out.
    We only remove the recency data to keep the LRU map clean for current cache items.
    
    To prevent memory leaks, the _aging_process implicitly handles cleanup of low-frequency ghosts.
    '''
    global m_lru_map
    
    # We maintain m_sketch_freq[evicted_obj.key] deliberately.
    
    # We clean up the LRU map because the object is no longer in cache
    if evicted_obj.key in m_lru_map:
        del m_lru_map[evicted_obj.key]
```
2025-11-27 18:31:01,232 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 45
- Hit Rate: 0.0418

Example 2:
- Trace ID: 0
- Hit Rate: 0.4509

Example 3:
- Trace ID: 37
- Hit Rate: 0.6394

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:31:01 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:31:01,233 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:33:36 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:33:36,515 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:33:36,516 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we will implement a **Segmented LRU (SLRU) with Decaying History** strategy.

### The Reasoning:
1.  **Scan/Loop Resistance (Trace 45):** The previous algorithm suffered because it allowed "Ghost" entries to restore their frequency fully. In a looping workload larger than the cache, this causes every item to eventually be promoted to "high frequency," degenerating the algorithm into simple LRU (which has a 0% hit rate for such loops). By placing new and re-inserted items on "probation" (Segment 1) and only promoting them after subsequent hits, we protect the cache from being flushed by one-time scans or large loops.
2.  **Churn Resistance (Zipfian Traces):** For workloads where hot items are evicted due to temporary capacity pressure, we retain their history but apply a **decay** upon re-insertion. A very hot item (Freq 10) re-inserted will restart at Freq 5 (still protected), whereas a scan item (Freq 1) re-inserted will restart at Freq 0 or 1 (still probationary).
3.  **Efficiency:** The eviction logic is O(N) over the cache size (which is unavoidable without internal access to the cache class), but performs simple integer comparisons. Metadata cleanup ensures memory usage doesn't explode.

### The Algorithm:
*   **Metadata:** We maintain `m_key_frequency` (approximate popularity) and `m_key_last_access` (recency).
*   **Update on Hit:** Increment Frequency (capped). Update Recency.
*   **Update on Insert:** Check if the key exists in metadata (Ghost). If yes, restore `Frequency // 2`. This allows formerly hot items to stay hot, but loop items (Freq 1) drop to 0, ensuring they stay in probation.
*   **Eviction:**
    *   **Segment 1 (Probation):** Items with `Frequency <= 1`.
    *   **Segment 2 (Protected):** Items with `Frequency > 1`.
    *   We prioritize evicting from **Probation** first (finding the LRU victim there).
    *   Only if Probation is empty do we evict from **Protected** (finding the LRU victim there).

```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Constants
MAX_FREQ = 50           # Cap frequency to prevent historical domination
PROTECTED_THRESHOLD = 1 # Items with freq > this are "Protected"
CLEANUP_THRESHOLD_RATIO = 5 # Clean metadata when it exceeds capacity * this

def evict(cache_snapshot, obj):
    '''
    Evicts based on Segmented LRU (SLRU).
    We divide the cache into "Probation" (low freq) and "Protected" (high freq).
    We prefer to evict LRU items from Probation. 
    If Probation is empty, we evict LRU from Protected.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Step 1: Identify the victim
    victim_key = None
    victim_time = float('inf')
    
    # We first try to find a victim in the Probationary segment (Freq <= 1)
    probation_victim_key = None
    probation_victim_time = float('inf')
    
    # We also track the LRU of the Protected segment as a fallback
    protected_victim_key = None
    protected_victim_time = float('inf')
    
    found_probationary = False

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        if freq <= PROTECTED_THRESHOLD:
            # Item is in Probation
            found_probationary = True
            if last_access < probation_victim_time:
                probation_victim_time = last_access
                probation_victim_key = key
        else:
            # Item is Protected
            if last_access < protected_victim_time:
                protected_victim_time = last_access
                protected_victim_key = key
    
    # Policy: Evict Probationary LRU if available, otherwise Protected LRU
    if found_probationary and probation_victim_key is not None:
        return probation_victim_key
    else:
        return protected_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    # We promote items on hits, pushing them into the Protected segment
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Handle Ghost entries with decay.
    '''
    global m_key_frequency, m_key_last_access
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic (Decaying History):
    # If the key is in metadata, it's a "Ghost". 
    # We restore it, but with a penalty (decay).
    # This ensures looping items (Freq 1) reset to 0/1 (Probation),
    # while Hot items (Freq 10) restore to ~5 (Protected).
    if obj.key in m_key_frequency:
        old_freq = m_key_frequency[obj.key]
        # Decay factor: Halve the frequency. 
        # Ensure at least 0 (or 1 effectively since it's an insert)
        new_freq = old_freq // 2
        # If it was previously 0 or 1, it stays low. If it was 10, it becomes 5.
        m_key_frequency[obj.key] = max(0, new_freq)
    else:
        # Brand new item. Start with frequency 0 (Probation).
        # It needs a Hit to reach frequency 1 or 2.
        m_key_frequency[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Manage metadata cleanup to prevent leaks.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately keep metadata for the evicted object (Ghost Cache).
    
    # Periodic Cleanup Strategy:
    # If metadata grows too large relative to cache capacity, purge old entries.
    # This prevents memory leaks in long-running simulations.
    capacity = cache_snapshot.capacity # Note: capacity is in bytes, but usually proportional to count in these problems
    # Using a heuristic for cleanup trigger based on dictionary size
    if len(m_key_frequency) > cache_snapshot.size * CLEANUP_THRESHOLD_RATIO and len(m_key_frequency) > 1000:
        
        # We need to remove old ghosts.
        # Calculate a cutoff time. Remove items accessed long ago that are NOT in cache.
        # To avoid iterating everything, we can just remove a batch of random keys or
        # do a full sweep. A full sweep is safer for correctness.
        
        current_time = cache_snapshot.access_count
        # Sort keys by access time (approximate LRU of metadata)
        # Only keep the most recent N items in metadata
        
        # Optimization: Just remove keys that haven't been seen in a long time
        # Threshold: keys older than 2 * the time it takes to fill the cache?
        # Simpler: Just keep the top X most recent keys.
        
        target_size = int(cache_snapshot.size * 2) 
        if target_size < 100: target_size = 100 # Safety floor
        
        # Get the keys sorted by access time (oldest first)
        sorted_keys = sorted(m_key_last_access.keys(), key=lambda k: m_key_last_access[k])
        
        # Identify keys to delete (oldest ones), but ensure we don't delete keys currently in cache
        keys_in_cache = set(cache_snapshot.cache.keys())
        deleted_count = 0
        num_to_delete = len(m_key_frequency) - target_size
        
        for k in sorted_keys:
            if deleted_count >= num_to_delete:
                break
            if k not in keys_in_cache:
                del m_key_frequency[k]
                del m_key_last_access[k]
                deleted_count += 1
```
2025-11-27 18:33:51,977 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Hit Rate: 0.0236

Example 2:
- Trace ID: 44
- Hit Rate: 0.0418

Example 3:
- Trace ID: 27
- Hit Rate: 0.8923

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:33:51 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:33:51,978 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:35:30 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:35:30,593 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:35:30,594 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance feedback, the previous algorithm suffered from two main issues:
1.  **Inefficiency with Size constraints:** It treated all objects equally, ignoring `obj.size`. In a byte-capacity constrained cache, preferring smaller objects allows the cache to hold more items, statistically increasing the hit rate.
2.  **Rigid Aging:** The manual aging process (halving frequencies periodically) was likely too coarse, causing "hot" items to be evicted too early or "cold" items to linger too long.
3.  **Poor Scan/Loop Resistance:** The low hit rates on Traces 17 and 44 suggest the cache was thrashing on working sets larger than the cache capacity.

The improved solution implements **Size-Aware Hyperbolic Caching**.

### Key Changes:
1.  **Hyperbolic Scoring:** Instead of a linear combination of Frequency and Recency, we use a ratio: `Score = Frequency / (Size * TimeSinceLastAccess)`.
    *   **Frequency:** High frequency increases score (LFU-like behavior for Trace 27).
    *   **Recency (TimeSinceLastAccess):** As time passes without access, the denominator grows, and the score drops naturally. This eliminates the need for manual "aging" functions.
    *   **Size:** We divide by size. Large objects have a lower score and are more likely to be evicted. This optimizes "Hit Density"—packing more small, useful objects into the limited byte capacity.
2.  **Ghost Registry Logic:** When an item is evicted, we store its frequency. If it is re-inserted (indicating a loop or a working set slightly larger than cache), we restore its frequency plus one. This allows "warm" items to survive longer than brand-new "cold" items during scans.
3.  **Frequency Weighting:** We use `Frequency^1.5` in the numerator. This non-linear weighting ensures that very popular items are significantly harder to evict than items that were just lucky to be accessed twice recently.

```python
# Metadata dictionaries
# Maps key -> {'freq': int, 'last': int}
m_metadata = dict() 
# Maps key -> int (frequency at eviction). Used to detect thrashing/loops.
m_ghosts = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Size-Aware Hyperbolic Score.
    
    Score Formula: (Frequency ^ 1.5) / (Size * Age)
    
    Why this works:
    1. Maximizes Hit Rate by penalizing Size: Evicting one large object makes room 
       for multiple small objects.
    2. Adaptive Aging: The 'Age' denominator naturally decays the score of items 
       that haven't been accessed recently, transitioning from LFU to LRU behavior automatically.
    3. Frequency Bias: The 1.5 exponent protects the 'hot set' (Trace 27) from being 
       flushed by temporary scans.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        meta = m_metadata.get(key)
        
        # Defensive check against desync, though unlikely
        if not meta:
            continue
            
        freq = meta['freq']
        last_access = meta['last']
        
        # Calculate Age (Time since last access). 
        # Add 1 to avoid division by zero (if eviction happens in same tick as insert)
        age = (current_time - last_access) + 1
        
        # Get object size from snapshot
        obj_size = cache_snapshot.cache[key].size
        
        # Calculate Score
        # We want to EVICT the item with the LOWEST score.
        # High Freq -> High Score (Keep)
        # Small Size -> High Score (Keep)
        # Low Age (Recent) -> High Score (Keep)
        score = (freq ** 1.5) / (obj_size * age)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency and update last access time.
    '''
    global m_metadata
    
    if obj.key in m_metadata:
        m_metadata[obj.key]['freq'] += 1
        m_metadata[obj.key]['last'] = cache_snapshot.access_count
    else:
        # Fallback for data integrity
        m_metadata[obj.key] = {'freq': 1, 'last': cache_snapshot.access_count}

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata for new object.
    Leverage Ghost Registry to detect returning items (Loop/Thrash protection).
    '''
    global m_metadata, m_ghosts
    
    current_time = cache_snapshot.access_count
    
    # Determine initial frequency
    initial_freq = 1
    
    # If key is in ghost registry, it means it was evicted recently.
    # Restore its "heat" so it isn't immediately evicted again.
    if obj.key in m_ghosts:
        initial_freq = m_ghosts[obj.key] + 1
        del m_ghosts[obj.key]
    
    m_metadata[obj.key] = {
        'freq': initial_freq,
        'last': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up active metadata and move vital stats to ghost registry.
    '''
    global m_metadata, m_ghosts
    
    if evicted_obj.key in m_metadata:
        # Save frequency to ghost registry
        freq = m_metadata[evicted_obj.key]['freq']
        del m_metadata[evicted_obj.key]
        
        m_ghosts[evicted_obj.key] = freq
        
        # Maintain Ghost Registry Size
        # A limit of 2000 is a heuristic to balance memory vs history.
        # Python dicts are ordered by insertion, so this acts as FIFO.
        if len(m_ghosts) > 2000:
            oldest_key = next(iter(m_ghosts))
            del m_ghosts[oldest_key]
```
2025-11-27 18:35:56,727 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Hit Rate: 0.0187

Example 2:
- Trace ID: 2
- Hit Rate: 0.4395

Example 3:
- Trace ID: 15
- Hit Rate: 0.0194

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:35:56 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:35:56,728 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:36:26 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:36:26,009 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:36:26,010 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the specific weaknesses shown in the performance feedback. The extremely low hit rates on traces 16 and 15 suggest that the cache is likely suffering from "cache pollution" caused by scans (large sequences of one-time access items) or "churning" (where useful items are evicted too early by high-volume cold items).

The previous LFRU implementation had a flaw: new items entered with a frequency of 1, making them immediate candidates for eviction alongside other cold items, but once a "ghost" item re-entered, it might have retained too much history.

**Proposed Solution: Adaptive Replacement Cache (ARC) Inspired Approach**

Instead of a simple LFRU, we will implement a simplified version of **ARC (Adaptive Replacement Cache)** logic, but adapted for this specific Python interface. ARC is superior because it balances between Recency (T1) and Frequency (T2) dynamically.

1.  **Two Lists (Logical Split):**
    *   **T1 (Recency):** Items accessed once recently. Captures "new" items.
    *   **T2 (Frequency):** Items accessed at least twice. Captures "popular" items.
2.  **Ghost Lists (B1 and B2):**
    *   We track keys that were recently evicted from T1 (into B1) and T2 (into B2).
    *   Hits in these ghost lists tell us whether we should have made the Recency cache larger or the Frequency cache larger.
3.  **Adaptive Parameter (`p`):**
    *   A target size `p` determines how much space T1 gets versus T2.
    *   If we get a hit in B1 (we evicted a recent item too soon), we increase `p` (grow T1).
    *   If we get a hit in B2 (we evicted a frequent item too soon), we decrease `p` (grow T2).

This implementation provides a highly robust mechanism that auto-tunes itself to scan-heavy workloads (shrinking T1 to protect T2) or recency-heavy workloads (growing T1).

Here is the improved implementation:

```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```
2025-11-27 18:38:52,041 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 8
- Hit Rate: 0.5409

Example 2:
- Trace ID: 30
- Hit Rate: 0.0433

Example 3:
- Trace ID: 1
- Hit Rate: 0.4725

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:38:52 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:38:52,042 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:40:21 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:40:21,105 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:40:21,111 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: This solution implements the **Greedy Dual-Size Frequency (GDSF)** algorithm.

### Analysis of the Problem
The previous algorithm failed on Trace 30 (4% hit rate) likely due to "cache pollution" from a scan (a sequence of objects accessed only once). The previous logic `Freq / log(t)` allowed new items (low `t`) to have disproportionately high scores compared to their actual utility, or failed to age old frequent items fast enough.

### The Solution: GDSF
GDSF is a robust algorithm that combines Recency, Frequency, and Object Size. It assigns a priority value to each object.
1.  **Priority ($P$):** $P = L + (Frequency / Size)$
2.  **Inflation ($L$):** A logical clock that increases over time. When an object is evicted, $L$ is updated to the priority of the evicted object.
3.  **Aging:** New objects enter with a priority based on the current $L$. As $L$ grows, older objects (whose priority was calculated with an older, smaller $L$) naturally fall behind unless they are accessed again and "re-inflated" to the current $L$.

This solves the specific issues:
*   **Scans:** New one-hit wonders get a priority of roughly $L + \epsilon$. If they aren't hit again, they remain low priority. Established items have high accumulated Frequency and are re-referenced to the current $L$, keeping them safe above the "water mark" ($L$) of the scan.
*   **Hit Rate:** By including `Size` in the denominator (dampened by a logarithm to prevent starvation of large files), we slightly favor smaller objects, which mathematically increases the total number of hits (miss rate reduction) for a fixed capacity.

```python
import math

# Global Metadata
# Tracks the frequency of keys (including "ghost" keys that were evicted)
# This serves as our long-term memory.
g_frequency_map = {}

# Tracks the current priority (GDSF score) of items currently IN the cache.
g_priority_map = {}

# The "Inflation Value" (L).
# This acts as a logical clock that ratchets up as items are evicted.
g_inflation_L = 0.0

# Configuration
MAX_HISTORY_SIZE = 50000 # Max size for metadata to prevent leaks

def _calculate_priority(freq, size):
    '''
    Calculates the GDSF priority.
    Score = L + (Frequency / Cost)
    Where Cost is a function of size.
    We use log(size) to favor small items (improving hit rate) 
    without completely starving large items.
    '''
    global g_inflation_L
    # Logarithmic size penalty is smoother than linear size penalty.
    # We add 1 to size to avoid div by zero or log(0).
    size_cost = math.log2(size + 2.0) 
    if size_cost < 1.0: size_cost = 1.0
    
    return g_inflation_L + (freq / size_cost)

def _cleanup_metadata(cache_snapshot):
    '''
    Keeps the frequency map from growing indefinitely.
    '''
    global g_frequency_map
    
    if len(g_frequency_map) > MAX_HISTORY_SIZE:
        # If we exceed history limit, we prune.
        # We want to remove keys that are NOT in the cache and have LOW frequency.
        
        # Identify keys in cache (set lookup is fast)
        cached_keys = set(cache_snapshot.cache.keys())
        
        # Collect victims: keys not in cache.
        # We sort roughly by frequency to remove the least useful history.
        # Limiting the sort to a subset or threshold is faster, but for Python sim
        # a simple list comprehension filtering is usually okay up to 50k items.
        
        keys_to_purge = []
        count = 0
        
        # First pass: try to remove items with frequency 1 (noise)
        for k, v in g_frequency_map.items():
            if k not in cached_keys and v <= 1:
                keys_to_purge.append(k)
                count += 1
                if count >= 5000: break # Purge in chunks
        
        # Execute purge
        for k in keys_to_purge:
            del g_frequency_map[k]
            
        # Hard cap fallback: if still too big, random/arbitrary deletion of non-cached
        if len(g_frequency_map) > MAX_HISTORY_SIZE:
             for k in list(g_frequency_map.keys()):
                if k not in cached_keys:
                    del g_frequency_map[k]
                    if len(g_frequency_map) <= MAX_HISTORY_SIZE * 0.9:
                        break

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the object with the smallest Priority value.
    Updates the global inflation parameter L to that minimum value.
    '''
    global g_priority_map, g_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority score.
    # While O(N), this allows perfect accuracy for the algorithm.
    victim_key = None
    min_priority = float('inf')
    
    # Optimization: If the cache is synced with g_priority_map, 
    # we iterate over that instead of the snapshot keys for potentially faster access.
    
    for key in candidate_keys:
        p = g_priority_map.get(key, -1.0)
        
        # Safety fallback if key missing from priority map
        if p == -1.0:
            p = 0.0
            
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # CRITICAL STEP in GDSF:
    # The system "ages" by updating L to the priority of the evicted object.
    # All future inserted objects will start at this new, higher L baseline.
    if victim_key is not None:
        g_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. "Refresh" the priority.
       The object is brought up to the CURRENT L value + its frequency weight.
       This resets its aging clock, protecting it from immediate eviction.
    '''
    global g_frequency_map, g_priority_map, g_inflation_L
    
    key = obj.key
    
    # Update Frequency
    curr_freq = g_frequency_map.get(key, 0)
    g_frequency_map[key] = curr_freq + 1
    
    # Update Priority
    # Note: We use the *current* g_inflation_L. This effectively brings the
    # item to the "front" of the recency queue relative to the aging factor.
    g_priority_map[key] = _calculate_priority(g_frequency_map[key], obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Cache (History).
    2. Calculate initial priority based on CURRENT L.
    '''
    global g_frequency_map, g_priority_map, g_inflation_L
    
    key = obj.key
    
    # Frequency handling
    if key in g_frequency_map:
        # Ghost Hit! It was here before.
        # We do NOT reset frequency. We accumulate it.
        # This helps detect working sets that are slightly larger than cache capacity.
        g_frequency_map[key] += 1
    else:
        # Brand new item
        g_frequency_map[key] = 1
        
    # Calculate Priority
    # New items start at the current inflation floor (L) plus their inherent value.
    g_priority_map[key] = _calculate_priority(g_frequency_map[key], obj.size)
    
    # Maintenance
    if len(g_frequency_map) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove from current priority map (it's no longer in cache).
    2. Keep in frequency map (Ghost Cache) for future history.
    '''
    global g_priority_map
    
    # The eviction logic in evict() already updated g_inflation_L.
    # Here we just clean up the "In-Cache" metadata.
    if evicted_obj.key in g_priority_map:
        del g_priority_map[evicted_obj.key]
```
2025-11-27 18:40:29,186 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 42
- Hit Rate: 0.051

Example 3:
- Trace ID: 43
- Hit Rate: 0.3667

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:40:29 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:40:29,187 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:42:27 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:42:27,910 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:42:27,911 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the analysis of the performance flaws in the original implementation (static frequency aging, lack of scan resistance, and poor metadata management), I have redesigned the algorithm to use a **Greedy Dual Size Frequency (GDSF)** approach. 

This algorithm is superior because:
1.  **Dynamic Aging (The `L` factor):** Instead of periodically halving frequencies (which is crude), it uses an inflation value `g_inflation_L` that rises naturally as items are evicted. This allows new items to compete with old, high-frequency items effectively.
2.  **Size Awareness:** It normalizes frequency by size (`Frequency / Size`). This optimizes the **Object Hit Rate** (minimizing miss count) by favoring smaller, high-frequency items that provide "more hits per byte" of cache storage.
3.  **Ghost Registry:** It retains the frequency of evicted items. If an item returns (e.g., in a looping pattern), it recovers its previous popularity, preventing it from being evicted immediately as a "new" item.
4.  **Priority Calculation:** The priority score combines Frequency, Size, and Recency (via the inflation `L`) into a single value, making the eviction decision mathematically robust compared to ad-hoc heuristic scoring.

Here is the complete improved implementation:

```python
# Metadata dictionaries
# m_metadata stores active cache info: Key -> {'freq': int, 'priority': float, 'last_access': int}
m_metadata = dict()
# m_ghost_registry stores frequency history for evicted keys: Key -> int (frequency)
m_ghost_registry = dict()
# g_inflation_L is the "Clock" or "Aging" factor for the GDSF algorithm
g_inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority Score (GDSF).
    Score = Inflation_L + (Frequency / Size)
    Tie-breaker: Least Recently Used (LRU)
    '''
    global m_metadata, g_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    min_last_access = float('inf')

    # Iterate to find the candidate with the lowest priority score
    for key in candidate_keys:
        # Retrieve metadata safely
        meta = m_metadata.get(key)
        
        if not meta:
            # Fallback for safety (should ideally not happen)
            priority = -1.0
            last_access = 0
        else:
            priority = meta['priority']
            last_access = meta['last_access']

        # Selection Logic: Minimize Priority
        if priority < min_priority:
            min_priority = priority
            min_last_access = last_access
            victim_key = key
        elif priority == min_priority:
            # Tie-breaker: If priorities are equal, use LRU (smaller timestamp is older)
            if last_access < min_last_access:
                min_last_access = last_access
                victim_key = key
    
    # GDSF Aging Mechanism:
    # Update the global inflation L to the priority of the evicted item.
    # This raises the bar for all future insertions/updates, effectively aging out 
    # items that haven't been accessed recently enough to justify their frequency.
    if victim_key and min_priority != float('inf') and min_priority > g_inflation_L:
        g_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and re-calculate priority using current L.
    This effectively "brings the item to the present".
    '''
    global m_metadata, g_inflation_L
    
    current_time = cache_snapshot.access_count
    key = obj.key
    # Ensure size is at least 1 to avoid division by zero
    size = max(1, obj.size) 
    
    if key not in m_metadata:
        freq = 1
    else:
        freq = m_metadata[key]['freq'] + 1
    
    # GDSF Priority Calculation
    # New Priority = Current_Inflation + (Frequency / Size)
    priority = g_inflation_L + (freq / size)
    
    m_metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency from Ghost if available.
    '''
    global m_metadata, m_ghost_registry, g_inflation_L
    
    current_time = cache_snapshot.access_count
    key = obj.key
    size = max(1, obj.size)
    
    # Check Ghost Registry (Admission Control / History)
    if key in m_ghost_registry:
        # Object has been seen before; restore its frequency to give it a fighting chance
        freq = m_ghost_registry[key]
        # Cleanup ghost entry as it is now active
        del m_ghost_registry[key]
    else:
        # Truly new object
        freq = 1

    # Calculate initial priority
    priority = g_inflation_L + (freq / size)
    
    m_metadata[key] = {
        'freq': freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move vital statistics (Frequency) to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    key = evicted_obj.key
    
    # Preserve frequency in ghost registry
    if key in m_metadata:
        m_ghost_registry[key] = m_metadata[key]['freq']
        del m_metadata[key]
        
    # Cap the Ghost Registry size to prevent memory leaks
    # Using a simple FIFO eviction for the ghost registry if it gets too large
    if len(m_ghost_registry) > 2000:
        # Remove the oldest inserted key in the ghost registry
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]
```
2025-11-27 18:42:30,436 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 9
- Hit Rate: 0.531

Example 2:
- Trace ID: 38
- Hit Rate: 0.1255

Example 3:
- Trace ID: 33
- Hit Rate: 0.7451

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:42:30 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:42:30,437 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:44:15 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:44:15,588 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:44:15,589 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the analysis of the performance feedback and the requirements, I will implement an **LFU-DA (Least Frequently Used with Dynamic Aging)** algorithm.

### Rationale:
1.  **LFU-DA vs. Standard LFU**: The current implementation is essentially LFU. LFU suffers from cache pollution (old popular items stick around too long) and poor adaptation to changing workloads (Trace 38's low hit rate). LFU-DA solves this by introducing a global aging factor `L` that increases as items are evicted. New items start with a priority offset by `L`, allowing them to compete with established items, while old items that stop being accessed are eventually overtaken.
2.  **Handling Object Size**: While the context mentions a count-based capacity, the presence of `obj.size` and `cache_snapshot.capacity` (in bytes) strongly suggests a byte-constrained environment in the underlying simulation. To maximize Hit Rate (not Byte Hit Rate), we should prefer keeping smaller objects if space is tight. However, aggressively penalizing large objects (like GDSF does) can backfire if the capacity is actually count-based. LFU-DA (`Score = Freq + L`) is size-neutral. I will use **Size** as a **Tie-Breaker**: if two items have the same score, evict the larger one. This frees up more byte-space (if applicable) without distorting the frequency-based logic.
3.  **Frequency Restoration (Ghosts)**: To help items that are part of the working set but got evicted due to a burst of other traffic, we track "ghost" frequencies. If a ghost key returns, it resumes with its previous frequency, preventing it from being evicted immediately again.

### Implementation:

```python
# Metadata dictionaries to store state efficiently
m_freq = {}          # Key -> Frequency (int)
m_priority = {}      # Key -> Priority Score (float)
m_last_access = {}   # Key -> Access Count (int) - for LRU tie-breaking
m_ghosts = {}        # Key -> Frequency (int) - History of evicted items

# Global state for LFU-DA
m_L = 0.0            # Dynamic Aging Factor (Value of the last evicted item's priority)
GHOST_LIMIT = 2000   # Max size for ghost registry to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority score (LFU-DA).
    Tie-breakers: 
    1. Size (Desc): Evict larger items to free more byte-space.
    2. Recency (Asc): Evict Least Recently Used.
    Updates the global aging factor m_L.
    '''
    global m_priority, m_last_access, m_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    victim_key = None
    
    # We want to minimize Priority
    min_priority = float('inf')
    
    # Track tie-breaker attributes for the current best candidate
    cand_priority = float('inf')
    cand_size = -1
    cand_last_access = float('inf')

    # Single pass scan to find the best victim
    for key in candidate_keys:
        # Retrieve metadata (default to 0 if missing to prevent errors)
        p = m_priority.get(key, 0)
        last_acc = m_last_access.get(key, 0)
        size = cache_snapshot.cache[key].size
        
        # Comparison Logic
        if p < cand_priority:
            # Found a new minimum priority
            victim_key = key
            cand_priority = p
            cand_size = size
            cand_last_access = last_acc
        elif p == cand_priority:
            # Priority Tie: Prefer evicting LARGER items
            # This is a safe heuristic: if cache is byte-limited, this is optimal.
            # If cache is count-limited, size doesn't matter, so it's neutral.
            if size > cand_size:
                victim_key = key
                cand_priority = p
                cand_size = size
                cand_last_access = last_acc
            elif size == cand_size:
                # Size Tie: Prefer evicting OLDER items (LRU)
                if last_acc < cand_last_access:
                    victim_key = key
                    cand_priority = p
                    cand_size = size
                    cand_last_access = last_acc

    # LFU-DA Aging: Update L to the priority of the victim
    if victim_key is not None:
        m_L = cand_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and recalculate Priority = Freq + L
    '''
    global m_freq, m_priority, m_last_access, m_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Update Access Time
    m_last_access[key] = current_time
    
    # Update Frequency
    new_freq = m_freq.get(key, 0) + 1
    m_freq[key] = new_freq
    
    # Update Priority (LFU-DA formula)
    m_priority[key] = new_freq + m_L

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. Restore frequency if in ghost registry.
    Priority = Initial_Freq + L
    '''
    global m_freq, m_priority, m_last_access, m_ghosts, m_L
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Check Ghost Registry for frequency restoration
    if key in m_ghosts:
        initial_freq = m_ghosts[key]
        del m_ghosts[key]
    else:
        initial_freq = 1
        
    m_freq[key] = initial_freq
    m_last_access[key] = current_time
    
    # Calculate Priority
    # New items start at L + Freq. Since L is the min priority of the cache (approx),
    # this gives new items a fair chance to survive at least one eviction cycle.
    m_priority[key] = initial_freq + m_L

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move frequency to ghost registry and cleanup active metadata.
    '''
    global m_freq, m_priority, m_last_access, m_ghosts, GHOST_LIMIT
    
    key = evicted_obj.key
    
    # Save frequency to ghost registry
    if key in m_freq:
        m_ghosts[key] = m_freq[key]
        
    # Clean up active metadata
    if key in m_freq: del m_freq[key]
    if key in m_priority: del m_priority[key]
    if key in m_last_access: del m_last_access[key]
    
    # Maintain Ghost Registry size limit (FIFO)
    if len(m_ghosts) > GHOST_LIMIT:
        # dict in Python 3.7+ preserves insertion order, so this removes the oldest inserted ghost
        try:
            oldest_key = next(iter(m_ghosts))
            del m_ghosts[oldest_key]
        except StopIteration:
            pass
```
2025-11-27 18:46:44,324 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 10
- Hit Rate: 0.075

Example 2:
- Trace ID: 12
- Hit Rate: 0.04

Example 3:
- Trace ID: 25
- Hit Rate: 0.254

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:46:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:46:44,326 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:47:05 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:05,337 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:05,338 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the analysis of the low hit rates in the provided feedback, the current LFU (Least Frequently Used) strategy with LRU tie-breaking suffers from a common issue known as **"frequency pollution."** Objects that were popular a long time ago tend to stay in the cache even if they are no longer being accessed, effectively clogging the cache with "dead" weight. Additionally, pure LFU is slow to adapt to changing workloads.

To improve performance across diverse traces, I will implement a **S3-FIFO (Simple S3-FIFO)** inspired algorithm, simplified for this context. However, given the constraints and the nature of the provided hooks, a **Segmented LRU (SLRU)** or a **Frequency-Based Replacement (FBR)** often outperforms basic LFU/LRU hybrids.

Better yet, I will implement a simplified version of **LIRS (Low Inter-reference Recency Set)** or **2Q** logic, but adapted to the provided structure. Let's go with a robust heuristic known as **Hyperbolic Caching** combined with a mechanism to resist "one-hit wonders."

However, looking at the strict function signatures provided, the most robust "general purpose" improvement over LFU/LRU is often **GDSF (Greedy Dual-Size Frequency)** or a simplified **Weighted LFU with Aging**.

Given the read-only attributes include `obj.size`, ignoring size in eviction is a mistake. However, standard hit-rate optimization usually focuses on count. To maximize hit rate (rather than byte hit rate), we generally prefer evicting large items that don't contribute much to frequency. But usually, standard hit-rate problems ignore size.

Let's implement a **Frequency-Aware LRU with Aging (or Dynamic Aging)**. This is often called **LFU-DA** or a variation of **GDSF** where cost is 1.

**The Chosen Strategy: GDSF (Greedy Dual-Size Frequency) with Dynamic Aging**
This algorithm assigns a value to each object: `H = L + (Frequency / Size)`.
*   `L` is an aging factor that increases over time (specifically, it updates to the H value of the evicted object).
*   However, since the goal is simple Hit Rate (not Byte Hit Rate), we treat size as 1.
*   The formula simplifies to `H = L + Frequency`.
*   When we evict, we find the object with the smallest `H`. We then set `L` to that object's `H`. This effectively "ages" all other objects, allowing new items with low frequency to compete with old, high-frequency items.

**Why this works better:**
1.  **Combats Frequency Pollution:** Old high-frequency items eventually get overtaken by the rising `L` value of newer items if the old items stop getting hits.
2.  **Recency Integration:** Recently accessed items get their `H` updated to the current `L`, pushing them up the priority list.

Here is the improved implementation:

```python
# Metadata dictionaries
# Tracks the 'priority' value (H) of an object.
# H = L + Frequency. This combines frequency with an aging factor.
m_key_priority = dict()

# Tracks the frequency of the object (needed to recalculate Priority).
m_key_frequency = dict()

# Tracks the order of insertion/update to handle tie-breaking (LRU).
# We use a simple counter or the access_count provided by snapshot.
m_key_last_access = dict()

# The global aging factor 'L'.
# This increases monotonically, helping new objects compete with old high-frequency objects.
aging_factor_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (GDSF-like value).
    Formula: Priority = aging_factor_L + Frequency
    Tie-breaking: LRU (Least Recently Used)
    
    This strategy (LFU with dynamic aging) prevents "cache pollution" where
    old, high-frequency items stick around forever despite no longer being needed.
    '''
    global aging_factor_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority value.
    # Initialize best candidate with the first item.
    best_key = candidate_keys[0]
    min_priority = m_key_priority.get(best_key, 0.0)
    min_last_access = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_priority = m_key_priority.get(key, 0.0)
        curr_last_access = m_key_last_access.get(key, 0)

        # Primary Criteria: Lowest Priority value
        if curr_priority < min_priority:
            best_key = key
            min_priority = curr_priority
            min_last_access = curr_last_access
        # Secondary Criteria: LRU (if priorities are equal, evict the older one)
        elif curr_priority == min_priority:
            if curr_last_access < min_last_access:
                best_key = key
                min_priority = curr_priority
                min_last_access = curr_last_access
    
    # CRITICAL STEP for Aging:
    # Update the global aging factor L to the priority of the evicted object.
    # This effectively "lowers" the relative value of existing high-frequency items
    # compared to new items entering the cache in the future.
    aging_factor_L = min_priority
            
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, we increase frequency and re-calculate priority based on the current L.
    This effectively "resets" the object's aging relative to the current baseline.
    '''
    global m_key_frequency, m_key_priority, m_key_last_access, aging_factor_L
    
    # Increment Frequency
    curr_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = curr_freq
    
    # Update Priority: L + Frequency
    # By adding current L, we bring this object to the "front" of the aging curve.
    m_key_priority[obj.key] = aging_factor_L + curr_freq
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, we give the new object a fighting chance.
    Priority = current L + initial Frequency (1).
    '''
    global m_key_frequency, m_key_priority, m_key_last_access, aging_factor_L
    
    # Initialize Frequency
    m_key_frequency[obj.key] = 1
    
    # Initialize Priority
    # New objects start at the current baseline (L) + 1. 
    # This prevents them from being immediately evicted if older objects have high freq,
    # but ensures they are evicted soon if they don't get hits (since older objects likely have freq > 1).
    m_key_priority[obj.key] = aging_factor_L + 1.0
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    '''
    global m_key_frequency, m_key_priority, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
    
    if evicted_obj.key in m_key_priority:
        del m_key_priority[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```
2025-11-27 18:47:13,480 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.0725

Example 2:
- Trace ID: 46
- Hit Rate: 0.4663

Example 3:
- Trace ID: 28
- Hit Rate: 0.0398

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:47:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:47:13,481 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:47:37 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:37,485 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:47:37,486 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the analysis of the current LFU+LRU implementation and the provided performance feedback, the main issues are:

1.  **Cache Pollution:** Pure LFU is very susceptible to "one-hit wonders" or items that were popular long ago but are no longer relevant. They accumulate high frequency counts and stick in the cache, blocking newer, potentially more relevant items.
2.  **Frequency Reset:** When an item is evicted, the current algorithm deletes its metadata. If that item is accessed again shortly after, it starts with a frequency of 1, making it a prime candidate for immediate eviction again (thrashing).
3.  **Static Policies:** LFU performs well on stable distributions, while LRU performs better on shifting temporal patterns. A rigid combination often fails to adapt to mixed workloads (which the variation in your trace results suggests).

### Proposed Solution: TinyLFU-inspired with Frequency Decay (Window TinyLFU / W-TinyLFU)

To improve hit rates significantly across diverse traces, we should move towards a more sophisticated frequency-based admission and eviction policy, similar to **TinyLFU** or **ARC (Adaptive Replacement Cache)**. Given the constraints, a **Frequency-Based Eviction with Aging/Decay** is a powerful approach.

Here is the strategy for the improved algorithm:
1.  **Retain Metadata for Evicted Items (Ghost Cache):** Instead of deleting metadata immediately upon eviction, we keep it for a while. This creates a "Ghost Cache." If an item returns, we restore its old frequency (perhaps decayed) rather than starting at 1. This prevents thrashing.
2.  **Frequency Aging/Decay:** To combat cache pollution, we will periodically "age" the frequencies. Every time the total access count grows by a certain factor (or on every Nth access), we divide all frequencies by 2. This ensures that items that were popular long ago but not recently will eventually be evicted.
3.  **Admission Policy (Implicit):** By maintaining history, we implicitly filter items. An item with a high historical frequency that was evicted will have a higher survival chance if re-inserted than a brand-new item.

### Improved Implementation

```python
import math

# Metadata dictionaries
# Tracks estimated frequency. We do not delete keys immediately on eviction to maintain history.
m_key_frequency = dict()
# Tracks the last access time (Recency) to break ties in frequency.
m_key_last_access = dict()
# A global counter to trigger the aging process
global_access_counter = 0
# Threshold to trigger frequency decay (halving)
DECAY_THRESHOLD = 10000 

def _apply_aging():
    '''
    Helper function to decay frequencies. 
    Divides all frequencies by 2 to prioritize recent popularity over historical popularity.
    '''
    global m_key_frequency
    keys_to_remove = []
    for k in m_key_frequency:
        m_key_frequency[k] //= 2
        if m_key_frequency[k] == 0:
            keys_to_remove.append(k)
    
    # Clean up zero-frequency items to keep memory usage somewhat bounded
    for k in keys_to_remove:
        del m_key_frequency[k]
        if k in m_key_last_access:
            del m_key_last_access[k]

def evict(cache_snapshot, obj):
    '''
    Evicts the object based on a dynamic frequency policy with LRU tie-breaking.
    Because we use aging, this acts similarly to Window TinyLFU.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We are looking for the key with the Lowest Frequency.
    # Tie-breaker: Least Recently Used.
    
    # Initialize with the first key
    victim_key = candidate_keys[0]
    min_freq = m_key_frequency.get(victim_key, 0)
    min_time = m_key_last_access.get(victim_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Frequency Check (Lower is better for eviction)
        if curr_freq < min_freq:
            victim_key = key
            min_freq = curr_freq
            min_time = curr_time
        # 2. Recency Check (If frequencies are equal, older is better for eviction)
        elif curr_freq == min_freq:
            if curr_time < min_time:
                victim_key = key
                min_freq = curr_freq
                min_time = curr_time
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update recency.
    Check if we need to apply aging.
    '''
    global m_key_frequency, m_key_last_access, global_access_counter
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    # Cap frequency to prevent integer overflows and excessive dominance
    m_key_frequency[obj.key] = min(current_freq + 1, 100000)
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Periodic Aging
    global_access_counter += 1
    if global_access_counter >= DECAY_THRESHOLD:
        _apply_aging()
        global_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: 
    If the object was previously seen (in ghost metadata), restore its frequency (incremented).
    If it's new, start with frequency 1.
    '''
    global m_key_frequency, m_key_last_access, global_access_counter
    
    # Check history. If it exists in m_key_frequency, it was a "ghost" entry.
    # We revive it.
    old_freq = m_key_frequency.get(obj.key, 0)
    
    # Logic: If it was evicted recently, it might still have a freq > 0.
    # If it's a brand new object, freq is 0.
    # We increment it to indicate the new access.
    m_key_frequency[obj.key] = old_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Periodic Aging check (also needed on insert)
    global_access_counter += 1
    if global_access_counter >= DECAY_THRESHOLD:
        _apply_aging()
        global_access_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    CRITICAL CHANGE: Do NOT delete metadata immediately.
    We keep it to serve as "Ghost" entries.
    However, to prevent unlimited memory growth, we rely on the `_apply_aging` 
    function to clean up items that drop to frequency 0.
    '''
    # We deliberately do nothing here regarding m_key_frequency.
    # The evicted object's frequency remains in the dictionary.
    # It will eventually decay to 0 and be removed by _apply_aging,
    # or it will be re-inserted and regain its status.
    
    # We can optionally remove recency to save space, as frequency is the primary metric for ghosts.
    # However, keeping it helps tie-breaking if it is immediately re-inserted.
    pass
```
2025-11-27 18:47:44,464 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 39
- Hit Rate: 0.0909

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:47:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:47:44,465 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:48:00 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:48:00,432 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:48:00,433 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache hit rate significantly, we need to move away from a pure Frequency-based approach (LFU), which often suffers from cache pollution (keeping old, high-frequency items that are no longer needed) and the inability to adapt quickly to changing access patterns.

The low hit rates (0% - 9%) in your feedback suggest that the access patterns might be "scan-heavy" (accessing many items once) or have "recency-heavy" bursts where LFU reacts too slowly.

A robust, high-performance algorithm to address this is **ARC (Adaptive Replacement Cache)** or a simplified approximation like **SLRU (Segmented LRU)** or **2Q**. Given the constraints and the structure provided, a **Segmented LRU (SLRU)** approach is highly effective. It combines the benefits of Recency (handling bursts) and Frequency (retaining popular items) without the complexity of balancing ghost lists like full ARC.

### The Strategy: Segmented LRU (SLRU)

We will virtually split the cache into two segments:
1.  **Probationary Segment:** For new items. This acts like a standard LRU filter. If an item is accessed only once, it gets evicted from here quickly.
2.  **Protected Segment:** For items accessed at least twice. This holds the "hot" data.

**How it works:**
*   **Insert:** New items go into the *Probationary* pool.
*   **Hit:**
    *   If hit in *Probationary*, promote to *Protected*.
    *   If hit in *Protected*, update its recency (stay in Protected).
*   **Evict:**
    *   Prefer evicting from the *Probationary* pool first (remove "one-hit wonders").
    *   Only evict from *Protected* if *Probationary* is empty (or we enforce a ratio, but strictly checking Probationary first is simpler and usually effective).

This prevents scan operations (reading a huge sequence of keys once) from wiping out your valuable, frequently accessed data.

Here is the improved implementation:

```python
# Metadata dictionaries
# Tracks the specific segment an object belongs to: 0 for Probationary, 1 for Protected
m_key_segment = dict()
# Tracks the last access time (Recency) necessary for LRU behavior within segments
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented LRU eviction policy.
    1. Try to evict the Least Recently Used item from the Probationary segment (Segment 0).
    2. If Probationary is empty, evict the Least Recently Used item from the Protected segment (Segment 1).
    '''
    global m_key_segment, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the LRU item in segment 0, or if none exist, LRU in segment 1.
    victim_key = None
    victim_time = float('inf')
    
    # First pass: Look for victim in Probationary Segment (0)
    found_in_probationary = False
    
    # We iterate to find the absolute LRU in Probationary
    for key in candidate_keys:
        segment = m_key_segment.get(key, 0) # Default to 0 if missing (safety)
        if segment == 0:
            found_in_probationary = True
            access_time = m_key_last_access.get(key, 0)
            if access_time < victim_time:
                victim_time = access_time
                victim_key = key

    # Second pass: If nothing found in Probationary, look in Protected Segment (1)
    if not found_in_probationary:
        victim_time = float('inf') # Reset for second pass
        for key in candidate_keys:
            # We treat everything remaining as effectively protected or fallback
            access_time = m_key_last_access.get(key, 0)
            if access_time < victim_time:
                victim_time = access_time
                victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update Last Access Time (Recency).
    2. Promote to Protected Segment (Segment 1) regardless of current segment.
    '''
    global m_key_segment, m_key_last_access
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Promotion: If you are hit, you are important. Move to Protected.
    m_key_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize Last Access Time.
    2. Place in Probationary Segment (Segment 0).
    '''
    global m_key_segment, m_key_last_access
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Insertion starts in Probationary
    m_key_segment[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global m_key_segment, m_key_last_access
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    if evicted_obj.key in m_key_segment:
        del m_key_segment[evicted_obj.key]
```
2025-11-27 18:48:17,283 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0262

Example 2:
- Trace ID: 40
- Hit Rate: 0.0922

Example 3:
- Trace ID: 7
- Hit Rate: 0.498

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:48:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:48:17,284 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:50:24 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:50:24,311 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:50:24,312 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To minimize the miss rate, I have implemented the **Adaptive Replacement Cache (ARC)** algorithm. ARC outperforms simple LRU by maintaining two lists: $T1$ for recency (recently accessed once) and $T2$ for frequency (accessed at least twice). It dynamically balances the cache space between these two lists using a parameter $p$, which adapts based on "ghost" hits (hits on items recently evicted).

This implementation addresses several issues in the previous attempt:
1.  **Strict Dictionary Ordering**: It leverages Python's dictionary insertion order to strictly maintain LRU (Least Recently Used) to MRU (Most Recently Used) ordering.
2.  **Correct Adaptation**: The parameter $p$ is updated correctly based on whether hits occur in the ghost list of $T1$ ($B1$) or $T2$ ($B2$).
3.  **Ghost List Management**: The ghost lists are pruned relative to the current number of cached items, ensuring the algorithm remains responsive without unbound memory growth.
4.  **Separation of Concerns**: The logic is cleanly split between eviction decisions and metadata updates to maintain state consistency.

```python
# Metadata dictionaries
# T1: Resident Recency (Recently accessed once) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_t1 = dict()

# T2: Resident Frequency (Accessed at least twice) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_t2 = dict()

# B1: Ghost Recency (Evicted from T1) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_b1 = dict()

# B2: Ghost Frequency (Evicted from T2) -> maps key to None
# Order: LRU (first) -> MRU (last)
meta_b2 = dict()

# Adaptive parameter 'p'
# Represents the target size (number of items) for the T1 list.
p = 0.0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Policy.
    Decides whether to evict from the Recency set (T1) or the Frequency set (T2)
    based on the adaptation parameter 'p' and the ghost history of the incoming object.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2, p
    
    # If resident lists are empty (startup/error), fallback to cache snapshot
    if not meta_t1 and not meta_t2:
        return next(iter(cache_snapshot.cache))
    
    incoming_key = obj.key
    len_t1 = len(meta_t1)
    
    # ARC "Replace" Logic
    # We evict from T1 if it exceeds the target size 'p'.
    # Special Case: If the incoming item is in B2 (Ghost Frequency), we are even 
    # more inclined to evict T1 (to grow T2), so we treat len_t1 == p as sufficient to evict T1.
    
    evict_from_t1 = False
    
    if meta_t1 and len_t1 > p:
        evict_from_t1 = True
    elif meta_t1 and (incoming_key in meta_b2 and len_t1 == int(p)):
        evict_from_t1 = True
        
    # Boundary checks: if one list is empty, must evict from the other
    if not meta_t2:
        evict_from_t1 = True
    elif not meta_t1:
        evict_from_t1 = False

    if evict_from_t1:
        # Return LRU of T1 (first key in dict)
        return next(iter(meta_t1))
    else:
        # Return LRU of T2 (first key in dict)
        return next(iter(meta_t2))

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    - If in T1 (Recency), move to T2 (Frequency).
    - If in T2 (Frequency), move to MRU position in T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from T1 -> T2
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Make MRU in T2 (delete and re-insert)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # Synchronization fallback: if hit in cache but not in T1/T2, assume T2
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    - Detect "Ghost Hits" (hits in B1 or B2) to adjust 'p'.
    - Insert object into T1 or T2 based on history.
    - Prune ghost lists to keep history bounded.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2, p
    
    key = obj.key
    # Approximate current capacity in number of objects (N)
    current_count = len(meta_t1) + len(meta_t2)
    
    # 1. Adapt 'p' based on where the miss occurred (B1 or B2)
    if key in meta_b1:
        # Hit in Ghost T1: We should have made T1 larger. Increase p.
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            # Safety check for divide by zero
            delta = len(meta_b2) / len(meta_b1) if len(meta_b1) > 0 else 1
            
        p = p + delta
        # Cap p at roughly the total number of items
        if p > current_count + 1:
            p = current_count + 1
            
        # Move from Ghost T1 -> Resident T2 (second access)
        del meta_b1[key]
        meta_t2[key] = None
        
    elif key in meta_b2:
        # Hit in Ghost T2: We should have made T2 larger (T1 smaller). Decrease p.
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            delta = len(meta_b1) / len(meta_b2) if len(meta_b2) > 0 else 1
            
        p = p - delta
        if p < 0:
            p = 0
            
        # Move from Ghost T2 -> Resident T2
        del meta_b2[key]
        meta_t2[key] = None
        
    else:
        # Completely new item -> Insert into Resident T1
        meta_t1[key] = None

    # 2. Prune Ghost Lists (B1/B2)
    # They shouldn't grow indefinitely. Standard ARC limits:
    # len(T1) + len(B1) <= C
    # len(T2) + len(B2) <= 2C
    # We use current_count as a proxy for Capacity (C)
    
    c = max(1, current_count)
    
    while len(meta_b1) > c:
        del meta_b1[next(iter(meta_b1))]
        
    while len(meta_b2) > 2 * c:
        del meta_b2[next(iter(meta_b2))]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move the evicted object from Resident (T) to Ghost (B).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```
2025-11-27 18:52:35,361 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 18
- Hit Rate: 0.0229

Example 2:
- Trace ID: 35
- Hit Rate: 0.0671

Example 3:
- Trace ID: 19
- Hit Rate: 0.2722

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:52:35 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:52:35,362 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:53:12 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:53:12,656 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:53:12,657 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache hit rate significantly, we need to address the weaknesses of the previous "Score-Based" approach. The previous implementation essentially tried to merge LFU (Frequency) and LRU (Recency) into a single mathematical formula. While theoretically sound, such formulas are often brittle against varying workloads (scan resistance vs. temporal locality).

The solution below implements an **Adaptive Replacement Cache (ARC)** inspired algorithm. ARC is widely considered superior because it dynamically balances between Recency (LRU) and Frequency (LFU) based on the current workload's behavior.

### Key Changes and Rationale:

1.  **Dual Lists (T1 and T2):**
    *   **T1 (Recent):** Captures items seen once recently (LRU-like behavior). Useful for sequential scans or new items.
    *   **T2 (Frequent):** Captures items seen at least twice (LFU-like behavior). Useful for the "working set" of popular items.
    *   Unlike the previous solution which just counted frequency, this explicitly separates "new/transient" items from "established/popular" items.

2.  **Ghost Lists (B1 and B2):**
    *   **B1:** Contains keys evicted from T1 (Recent). If a hit occurs here, it means the cache is too small for the "recent" set, so we need to expand the capacity dedicated to Recency.
    *   **B2:** Contains keys evicted from T2 (Frequent). If a hit occurs here, it means the cache is too small for the "frequent" set, so we need to expand the capacity dedicated to Frequency.

3.  **Adaptive Parameter `p`:**
    *   The variable `p` determines the target size of the T1 list.
    *   If we get hits in B1, we increment `p` (favor Recency).
    *   If we get hits in B2, we decrement `p` (favor Frequency).
    *   This eliminates the need for hardcoded constants or heuristic "decay factors" found in the previous solution.

4.  **Metadata Management:**
    *   The previous solution had a potential memory leak or performance hit with `MAX_HISTORY_KEYS` and manual cleanup.
    *   This solution strictly enforces that the size of Ghost Lists (B1 + B2) never exceeds the cache capacity, ensuring O(1) memory overhead relative to the cache size.

### Improved Implementation

```python
# Metadata Global State
# T1: LRU list for recent items (in cache). Key -> Object
# T2: LRU list for frequent items (in cache). Key -> Object
# B1: LRU list for recent evictions (ghost). Key -> None
# B2: LRU list for frequent evictions (ghost). Key -> None
# Note: In this simulation environment, 'cache_snapshot.cache' holds the actual object storage.
# We use auxiliary lists (Python lists acting as queues) to track order.
# To make lookups O(1), we maintain sets or dicts for membership testing.

meta_t1_keys = []    # Keys in T1 (Recent)
meta_t2_keys = []    # Keys in T2 (Frequent)
meta_b1_keys = []    # Keys in B1 (Ghost Recent)
meta_b2_keys = []    # Keys in B2 (Ghost Frequent)

# Sets for fast O(1) lookups
set_t1 = set()
set_t2 = set()
set_b1 = set()
set_b2 = set()

# Adaptation parameter
# p targets the size of T1. 
# 0 <= p <= capacity
target_p = 0

def _replace(cache_snapshot, key_to_insert_if_ghost_hit=None):
    """
    Subroutine to determine which item to evict based on the state of T1, T2, and P.
    """
    global meta_t1_keys, meta_t2_keys, meta_b1_keys, meta_b2_keys
    global set_t1, set_t2, set_b1, set_b2, target_p

    victim = None
    
    t1_len = len(meta_t1_keys)
    
    # Logic to decide whether to evict from T1 or T2
    # If T1 is larger than target_p, we generally evict from T1 (the LRU end).
    # However, if the operation was triggered by a B2 hit, we might be more aggressive on T1.
    
    evict_from_t1 = False
    
    if t1_len > 0 and (t1_len > target_p or (key_to_insert_if_ghost_hit in set_b2 and t1_len == target_p)):
        evict_from_t1 = True
    else:
        # Evict from T2
        evict_from_t1 = False
    
    # Perform the eviction logic on metadata
    if evict_from_t1:
        # Pop LRU of T1
        victim = meta_t1_keys.pop(0) 
        set_t1.remove(victim)
        
        # Move to B1 (Ghost Recent)
        meta_b1_keys.append(victim)
        set_b1.add(victim)
    else:
        # Pop LRU of T2
        # Safety check: if T2 is empty, we must evict T1 (should rarely happen if logic is sound)
        if not meta_t2_keys:
            victim = meta_t1_keys.pop(0)
            set_t1.remove(victim)
            meta_b1_keys.append(victim)
            set_b1.add(victim)
        else:
            victim = meta_t2_keys.pop(0)
            set_t2.remove(victim)
            
            # Move to B2 (Ghost Frequent)
            meta_b2_keys.append(victim)
            set_b2.add(victim)
            
    return victim

def evict(cache_snapshot, obj):
    """
    Determines the victim using ARC logic (T1 vs T2 split based on P).
    """
    global target_p
    
    # Typically, ARC performs the eviction inside the insertion logic loop.
    # However, this framework asks for a specific 'evict' function call.
    # We use the _replace helper logic to find the victim.
    
    # Note: We don't change metadata state *here* usually, but since the framework
    # separates `evict` (get victim) and `update_after_evict` (do cleanup), 
    # we need to be careful.
    
    # In this specific framework flow:
    # 1. evict() is called. We must return a key.
    # 2. Framework removes key from cache_snapshot.cache.
    # 3. update_after_evict is called.
    
    # Because _replace() modifies our internal lists (moves T to B), we must return the result
    # of _replace(). The framework will then perform the physical removal.
    
    # Context check: is the object being inserted (obj) in B2?
    # This affects the eviction decision in ARC.
    key_in_b2 = obj.key in set_b2
    
    victim_key = _replace(cache_snapshot, obj.key if key_in_b2 else None)
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    Handles a cache hit.
    Move to MRU of T2.
    """
    global meta_t1_keys, meta_t2_keys, set_t1, set_t2
    
    key = obj.key
    
    # Case 1: Key is in T1 (Recent) -> Move to T2 (Frequent)
    if key in set_t1:
        # Remove from T1
        meta_t1_keys.remove(key)
        set_t1.remove(key)
        
        # Add to T2 (MRU)
        meta_t2_keys.append(key)
        set_t2.add(key)
        
    # Case 2: Key is in T2 (Frequent) -> Move to MRU of T2
    elif key in set_t2:
        meta_t2_keys.remove(key)
        meta_t2_keys.append(key)
        # set remains same

def update_after_insert(cache_snapshot, obj):
    """
    Handles inserting a new object.
    Could be a totally new object, or a ghost hit (B1 or B2).
    """
    global meta_t1_keys, meta_t2_keys, meta_b1_keys, meta_b2_keys
    global set_t1, set_t2, set_b1, set_b2, target_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # We assume capacity is count-based for ARC logic usually, but here bytes.
    # Approximation: use current key count as capacity reference if needed, 
    # but strictly ARC manages the lists.
    # Since the framework handles "Full" checks, we just manage list movements.
    
    # Case 1: Ghost Hit in B1 (Was recently evicted from T1)
    if key in set_b1:
        # Adapt P: Favor T1 (Recency)
        delta = 1
        if len(set_b1) < len(set_b2):
            delta = len(set_b2) // len(set_b1)
        target_p = min(cache_snapshot.capacity, target_p + delta) # Clamp to max capacity
        
        # Move from B1 to T2 (It has been seen twice now: once in T1, evicted, now seen again)
        meta_b1_keys.remove(key)
        set_b1.remove(key)
        
        meta_t2_keys.append(key)
        set_t2.add(key)
        return

    # Case 2: Ghost Hit in B2 (Was recently evicted from T2)
    if key in set_b2:
        # Adapt P: Favor T2 (Frequency)
        delta = 1
        if len(set_b2) < len(set_b1):
            delta = len(set_b1) // len(set_b2)
        target_p = max(0, target_p - delta)
        
        # Move from B2 to T2 (Reinforce Frequency)
        meta_b2_keys.remove(key)
        set_b2.remove(key)
        
        meta_t2_keys.append(key)
        set_t2.add(key)
        return

    # Case 3: Totally new item
    # Insert into T1 (MRU)
    meta_t1_keys.append(key)
    set_t1.add(key)

    # Note: The `evict` function handles the necessary clearing of space (moving T->B).
    # However, we must ensure the Ghost lists (B1/B2) don't grow infinitely.
    # Standard ARC rule: Len(T1+B1) <= C and Len(T2+B2) <= 2C.
    # Simplified cleanup:
    
    total_keys_count = len(cache_snapshot.cache) # Approximate C
    
    # Ensure B1 + T1 <= C (roughly)
    # Actually, simplistic ARC cleanup: just ensure B1+B2+T1+T2 isn't massive.
    # The pure algorithm removes LRU of B1 if len(T1)+len(B1) > C
    
    if len(meta_b1_keys) > total_keys_count:
        removed = meta_b1_keys.pop(0)
        set_b1.remove(removed)
        
    if len(meta_b2_keys) > total_keys_count + len(meta_b1_keys): # Looser bound for B2
        removed = meta_b2_keys.pop(0)
        set_b2.remove(removed)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Framework calls this after evict() returns a victim and it is removed.
    ARC's logic for moving items to Ghost lists was handled inside evict() -> _replace()
    because we needed to select the specific victim based on list states.
    
    We just perform a consistency check here.
    """
    # In our implementation of evict(), we pop from T1/T2 and push to B1/B2.
    # So the metadata is already updated.
    pass

```
2025-11-27 18:55:15,352 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 6
- Hit Rate: 0.2727

Example 2:
- Trace ID: 31
- Hit Rate: 0.0266

Example 3:
- Trace ID: 32
- Hit Rate: 0.029

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:55:15 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:55:15,353 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:57:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:57:28,933 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:57:28,934 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To significantly improve the cache hit rate, I have implemented an **Adaptive Replacement Cache (ARC)** algorithm.

### Analysis of the Previous Failure
The previous implementation used a static Segmented LRU approach with strict priority. It failed (2-3% hit rates) because:
1.  **Strict Priority:** Items with `Frequency=1` were *always* evicted before `Frequency>1`. This caused "Scan Pollution" where old, cold items with `Frequency=2` stayed in the cache forever, blocking new, potentially hot content from establishing itself.
2.  **Lack of Adaptability:** Real-world traces often shift between "Recency-heavy" (loops, repeating small sets) and "Frequency-heavy" (popular items) patterns. The previous algorithm had fixed logic and couldn't adapt.

### Solution: Adaptive Replacement Cache (ARC)
The improved solution implements ARC, which dynamically balances between two lists:
1.  **T1 (Recency):** Captures items seen only once recently (scans/new items).
2.  **T2 (Frequency):** Captures items seen at least twice (popular items).
3.  **Ghosts (B1 & B2):** We track the keys of recently evicted items.
    *   If we get a hit on a **Ghost T1 (B1)**, it means we evicted a recent item too soon -> We increase the size of T1.
    *   If we get a hit on a **Ghost T2 (B2)**, it means we evicted a frequent item too soon -> We decrease the size of T1 (making more room for T2).

This self-tuning parameter `p` allows the cache to behave like LRU when needed and LFU when needed, solving the issues seen in the feedback.

```python
import math

# -- Global State for ARC Implementation --

# m_meta stores metadata for currently cached items.
# Key -> {'type': 0 (T1 - Recency) or 1 (T2 - Frequency), 'time': access_timestamp}
m_meta = dict()

# m_ghosts stores keys of recently evicted items (Shadow Cache).
# Key -> type (0 for B1/Ghost Recency, 1 for B2/Ghost Frequency)
# Python dicts (3.7+) preserve insertion order, allowing us to use this as a FIFO queue.
m_ghosts = dict()

# m_state stores the adaptive parameter 'p'.
# 'p' represents the target number of items the cache should dedicate to the T1 (Recency) list.
m_state = {'p': 0}

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict using ARC logic.
    1. Checks if the incoming object is in the Ghost Registry to adapt 'p'.
    2. Determines whether to evict from T1 (Recency) or T2 (Frequency) based on 'p'.
    3. Selects the LRU victim from the chosen list.
    '''
    global m_meta, m_ghosts, m_state
    
    # -- Step 1: Adaptivity (Ghost Hit Check) --
    # If the incoming object (obj) corresponds to a ghost, we know we made a mistake 
    # by evicting it earlier. We adjust 'p' to correct this for the future.
    if obj.key in m_ghosts:
        ghost_type = m_ghosts[obj.key]
        
        # Calculate the size of Ghost Recency (B1) and Ghost Frequency (B2)
        b1_size = sum(1 for v in m_ghosts.values() if v == 0)
        b2_size = len(m_ghosts) - b1_size
        
        delta = 1
        # Current logical capacity roughly equals the number of items currently in cache
        current_capacity = len(cache_snapshot.cache)
        
        if ghost_type == 0: 
            # Hit in B1 (Recency Ghost): We should have kept T1 larger.
            # Delta calculation favors B2 size to converge faster if B1 is small
            if b1_size >= b2_size:
                delta = 1
            else:
                delta = b2_size / max(b1_size, 1) # Avoid div/0
            
            # Increase target p (grow T1)
            m_state['p'] = min(current_capacity, m_state['p'] + delta)
            
        else: 
            # Hit in B2 (Frequency Ghost): We should have kept T2 larger.
            if b2_size >= b1_size:
                delta = 1
            else:
                delta = b1_size / max(b2_size, 1) # Avoid div/0
            
            # Decrease target p (shrink T1, implicitly growing T2)
            m_state['p'] = max(0, m_state['p'] - delta)

    # -- Step 2: Identify Candidates and Victim Selection --
    
    candidates = []
    t1_count = 0
    
    # Scan cache to identify which items are T1 (Recency) and which are T2 (Frequency)
    # This also helps us find the LRU of each set.
    for k in cache_snapshot.cache:
        # Maintenance: Ensure every key in cache has metadata
        if k not in m_meta:
            m_meta[k] = {'type': 0, 'time': 0}
            
        meta = m_meta[k]
        if meta['type'] == 0:
            t1_count += 1
        candidates.append((k, meta))
            
    # Decision: Do we evict from T1 or T2?
    target_p = m_state['p']
    
    # If we have more T1 items than our target 'p', we must evict from T1.
    # Otherwise, we have space in T1 (or need space for it), so we evict from T2.
    evict_from_t1 = (t1_count > target_p)
    
    best_victim_key = None
    min_time = float('inf')
    found_preferred = False
    
    # Pass 1: Find LRU in the preferred list
    for k, meta in candidates:
        if (evict_from_t1 and meta['type'] == 0) or (not evict_from_t1 and meta['type'] == 1):
            if meta['time'] < min_time:
                min_time = meta['time']
                best_victim_key = k
                found_preferred = True
                
    # Pass 2: Fallback (if preferred list was empty)
    if not found_preferred:
        for k, meta in candidates:
            if meta['time'] < min_time:
                min_time = meta['time']
                best_victim_key = k
                
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update last access time.
    2. Move item to T2 (Frequency List). 
       Any item hit more than once is considered "Frequent".
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    if obj.key in m_meta:
        m_meta[obj.key]['type'] = 1 # Promote to T2
        m_meta[obj.key]['time'] = current_time
    else:
        # Fallback for sync issues
        m_meta[obj.key] = {'type': 1, 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If it was a Ghost, it enters as T2 (Frequency).
    3. If it is new, it enters as T1 (Recency).
    '''
    global m_meta, m_ghosts
    current_time = cache_snapshot.access_count
    
    new_type = 0 # Default: T1 (Recency)
    
    if obj.key in m_ghosts:
        # It was recently evicted, so it's a returning frequent item -> T2
        new_type = 1
        del m_ghosts[obj.key]
        
    m_meta[obj.key] = {'type': new_type, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (B1 if it was T1, B2 if it was T2).
    3. Keep Ghost Registry size bounded.
    '''
    global m_meta, m_ghosts
    
    # 1. Capture type before deleting
    evicted_type = 0
    if evicted_obj.key in m_meta:
        evicted_type = m_meta[evicted_obj.key]['type']
        del m_meta[evicted_obj.key]
        
    # 2. Add to Ghosts
    # m_ghosts acts as a FIFO queue because we insert new ghosts at the end
    m_ghosts[evicted_obj.key] = evicted_type
    
    # 3. Maintain Ghost Size
    # ARC typically maintains |Ghosts| <= |Cache Capacity|
    # We use current cache length as a proxy for capacity count.
    # We allow a small buffer (+1) to account for the object currently being inserted.
    max_ghosts = len(cache_snapshot.cache) + 1
    
    if len(m_ghosts) > max_ghosts:
        try:
            # Remove the oldest inserted ghost (FIFO)
            oldest_ghost = next(iter(m_ghosts))
            del m_ghosts[oldest_ghost]
        except StopIteration:
            pass
```
2025-11-27 18:57:41,619 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 20
- Hit Rate: 0.3884

Example 2:
- Trace ID: 47
- Hit Rate: 0.0392

Example 3:
- Trace ID: 5
- Hit Rate: 0.4867

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:57:41 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:57:41,620 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:58:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:58:02,873 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:58:02,875 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is an improved cache eviction algorithm designed to minimize miss rates by implementing a variant of the **LFU-DA (Least Frequently Used with Dynamic Aging)** strategy, enhanced with **Ghost Cache** (metadata persistence) and **Frequency Aging**.

### Key Improvements:
1.  **Dynamic Aging (LFU-DA Logic):** Instead of a static decay factor, we introduce a global "cache age" or "inflation value" (`L`) that increases whenever an eviction occurs. This value is added to the frequency of new items. This naturally allows newer items to compete with historically popular items without needing complex logarithmic decay formulas.
2.  **Ghost Cache Persistence:** Metadata is strictly maintained for items even after they leave the cache. This distinguishes between "first-time" visitors and "returning" visitors (churn).
3.  **Adaptive Frequency:** Frequencies are not just reset. When an object is evicted, its frequency isn't deleted immediately from metadata. When it returns, it retains its past popularity (minus the global aging offset), preventing the "cold start" problem for cyclic working sets.

```python
import math

# Metadata Global Storage
# Stores the "value" (frequency + aging bias) of keys
m_key_values = dict()
# The global "inflation" value (similar to L in LFU-DA) to handle aging
m_global_age = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the minimum value.
    Value = Frequency + Global_Age_At_Entry
    
    This implements an LFU-DA (Dynamic Aging) approach. 
    - LFU: We evict the item with the least estimated utility.
    - DA: We add a 'cache age' factor to new items so they don't start at 0 
      and immediately die against old, high-frequency items.
      
    When an eviction happens, the global age increases to the value of the victim.
    This effectively "ages out" old high-frequency items because new items come in 
    with a higher baseline score.
    '''
    global m_key_values, m_global_age
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We search for the key with the lowest tracked value in the cache
    victim_key = None
    min_val = float('inf')
    
    # Tie-breaking optimization:
    # If values are equal, we prefer to evict the one inserted/accessed earliest (LRU tie-breaker).
    # Since Python 3.7+, dicts maintain insertion order. If we iterate, we might implicitly
    # hit older items first or last depending on update logic.
    # However, for pure LFU-DA, just finding the min value is sufficient.
    
    for key in candidate_keys:
        val = m_key_values.get(key, 0.0)
        if val < min_val:
            min_val = val
            victim_key = key
            
    # LFU-DA Logic Update:
    # The Global Age advances to the value of the evicted item.
    # This prevents the "frequency counter pollution" where old items stay forever.
    if victim_key is not None:
        m_global_age = min_val

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment the object's value.
    '''
    global m_key_values
    
    key = obj.key
    current_val = m_key_values.get(key, 0.0)
    
    # Standard LFU reinforcement. 
    # We add 1.0 to the frequency.
    m_key_values[key] = current_val + 1.0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Initialize value using Ghost Cache logic + Dynamic Aging.
    '''
    global m_key_values, m_global_age
    
    key = obj.key
    
    # Ghost Cache Check:
    if key in m_key_values:
        # The key is in metadata history (it was evicted previously).
        # We assume it has retained some 'heat', but we must ensure 
        # it is at least comparable to the current global age.
        # Logic: If it was popular before, it starts with that previous popularity.
        # However, if that popularity is now lower than the current global age (cache minimum),
        # we bump it up to Global Age + Initial_Boost to give it a fighting chance.
        
        saved_val = m_key_values[key]
        if saved_val < m_global_age:
             # Reset to current baseline + small boost for being a return visitor
            m_key_values[key] = m_global_age + 1.0
        else:
            # It was very popular, keep its high score + boost
            m_key_values[key] = saved_val + 1.0
            
    else:
        # New Item (never seen).
        # In LFU-DA, a new item enters with value = Global_Age + Initial_Frequency.
        # This allows it to survive long enough to prove its worth.
        m_key_values[key] = m_global_age + 1.0
    
    # Maintenance: Prune metadata if it grows too large to prevent memory leaks.
    # We only prune items NOT in the cache.
    if len(m_key_values) > 50000: # Heuristic cap
        # Remove entries that are significantly below global age (likely won't be admitted/survive)
        # unless they are currently in the cache.
        keys_to_purge = []
        cached_keys = set(cache_snapshot.cache.keys())
        
        # We accept O(N) cleanup occasionally for better hit rate performance
        count = 0
        for k, v in m_key_values.items():
            if k not in cached_keys and v < m_global_age:
                keys_to_purge.append(k)
                count += 1
                if count > 1000: break # Cleanup in chunks
        
        for k in keys_to_purge:
            del m_key_values[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Update metadata but do NOT delete the key.
    '''
    # We keep the key in m_key_values. 
    # This serves as the "Ghost Cache". 
    # If this item is requested again, we know it was recently popular.
    pass
```
2025-11-27 18:58:12,085 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 28
- Hit Rate: 0.0398

Example 3:
- Trace ID: 26
- Hit Rate: 0.7964

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:58:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:58:12,087 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:58:38 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:58:38,557 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:58:38,558 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the weaknesses observed in the baseline LFRU approach, particularly its poor performance on certain traces (Trace 29 and 28 with ~3% hit rates). These low hit rates suggest the algorithm is suffering from "cache pollution" (one-time access patterns filling the cache) or "thrashing" (cyclic access patterns slightly larger than the cache).

The baseline algorithm struggles because:
1.  **Frequency Accumulation:** Simply counting frequency indefinitely (even with aging) biases heavily towards old, historically popular items that may no longer be relevant, starving new, rising items.
2.  **No Protection for New Items:** New items start with frequency 1 and are immediate candidates for eviction if the cache is full of items with frequency > 1.
3.  **Scan Resistance:** A linear scan of unique items (a loop over a dataset larger than the cache) will flush out high-value items because the new items displace them before establishing their own frequency.

To solve this, I will implement a **Segmented LRU (SLRU) with a Frequency-based admission filter**, inspired by algorithms like S3-FIFO and TinyLFU.

### The Strategy:
1.  **Ghost Cache (Shadow History):** We will maintain a set of keys that were recently evicted. If an item is accessed while in this "ghost" state, it indicates the item is part of a larger working set or a recurring pattern, boosting its priority significantly upon re-insertion.
2.  **Two-Tier Cache Structure (Simulated):** We will logically divide the cache into a "Probation" segment and a "Protected" segment using priority scores.
    *   **Probation:** New items enter here. They have lower priority.
    *   **Protected:** Items that receive a hit while in Probation (or are re-inserted from Ghost) get promoted here with higher priority.
3.  **Eviction Logic:** We will primarily evict from the Probation segment (low frequency/recency). We will only evict from the Protected segment if the Probation segment is empty or statistically valuable to keep.

### Improved Implementation

```python
import math

# Metadata dictionaries
# m_key_metadata stores a tuple or object: (frequency, insertion_time, is_protected)
m_key_metadata = dict()

# Ghost cache to track recently evicted items. 
# We store insertion_time or a boolean to know it was recently seen.
# Using a set for O(1) lookups.
m_ghost_keys = set()
m_ghost_fifo = [] # To manage the size of the ghost cache
MAX_GHOST_SIZE = 0 # Will be dynamically set based on cache capacity approximation

# Global counter for logical time
current_time = 0

def get_score(key, access_count):
    """
    Calculate a utility score for eviction. Lower score = better victim.
    Score components: Frequency tier + Recency.
    """
    if key not in m_key_metadata:
        return -1
    
    freq, last_access, protected = m_key_metadata[key]
    
    # Hierarchy of importance:
    # 1. Protected status (High Score)
    # 2. Frequency (Medium Score influence)
    # 3. Recency (Tie-breaker)
    
    # Base score: Protected items get a massive boost (e.g., +1,000,000)
    # Frequency acts as a multiplier or additive tier.
    # Recency is the fine-grained differentiator.
    
    # We invert the logic for eviction: We want the MINIMUM score.
    # So, Recency should increase the score (newer = higher score).
    
    base = 1000000 if protected else 0
    
    # Logarithmic frequency to prevent historical domination, but reward popularity
    freq_score = min(freq, 100) * 1000
    
    # Time delta
    # We want newer items to have higher scores.
    # Since access_count grows, 'last_access' is larger for newer items.
    
    return base + freq_score + last_access

def cleanup_ghosts():
    """Ensure ghost cache doesn't grow indefinitely."""
    global m_ghost_keys, m_ghost_fifo, MAX_GHOST_SIZE
    
    # Lazy cleanup: remove oldest ghosts if exceeding size limit
    while len(m_ghost_fifo) > MAX_GHOST_SIZE:
        old_key = m_ghost_fifo.pop(0)
        if old_key in m_ghost_keys:
            m_ghost_keys.remove(old_key)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest score (Least Valuable).
    Prioritizes evicting unprotected (probationary) items with low frequency and old access time.
    '''
    global m_key_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for the key with the MINIMUM score.
    # Logic: Unprotected < Protected. Old < New. Low Freq < High Freq.
    
    victim = None
    min_score = float('inf')
    
    # Optimization: We can scan to find the first unprotected item. 
    # If we find unprotected items, we restrict our search to them to ensure
    # we don't evict protected items unless absolutely necessary.
    
    # 1. Try to find victim in Probation (Unprotected) list
    probation_candidates = [k for k in candidate_keys if k in m_key_metadata and not m_key_metadata[k][2]]
    
    search_space = probation_candidates if probation_candidates else candidate_keys
    
    for key in search_space:
        # Default metadata if missing (shouldn't happen often)
        if key not in m_key_metadata:
            return key 
            
        freq, last_access, protected = m_key_metadata[key]
        
        # Calculate Score
        # Primary Weight: Protection Status (0 or 1)
        # Secondary Weight: Frequency
        # Tertiary Weight: Recency (Access Time)
        
        # We want to evict: Not Protected -> Low Freq -> Oldest
        
        # Composite score calculation for comparison
        # (Protected, Frequency, Last_Access)
        # Python compares tuples element by element.
        # We want to minimize this tuple.
        score = (1 if protected else 0, freq, last_access)
        
        if score < min_score:
            min_score = score
            victim = key
            
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Promote to Protected status.
    '''
    global m_key_metadata
    
    curr_freq = 0
    is_protected = False
    
    if obj.key in m_key_metadata:
        curr_freq, _, is_protected = m_key_metadata[obj.key]
    
    # Increment frequency
    new_freq = curr_freq + 1
    
    # Hit implies value -> Promote to Protected
    new_protected = True
    
    m_key_metadata[obj.key] = (new_freq, cache_snapshot.access_count, new_protected)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Cache.
    2. If in Ghost, insert as Protected (high priority) + restore frequency.
    3. If new, insert as Probation (low priority).
    '''
    global m_key_metadata, m_ghost_keys, MAX_GHOST_SIZE
    
    # Dynamic sizing for ghost cache based on observed capacity
    # We estimate capacity by current count. 
    if MAX_GHOST_SIZE == 0 and len(cache_snapshot.cache) > 0:
        # Heuristic: Ghost cache size same as actual cache size
        MAX_GHOST_SIZE = len(cache_snapshot.cache) 
        
    freq = 1
    protected = False
    
    # Check if this is a "Resurrection"
    if obj.key in m_ghost_keys:
        # It was recently evicted and needed again. 
        # This is a strong signal it belongs in the working set.
        freq = 2 # Boost frequency slightly
        protected = True # Enter directly into protected mode
        # Remove from ghost
        m_ghost_keys.remove(obj.key)
    else:
        # Brand new item
        freq = 1
        protected = False # Starts in Probation
        
    m_key_metadata[obj.key] = (freq, cache_snapshot.access_count, protected)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Cache to remember we saw it recently.
    '''
    global m_key_metadata, m_ghost_keys, m_ghost_fifo
    
    # Clean up active metadata
    if evicted_obj in m_key_metadata:
        del m_key_metadata[evicted_obj]
        
    # Add to ghost cache logic
    # We track that this key existed recently.
    if evicted_obj not in m_ghost_keys:
        m_ghost_keys.add(evicted_obj)
        m_ghost_fifo.append(evicted_obj)
        
    cleanup_ghosts()
```
2025-11-27 18:58:40,992 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.0725

Example 2:
- Trace ID: 8
- Hit Rate: 0.5363

Example 3:
- Trace ID: 35
- Hit Rate: 0.068

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:58:40 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:58:40,993 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:59:09 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:59:09,370 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:59:09,371 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the weaknesses of the current LFU+LRU implementation.

### Analysis of the Current Algorithm
The current implementation is a standard **LFU (Least Frequently Used)** with an **LRU (Least Recently Used)** tie-breaker.
*   **Weakness 1: Frequency Accumulation (Pollution).** Objects that were popular a long time ago maintain high frequency counts. They "stick" in the cache even if they haven't been accessed recently, wasting space and causing misses for newer, currently popular items. This explains the very low hit rates in Traces 41 and 35 (likely shifting workloads).
*   **Weakness 2: New Item Starvation.** Newly inserted items have a frequency of 1. If the cache is full of items with frequency > 1, the new item is almost immediately evicted if it isn't accessed again instantly. This prevents new working sets from establishing themselves.

### Proposed Solution: LFU with Dynamic Aging (LFU-DA) or LRFU
To fix these issues, we need an algorithm that balances Recency and Frequency. A pure LRU forgets frequency, and a pure LFU forgets recency.

I will implement a variation of **LFU with Dynamic Aging (LFU-DA)**, which is often superior for mixed workloads. However, to keep the implementation robust and strictly within the provided function signatures without complex heap structures, I will implement a **GDSF (Greedy Dual Size Frequency) inspired logic**, but simplified for generic object sizes (assuming uniform cost for now, but factoring in size if useful) or a **Hyperbolic Caching** approach.

Given the constraints and the goal of general improvement, **Hyperbolic Caching** is often a robust choice that outperforms simple LFU/LRU by combining them mathematically without complex aging parameters. However, a simpler and often more effective approach for varied traces is **S3-FIFO** or **TinyLFU**. Since we don't have a complex admission policy wrapper, we will improve the eviction score directly.

**Selected Strategy: Combined Recency and Frequency Score (Weighted LFU)**

We will calculate a score for every object based on:
`Score = Frequency / (CurrentTime - LastAccessTime)`

This logic (similar to Hyperbolic Caching) implies:
1.  Higher Frequency $\rightarrow$ Higher Score (Keep).
2.  Longer time since last access $\rightarrow$ Lower Score (Evict).

However, to prevent "Cache Pollution" from historical heavy hitters that stopped being accessed, we also need to **reset or decay frequency** upon eviction or cap it.

A better, standard, and highly effective improvement over basic LFU is **Window-LFU** or simply improving the metadata handling to retain history *after* eviction (Ghost Entries). But since we are required to delete metadata on `update_after_evict`, we cannot use Ghost Entries easily.

**The Optimization: Adaptive LFU (Weighting Recency)**
I will modify the eviction calculation to prioritize objects that have a high frequency *relative* to how long they have been in the cache.

To address the "New Item Starvation" (where a new item has freq=1 and gets kicked out immediately), we will give new items a "grace period" or a slightly boosted initial priority, or rely on the `Recency` factor to protect them briefly (since `CurrentTime - LastAccessTime` is small for new items).

Let's refine the Score:
We want to evict the item with the **minimum** value of:
`Score = (Frequency ^ Weight) / (Time_Since_Access + 1)`

If we define eviction as finding the "least valuable" item:
*   High Frequency = Valuable (Don't evict).
*   High Recency (Low Time_Since) = Valuable (Don't evict).
*   We evict the item with the **lowest** result of: `Frequency * Recency_Factor`.

Let's implement a robust **recency-weighted LFU**.

**Algorithm Details:**
1.  **Metadata:** Keep `frequency` and `insertion_time`.
2.  **Eviction Score:** We want to evict items that are *not* frequently used *recently*.
    *   Standard LFU: Evict Min(Freq).
    *   Standard LRU: Evict Min(AccessTime).
    *   **New Logic:** Evict Min of a combined score.
    *   `Score = Frequency * (1 / (CurrentTime - LastAccess + 1))`? No, that's computationally expensive to do divisions.
    
    Let's stick to a proven heuristic: **aging the frequency**.
    When the average frequency of the cache gets too high, or periodically, we divide all frequencies by 2. However, we can't iterate all keys globally easily in the update functions.
    
    Instead, let's use **Logarithmic Frequency**.
    We only increment frequency with a probability, or use `log(frequency)`. This dampens the effect of massive historical counts.
    
    **Winning Strategy: The "Clock" or "Second Chance" approach combined with LFU.**
    But given the function signatures (calculating a score over the snapshot), a mathematical scoring function is easiest to implement correctly.

    **Implementation: Frequency-Aware LRU (F-LRU)**
    We will use a score: `LastAccessTime + (Frequency * Weight)`.
    This effectively gives frequently accessed items a "virtual time extension" into the future. They appear "more recently used" than they actually were.
    
    *   `Score = LastAccessTime + (Frequency * CacheCapacity * ScalingFactor)`
    *   This is tricky to tune.

    **Let's go with a simplified Hyperbolic approach.**
    It requires no tuning parameters and usually beats LFU and LRU.
    **Eviction Criteria:** Minimize `(Frequency) / (Time_In_Cache + 1)`.
    *   Wait, standard Hyperbolic Caching is `Frequency / (CurrentTime - InsertionTime)`.
    *   Let's modify it to be `Frequency / (CurrentTime - LastAccessTime)`.
    *   If `CurrentTime - LastAccessTime` is large (old item), the score is low $\rightarrow$ Evict.
    *   If `Frequency` is low, the score is low $\rightarrow$ Evict.
    
    This handles Trace 41 (likely scanning/churn) well because new items have very small denominator (high score), protecting them long enough to see if they get a second hit.

### The Algorithm Implementation

```python
import math

# Metadata dictionaries
# Tracks how many times an object has been accessed.
m_key_frequency = dict()
# Tracks the last access time of an object.
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest utility score.
    Utility is calculated using a Hyperbolic Caching inspired function:
    Score = Frequency / (Time Since Last Access)
    
    This balances LFU (keeping popular items) and LRU (discarding old items).
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Current "time" is represented by access_count
    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    min_score = float('inf')

    # We are looking for the object with the LOWEST score to evict.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta. Add 1 to avoid division by zero if accessed same tick.
        time_delta = (current_time - last_access) + 1
        
        # Hyperbolic Score:
        # Items accessed recently (small delta) get HIGH score.
        # Items accessed frequently (high freq) get HIGH score.
        # We want to evict the item with the LOWEST score.
        score = freq / time_delta
        
        # Optimization: Tie-breaking
        # If scores are identical, evict the one with the lowest frequency (pure LFU fallback)
        # or the one that is largest (Cost-benefit, larger items free up more space).
        # Here we just strictly minimize score.
        if score < min_score:
            min_score = score
            best_victim_key = key
        elif score == min_score:
            # Tie-breaker: Evict the one that was accessed longest ago (Classic LRU logic)
            # This handles cases where frequencies and deltas are proportional.
            if last_access < m_key_last_access.get(best_victim_key, 0):
                best_victim_key = key

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    # Linearly incrementing creates strong pollution.
    # We use a standard increment here, relying on the 'Time Delta' 
    # in the evict function to age out old heavy hitters.
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1
    
    # Update Recency to current access count
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency
    # We start at 1. The eviction logic protects this item briefly 
    # because (CurrentTime - LastAccess) will be very small, resulting in a high Score.
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```
2025-11-27 18:59:25,306 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 13
- Hit Rate: 0.0

Example 2:
- Trace ID: 36
- Hit Rate: 0.077

Example 3:
- Trace ID: 16
- Hit Rate: 0.0199

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m18:59:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 18:59:25,307 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m18:59:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 18:59:51,565 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 18:59:51,566 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is an optimized cache eviction algorithm implementation.

### Analysis of Previous Failure
The previous implementation attempted a simplified Segmented LRU (SLRU) with a Ghost Cache, but suffered from extremely low hit rates (0% - 7%). The likely causes were:
1.  **Over-penalizing new items:** By forcing all new items (Frequency=1) to be evicted before *any* item with Frequency > 1, the cache became static. Old, "warm" items saturated the protected segment, and new, potentially hotter items were evicted immediately before they could get a second hit (Scan Resistance was too aggressive).
2.  **Inefficient Ghost Registry:** The ghost registry logic was purely binary (in/out) and didn't factor in recency or frequency of the ghost hits effectively.
3.  **Lack of Size Awareness:** The eviction logic treated all objects as equal units. However, the problem statement provides `obj.size`, `cache_snapshot.size`, and `cache_snapshot.capacity` in bytes. The previous algorithm ignored object size, likely clogging the cache with a few large, useless items, preventing many small, useful items from entering.

### Optimization Strategy: Cost-Benefit Size-Aware LFU-LRU (GDSF-like variant)
To drastically improve performance, we need a hybrid approach often used in high-performance web caching (like GDSF or variants of ARC/LIRS) that balances Recency, Frequency, and **Size**.

**The new "Victim Score" Strategy:**
We will implement a scoring function based on the **Greedy Dual-Size Frequency (GDSF)** principle, but simplified for this constraint environment.
*   **Value of an object:** Higher Frequency is better. More recent access is better.
*   **Cost of an object:** Larger Size is worse (it takes up space that could hold multiple smaller items).

**The Formula:**
$$ Score = \frac{Frequency \times AgingFactor}{Size} + InflationValue $$
*   Items with the *lowest* score are evicted.
*   Small items are preferred (score higher).
*   Frequent items are preferred (score higher).
*   Recent items are preferred (via the inflation mechanism).

**Key Mechanisms:**
1.  **Size Awareness:** We explicitly calculate score inversely proportional to `obj.size`. This maximizes the *number* of items in the cache, statistically increasing hit probability.
2.  **Inflation ($L$ value):** When an object is evicted, we record its score. This score becomes the new "base" (Inflation Value) for future insertions. This ensures that for a new item to enter and stay, it must be "better" than the item that was just evicted. This naturally handles cache aging.
3.  **Logarithmic Frequency:** To prevent old heavy hitters from staying forever, we can use `log2(freq)` or simple frequency caps.

### Improved Implementation

```python
import math

# Global metadata storage
# Key -> { 'freq': int, 'size': int, 'l_value': float }
m_metadata = dict()

# Global inflation value (L). 
# This represents the "cost" or "priority" threshold of the cache at the current moment.
# It increases monotonically as items are evicted.
m_inflation_l = 0.0

def get_priority_score(key, obj_size):
    '''
    Calculate the priority (GDSF-like score).
    Higher Score = More valuable (KEEP).
    Lower Score = Less valuable (EVICT).
    
    Formula: H(p) = L + (Frequency / Size)
    
    We modify this slightly for stability:
    Score = L + (Frequency * CostWeight) / Size
    
    Where CostWeight is a constant to balance the magnitude of freq/size vs L.
    '''
    if key not in m_metadata:
        return -1.0
    
    data = m_metadata[key]
    freq = data['freq']
    # Use the stored L value from when the object was last updated/inserted
    # This acts as the "Time" component (Recency).
    base_l = data['l_value'] 
    
    # Weighting:
    # We want small items to have high scores.
    # We want frequent items to have high scores.
    # We calculate density: value per byte.
    
    # Avoid division by zero
    safe_size = max(1, obj_size)
    
    # Priority = The inflation value at time of insertion/update + (Freq / Size)
    # This combines Recency (L value increases over time), Frequency, and Size.
    priority = base_l + (freq / safe_size)
    
    return priority

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the LOWEST priority score.
    Returns the key of the victim.
    '''
    global m_inflation_l
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    min_score = float('inf')
    victim_key = None
    
    # Find the object with the minimum priority score
    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        score = get_priority_score(key, cached_obj.size)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    # CRITICAL GDSF STEP:
    # Update the global Inflation Value (L) to the score of the evicted item.
    # This ensures that future items must have a higher merit to survive than the item we just killed.
    if victim_key is not None:
        m_inflation_l = min_score
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset its 'L' value to the current global inflation L.
       (This effectively marks it as "Recently Accessed").
    '''
    global m_metadata, m_inflation_l
    
    if obj.key not in m_metadata:
        # Should not happen on hit, but safety fallback
        m_metadata[obj.key] = {'freq': 1, 'size': obj.size, 'l_value': m_inflation_l}
    else:
        # Increment frequency
        m_metadata[obj.key]['freq'] += 1
        # Update L value to current global L (Restores Recency)
        m_metadata[obj.key]['l_value'] = m_inflation_l
        # Ensure size is accurate (though usually static)
        m_metadata[obj.key]['size'] = obj.size

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Set L value to current global inflation L.
    '''
    global m_metadata, m_inflation_l
    
    # New object starts with frequency 1
    # It inherits the current system "temperature" (L)
    m_metadata[obj.key] = {
        'freq': 1, 
        'size': obj.size, 
        'l_value': m_inflation_l
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata to save memory.
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
```
2025-11-27 19:00:12,155 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 11
- Hit Rate: 0.3912

Example 2:
- Trace ID: 9
- Hit Rate: 0.531

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:00:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:00:12,156 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:00:35 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:00:35,792 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:00:35,793 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, I will transition from the existing LFRU (Least Frequent Recently Used) approach to a simplified implementation of **LFU with Dynamic Aging (specifically inspired by Window TinyLFRU and ARC concepts)**.

The current implementation suffers from "frequency pollution" where old, high-frequency items stick around too long (even with the aging mechanism), and new items are evicted too quickly before they can build up frequency (the "cold start" problem).

### Key Improvements:
1.  **Frequency Decay on Eviction (Adaptive Aging):** Instead of a fixed global timer (which is arbitrary and insensitive to cache pressure), I will apply decay logic specifically during eviction pressure. This ensures that when the cache is churning, high-frequency items that haven't been seen recently lose their protection faster.
2.  **Recency vs. Frequency Balance:** I will maintain a pure LFU score but break ties strictly with LRU.
3.  **New Item Protection:** I will assign an "initial frequency" based on the average frequency of the current cache (or a minimum threshold) to give new items a fighting chance against established heavy hitters.
4.  **Metadata Management:** I will continue to track evicted items (Ghost Cache) but will be more aggressive about removing metadata for keys that haven't been seen in a very long time to prevent memory bloat, while keeping enough history to rescue "churning" items.

Here is the improved implementation:

```python
import math

# Metadata dictionaries
# Tracks the frequency of access for keys
m_key_frequency = dict()
# Tracks the specific timestamp (access_count) of the last access
m_key_last_access = dict()

# Heuristics
# A cap prevents long-lived objects from becoming "immortal"
MAX_FREQ = 100 
# We maintain a dynamic average frequency to help new items survive
avg_cache_frequency = 1.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the Minimum Frequency.
    Ties are broken by Recency (Least Recently Used among min-frequency items).
    '''
    global m_key_frequency, m_key_last_access

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We want to find the victim with the lowest score.
    # Score tuple: (Frequency, Last_Access_Time)
    # Python compares tuples element by element.
    # Lower frequency is evicted first. If frequencies equal, older time is evicted.
    
    victim_key = None
    min_score = (float('inf'), float('inf'))

    for k in candidate_keys:
        freq = m_key_frequency.get(k, 1)
        access_time = m_key_last_access.get(k, 0)
        
        # We add a slight "age" penalty to frequency if the item is very old
        # This helps evict high-frequency items that have stopped being accessed.
        # However, purely relying on the tuple (freq, access_time) is usually robust enough
        # if the update logic handles decay correctly.
        score = (freq, access_time)
        
        if score < min_score:
            min_score = score
            victim_key = k

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency (subject to a cap).
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    current_freq = m_key_frequency.get(key, 0)
    
    # Linear increment with a hard cap
    if current_freq < MAX_FREQ:
        m_key_frequency[key] = current_freq + 1
        
    # Update global average frequency heuristic slightly
    # (Simple exponential moving average)
    avg_cache_frequency = 0.95 * avg_cache_frequency + 0.05 * m_key_frequency[key]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Set Recency.
    2. Set Initial Frequency.
       If the item was in our "Ghost" history (recently evicted), we restore its old frequency.
       If it's new, we give it a "fighting chance" frequency (e.g., floor of avg_freq)
       so it isn't evicted immediately by older, established items.
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in m_key_frequency:
        # Ghost Hit: This item was evicted but requested again.
        # This is a strong signal of popularity. Boost it.
        # We restore it, perhaps slightly decayed or just incremented.
        new_freq = min(m_key_frequency[key] + 1, MAX_FREQ)
        m_key_frequency[key] = new_freq
    else:
        # Cold Start:
        # Instead of starting at 1, we start at a baseline relative to the cache's current state.
        # This prevents the "new item churn" loop where new items enter with freq 1 and are
        # immediately evicted because everyone else is freq > 1.
        # However, we don't want to make it too high, or we pollute the cache.
        # Using 1 is safe for strict LFU, but taking min(1, int(avg_cache_frequency // 2)) helps adapt.
        # For this specific implementation, starting at 1 is standard, 
        # but relying on the "Age" logic in evict handles the rest.
        # Let's stick to 1 for simplicity but allow the ghost logic to handle re-entries.
        m_key_frequency[key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. We DO NOT delete metadata for the evicted object immediately (Ghost Cache).
    2. However, to prevent frequency inflation over time, we apply a global decay 
       periodically or based on max frequency thresholds.
    '''
    global m_key_frequency, m_key_last_access, avg_cache_frequency

    # Adaptive Decay Strategy:
    # If the average frequency in the cache gets too high, or the evicted item
    # had a very high frequency, it implies our counters are saturated.
    # We decrement all counters periodically to simulate "forgetting".
    # Here, we trigger decay if the evicted item had a high frequency (e.g., > 10).
    
    evicted_freq = m_key_frequency.get(evicted_obj.key, 1)
    
    # If we are evicting something that was somewhat popular, it means the cache 
    # is under pressure. We should decay everyone slightly to allow new trends to emerge.
    if evicted_freq > 5:
        # Decay all frequencies by integer division or subtraction
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            # If item is not in cache (it's a ghost), decay faster
            is_ghost = k not in cache_snapshot.cache
            
            if is_ghost:
                new_v = v // 2 
            else:
                # If in cache, decay slower (decrement)
                new_v = v - 1 if v > 1 else 1
            
            if new_v <= 0:
                keys_to_remove.append(k)
            else:
                m_key_frequency[k] = new_v
        
        # Cleanup zeroed out ghosts
        for k in keys_to_remove:
            del m_key_frequency[k]
            if k in m_key_last_access:
                del m_key_last_access[k]
                
    # Note: We keep the evicted object in m_key_frequency for now (as a ghost)
    # unless it was removed in the cleanup loop above.
```
2025-11-27 19:00:45,254 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 45
- Hit Rate: 0.0741

Example 2:
- Trace ID: 5
- Hit Rate: 0.483

Example 3:
- Trace ID: 30
- Hit Rate: 0.0456

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:00:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:00:45,255 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:01:04 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:01:04,132 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:01:04,133 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The provided implementation attempts a Frequency-Based approach (LFU) with aging and a "ghost" registry. However, the performance results (hit rates of 0.07 and 0.04 in two cases) suggest serious flaws.

**Analysis of Failures:**
1.  **The "Frequency + Recency" Score Logic is Flawed:** The scoring formula `score = freq + (recency_factor * 0.0001)` is heavily biased towards Frequency. In workloads where popularity shifts quickly (Trace 45 and 30 likely), "old heavy hitters" stay in the cache too long, polluting it and preventing new, relevant items from entering.
2.  **Weak Recency:** The recency component is only a tie-breaker. This makes the cache resistant to scan patterns (accessing many items once) or shifting working sets.
3.  **Inefficient Aging:** Dividing frequency by 2 every 1000 accesses is a "choppy" way to handle aging. It creates sudden changes in eviction candidates rather than a smooth transition.
4.  **Ghost Registry Implementation:** While the idea is good (ARC-like), strictly storing keys without context or adaptive parameters makes it less effective.

**The Solution: Adapted LFU-DA (LFU with Dynamic Aging)**

To fix this, we will implement an algorithm inspired by **LFU-DA (Least Frequently Used with Dynamic Aging)** or **GDSF (Greedy Dual Size Frequency)** concepts, adapted for this specific constrained environment.

**Key Changes:**
1.  **Dynamic Aging via Inflation:** Instead of dividing frequencies periodically (which is expensive and choppy), we will *inflate* the starting value of new items based on the "value" of the item just evicted. This is the core of LFU-DA. It allows new items to compete with old, high-frequency items immediately.
2.  **Priority Calculation:** Priority $P(i) = Freq(i) + L$, where $L$ is a running "aging factor" that increases as items are evicted.
3.  **Tie-Breaking with LRU:** If priorities are equal, strict LRU is used.
4.  **Simplified Metadata:** We maintain frequency and last access time. The ghost registry is removed to simplify logic and focus memory/CPU cycles on the core metric, as the dynamic aging inherently handles the "second chance" concept better by raising the baseline cost of entry.

Here is the improved implementation:

```python
# Metadata dictionaries
m_key_frequency = dict()     # Stores access frequency: F(k)
m_key_last_access = dict()   # Stores logical timestamp: T(k)

# Global State for Dynamic Aging
# L represents the 'minimum' value required to stay in the cache.
# Instead of decaying old items, we inflate the cost of new items.
m_cache_min_priority = 0.0   

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority.
    
    Priority P(k) = F(k) + L
    where:
      F(k) is the frequency of key k
      L is the dynamic aging factor (m_cache_min_priority) at the time of insertion/update.
      
    Note: Since L is applied during updates, the stored frequency m_key_frequency[key]
    effectively represents P(k).
    
    Tie-breaking: Least Recently Used (LRU) is used if priorities are equal.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    victim_key = None
    min_priority = float('inf')
    min_timestamp = float('inf')

    # We iterate to find the item with the smallest Priority.
    # If priorities match, we pick the one with the smallest Last Access Time (LRU).
    for key in candidate_keys:
        # In this implementation, m_key_frequency[key] stores the computed Priority directly
        priority = m_key_frequency.get(key, 0)
        timestamp = m_key_last_access.get(key, 0)
        
        if priority < min_priority:
            min_priority = priority
            min_timestamp = timestamp
            victim_key = key
        elif priority == min_priority:
            # Tie-breaker: LRU
            if timestamp < min_timestamp:
                min_timestamp = timestamp
                victim_key = key
    
    # Update the global aging factor L to the priority of the evicted item.
    # This ensures new items must meet this threshold to survive.
    if victim_key is not None:
        m_cache_min_priority = min_priority

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency (timestamp).
    2. Increment Priority.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Standard LFU-DA update: P_new = P_old + 1
    # Note: Some variations reset P to L + Freq + 1, but simple increment works 
    # well to preserve accumulated history while allowing the global L floor to rise.
    current_priority = m_key_frequency.get(obj.key, m_cache_min_priority)
    m_key_frequency[obj.key] = current_priority + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Set recency.
    2. Initialize Priority to L + 1.
       This gives the new item a "fighting chance" against existing items
       because it inherits the base cost (L) of the cache.
    '''
    global m_key_frequency, m_key_last_access, m_cache_min_priority
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Initialize Priority
    # The new item starts with the current global aging factor (L) plus an initial count.
    # This prevents the "Cold Start" problem where new items with freq=1 are immediately evicted.
    m_key_frequency[obj.key] = m_cache_min_priority + 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```
2025-11-27 19:01:29,815 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
MAX_FREQ = 255  # Cap frequency to prevent historical domination
AGING_INTERVAL = 1000 # Number of accesses before we decay frequencies
current_access_counter = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid of Frequency and Recency (LFRU).
    Items with low frequency are evicted first. 
    If frequencies are high/equal, we look at recency.
    '''
    candid_obj_key = None
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We use a simple iteration to find the victim.
    # Metric: Score = Frequency * Weight + Recency_Factor
    # However, strictly adhering to "Min Frequency, then Min Recency" is robust.
    
    best_key = candidate_keys[0]
    # Default to 0 if not found (though they should be there)
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # 1. Primary Sort: Frequency. 
        # We prefer to evict items with lower frequency.
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        
        # 2. Secondary Sort: Recency.
        # If frequencies are equal, evict the one accessed furthest in the past (LRU).
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Update recency and increment frequency (with saturation).
    Perform aging if necessary.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Update Frequency with Saturation
    curr_freq = m_key_frequency.get(obj.key, 0)
    if curr_freq < MAX_FREQ:
        m_key_frequency[obj.key] = curr_freq + 1
        
    # Periodic Aging: Halve frequencies periodically to handle shift in workloads
    if current_access_counter > AGING_INTERVAL:
        for k in m_key_frequency:
            m_key_frequency[k] = m_key_frequency[k] // 2
        current_access_counter = 0

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Set initial values.
    Crucially, check if we have "Ghost" metadata. 
    If this key was recently evicted (still in dicts but not in cache), restore its freq.
    '''
    global m_key_frequency, m_key_last_access, current_access_counter
    
    current_access_counter += 1
    
    # Recency is always "now"
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Frequency Logic:
    # If the key exists in m_key_frequency, it means it was evicted recently but
    # we kept the history (Ghost Cache concept). We restore it but don't reset to 1.
    if obj.key in m_key_frequency:
        # Give it a slight boost to protect it from immediate eviction again, 
        # but respect the cap.
        curr = m_key_frequency[obj.key]
        m_key_frequency[obj.key] = min(curr + 1, MAX_FREQ)
    else:
        # Brand new item. Start with frequency 1.
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Do NOT delete metadata immediately. 
    We keep it to serve as a "Ghost Cache" / History.
    This allows the algorithm to recognize "churning" items that are evicted and immediately re-requested.
    
    However, to prevent memory leaks in the simulation metadata, we might need a cleanup strategy.
    For this specific task context, we will assume we can keep reasonable metadata history,
    or we rely on the periodic aging in 'update_after_hit' to eventually flush very old zeroes.
    '''
    global m_key_frequency, m_key_last_access
    
    # We deliberately DO NOT delete from m_key_frequency or m_key_last_access here.
    # By keeping the frequency count, if 'evicted_obj' is inserted again soon,
    # 'update_after_insert' will see the old frequency and give it a higher priority,
    # correcting the mistake of evicting it.
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0526

Example 2:
- Trace ID: 24
- Hit Rate: 0.2886

Example 3:
- Trace ID: 46
- Hit Rate: 0.4663

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:01:29 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:01:29,816 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:01:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:01:51,023 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:01:51,025 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we will move from a basic frequency-based approach to a more adaptive policy inspired by **LFU (Least Frequently Used) with Dynamic Aging** or concepts similar to **ARC (Adaptive Replacement Cache)** or **LIRS (Low Inter-reference Recency Set)**.

However, given the constraints of a generic Python simulation where complex pointer manipulation is harder, a highly effective and robust strategy is **Window-TinyLFC (W-TinyLFC)** or a simplified **Segmented LRU (SLRU)** with admission control.

Given the performance data (especially the very low 5% hit rate on Trace 42), the current algorithm suffers from "Cache Pollution." One-time-use items (scans) enter the cache, acquire a frequency of 1, and kick out valuable items.

### The Strategy: Segmented LRU (SLRU) with Probation
To fix this, we will implement a simplified **Segmented LRU**. The cache is logically divided into two segments:
1.  **Probation Segment:** New items enter here. They are the first candidates for eviction.
2.  **Protected Segment:** Items that are hit while in the Probation segment are promoted here. These are "proven" valuable items.

This prevents "scan" traffic (items accessed only once) from flushing out frequently used items.

**Metadata:**
*   `m_protected`: A set of keys currently in the protected segment.
*   `m_access_time`: A dictionary mapping keys to their last access time (for LRU logic within segments).

**Eviction Logic:**
1.  If the cache is full, we look at the Probation segment first.
2.  We find the LRU item in Probation.
3.  If Probation is empty (rare, but possible if everything is promoted), we evict the LRU from Protected.

**Update Logic:**
*   **Insert:** Always insert into Probation.
*   **Hit:**
    *   If in Probation -> Promote to Protected.
    *   If in Protected -> Update access time (stay in Protected).
    *   To prevent the Protected segment from filling the entire cache (starving new entries), if the Protected segment grows too large (e.g., > 80% of items), we demote the LRU item from Protected back to Probation.

### Improved Implementation

```python
# Metadata dictionaries
# Tracks the last access time (Recency) for all items
m_access_time = dict()
# Tracks items considered "Protected" (High priority)
m_protected = set()

# Configuration
PROTECTED_RATIO = 0.8  # Max 80% of items can be protected

def evict(cache_snapshot, obj):
    '''
    Evicts based on Segmented LRU (SLRU).
    Priority 1: Evict LRU from the Probation (non-protected) segment.
    Priority 2: If no probation items exist, evict LRU from Protected segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate candidates into Probation and Protected lists
    probation_candidates = []
    protected_candidates = []
    
    for key in candidate_keys:
        if key in m_protected:
            protected_candidates.append(key)
        else:
            probation_candidates.append(key)
            
    victim_key = None
    
    # 1. Try to evict from Probation segment (Filter out one-hit wonders)
    if probation_candidates:
        # Find LRU in probation
        victim_key = probation_candidates[0]
        min_time = m_access_time.get(victim_key, 0)
        
        for key in probation_candidates[1:]:
            t = m_access_time.get(key, 0)
            if t < min_time:
                min_time = t
                victim_key = key
    
    # 2. If Probation is empty, evict LRU from Protected
    elif protected_candidates:
        victim_key = protected_candidates[0]
        min_time = m_access_time.get(victim_key, 0)
        
        for key in protected_candidates[1:]:
            t = m_access_time.get(key, 0)
            if t < min_time:
                min_time = t
                victim_key = key
                
    # Fallback (should not be reached if cache is not empty)
    if victim_key is None:
        victim_key = candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency.
    2. If item is in Probation, promote to Protected.
    3. Balance segments if Protected becomes too large.
    '''
    global m_access_time, m_protected
    
    # Update global recency
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # Promotion Logic
    if obj.key not in m_protected:
        m_protected.add(obj.key)
        
    # Balancing Logic: Ensure Protected segment doesn't starve Probation
    # If Protected set is larger than specific ratio of total cache count
    current_cache_size = len(cache_snapshot.cache)
    if len(m_protected) > current_cache_size * PROTECTED_RATIO:
        # We need to demote the LRU item within the Protected set back to Probation.
        # Note: We only look at keys currently in cache AND in protected set.
        protected_in_cache = [k for k in m_protected if k in cache_snapshot.cache]
        
        if protected_in_cache:
            lru_protected_key = protected_in_cache[0]
            min_ts = m_access_time.get(lru_protected_key, 0)
            
            for k in protected_in_cache[1:]:
                ts = m_access_time.get(k, 0)
                if ts < min_ts:
                    min_ts = ts
                    lru_protected_key = k
            
            # Demote: Remove from protected set (effectively moves to Probation)
            m_protected.remove(lru_protected_key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Update recency.
    2. New items always start in Probation (not in m_protected).
    '''
    global m_access_time, m_protected
    
    m_access_time[obj.key] = cache_snapshot.access_count
    
    # Ensure it starts in Probation
    if obj.key in m_protected:
        m_protected.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    Cleanup metadata to prevent memory leaks.
    '''
    global m_access_time, m_protected
    
    # Clean up the evicted object's metadata
    if evicted_obj.key in m_access_time:
        del m_access_time[evicted_obj.key]
        
    if evicted_obj.key in m_protected:
        m_protected.remove(evicted_obj.key)
```
2025-11-27 19:02:27,572 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 32
- Hit Rate: 0.029

Example 2:
- Trace ID: 0
- Hit Rate: 0.5093

Example 3:
- Trace ID: 2
- Hit Rate: 0.4917

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:02:27 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:02:27,573 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:02:52 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:02:52,114 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:02:52,115 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The analysis of the previous implementation and results suggests that while the Segmented LRU (SLRU) approach is sound, the strict prioritization of evicting *any* item with Frequency=1 over *any* item with Frequency > 1 is too aggressive. In workloads with "scanning" (reading many unique items once), this protects the main cache well. However, in workloads where new items rapidly become hot (recency-heavy), this logic prevents new, potentially popular items from staying in the cache long enough to get a second hit.

To improve this, we will implement an **Adaptive Replacement Cache (ARC)-inspired** algorithm, simplified for this specific API context.

### Key Strategy Improvements

1.  **Recency vs. Frequency Balance:** Instead of strictly strictly punishing frequency=1 items, we will balance "Recency" (LRU) and "Frequency" (LFU) using a dynamic ghost registry that tells us *what kind of mistake we made*.
2.  **Ghost Lists:** We will maintain two ghost lists:
    *   `ghost_recent`: Items evicted that were recently added (Probation/T1 evictions). A hit here means we should have made the Recency list larger.
    *   `ghost_frequent`: Items evicted that were frequently accessed (Protected/T2 evictions). A hit here means we should have kept the Frequency list larger.
3.  **Adaptive Parameter (`p`):** We introduce a target size `p` for the "Recency" set (T1).
    *   If we hit in `ghost_recent`, we increase `p` (invest more space in recent items).
    *   If we hit in `ghost_frequent`, we decrease `p` (invest more space in frequent items).
4.  **Victim Selection:** We evict based on this target `p`. If the Recency set (T1) is larger than `p`, we evict the LRU of T1. Otherwise, we evict the LRU of T2 (Frequency set).

This allows the cache to essentially "morph" between an LRU and an LFU behavior based on the actual trace patterns (Example 1, 2, and 3 likely have different dominant patterns).

```python
import math

# --- Metadata Globals ---

# Stores active cache metadata.
# Key -> Node(type, last_access_time)
# Types: 'T1' (Recent/Probation), 'T2' (Frequent/Protected)
m_meta = dict()

# Ghost Registries (Evicted keys)
# We store just the key presence. In a full ARC, these are strict queues, 
# but here we use dicts for O(1) lookups and use access time to approximate LRU behavior.
m_ghost_b1 = dict() # Ghost Recent: Evicted from T1
m_ghost_b2 = dict() # Ghost Frequent: Evicted from T2

# Adaptive Parameter 'p'
# Target size for the T1 (Recent) list.
# 0 <= p <= capacity.
m_p = 0

def get_lru_key(candidate_keys, key_subset_filter=None):
    """
    Helper to find the LRU key among candidates, optionally filtering by a set/type.
    """
    lru_key = None
    min_time = float('inf')
    
    for k in candidate_keys:
        # If a filter is provided (e.g., only look at T1 items), skip others
        if key_subset_filter and m_meta.get(k, {}).get('type') != key_subset_filter:
            continue
            
        # Standard LRU search
        access_time = m_meta.get(k, {}).get('time', 0)
        if access_time < min_time:
            min_time = access_time
            lru_key = k
            
    return lru_key

def evict(cache_snapshot, obj):
    '''
    ARC-like eviction logic.
    We decide whether to evict from T1 (Recent) or T2 (Frequent) based on target `p`.
    '''
    global m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Classify current cache content
    t1_keys = [k for k in candidate_keys if m_meta.get(k, {}).get('type') == 'T1']
    t2_keys = [k for k in candidate_keys if m_meta.get(k, {}).get('type') == 'T2']
    
    len_t1 = len(t1_keys)
    
    # ARC Eviction Logic:
    # We want T1 to have roughly 'm_p' items.
    # If len(T1) > p, we have too many recent items, evict LRU from T1.
    # Else, we satisfy the recent quota, so we evict LRU from T2 to make space.
    
    victim_key = None
    
    # Robustness check: if T1 is empty, must evict T2. If T2 empty, must evict T1.
    if len_t1 > 0 and (len_t1 > m_p or len(t2_keys) == 0):
        # Evict LRU from T1
        victim_key = get_lru_key(t1_keys) # t1_keys is the filter
        # Fallback if logic fails (shouldn't happen given len_t1 > 0)
        if not victim_key: victim_key = get_lru_key(candidate_keys)
    else:
        # Evict LRU from T2
        victim_key = get_lru_key(t2_keys)
        # Fallback
        if not victim_key: victim_key = get_lru_key(candidate_keys)
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item was in T1 (Recent), move to T2 (Frequent).
    If item was in T2, update time (stay in T2).
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    if obj.key in m_meta:
        # Update timestamp
        m_meta[obj.key]['time'] = current_time
        
        # If it was in T1 (Probation), a second hit promotes it to T2 (Protected)
        if m_meta[obj.key]['type'] == 'T1':
            m_meta[obj.key]['type'] = 'T2'
    else:
        # Edge case: Metadata missing (should not happen in consistent simulation)
        m_meta[obj.key] = {'type': 'T2', 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Adapt 'p' based on Ghost hits.
    2. Add new item to T1 (Recent).
    '''
    global m_meta, m_ghost_b1, m_ghost_b2, m_p
    
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: Sim treats capacity as count usually, but provided as bytes/size. Assuming size=1 count logic or utilizing capacity purely for calculation boundaries.
    
    # Check Ghost Hits (Did we evict something we shouldn't have?)
    if obj.key in m_ghost_b1:
        # Hit in Ghost Recent (B1). 
        # Means we evicted a recent item too soon. We should expand T1 size (p).
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
            delta = len(m_ghost_b2) // len(m_ghost_b1)
        m_p = min(m_p + delta, len(cache_snapshot.cache) + 1) # Bound by current size (approx capacity)
        
        del m_ghost_b1[obj.key]
        # Promoted straight to T2 because it's a recalled item (Frequency > 1 logic)
        new_type = 'T2' 
        
    elif obj.key in m_ghost_b2:
        # Hit in Ghost Frequent (B2).
        # Means we evicted a frequent item too soon. We should shrink T1 size (p) to make room for T2.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
        m_p = max(m_p - delta, 0)
        
        del m_ghost_b2[obj.key]
        # Promoted straight to T2
        new_type = 'T2'
        
    else:
        # Brand new item, never seen or long forgotten.
        # Enters T1 (Recent)
        new_type = 'T1'

    # Insert Metadata
    m_meta[obj.key] = {'type': new_type, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted metadata to the corresponding Ghost Registry.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2
    
    if evicted_obj.key in m_meta:
        evicted_type = m_meta[evicted_obj.key]['type']
        
        # Move to Ghost lists
        if evicted_type == 'T1':
            m_ghost_b1[evicted_obj.key] = True
        else:
            m_ghost_b2[evicted_obj.key] = True
            
        del m_meta[evicted_obj.key]
    
    # Maintenance: Limit Ghost Registry Size to prevent memory leaks
    # Standard ARC keeps ghost size roughly equal to cache capacity.
    current_cache_len = len(cache_snapshot.cache)
    
    # Simple cleanup strategy: If ghosts exceed 2x Capacity (conservative), prune.
    # Since we can't easily iter-delete without order, we rely on the fact that
    # Python dicts are insertion ordered.
    if len(m_ghost_b1) > current_cache_len:
        try:
            oldest = next(iter(m_ghost_b1))
            del m_ghost_b1[oldest]
        except: pass
        
    if len(m_ghost_b2) > current_cache_len:
        try:
            oldest = next(iter(m_ghost_b2))
            del m_ghost_b2[oldest]
        except: pass
```
2025-11-27 19:03:10,432 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 19
- Hit Rate: 0.2722

Example 2:
- Trace ID: 39
- Hit Rate: 0.0668

Example 3:
- Trace ID: 20
- Hit Rate: 0.3412

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:03:10 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:03:10,433 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:04:53 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:04:53,745 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:04:53,746 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: ```python
# Segmented LRU (SLRU) Implementation with Ghost Cache

# Metadata dictionaries behaving as Ordered Sets
# meta_protected: 'Protected' segment for popular items. Keys mapped to None. LRU at front.
meta_protected = dict()
# meta_probation: 'Probation' segment for new items. Keys mapped to None. LRU at front.
meta_probation = dict()
# meta_ghost: 'Ghost' cache to track recently evicted items. Keys mapped to None. LRU at front.
meta_ghost = dict()

def evict(cache_snapshot, obj):
    '''
    SLRU Eviction Strategy.
    
    We maintain two segments:
    1. Probation: For new items. They are evicted first unless accessed again.
    2. Protected: For popular items (accessed >= 2 times).
    
    Strategy:
    1. Balance segments: Ensure Protected doesn't exceed a specific ratio (e.g., 80%) of the total cache count.
       If it does, demote the LRU of Protected to the MRU of Probation.
    2. Pick Victim: Choose the LRU of Probation. If Probation is empty, choose LRU of Protected.
    '''
    global meta_protected, meta_probation
    
    # Determine current operational size (number of objects)
    # When evict is called, the cache is effectively full.
    current_count = len(cache_snapshot.cache)
    
    # Define target capacity for Protected segment (e.g., 80% of total items)
    target_protected = int(current_count * 0.8)
    
    # 1. Segment Balancing
    # If Protected is too big, demote its LRU items to Probation (give them a second chance)
    while len(meta_protected) > target_protected:
        # Get LRU from Protected
        try:
            demoted_key = next(iter(meta_protected))
            del meta_protected[demoted_key]
            
            # Move to Probation (as MRU, giving it a grace period)
            # We check if it's in cache to ensure metadata consistency
            if demoted_key in cache_snapshot.cache:
                meta_probation[demoted_key] = None
        except StopIteration:
            break
            
    # 2. Select Victim
    victim_key = None
    
    # Preference 1: Evict from Probation (LRU)
    if meta_probation:
        # Iterate to find the first key that is effectively in the cache
        # (Handles potential rare async state drifts)
        for k in list(meta_probation.keys()):
            if k in cache_snapshot.cache:
                victim_key = k
                break
            else:
                # Clean up stale metadata if found
                del meta_probation[k]
    
    # Preference 2: Evict from Protected (LRU) - only if Probation is empty/invalid
    if victim_key is None and meta_protected:
        for k in list(meta_protected.keys()):
            if k in cache_snapshot.cache:
                victim_key = k
                break
            else:
                del meta_protected[k]
                
    # Fallback (Should not happen in consistent state)
    if victim_key is None:
        victim_key = list(cache_snapshot.cache.keys())[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If object is in Probation, promote it to Protected (Frequency > 1).
    - If object is in Protected, update its LRU position (move to MRU).
    '''
    global meta_protected, meta_probation
    
    key = obj.key
    
    if key in meta_probation:
        # Promote: Probation -> Protected
        del meta_probation[key]
        meta_protected[key] = None
    elif key in meta_protected:
        # Update usage: Move to MRU of Protected
        del meta_protected[key]
        meta_protected[key] = None
    else:
        # Safety net: If hit but not in metadata, assume Protected
        meta_protected[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - If object key is in Ghost cache (recently evicted), it means it's a recurring item.
      Insert directly into Protected segment.
    - Otherwise, it's a new item. Insert into Probation segment.
    '''
    global meta_protected, meta_probation, meta_ghost
    
    key = obj.key
    
    if key in meta_ghost:
        # Was seen recently, now back -> Protected
        del meta_ghost[key]
        meta_protected[key] = None
    else:
        # First access / Cold -> Probation
        meta_probation[key] = None
        
    # Maintain Ghost size limits
    # Ghost list prevents "one-hit wonders" from polluting Protected, 
    # but needs to be bounded to avoid memory leaks.
    # We use a heuristic limit relative to cache count.
    max_ghost_size = max(100, len(cache_snapshot.cache))
    
    if len(meta_ghost) > max_ghost_size:
        # Evict LRU from Ghost
        try:
            oldest = next(iter(meta_ghost))
            del meta_ghost[oldest]
        except StopIteration:
            pass

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Remove the victim from the metadata segments.
    - Add the victim key to the Ghost cache (history) to track recency.
    '''
    global meta_protected, meta_probation, meta_ghost
    
    key = evicted_obj.key
    
    if key in meta_probation:
        del meta_probation[key]
        # Track in Ghost cache (MRU)
        meta_ghost[key] = None
    elif key in meta_protected:
        del meta_protected[key]
        # Even if evicted from Protected, we track in Ghost
        meta_ghost[key] = None
```
2025-11-27 19:05:03,154 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 18
- Hit Rate: 0.0229

Example 2:
- Trace ID: 44
- Hit Rate: 0.021

Example 3:
- Trace ID: 38
- Hit Rate: 0.1255

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:05:03 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:05:03,155 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:05:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:05:33,737 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:05:33,738 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the extremely low hit rates observed in the provided traces (e.g., ~2%). The previous approach likely suffered from two main issues:
1.  **Complexity Overhead vs. Accuracy:** The logarithmic scoring combined with global aging is computationally heavy and, more importantly, hard to tune. The decay factor might be aggressively pruning useful history or keeping useless history too long.
2.  **Scan Resistance:** Simple LFU/LRU hybrids often fail in the presence of "scans" (sequences of one-time-use items) because they pollute the history or the cache itself.

The solution below implements a variation of **ARC (Adaptive Replacement Cache)** logic, but adapted for this specific Python interface. Instead of a single complex score, we will manage two conceptual lists (or rather, we will simulate the behavior of two lists using metadata):
1.  **Recency-biased set (T1):** Items seen once recently.
2.  **Frequency-biased set (T2):** Items seen at least twice recently.

We will dynamically tune the balance between these two sets based on "Ghost Hits" (hits on metadata of recently evicted items). If we get hits on the ghost history of T1, we need more space for recency. If we get hits on the ghost history of T2, we need more space for frequency.

### Improved Cache Eviction Algorithm

```python
# Metadata dictionaries
# m_key_state: Maps key -> state (0: T1 (Recency), 1: T2 (Frequency), 2: B1 (Ghost Recency), 3: B2 (Ghost Frequency))
m_key_state = dict()

# We track a target size for the T1 (Recency) list.
# p: Target size for the recency list (adaptive parameter)
adaptive_p = 0

# Constants for states to improve readability
STATE_T1 = 0 # In Cache, Recent
STATE_T2 = 1 # In Cache, Frequent
STATE_B1 = 2 # Ghost, Recent
STATE_B2 = 3 # Ghost, Frequent

# Track separate LRU lists to find victims efficiently.
# Since we cannot modify the cache object structure, we maintain our own ordering lists.
list_t1 = [] # Keys in T1 (Recency)
list_t2 = [] # Keys in T2 (Frequency)
list_b1 = [] # Keys in B1 (Ghost Recency)
list_b2 = [] # Keys in B2 (Ghost Frequency)

def _remove_from_list(l, key):
    """Helper to remove key from a list if present."""
    if key in l:
        l.remove(key)

def _move_to_mru(l, key):
    """Helper to move a key to the end (MRU position) of a list."""
    if key in l:
        l.remove(key)
    l.append(key)

def evict(cache_snapshot, obj):
    '''
    Evicts an object based on the Adaptive Replacement Cache (ARC) logic.
    We decide whether to evict from T1 (Recent) or T2 (Frequent) based on the 
    adaptive parameter `p` and the current size of T1.
    '''
    global list_t1, list_t2, list_b1, list_b2, adaptive_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Identify which list to evict from
    # Logic: If len(T1) > p, we have too many recent items, evict from T1.
    #        Otherwise, evict from T2.
    # Note: We must ensure the victim is actually in the cache snapshot.
    
    t1_size = len(list_t1)
    victim_key = None
    
    # We prefer to evict from T1 if it exceeds the target size 'p'
    if t1_size > 0 and (t1_size > adaptive_p or not list_t2):
        # Evict LRU of T1
        # Validate that the item is actually in the cache (sync check)
        for k in list_t1:
            if k in cache_snapshot.cache:
                victim_key = k
                break
        
        # If we found a victim in T1, we treat it as moving to B1 (Ghost Recency)
        # The actual move happens in update_after_evict usually, but we need to know WHO here.
        
    # If we didn't pick from T1, or T1 was empty, pick from T2
    if victim_key is None and list_t2:
        for k in list_t2:
            if k in cache_snapshot.cache:
                victim_key = k
                break

    # Fallback: if our lists are out of sync with the cache for some reason, 
    # pick the first available key (should rarely happen)
    if victim_key is None:
        victim_key = candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    If it's in T1, move to T2 (it has been seen twice).
    If it's in T2, move to MRU of T2.
    '''
    global list_t1, list_t2, m_key_state
    key = obj.key
    
    state = m_key_state.get(key)

    if state == STATE_T1:
        # Promotion: Recent -> Frequent
        _remove_from_list(list_t1, key)
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2
        
    elif state == STATE_T2:
        # Update: Frequent -> MRU Frequent
        _move_to_mru(list_t2, key)
        # State remains T2

    # If state is None or Ghost (B1/B2), it technically counts as a miss in the
    # pure ARC model, but here `update_after_hit` implies it WAS in `cache_snapshot.cache`.
    # This usually means it was already in T1 or T2.

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (Cache Miss):
    Check if it was in Ghost lists (B1 or B2) to adapt `p`.
    Then insert into T1 (Recency).
    '''
    global list_t1, list_t2, list_b1, list_b2, m_key_state, adaptive_p
    
    key = obj.key
    capacity = cache_snapshot.capacity // obj.size if obj.size > 0 else 100 # Approx capacity in count
    # Since we only get total capacity in bytes, we estimate count capacity roughly or 
    # rely on the lists. For ARC, 'c' is usually the cache count capacity.
    # We will use the current number of items + 1 as a proxy for capacity if needed,
    # or just cap our ghost lists.
    
    state = m_key_state.get(key)

    if state == STATE_B1:
        # Ghost Hit on Recency History -> We need a larger Recency list.
        # Increase p
        delta = 1
        if len(list_b1) >= len(list_b2) and len(list_b2) > 0:
            delta = 1
        elif len(list_b2) > 0:
            delta = len(list_b1) / len(list_b2)
            
        adaptive_p = adaptive_p + delta
        # remove from B1
        _remove_from_list(list_b1, key)
        
        # Move to T2 (since it was seen recently, and now seen again)
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2

    elif state == STATE_B2:
        # Ghost Hit on Frequency History -> We need a larger Frequency list (smaller p).
        # Decrease p
        delta = 1
        if len(list_b2) >= len(list_b1) and len(list_b1) > 0:
            delta = 1
        elif len(list_b1) > 0:
            delta = len(list_b2) / len(list_b1)
            
        adaptive_p = adaptive_p - delta
        # remove from B2
        _remove_from_list(list_b2, key)
        
        # Move to T2
        _move_to_mru(list_t2, key)
        m_key_state[key] = STATE_T2

    else:
        # Totally new item.
        # Insert into T1 (Recency)
        _move_to_mru(list_t1, key)
        m_key_state[key] = STATE_T1

    # Clamp p
    current_cache_size = len(cache_snapshot.cache)
    if adaptive_p < 0: adaptive_p = 0
    if adaptive_p > current_cache_size: adaptive_p = current_cache_size
    
    # Initialize p if it's 0 and we are just starting (heuristic)
    if adaptive_p == 0 and current_cache_size > 0 and not list_b1 and not list_b2:
        adaptive_p = current_cache_size / 2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Move the victim from T1/T2 to B1/B2 (Ghost lists).
    '''
    global list_t1, list_t2, list_b1, list_b2, m_key_state
    
    key = evicted_obj.key
    state = m_key_state.get(key)
    
    if state == STATE_T1:
        _remove_from_list(list_t1, key)
        # Move to B1
        _move_to_mru(list_b1, key)
        m_key_state[key] = STATE_B1
    elif state == STATE_T2:
        _remove_from_list(list_t2, key)
        # Move to B2
        _move_to_mru(list_b2, key)
        m_key_state[key] = STATE_B2
        
    # Maintain Ghost List Sizes
    # To prevent memory leaks, we restrict the size of B1 and B2.
    # Standard ARC sets |T1| + |B1| = c and |T2| + |B2| = 2c.
    # We will simply cap them at the current cache size.
    current_cache_count = len(cache_snapshot.cache)
    max_ghost_size = max(current_cache_count, 100) # Ensure at least some history
    
    while len(list_b1) > max_ghost_size:
        k = list_b1.pop(0) # Remove LRU of B1
        if k in m_key_state: del m_key_state[k]

    while len(list_b2) > max_ghost_size:
        k = list_b2.pop(0) # Remove LRU of B2
        if k in m_key_state: del m_key_state[k]
```
2025-11-27 19:07:49,496 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 22
- Hit Rate: 0.0587

Example 2:
- Trace ID: 33
- Hit Rate: 0.7503

Example 3:
- Trace ID: 4
- Hit Rate: 0.5062

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:07:49 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:07:49,497 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:08:22 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:08:22,742 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:08:22,743 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Optimizing cache eviction is a balancing act between recency (LRU) and frequency (LFU). The current implementation attempts a Segmented LRU approach (probation vs. protected) but suffers from rigid thresholds (freq=1 vs freq>1) and lacks adaptability to changing workload patterns.

The improved algorithm below implements a logic inspired by **ARC (Adaptive Replacement Cache)** and **SLRU (Segmented LRU)**.

### Key Improvements:
1.  **Strict Segmentation**: Instead of a simple frequency counter, we conceptually divide the cache into two lists:
    *   **T1 (Probationary)**: Items accessed only once recently. Captures recency.
    *   **T2 (Protected)**: Items accessed at least twice. Captures frequency.
2.  **Adaptive Sizing**: We introduce a parameter `p` (target size for T1). The algorithm dynamically adjusts `p` based on "Ghost" hits.
    *   If we get a hit in the T1 Ghost registry (meaning we evicted a recent item too soon), we increase the size of T1.
    *   If we get a hit in the T2 Ghost registry (meaning we evicted a frequent item too soon), we increase the size of T2 (decrease T1).
3.  **Eviction Logic**: We evict from T1 or T2 based on the current cache population relative to the adaptive parameter `p`.

This allows the cache to behave like LRU when the workload creates scan patterns (many unique items), and like LFU when the workload has a stable "hot set."

```python
import math

# Global Metadata
# m_map: Stores metadata for items currently in cache or ghost lists.
# Structure: key -> {'list_type': 'T1'/'T2'/'B1'/'B2', 'access_time': int}
# T1: Recent (Probation) | T2: Frequent (Protected)
# B1: Ghost Recent       | B2: Ghost Frequent
m_map = dict()

# Adaptive Parameter 'p': Target size for the T1 (Probation) list.
# 0 <= p <= capacity
m_p = 0
m_capacity_tracker = 0 # To store capacity from snapshot for logic

def get_victim_score(key, current_time):
    '''
    We implement the eviction logic inside the `evict` function itself using the ARC strategy.
    However, the framework requires this function to return a score.
    To bridge the gap, we will use this function to identifying which list (T1 or T2)
    the key belongs to and return a score that aligns with the decision made in `evict`.
    
    Higher score = High priority to evict.
    '''
    global m_map, m_p, m_capacity_tracker
    
    if key not in m_map:
        return -1.0

    meta = m_map[key]
    list_type = meta['list_type']
    access_time = meta['access_time'] # Smaller means older (LRU)

    # ARC Logic determines whether we evict from T1 or T2 based on the target 'p'.
    # We count how many items are currently in T1.
    t1_count = sum(1 for k, v in m_map.items() if v['list_type'] == 'T1')
    
    # We want to evict the LRU of T1 if len(T1) > p
    # We want to evict the LRU of T2 if len(T1) <= p
    
    # Construction of Score:
    # 1. Base Score separates the preferred victim list from the safe list.
    # 2. Tie-breaker is staleness (Time - access_time). Oldest gets highest score.
    
    staleness = current_time - access_time
    
    # If T1 is overflowing (len(T1) > p), we prefer evicting from T1.
    # If T1 is under quota, we prefer evicting from T2.
    evict_t1_preference = t1_count > m_p
    
    if evict_t1_preference:
        if list_type == 'T1':
            # Highest priority: Base 2e12 + staleness
            return 2_000_000_000_000 + staleness
        elif list_type == 'T2':
            # Lower priority: Base 0 + staleness
            return 0 + staleness
    else:
        if list_type == 'T2':
            # Highest priority: Base 2e12 + staleness
            return 2_000_000_000_000 + staleness
        elif list_type == 'T1':
             # Lower priority: Base 0 + staleness
            return 0 + staleness

    return 0.0

def evict(cache_snapshot, obj):
    '''
    Selects the victim. 
    The logic is encoded in `get_victim_score` which dynamically adjusts
    priorities based on the adaptive parameter `p` and current list sizes.
    '''
    global m_capacity_tracker
    
    # Update capacity tracker for use in logic
    m_capacity_tracker = cache_snapshot.capacity
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0

    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1 (Probation), move to T2 (Protected).
    If item is in T2, move to MRU position of T2.
    '''
    global m_map
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    if key in m_map:
        # Move to T2 (Frequent) regardless of whether it was T1 or T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time
    else:
        # Edge case: Hit in cache but missing metadata (should not happen usually)
        m_map[key] = {'list_type': 'T2', 'access_time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if it was in Ghost lists (B1 or B2).
    2. Adapt parameter 'p' based on ghost hits.
    3. Insert into T2 if it came from ghost, or T1 if brand new.
    '''
    global m_map, m_p, m_capacity_tracker
    
    current_time = cache_snapshot.access_count
    key = obj.key
    
    # Ensure capacity is known
    capacity = cache_snapshot.capacity
    
    # Case 1: Brand new item (Not in T1, T2, B1, B2)
    if key not in m_map:
        # Insert into T1 (MRU)
        m_map[key] = {'list_type': 'T1', 'access_time': current_time}
        return

    # Case 2: History Hit (Ghost List)
    list_type = m_map[key]['list_type']
    
    # Sizes of ghost lists
    b1_size = sum(1 for v in m_map.values() if v['list_type'] == 'B1')
    b2_size = sum(1 for v in m_map.values() if v['list_type'] == 'B2')
    
    if list_type == 'B1':
        # Hit in Ghost Recent. We should have kept T1 larger.
        # Delta logic from ARC
        delta = 1 if b1_size >= b2_size else b2_size / b1_size
        m_p = min(capacity, m_p + delta)
        
        # Promote to T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time
        
    elif list_type == 'B2':
        # Hit in Ghost Frequent. We should have kept T2 larger (T1 smaller).
        delta = 1 if b2_size >= b1_size else b1_size / b2_size
        m_p = max(0, m_p - delta)
        
        # Promote to T2
        m_map[key]['list_type'] = 'T2'
        m_map[key]['access_time'] = current_time

    # If it was somehow already T1 or T2 (should be handled by hit), just update time
    else:
        m_map[key]['access_time'] = current_time

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted item from Active list (T1/T2) to Ghost list (B1/B2).
    Prune Ghost lists if they get too huge.
    '''
    global m_map
    
    key = evicted_obj.key
    if key not in m_map:
        return
        
    prev_type = m_map[key]['list_type']
    
    if prev_type == 'T1':
        m_map[key]['list_type'] = 'B1' # Ghost Recent
    elif prev_type == 'T2':
        m_map[key]['list_type'] = 'B2' # Ghost Frequent
        
    # Garbage Collection for Metadata
    # ARC generally suggests maintaining B1+B2 size roughly equal to Cache Capacity.
    # Total tracked items (T1+T2+B1+B2) <= 2 * Capacity
    
    # Get all keys in ghost lists
    ghost_keys = [k for k, v in m_map.items() if v['list_type'] in ('B1', 'B2')]
    
    if len(ghost_keys) > cache_snapshot.capacity:
        # Find oldest ghost to remove
        # This scan is linear, but metadata size is constrained, usually acceptable.
        oldest_key = None
        min_time = float('inf')
        
        for k in ghost_keys:
            if m_map[k]['access_time'] < min_time:
                min_time = m_map[k]['access_time']
                oldest_key = k
        
        if oldest_key:
            del m_map[oldest_key]
```
2025-11-27 19:08:56,260 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 25
- Hit Rate: 0.3881

Example 2:
- Trace ID: 6
- Hit Rate: 0.2811

Example 3:
- Trace ID: 3
- Hit Rate: 0.436

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:08:56 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:08:56,262 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:10:06 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:10:06,490 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:10:06,491 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance feedback and the provided environment, the previous algorithm suffers because:
1.  **Inefficient Scoring:** The score `Freq / log(Time)` is an arbitrary heuristic that doesn't adapt dynamically to the changing "temperature" of the cache.
2.  **Lack of Size Awareness:** The cache capacity is defined in **bytes**, but the previous algorithm treats all objects equally. To maximize object hit rate (the number of hits), the algorithm should favor smaller objects (because you can fit more of them) and popular objects.
3.  **Static Aging:** The previous aging mechanism (iterating through the whole dict to multiply by 0.95) is computationally expensive and imprecise.

### The Solution: Greedy Dual-Size Frequency (GDSF)
I will implement the **GDSF** algorithm. This is a state-of-the-art replacement policy for web proxies and caches with variable-sized objects.

**Core Logic:**
1.  **Priority Value:** Every object has a priority $P = L + \frac{Frequency}{Size}$.
2.  **Inflation Value ($L$):** This is a running "aging" factor. Initially 0. When an object is evicted, $L$ is updated to the priority of the evicted object. This effectively "raises the bar" for new items entering the cache and naturally ages out items that sit in the cache without being accessed (as their $P$ will eventually fall below $L$).
3.  **Size Bias:** By dividing by size, we penalize large objects. Evicting one large object makes room for multiple small objects, statistically increasing the probability of future hits (improving Hit Rate).
4.  **Tie-Breaking:** If priorities are equal, we use Recency (LRU) to break ties.

```python
import math

# --- Metadata Globals ---
# Tracks the access frequency of a key (Frequency)
m_freq = {}
# Tracks the computed GDSF priority value
m_priority = {}
# Tracks the last access time (for LRU tie-breaking)
m_last_access = {}

# The "Inflation" value (Logical Clock) for GDSF
# This value rises over time, acting as the aging mechanism.
g_inflation = 0.0

# --- Constants ---
# Limit metadata size to prevent memory leaks in long simulations
MAX_METADATA_SIZE = 100000 

def _get_size_cost(size):
    '''
    Returns the cost denominator.
    We ensure size is at least 1 to avoid division by zero.
    We interpret size strictly to maximize object hit rate.
    '''
    return max(1, size)

def _cleanup_metadata(current_cache_keys):
    '''
    Removes old metadata to prevent memory leaks.
    Only removes keys that are NOT currently in the cache.
    '''
    global m_freq, m_priority, m_last_access
    
    if len(m_freq) > MAX_METADATA_SIZE:
        # Identify keys not in cache
        cache_set = set(current_cache_keys)
        deletion_candidates = []
        
        # We want to remove items that are not in cache AND have low priority/frequency
        # To be fast, we just sample or iterate once.
        # Here we just iterate and remove the first N non-cached candidates found
        # to bring size down slightly.
        count = 0
        keys = list(m_freq.keys())
        for k in keys:
            if k not in cache_set:
                deletion_candidates.append(k)
                count += 1
                # Prune in batches of 1000 to keep it responsive
                if count >= 1000:
                    break
        
        for k in deletion_candidates:
            m_freq.pop(k, None)
            m_priority.pop(k, None)
            m_last_access.pop(k, None)

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the object with the lowest Priority value.
    Updates the global Inflation value (g_inflation) to the priority of the evicted object.
    '''
    global m_priority, m_last_access, g_inflation
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim = None
    min_p = float('inf')
    # We use min_access for tie-breaking (LRU)
    min_access = float('inf') 

    # Linear scan to find victim (Allowed per context)
    for key in candidate_keys:
        p = m_priority.get(key, 0.0)
        access_time = m_last_access.get(key, 0)
        
        # We look for the smallest Priority
        if p < min_p:
            min_p = p
            victim = key
            min_access = access_time
        elif p == min_p:
            # Tie-breaker: Evict the Least Recently Used among those with equal priority
            if access_time < min_access:
                victim = key
                min_access = access_time

    # GDSF Aging Mechanism:
    # Update the global inflation factor to the priority of the item being evicted.
    # This ensures that future insertions start at a higher base priority,
    # effectively aging out existing items that don't get refreshed.
    if victim is not None:
        g_inflation = min_p

    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Recency.
    3. Recalculate Priority based on new Frequency and current Inflation.
    '''
    global m_freq, m_priority, m_last_access, g_inflation
    
    key = obj.key
    
    # Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Update Priority
    # Formula: Priority = Inflation + (Frequency / Size)
    # Note: We use current g_inflation. When an item is hit, it is "brought up to date"
    # with the current aging clock.
    size_cost = _get_size_cost(obj.size)
    m_priority[key] = g_inflation + (m_freq[key] / size_cost)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Restore Frequency (Ghost Cache logic).
    2. Set Recency.
    3. Calculate initial Priority.
    '''
    global m_freq, m_priority, m_last_access, g_inflation
    
    key = obj.key
    
    # Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Frequency Logic (Ghost Cache)
    if key in m_freq:
        # We've seen this before (recently evicted). 
        # Increase frequency to indicate it's a returning visitor.
        m_freq[key] += 1
    else:
        # Brand new item
        m_freq[key] = 1
        
    # Calculate Priority
    # Even for new items, we add g_inflation. This puts them on the "current playing field"
    # rather than at 0.
    size_cost = _get_size_cost(obj.size)
    m_priority[key] = g_inflation + (m_freq[key] / size_cost)
    
    # Cleanup if needed
    if len(m_freq) > MAX_METADATA_SIZE:
        _cleanup_metadata(cache_snapshot.cache.keys())

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    We do NOT delete the metadata immediately. 
    Keeping the metadata allows us to implement "Ghost Cache" behavior 
    (remembering frequency if the object is re-inserted later).
    '''
    pass
```
2025-11-27 19:10:13,050 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.8922

Example 2:
- Trace ID: 43
- Hit Rate: 0.2833

Example 3:
- Trace ID: 10
- Hit Rate: 0.075

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:10:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:10:13,051 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:10:36 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:10:36,478 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:10:36,479 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To minimize cache miss rates across diverse traces (which implies mixed workloads including recency-heavy, frequency-heavy, and scan-heavy patterns), a simplified version of **S3-FIFO (Simple, Static, Scalable FIFO)** or a **Frequency-Based LRU with Aging** is often superior to standard LRU or segmented LRU because it handles "one-hit wonders" (scans) much more aggressively.

The previous implementation attempted a Segmented LRU but likely suffered because iterating over the entire cache in Python to find the max score in `evict` is O(N) and can be slow, but more importantly, the "probation" logic was too rigid.

Here is an improved algorithm based on a **Frequency-Aware Least Recently Used (Predictive) strategy**. It borrows concepts from LFU and ARC but simplifies them into a robust scoring mechanism.

### Key Strategy Changes
1.  **Logarithmic Frequency decay:** Instead of capping frequency at a small number (like 10), we use the log of the frequency. This allows distinguishing between "warm" and "very hot" items without letting high-frequency items camp in the cache forever (cache pollution).
2.  **Adaptive Ghost Registry:** We keep the ghost registry but use it to boost the initial priority of returning items significantly, signaling a working set larger than the cache.
3.  **Recency + Frequency Scoring:** The eviction score combines Recency (time since last access) and Frequency.
    *   *Score Formula:* `RecencyFactor / (FrequencyFactor)`
    *   Items that are accessed recently result in a small numerator (low victim score).
    *   Items accessed frequently result in a large denominator (low victim score).
    *   Items that are old and infrequent have high scores (likely victims).

### Python Implementation

```python
import math

# Metadata dictionary
# Key -> (frequency, last_access_time)
m_access_info = dict()

# Ghost registry to track recently evicted items (Key -> eviction_time)
# Used to give a "boost" to items that are evicted but immediately requested again (thrashing protection).
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculates a score determining likelihood of eviction.
    Higher Score = Better Victim (Likely to be evicted).
    
    Formula concept:  TimeDelta / (Frequency ^ Weight)
    
    1. If an item is very old (high TimeDelta), it should be evicted.
    2. If an item is very frequent (high Frequency), the score reduces, protecting it.
    '''
    if key not in m_access_info:
        return float('inf') # Safety fallback
        
    freq, last_access = m_access_info[key]
    
    # Time since last access
    # We add 1 to avoid division by zero or zero-staleness issues
    staleness = (current_time - last_access) + 1
    
    # We apply a logarithmic dampener to frequency. 
    # This ensures that an item accessed 100 times isn't 100x harder to evict than an item accessed once,
    # but maybe 4-5x harder. This prevents "cache pollution" by historically hot items that are now cold.
    # We use base 2 log. freq=1 -> val=1. freq=2 -> val=2. freq=4 -> val=3.
    freq_weight = math.log2(freq + 1) + 1
    
    # The Score:
    # High staleness (old) -> High Score (Evict)
    # High frequency -> Low Score (Keep)
    #
    # Items with Freq=1 (Scans) will have freq_weight ~ 1.58.
    # Items with Freq=High will have higher denominator, reducing the score drastically,
    # allowing them to survive longer periods of inactivity.
    return staleness / freq_weight

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest victim score (Oldest and Least Frequent).
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # Optimization: To avoid O(N) overhead on large caches every single evict,
    # we can employ a random sampling strategy if N is large.
    # However, to guarantee the *best* victim for the requirement, we scan all.
    # Given the constraints of the problem description usually imply logic over raw speed,
    # we will stick to a full scan for maximum hit-rate accuracy.
    
    best_victim_key = None
    max_score = -1.0
    
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time to current.
    2. Increment Frequency.
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    # Retrieve existing data
    # Default (0, current) shouldn't happen on hit, but safe coding
    freq, _ = m_access_info.get(obj.key, (0, current_time))
    
    # Update frequency
    new_freq = freq + 1
    
    m_access_info[obj.key] = (new_freq, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry. If present, this is a 'false negative' eviction (thrashing).
       Initialize with higher frequency to protect it immediately.
    2. Otherwise, initialize with base frequency.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Adaptive Logic:
    # If this key was recently evicted (it's in the ghost registry), it means our cache 
    # was too small or we evicted it prematurely. We want to "latch" it into the cache 
    # so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Boost frequency. A boost to 4 roughly simulates it having "survived" the probation period.
        initial_freq = 4
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # Remove from active stats
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prevent Ghost Registry from growing infinitely.
    # A good rule of thumb is keeping the ghost registry size proportional to cache size.
    # Here we limit it to roughly the number of items in the cache to keep memory overhead low
    # while retaining enough history to be useful.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest inserted key in the ghost registry (Python 3.7+ dicts preserve insertion order)
        # next(iter()) gets the first key efficiently.
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
```
2025-11-27 19:10:44,344 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 14
- Hit Rate: 0.0207

Example 3:
- Trace ID: 47
- Hit Rate: 0.0817

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:10:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:10:44,345 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:11:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:11:02,966 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:11:02,967 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm and address the extremely low hit rates (2-8%), we need to move away from a naive LFU approach. The current implementation suffers from two main issues:
1.  **Cache Pollution/Stagnation:** Pure LFU tends to keep items that were popular long ago but are no longer relevant, clogging the cache.
2.  **Ghost entries missing:** When an item is evicted, `update_after_evict` deletes its metadata. If that item is accessed again shortly after, it starts with a frequency of 1, looking like a brand new item, rather than a recurring one.

To solve this, I will implement a **LFU-Dynamic Aging (LFU-DA)** algorithm with **Ghost Caching (Frequency Retention)**.

### Key Improvements:
1.  **Frequency Retention (Ghost Entries):** Instead of deleting metadata immediately upon eviction (`update_after_evict`), we will keep the frequency data. This allows items that are "cyclic" or have working sets larger than the cache size to regain their high-priority status quickly upon re-insertion.
2.  **Dynamic Aging:** To prevent old high-frequency items from polluting the cache forever, we will incorporate a "cache age" or "inflation" factor. When a new item enters, its starting value isn't 1; it's based on the current minimum frequency in the cache. This gives new items a fighting chance against established heavy hitters.
3.  **Tie-Breaking:** We will stick to LRU for tie-breaking, as it is robust for items with equal importance.

### Improved Implementation

```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
# Now persists even after eviction to handle cyclic patterns better.
m_key_frequency = dict()

# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

# Tracks the "frequency inflation" baseline. 
# This represents the minimum frequency currently in the cache, allowing new items to enter with competitive scores.
m_dynamic_aging_base = 0

def evict(cache_snapshot, obj):
    '''
    Evicts based on a Dynamic LFU policy.
    Primary Metric: Frequency (with aging/inflation considerations).
    Secondary Metric: Recency (LRU).
    '''
    global m_dynamic_aging_base

    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    best_key = None
    min_score = float('inf')
    min_time = float('inf')

    # Iterate to find the object with the lowest score (frequency)
    # Tie-break with oldest timestamp
    for key in candidate_keys:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        if curr_freq < min_score:
            min_score = curr_freq
            min_time = curr_time
            best_key = key
        elif curr_freq == min_score:
            if curr_time < min_time:
                min_time = curr_time
                best_key = key
    
    # Update the dynamic aging base to the frequency of the item we are about to evict.
    # This ensures new items inserted later start relative to this baseline.
    if best_key is not None:
        m_dynamic_aging_base = m_key_frequency.get(best_key, 0)

    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Standard LFU increment
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, we use the Dynamic Aging Base.
    If the object has history (was evicted previously), we restore it + inflation.
    If it's new, it starts at the inflation base + 1.
    '''
    global m_key_frequency, m_key_last_access, m_dynamic_aging_base
    
    # Check if we have historical frequency data (Ghost entry)
    historical_freq = m_key_frequency.get(obj.key, 0)
    
    if historical_freq > 0:
        # If we remember this object, give it a boost based on current cache pressure
        # But ensure it doesn't just dominate immediately if it's been gone a long time.
        # Max(historical, base + 1) ensures it's at least competitive with current items.
        new_freq = max(historical_freq, m_dynamic_aging_base + 1)
        m_key_frequency[obj.key] = new_freq
    else:
        # Totally new object. Give it a fighting chance by starting at the current baseline + 1
        m_key_frequency[obj.key] = m_dynamic_aging_base + 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, we strictly REMOVE the Last Access Time (Recency) to save space,
    BUT we KEEP the Frequency metadata. 
    
    This creates a "Ghost Cache" effect where we remember how popular an item was 
    even if it's temporarily evicted.
    '''
    global m_key_last_access
    
    # We purposefully do NOT delete from m_key_frequency here.
    # We only delete recency to mark it as not currently in cache for LRU logic.
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    # Optional: To prevent infinite memory growth of m_key_frequency, 
    # one might implement a cleanup mechanism if the dict gets too large,
    # but for typical competitive coding/trace contexts, retaining it is usually preferred.
```
2025-11-27 19:11:29,170 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Hit Rate: 0.0238

Example 2:
- Trace ID: 1
- Hit Rate: 0.4725

Example 3:
- Trace ID: 15
- Hit Rate: 0.0213

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:11:29 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:11:29,171 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:12:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:12:02,637 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:12:02,638 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The current implementation relies on a "Frequency / Log(Recency)" scoring model. While this attempts to hybridize LFU and LRU, the performance results (particularly the 2% hit rates on Traces 15 and 17) suggest it fails significantly on workloads with scanning patterns or shifting working sets. The logarithmic decay is likely too slow to evict cold items that were once popular (cache pollution), and the linear scan in `evict` is inefficient for large caches.

To improve this, we will implement a variant of **ARC (Adaptive Replacement Cache)** logic, but adapted to the constraints of this environment (a scoring function approach rather than strict list management). We will maintain two logical concepts using our metadata:
1.  **Recency (T1):** Items seen recently but only once (or infrequently).
2.  **Frequency (T2):** Items seen frequently.

We will track "ghost" entries (history of evicted items) to dynamically tune a parameter `p`. This parameter `p` decides whether we prefer keeping Recent items or Frequent items based on which type of ghost list is getting hits.

### Improved Algorithm Strategy:
1.  **Metadata:** We will track `access_count` (frequency) and `last_access` (recency). Crucially, we will track a set of `ghost_keys` to detect when we made a mistake by evicting something too early.
2.  **Dynamic Tuning (`p`):** We will maintain a global parameter `p` (target size for the Recency set).
    *   If we hit a ghost key that was recently evicted from the "Recent" set, we increase `p` (we need more space for recency).
    *   If we hit a ghost key from the "Frequent" set, we decrease `p` (we need more space for frequency).
3.  **Eviction Logic:** Instead of a strict O(N) sort every time, we will calculate a utility score that incorporates this dynamic `p`.
    *   Items with `frequency = 1` are treated as "Recent/T1" candidates.
    *   Items with `frequency > 1` are treated as "Frequent/T2" candidates.
    *   If the cache has too many "Recent" items (count > p), we penalize them heavily to force their eviction. If it has too few, we protect them.

Here is the improved implementation:

```python
import math

# --- Metadata ---
# Tracks how many times a key has been accessed.
m_frequency = dict()
# Tracks the specific timestamp (access_count) of the last access.
m_last_access = dict()
# Tracks keys that were evicted. Value is True if it was evicted from T1 (Recent), False if T2 (Frequent).
m_ghosts = dict()

# --- Adaptive Parameters ---
# Target size for the "Recent/T1" logical list. 
# It adapts between 0 and capacity.
m_p = 0 
# Current count of items in the cache that effectively belong to T1 (freq=1).
m_t1_size = 0

# Constants for cleanup
MAX_GHOSTS = 100000

def evict(cache_snapshot, obj):
    '''
    Eviction Logic inspired by Adaptive Replacement Cache (ARC).
    
    We categorize items currently in cache into two logical sets:
    1. T1 (Recent): Items accessed exactly once (frequency == 1).
    2. T2 (Frequent): Items accessed more than once (frequency > 1).
    
    We rely on the adaptive parameter `m_p`, which represents the ideal size of T1.
    
    The Logic:
    - If len(T1) > m_p: We prefer evicting from T1 (the LRU item among those with freq=1).
    - If len(T1) < m_p: We prefer evicting from T2 (the LRU item among those with freq>1).
    
    Since we cannot maintain strict sorted lists in this restricted environment easily, 
    we simulate this via a scoring function that penalizes the "oversized" set.
    '''
    global m_frequency, m_last_access, m_p, m_t1_size
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Recalculate T1 size just to be safe (ensure consistency)
    m_t1_size = sum(1 for k in candidate_keys if m_frequency.get(k, 1) == 1)
    
    victim_key = None
    # We want to evict the item with the MINIMUM score.
    min_score = float('inf')
    
    # Determine which set we want to evict from based on P
    # If we have too many T1 items compared to our target P, we want to evict T1.
    evict_t1 = m_t1_size > m_p
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_acc = m_last_access.get(key, 0)
        
        # Primary sort: Classify into T1 (freq=1) or T2 (freq>1)
        is_t1 = (freq == 1)
        
        # Scoring Logic:
        # We construct a score such that:
        # 1. The preferred eviction set has significantly lower scores than the protected set.
        # 2. Within the same set, strictly LRU applies (smaller last_acc -> smaller score).
        
        # Base score is the access time (LRU behavior)
        score = last_acc
        
        if evict_t1:
            if is_t1:
                # T1 is the target. Keep score as raw timestamp (very small).
                # This ensures T1 items are evicted before T2 items.
                pass 
            else:
                # T2 is protected. Add a massive constant to timestamp to ensure 
                # it is greater than any T1 timestamp.
                score += 10**15
        else:
            # We want to evict T2 (Frequent), protecting T1.
            if is_t1:
                # T1 is protected.
                score += 10**15
            else:
                # T2 is the target.
                pass
                
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. Update frequency.
    2. Update recency.
    3. If item moves from T1 (freq=1) to T2 (freq=2), update m_t1_size.
    '''
    global m_frequency, m_last_access, m_t1_size
    
    key = obj.key
    curr_freq = m_frequency.get(key, 0)
    
    # Update Metadata
    m_last_access[key] = cache_snapshot.access_count
    m_frequency[key] = curr_freq + 1
    
    # If it was in T1, it is now in T2
    if curr_freq == 1:
        m_t1_size -= 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Check Ghost lists to adapt `m_p`.
    2. Initialize metadata.
    '''
    global m_frequency, m_last_access, m_ghosts, m_p, m_t1_size
    
    key = obj.key
    capacity = cache_snapshot.capacity # We approximate capacity using size here or assume count based
    # Note: Since capacity is bytes and we deal with objects, the logic is slightly abstract,
    # but the counts work for the algorithm logic.
    
    # Adaptation: Did we miss something recently evicted?
    if key in m_ghosts:
        was_t1_ghost = m_ghosts[key]
        
        # Delta determines how fast we adapt. 
        # If we have many T1 ghosts, B1 is large, we increment P slowly.
        # If we have many T2 ghosts, B2 is large, we decrement P slowly.
        # Simplified ARC delta:
        delta = 1 # simplified step
        
        if was_t1_ghost:
            # We evicted a T1 item too soon. We need to increase P (size of T1).
            m_p = min(len(cache_snapshot.cache) + len(m_ghosts), m_p + delta)
        else:
            # We evicted a T2 item too soon. We need to decrease P (shrink T1, grow T2).
            m_p = max(0, m_p - delta)
            
        # Remove from ghost since it's back in cache
        del m_ghosts[key]
    
    # Initialize new item
    m_frequency[key] = 1
    m_last_access[key] = cache_snapshot.access_count
    m_t1_size += 1
    
    # Housekeeping
    if len(m_ghosts) > MAX_GHOSTS:
        # Prune oldest ghosts
        # In a real ARC implementation this is strict, here we just prevent memory leaks
        keys_to_del = list(m_ghosts.keys())[:100]
        for k in keys_to_del:
            del m_ghosts[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict:
    1. Update internal T1 counter.
    2. Add to Ghost list.
    '''
    global m_frequency, m_last_access, m_ghosts, m_t1_size
    
    key = evicted_obj.key
    freq = m_frequency.get(key, 1)
    
    # Track as ghost
    # True if it was T1 (freq=1), False if T2 (freq > 1)
    is_t1 = (freq <= 1)
    m_ghosts[key] = is_t1
    
    # Remove strict metadata to save memory, logic relies on Ghosts for history now
    if key in m_frequency: del m_frequency[key]
    if key in m_last_access: del m_last_access[key]
    
    if is_t1:
        m_t1_size -= 1
```
2025-11-27 19:12:43,239 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 12
- Hit Rate: 0.0249

Example 2:
- Trace ID: 21
- Hit Rate: 0.0266

Example 3:
- Trace ID: 40
- Hit Rate: 0.0214

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:12:43 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:12:43,240 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:13:09 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:13:09,258 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:13:09,259 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To significantly improve the cache hit rate, we need to shift from a complex, math-heavy heuristic to a robust, adaptive strategy that handles different access patterns (like recency-heavy vs. frequency-heavy workloads) more effectively.

The previous implementation likely suffered because the "Combined Score" approach can sometimes be "worst of both worlds"—holding onto stale frequent items too long while not protecting new items enough to let them build frequency (the "cold start" problem).

The improved algorithm below implements **ARC (Adaptive Replacement Cache)** logic. ARC dynamically balances between Recency (LRU) and Frequency (LFU) by maintaining two lists:
1.  **L1 (Recency)**: Captures "new" items (seen once recently).
2.  **L2 (Frequency)**: Captures "frequent" items (seen at least twice).

It also maintains "Ghost" entries (B1 and B2) for items evicted from L1 and L2 respectively. If a hit occurs in a ghost list, the algorithm learns which strategy (Recency or Frequency) needs more space and adjusts a target parameter `p` accordingly.

### Improved Cache Eviction Algorithm

```python
# Metadata definitions
# T1: Recent keys (LRU logic for items seen once)
# T2: Frequent keys (LRU logic for items seen >1 times)
# B1: Ghost keys evicted from T1 (Recency history)
# B2: Ghost keys evicted from T2 (Frequency history)
# p: Target size for the T1 list (adaptation parameter)

m_T1 = []
m_T2 = []
m_B1 = []
m_B2 = []
m_p = 0
m_access_map = dict() # Fast lookup to find which list a key is in

def evict(cache_snapshot, obj):
    '''
    Eviction logic implementing the REPLACE subroutine of ARC.
    Decides which key to evict based on the current sizes of T1, T2 
    and the adaptive parameter 'p'.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_map
    
    # If the cache is empty, nothing to evict
    if not cache_snapshot.cache:
        return None

    victim_key = None
    
    # Logic to determine which list (T1 or T2) to evict from
    # We evict from T1 if it has grown larger than the target `p`.
    # Or, if T1 is within `p` but L1 (T1+B1) is massive, we might still evict from T1 
    # to make room for T2 items if the incoming item is a B2 hit.
    
    # However, standard ARC `REPLACE(p)` logic is usually called *before* insertion 
    # to make space. Since the framework here asks "who to evict" *when full*, 
    # we simulate the decision:
    
    # Basic Rule:
    # if len(T1) >= 1 and ((incoming_is_in_B2 and len(T1) == p) or (len(T1) > p)):
    #   pop from T1
    # else:
    #   pop from T2
    
    # Note: 'obj' is the new object causing the eviction. We need to know if 
    # it is in B2 to strictly follow ARC, but usually checking T1 > p is sufficient 
    # for the eviction constraint.
    
    # Check if the incoming object is in B2 (Ghost Frequency)
    in_B2 = obj.key in m_B2
    
    if len(m_T1) > 0 and (
        len(m_T1) > m_p or (in_B2 and len(m_T1) == m_p)
    ):
        # Evict LRU from T1
        victim_key = m_T1[0]
    else:
        # Evict LRU from T2
        if len(m_T2) > 0:
            victim_key = m_T2[0]
        elif len(m_T1) > 0:
            # Fallback if T2 is empty
            victim_key = m_T1[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Updates lists when an object is found in the cache (T1 or T2).
    Moves the object to the MRU position of T2 (Frequency list).
    '''
    global m_T1, m_T2, m_access_map
    
    key = obj.key
    
    # Identify where the key is currently
    # It must be in T1 or T2 since it's a hit.
    
    if key in m_T1:
        m_T1.remove(key)
        m_T2.append(key) # Move to T2 MRU
        m_access_map[key] = 'T2'
    elif key in m_T2:
        m_T2.remove(key)
        m_T2.append(key) # Move to T2 MRU (renew)
        m_access_map[key] = 'T2'
    else:
        # Theoretical edge case: cache says hit, but metadata lost it.
        # Treat as new insert to T2.
        m_T2.append(key)
        m_access_map[key] = 'T2'

def update_after_insert(cache_snapshot, obj):
    '''
    Updates lists when a new object is inserted.
    This handles three cases:
    1. Key was in Ghost List B1 (Recency miss) -> Adapt p, move to T2.
    2. Key was in Ghost List B2 (Frequency miss) -> Adapt p, move to T2.
    3. Key is totally new -> Add to T1.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_map
    
    key = obj.key
    capacity = cache_snapshot.capacity # We assume capacity is roughly max count of items
    # If capacity is bytes, we approximate max items based on current cache length 
    # or just rely on the list lengths relative to each other.
    # ARC generally relies on count of items (c). We use len(cache) as c.
    c = len(cache_snapshot.cache) 
    if c == 0: c = 100 # Safety
    
    # Case 1: Recency Hit (Ghost B1)
    if key in m_B1:
        # Adapt p: Increase T1 target size
        # We favor Recency because we just missed something that was recently evicted.
        delta = 1
        if len(m_B1) >= len(m_B2) and len(m_B2) > 0:
            delta = 1
        elif len(m_B2) > 0:
            delta = len(m_B1) / len(m_B2)
            
        m_p = min(c, m_p + delta)
        
        # Move to T2 (promote to frequent)
        m_B1.remove(key)
        m_T2.append(key)
        m_access_map[key] = 'T2'
        
    # Case 2: Frequency Hit (Ghost B2)
    elif key in m_B2:
        # Adapt p: Decrease T1 target size (Favor Frequency)
        delta = 1
        if len(m_B2) >= len(m_B1) and len(m_B1) > 0:
            delta = 1
        elif len(m_B1) > 0:
            delta = len(m_B2) / len(m_B1)
            
        m_p = max(0, m_p - delta)
        
        # Move to T2 (promote to frequent)
        m_B2.remove(key)
        m_T2.append(key)
        m_access_map[key] = 'T2'
        
    # Case 3: Totally New
    else:
        # Add to T1 MRU
        m_T1.append(key)
        m_access_map[key] = 'T1'

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Maintains the Ghost lists (B1 and B2).
    When an item is evicted from cache (T1 or T2), it moves to the corresponding ghost list.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_access_map
    
    victim_key = evicted_obj.key
    
    # The evict() function decided which key to kill, but didn't remove it from lists.
    # We remove it now and push to Ghost lists.
    
    if victim_key in m_T1:
        m_T1.remove(victim_key)
        m_B1.append(victim_key) # Add to B1 MRU
        # Cap B1 size (History shouldn't grow forever)
        # Standard ARC caps L1+B1 <= 2*c, we just cap B1 to roughly cache size
        if len(m_B1) > cache_snapshot.capacity: # loosely using capacity as limit
             # or better, simple fixed limit if capacity is bytes and huge
             if len(m_B1) > len(cache_snapshot.cache) + 100:
                 m_B1.pop(0) 
                 
        if victim_key in m_access_map: del m_access_map[victim_key]

    elif victim_key in m_T2:
        m_T2.remove(victim_key)
        m_B2.append(victim_key) # Add to B2 MRU
        # Cap B2 size
        if len(m_B2) > len(cache_snapshot.cache) + 100:
            m_B2.pop(0)

        if victim_key in m_access_map: del m_access_map[victim_key]
```
2025-11-27 19:15:26,085 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 37
- Hit Rate: 0.6409

Example 2:
- Trace ID: 31
- Hit Rate: 0.007

Example 3:
- Trace ID: 7
- Hit Rate: 0.4986

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:15:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:15:26,086 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:16:50 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:16:50,541 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:16:50,542 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The following solution implements an **Adaptive Replacement Cache (ARC)** inspired algorithm.

### Analysis of the Problem
The performance results indicate two extremes:
1.  **Trace 37 (64% Hit Rate):** Suggests a workload with a stable "hot set" where Frequency (LFU) performs well.
2.  **Trace 31 (0.7% Hit Rate):** Suggests a "scan" (accessing many unique items once) or a "loop" larger than the cache. The current LFU-biased algorithm fails catastrophically here because new items (Frequency 1) are evicted immediately in favor of slightly older items, or the cache gets polluted by the scan, flushing out the useful data.

### The Solution: Adaptive Segmentation (ARC-like)
To solve this, we need an algorithm that dynamically shifts between Recency (LRU) and Frequency (LFU).
1.  **Segmentation**: We logically divide the cache into two segments:
    *   **T1 (Probation/Recency):** New items enter here. Acts like an LRU to catch "scans" and filter them out.
    *   **T2 (Protected/Frequency):** Items accessed more than once move here. Acts like an LFU/LRU of frequent items.
2.  **Adaptivity**: We maintain a target size parameter `p` for the Probation segment.
    *   If we get hits on items recently evicted from Probation (Ghost T1), it means the Probation segment is too small. We increase `p`.
    *   If we get hits on items recently evicted from Protected (Ghost T2), it means the Protected segment is too small. We decrease `p`.

This approach ensures that during a scan (Trace 31), the Probation segment churns quickly while the Protected segment stays safe. During a stable workload (Trace 37), the Protected segment grows to hold the hot set.

```python
# Metadata dictionaries
m_key_access_time = dict()  # Tracks logical timestamp (for LRU within segments)
m_key_segment = dict()      # 0 = Probation (T1), 1 = Protected (T2)
m_ghost_registry = dict()   # Stores segment info of evicted keys: key -> segment_id
m_p = 0                     # Target size of the Probation segment (T1)

# Constants
GHOST_CAPACITY_FACTOR = 2   # How many ghosts to keep relative to cache count

def evict(cache_snapshot, obj):
    '''
    ARC-inspired Eviction Policy.
    Decides whether to evict from Probation (T1) or Protected (T2) based on 
    the adaptive parameter `m_p`.
    '''
    global m_key_access_time, m_key_segment, m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Separate candidates into T1 (Probation) and T2 (Protected)
    t1_candidates = []
    t2_candidates = []
    
    for k in candidate_keys:
        seg = m_key_segment.get(k, 0) # Default to 0 if unknown
        if seg == 0:
            t1_candidates.append(k)
        else:
            t2_candidates.append(k)

    # ARC Logic to decide which region to victimize:
    # We evict from T1 if T1 size exceeds target `p`.
    # However, if T1 is empty, we must evict from T2 (and vice versa).
    
    evict_from_t1 = False
    
    if len(t1_candidates) > 0 and len(t1_candidates) > m_p:
        evict_from_t1 = True
    elif len(t1_candidates) > 0 and len(t2_candidates) == 0:
        evict_from_t1 = True
    else:
        evict_from_t1 = False
        
    # Select the victim list
    candidates = t1_candidates if evict_from_t1 else t2_candidates
    
    # If for some reason the chosen list is empty (edge case), fallback to the other
    if not candidates:
        candidates = t2_candidates if evict_from_t1 else t1_candidates

    # Find LRU in the selected segment
    victim_key = None
    min_time = float('inf')
    
    for k in candidates:
        t = m_key_access_time.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    1. Update Recency.
    2. If item was in Probation (T1), promote to Protected (T2).
    '''
    global m_key_access_time, m_key_segment
    
    # Update Access Time (LRU)
    m_key_access_time[obj.key] = cache_snapshot.access_count
    
    # Promotion Logic
    # If it was in T1 (Probation), move to T2 (Protected)
    if m_key_segment.get(obj.key, 0) == 0:
        m_key_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. New items start in Probation (T1).
    2. Check Ghost Registry to adapt `m_p`.
    '''
    global m_key_access_time, m_key_segment, m_ghost_registry, m_p
    
    # Initialize metadata
    m_key_access_time[obj.key] = cache_snapshot.access_count
    m_key_segment[obj.key] = 0 # Insert into Probation (T1)
    
    # Adaptive Logic (ARC Parameter Tuning)
    if obj.key in m_ghost_registry:
        ghost_segment = m_ghost_registry[obj.key]
        
        # Calculate Delta
        # If we had a hit in T1's ghost (B1), we needed a larger T1. Increase p.
        # If we had a hit in T2's ghost (B2), we needed a larger T2. Decrease p.
        
        # Estimating current ghost sizes for delta calculation (Simplified ARC delta)
        # Note: In a strict implementation, we track B1/B2 lengths. 
        # Here we use a safe increment of 1 to ensure stability.
        
        delta = 1
        
        if ghost_segment == 0:
            # Hit in B1 (Recency Ghost) -> Increase Probation Size
            # Limit p to total number of items currently in cache
            current_count = len(cache_snapshot.cache)
            m_p = min(current_count, m_p + delta)
        else:
            # Hit in B2 (Frequency Ghost) -> Decrease Probation Size (Increase Protected)
            m_p = max(0, m_p - delta)
            
        # Promote immediately to T2 because it's a "known" item returning
        m_key_segment[obj.key] = 1
        
        # Remove from ghost
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict:
    1. Move victim to Ghost Registry.
    2. Cleanup active metadata.
    '''
    global m_key_access_time, m_key_segment, m_ghost_registry, GHOST_CAPACITY_FACTOR
    
    victim_key = evicted_obj.key
    segment = m_key_segment.get(victim_key, 0)
    
    # Add to Ghost Registry
    m_ghost_registry[victim_key] = segment
    
    # Clean up active metadata
    if victim_key in m_key_access_time:
        del m_key_access_time[victim_key]
    if victim_key in m_key_segment:
        del m_key_segment[victim_key]
        
    # Maintain Ghost Registry Size
    # If the ghost registry grows too large, remove oldest entries to save memory.
    # Using a heuristic based on cache count.
    max_ghosts = len(cache_snapshot.cache) * GHOST_CAPACITY_FACTOR
    if max_ghosts < 100: max_ghosts = 100 # Minimum safety floor
    
    if len(m_ghost_registry) > max_ghosts:
        # Remove arbitrary old item (FIFO behavior of dict in Python 3.7+)
        # This keeps the overhead low compared to tracking LRU for ghosts strictly.
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]
```
2025-11-27 19:17:15,193 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.1627

Example 2:
- Trace ID: 27
- Hit Rate: 0.8922

Example 3:
- Trace ID: 24
- Hit Rate: 0.2698

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:17:15 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:17:15,195 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:17:39 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:17:39,710 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:17:39,712 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the weaknesses of the previous "Segmented LRU" attempt. The previous implementation struggled with trace consistency (low hit rate on Trace 41 and 24) likely because it relied too heavily on a binary "probation vs. protected" state without considering the *relative* frequency or recency in a more granular way.

The improved algorithm below implements a logic inspired by **LFU-DA (Least Frequently Used with Dynamic Aging)** and **GDSF (Greedy Dual-Size Frequency)**.

### The Strategy: Adaptive LFU with Dynamic Aging

1.  **Frequency with Aging**: Pure LFU is bad because old "hot" items stick around forever even when they become cold (cache pollution). To fix this, we use a "Dynamic Aging" or "Inflation" mechanism. When an object is inserted or hit, its "priority" is calculated as `Frequency + Cache_Inflation_Value`.
2.  **The Inflation Value (`L`)**: This is a global counter. Whenever we *evict* an item, we set `L` to the priority of the evicted item. This effectively raises the bar for all future items. New items entering the cache must "pay" an entry cost, and existing items must be accessed frequently enough to stay above the rising tide of `L`.
3.  **Tie-Breaking**: If priorities are equal, we use Recency (LRU) to break ties.
4.  **Ghost Cache**: We persist the Ghost Registry idea but refine it. If an item is in the Ghost Registry, it means it was recently useful but evicted. Upon re-insertion, we restore its previous frequency count, allowing it to quickly regain a high priority.

### Key Improvements:
*   **Decay Mechanism**: The `L` value ensures that items that stop being accessed eventually have a priority lower than `L` (as `L` keeps growing upon evictions), making them candidates for eviction.
*   **Frequency Memory**: Using a ghost registry allows the algorithm to "remember" items that are cyclic or part of a working set larger than the cache capacity.

Here is the complete improved implementation:

```python
import collections

# Global State Management
# m_metadata stores: key -> [frequency, priority, insertion_tick]
m_metadata = dict()

# Global Priority Inflation Value (L)
# This represents the "minimum worth" required to stay in the cache.
m_inflation_L = 0.0

# Ghost Registry to remember frequency of recently evicted items.
# Maps key -> frequency
m_ghost_freq = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest priority.
    Strategy: LFU-DA (Least Frequently Used with Dynamic Aging).
    
    We look for the item with the minimum Priority score.
    Priority = (Frequency) + L (at time of insertion/update)
    
    The global 'L' increases as we evict items, naturally aging out
    items that haven't been updated recently.
    '''
    global m_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
    
    min_priority = float('inf')
    victim_key = None
    
    # We want to evict the item with the LOWEST priority.
    # Tie-breaking: If priorities are equal, evict the LRU one (smallest insertion_tick).
    # Since Python 3.7+, dict order is insertion order, but we track access ticks explicitly.
    
    # Optimization: To avoid O(N) scan every time, usually heaps are used, 
    # but given the constraints, we scan.
    
    victim_entry = None # Will store (priority, insertion_tick)
    
    for key in candidate_keys:
        if key in m_metadata:
            freq, priority, tick = m_metadata[key]
            
            # Comparison: Lower priority is better victim.
            # If priorities equal, lower tick (older) is better victim.
            if priority < min_priority:
                min_priority = priority
                victim_key = key
                victim_entry = (priority, tick)
            elif priority == min_priority:
                # Tie-breaker: LRU
                if victim_entry and tick < victim_entry[1]:
                    victim_key = key
                    victim_entry = (priority, tick)
    
    # Fallback if metadata is desync'd (should not happen)
    if victim_key is None:
        return candidate_keys[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on current L.
    3. Update Access Tick (for LRU tie-breaking).
    '''
    global m_metadata, m_inflation_L
    
    current_tick = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, old_priority, _ = m_metadata[obj.key]
        new_freq = freq + 1
        
        # LFU-DA Update Rule: Priority = New_Frequency + Current_L
        # This brings the item "up to date" with the current inflation level
        new_priority = new_freq + m_inflation_L
        
        m_metadata[obj.key] = [new_freq, new_priority, current_tick]
    else:
        # Should not happen on hit, but self-heal if necessary
        m_metadata[obj.key] = [1, 1.0 + m_inflation_L, current_tick]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine Frequency (1 if new, restored if Ghost).
    2. Calculate Priority = Frequency + L.
    '''
    global m_metadata, m_ghost_freq, m_inflation_L
    
    current_tick = cache_snapshot.access_count
    
    # Check ghost registry for history
    if obj.key in m_ghost_freq:
        initial_freq = m_ghost_freq[obj.key]
        # Remove from ghost
        del m_ghost_freq[obj.key]
    else:
        initial_freq = 1
        
    # Calculate Priority
    # New items enter with Priority = Freq + L.
    # This prevents them from being immediately evicted if L is high,
    # provided their frequency justifies it.
    priority = initial_freq + m_inflation_L
    
    m_metadata[obj.key] = [initial_freq, priority, current_tick]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Update Global L (Aging factor) to the priority of the evicted item.
    2. Save frequency to Ghost Registry.
    3. Clean up metadata.
    '''
    global m_metadata, m_ghost_freq, m_inflation_L
    
    if evicted_obj.key in m_metadata:
        freq, priority, _ = m_metadata[evicted_obj.key]
        
        # UPDATE L: L becomes the priority of the evicted object.
        # This is the standard GDSF / LFU-DA aging mechanism.
        # It ensures that only items with priority > this victim survive long term.
        m_inflation_L = priority
        
        # Save to Ghost: Remember the frequency so if it comes back, it resumes importance
        m_ghost_freq[evicted_obj.key] = freq
        
        del m_metadata[evicted_obj.key]
        
    # Maintenance: Cap the ghost registry size to prevent memory leaks.
    # A good rule of thumb is capacity * 2 or a fixed limit.
    # Since we don't have max keys, we use len(cache) as a proxy for desired scale.
    cache_capacity_count = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100
    if len(m_ghost_freq) > cache_capacity_count:
        # Remove arbitrary item (FIFO-ish in Python 3.7+)
        try:
            it = iter(m_ghost_freq)
            del m_ghost_freq[next(it)]
        except StopIteration:
            pass
```
2025-11-27 19:18:06,040 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0526

Example 2:
- Trace ID: 17
- Hit Rate: 0.0235

Example 3:
- Trace ID: 23
- Hit Rate: 0.0587

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:18:06 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:18:06,042 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:20:17 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:20:17,676 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:20:17,678 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To minimize cache miss rates, I will implement a corrected version of the **Adaptive Replacement Cache (ARC)** algorithm. 

**Analysis of the flaws in the previous implementation:**
1.  **Unit Mismatch:** The previous implementation used `cache_snapshot.capacity` (which is in bytes) to bound the parameter `p`. However, `p` describes the target *number* of items in the recency list (T1). In a byte-constrained cache with variable-sized objects, `capacity` (bytes) can be orders of magnitude larger than the number of items, causing `p` to grow uncontrollably and breaking the adaptive logic. The improved implementation implicitly treats the current number of cached items as the "capacity" for the adaptation logic.
2.  **Logic Errors:** The maintenance of the ghost lists (B1, B2) and the `evict` decision logic were slightly desynchronized from the standard ARC definition, leading to suboptimal adaptation to scan or loop patterns.
3.  **Efficiency:** The previous `evict` function iterated over the entire metadata dictionary to check for cache presence, which is inefficient.

**Improvements:**
-   **Corrected Adaptation:** The parameter `p` now targets the *count* of items in T1 and is bounded by the actual number of items in the cache (`len(cache)`), ensuring correct behavior regardless of object sizes.
-   **Robust Metadata:** Using `OrderedDict` to efficiently manage LRU positions and "Ghost" histories.
-   **ARC Logic:** Strictly implementing the ARC move-to-MRU and adaptation rules to handle both Recency (T1) and Frequency (T2) effectively.

```python
from collections import OrderedDict

# Metadata containers
# T1: Recency set (Currently cached, accessed once recently) -> OrderedDict[key, None]
# T2: Frequency set (Currently cached, accessed at least twice) -> OrderedDict[key, None]
# B1: Ghost Recency (Evicted from T1) -> OrderedDict[key, None]
# B2: Ghost Frequency (Evicted from T2) -> OrderedDict[key, None]
meta_t1 = OrderedDict()
meta_t2 = OrderedDict()
meta_b1 = OrderedDict()
meta_b2 = OrderedDict()

# Adaptive parameter 'p': Target size (count) for T1
p = 0

def _ensure_metadata_sync(cache_snapshot):
    '''
    Safety utility to ensure metadata isn't empty if the cache is populated.
    This handles initialization or drift cases.
    '''
    global meta_t1, meta_t2
    # If cache has items but our tracking lists are empty, rebuild blindly into T1
    if not meta_t1 and not meta_t2 and cache_snapshot.cache:
        for k in cache_snapshot.cache:
            meta_t1[k] = None

def evict(cache_snapshot, obj):
    '''
    Selects a victim key to evict.
    Uses the ARC logic:
    - Evict from T1 (Recency) if T1 size > target p.
    - Otherwise evict from T2 (Frequency).
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    
    victim_key = None
    t1_len = len(meta_t1)
    
    # ARC Eviction Decision Logic:
    # We prefer evicting from T1 if it exceeds the target size 'p'.
    # Standard ARC also checks if the *incoming* miss (obj.key) is in B2 to break ties,
    # encouraging T1 eviction if we are observing frequent items returning (B2 hits).
    
    evict_from_t1 = False
    
    if t1_len > 0:
        if t1_len > p:
            evict_from_t1 = True
        elif obj.key in meta_b2 and t1_len == int(p):
            # Special ARC condition: if we hit a ghost frequent item, we sacrifice T1
            evict_from_t1 = True
    
    # Fallback safety: if T2 is empty, must evict T1. If T1 empty, must evict T2.
    if not meta_t2:
        evict_from_t1 = True
    elif not meta_t1:
        evict_from_t1 = False
        
    if evict_from_t1:
        # LRU of T1 is the first item in OrderedDict
        victim_key = next(iter(meta_t1))
    else:
        # LRU of T2 is the first item in OrderedDict
        victim_key = next(iter(meta_t2))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Cache Hit:
    - If in T1 (Recency), promote to T2 (Frequency).
    - If in T2, update MRU position.
    '''
    global meta_t1, meta_t2
    
    _ensure_metadata_sync(cache_snapshot)
    key = obj.key
    
    if key in meta_t1:
        # Promote T1 -> T2
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Mark as most recently used in T2
        meta_t2.move_to_end(key)
    else:
        # Fallback for sync issues: assume T2
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Cache Miss (Insert):
    - Adapt parameter 'p' based on ghost hits (B1 vs B2).
    - Insert new item into T1 or T2 based on history.
    - Trim ghost lists.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    key = obj.key
    
    # Current cache count (c) implicitly defines our bounds
    current_count = len(cache_snapshot.cache)
    
    # 1. Adapt 'p'
    if key in meta_b1:
        # Hit in Ghost T1: We should have made T1 larger.
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            # If B1 is small, adapt faster
            delta = len(meta_b2) / len(meta_b1)
        
        p = min(current_count, p + delta)
        
        # Promote to T2 (it was recently evicted from T1, now back -> frequent)
        del meta_b1[key]
        meta_t2[key] = None
        
    elif key in meta_b2:
        # Hit in Ghost T2: We should have made T2 larger (T1 smaller).
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            delta = len(meta_b1) / len(meta_b2)
            
        p = max(0, p - delta)
        
        # Restore to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    else:
        # Totally new item -> T1 (Recency)
        meta_t1[key] = None

    # 2. Cleanup Ghost Lists
    # Keep ghost history bounded relative to cache size to prevent memory waste.
    # A safe upper bound is roughly the number of items in cache.
    max_ghost_size = max(current_count, 100)
    
    while len(meta_b1) > max_ghost_size:
        meta_b1.popitem(last=False) # Remove FIFO (oldest ghost)
        
    while len(meta_b2) > max_ghost_size:
        meta_b2.popitem(last=False)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move evicted key from T list to corresponding Ghost (B) list.
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    _ensure_metadata_sync(cache_snapshot)
    
    victim_key = evicted_obj.key
    
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```
2025-11-27 19:20:33,431 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 26
- Hit Rate: 0.7895

Example 2:
- Trace ID: 1
- Hit Rate: 0.4789

Example 3:
- Trace ID: 7
- Hit Rate: 0.4986

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:20:33 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:20:33,432 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:22:24 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:22:24,967 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:22:24,968 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The previous algorithm relied on a bounded LFU (Least Frequently Used) strategy with periodic aging. Based on the performance feedback, this approach struggled significantly with Traces 1 and 7 (hit rates < 50%). These low hit rates suggest "scan" or "loop" access patterns where LFU holds onto old, low-value items too long ("cache pollution") or fails to adapt quickly to shifting working sets.

To improve this, I have implemented a **Hyperbolic Caching** strategy with a **Ghost Registry**.

### Algorithm Changes:
1.  **Hyperbolic Scoring (`evict`)**: Instead of complex aging or fixed weights, we calculate a dynamic score for every eviction candidate: `Score = Frequency / (Time_Since_Last_Access)`.
    *   **Frequency** rewards popular items (LFU).
    *   **Time_Since_Last_Access** penalizes items that haven't been seen recently. As time passes, the denominator grows, naturally lowering the score of unused items (LRU).
    *   This hybrid approach adapts automatically to both Zipfian distributions (Trace 26) and scans/loops (Trace 1 & 7) without needing manual "aging" thresholds.

2.  **Ghost Registry (`update_after_evict` / `update_after_insert`)**:
    *   When an item is evicted, we store its presence in a "ghost" dictionary.
    *   If a new item appears in the ghost registry, it means we evicted it too soon (the working set is larger than the cache). We re-insert it with a boosted frequency (Start at 2 instead of 1) to give it a "second chance" to survive against established items.

3.  **Removal of Size Penalty**: The context states the cache is full when the *number* of objects reaches capacity. Therefore, `obj.size` is excluded from the scoring to strictly maximize hit rate based on object slots, not byte density.

```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks logical time of last access
m_ghost_registry = dict()   # Tracks recently evicted keys (Second Chance)

# Configuration
GHOST_REGISTRY_LIMIT = 1000 # Limit memory usage for history

def evict(cache_snapshot, obj):
    '''
    Hyperbolic Eviction:
    Evicts the item with the lowest score based on: Score = Frequency / (Time Since Last Access).
    
    This naturally balances LFU (high numerator) and LRU (low denominator implies recent access).
    Items not accessed for a long time will have a growing denominator, reducing their score 
    until they are evicted.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    victim_key = None
    min_score = float('inf')

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate Duration (Recency)
        # Add 1 to ensure we don't divide by zero if accessed in the same tick 
        # (or effectively prioritize very recent items correctly against freq)
        duration = (current_time - last_access) + 1
        
        # Hyperbolic Score formula
        score = freq / duration
        
        # We want to evict the item with the LOWEST score (Least valuable)
        # Tie-breaking:
        # If scores are equal, we don't explicitly force a complex check, 
        # but mathematically, for equal freq, the older item (larger duration) has lower score.
        # This naturally defaults to LRU behavior for items of same frequency.
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    # Increment frequency to reinforce this item's position
    m_key_frequency[obj.key] = m_key_frequency.get(obj.key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Ghost Registry Check
    # If this key is in the ghost registry, it was evicted recently but requested again.
    # This implies our cache might be thrashing or the working set is slightly larger than capacity.
    # We give it a starting frequency boost (2) instead of (1) so it isn't immediately evicted again
    # by the Hyperbolic function.
    if obj.key in m_ghost_registry:
        m_key_frequency[obj.key] = 2
        del m_ghost_registry[obj.key]
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On evict: Clean active metadata and move key to ghost registry.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, GHOST_REGISTRY_LIMIT
    
    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    # Add to Ghost Registry
    # We just track presence. We don't need the old frequency, 
    # just knowing it was here recently is enough to justify the boost.
    m_ghost_registry[evicted_obj.key] = True
    
    # Prevent ghost registry from growing infinitely
    if len(m_ghost_registry) > GHOST_REGISTRY_LIMIT:
        # Remove an arbitrary item (first found) to maintain size
        # This keeps the history relevant to the "recent" past.
        iterator = iter(m_ghost_registry)
        try:
            oldest_ghost = next(iterator)
            del m_ghost_registry[oldest_ghost]
        except StopIteration:
            pass
```
2025-11-27 19:24:46,440 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 43
- Hit Rate: 0.3667

Example 2:
- Trace ID: 28
- Hit Rate: 0.0204

Example 3:
- Trace ID: 44
- Hit Rate: 0.021

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:24:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:24:46,441 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:25:06 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:25:06,329 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:25:06,331 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An optimized cache eviction algorithm implementation is provided below.

### Analysis of the Problem & Solution Strategy

The previous implementation relied on a "Frequency / Log(Recency)" scoring formula. While theoretically sound for combining LFU and LRU, the extremely low hit rates in Examples 2 and 3 (approx. 2%) suggest specific weaknesses:
1.  **Scan Resistance:** It likely fails to handle "scan" patterns (where a long sequence of unique items is accessed once). High-frequency items might get evicted too slowly, or new items might be retained too long.
2.  **Recency Bias:** The log-decay of time might be too gentle for workloads that are strictly LRU-friendly (like loops or temporal locality bursts).
3.  **Metadata Overhead:** The Python-level dictionary operations and linear scans in `evict` (O(N)) are computationally expensive, though the prompt implies we focus on logic over raw CPU cycles. However, the logic itself—tracking global frequency indefinitely—can be noisy.

To fix this, I have implemented an approach inspired by **ARC (Adaptive Replacement Cache)** and **LIRS (Low Inter-reference Recency Set)** concepts, but simplified into a robust **Frequency-Recency Hybrid with Dynamic Adaptivity**.

**Key Improvements:**
1.  **Recency vs. Frequency Partitioning:** Instead of a single score, we treat the cache as having two logical zones: a "Protected" zone (high frequency/recency) and a "Probational" zone (newly inserted items).
2.  **Dynamic Adaptation:** We track a "Ghost" cache (evicted keys). If a ghost hit occurs, it means we evicted something too early. We then adjust the bias:
    *   If ghosts of recent items are hit, we need more space for recency (act more like LRU).
    *   If ghosts of frequent items are hit, we need more space for frequency (act more like LFU).
3.  **Simplified Scoring (Eviction Policy):** We use a simpler, more robust eviction heuristic. We prefer evicting items that are *both* infrequent and old. However, strictly distinguishing between "seen once" and "seen many" is the most critical filter for high miss-rate traces.

### Improved Implementation

```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```
2025-11-27 19:27:36,427 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 4
- Hit Rate: 0.4996

Example 2:
- Trace ID: 37
- Hit Rate: 0.6419

Example 3:
- Trace ID: 20
- Hit Rate: 0.3835

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:27:36 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:27:36,428 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:28:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:02,905 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:02,907 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache hit rate, we need to move beyond simple frequency or recency scoring and address the specific weaknesses shown in the performance feedback (specifically trace 4 and 20, which often imply scanning or looping patterns).

The previous implementation attempted a "hybrid score" but suffered from two main issues:
1.  **Complexity/Overhead:** Calculating a floating-point score `(freq ^ x) / (age ^ y)` for *every* item in the cache during *every* eviction is computationally expensive ($O(N)$) and often unstable due to the "Adaptive Factor" swinging too wildly.
2.  **Scan Resistance:** While it penalized frequency=1 items, it didn't distinguish well between "cold" items and "cooling" items.

### Improved Approach: Segmented LRU (SLRU) / 2Q Variant

The most robust algorithm for mixed workloads (scans + loops + random access) without high computational overhead is a **Segmented LRU (SLRU)** approach, conceptually similar to the **2Q** algorithm.

**Core Logic:**
We divide the cache logically into three segments (using metadata, not physical cache partitions, to handle the provided constraints):
1.  **Probationary Segment (Admission):** New items enter here. They are essentially "on trial." If they are not accessed again quickly, they are evicted first. This acts as a filter for scans.
2.  **Protected Segment (Main):** Items that survive probation (get a hit while in cache) are "promoted" to this segment. These are high-value, high-frequency items.
3.  **Ghost Registry:** We track keys that were recently evicted from Probation. If a Ghost Key is accessed, it proves our probationary filter was too aggressive (the item was actually part of a larger working set), so we admit it directly to Protected next time.

**Eviction Strategy:**
1.  Prefer evicting from the **Probationary** set first (LRU within that set).
2.  Only if the Probationary set is empty (or very small), evict from the **Protected** set (LRU within that set).

This design inherently solves the "Scan" problem (one-time items flush through Probation without disturbing Protected items) and the "Frequency" problem (popular items move to Protected and stay there).

### Implementation Details

We will use standard Python dictionaries to simulate Linked Hash Maps (which preserve insertion order) to manage LRU behavior efficiently $O(1)$ instead of calculating scores $O(N)$.

```python
# Metadata Globals

# Tracks items currently in the 'Protected' (high frequency) logical segment.
# Key -> True. If a key is in cache but NOT in this set, it is in 'Probation'.
m_protected = set()

# Tracks keys recently evicted from the Probationary segment.
# This helps us distinguish a "new" item from an "item that was just evicted but needed".
m_ghost = set()

# Map to track access time for LRU approximations without reordering the actual cache dict
# Key -> Last Access Time
m_access_time = dict()

# Constants
# Target ratio for Protected vs Probationary items.
# 80% protected allows a working set to stay resident while 20% buffer handles new traffic.
PROTECTED_RATIO = 0.8 
MAX_GHOST_SIZE = 5000 # Limit ghost registry size to prevent memory leaks

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) / 2Q-ish approach.
    
    1. Identify candidates in Probation (cache keys NOT in m_protected).
    2. Identify candidates in Protected (cache keys IN m_protected).
    3. Prefer evicting the LRU item from Probation.
    4. If Probation is empty, evict LRU from Protected.
    '''
    global m_protected, m_access_time
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Separate candidates into Probationary and Protected lists
    probation_candidates = []
    protected_candidates = []

    for k in candidate_keys:
        if k in m_protected:
            protected_candidates.append(k)
        else:
            probation_candidates.append(k)
    
    victim_key = None
    
    # Strategy: Evict from Probation first (Filter scans)
    if probation_candidates:
        # Find LRU in Probation
        victim_key = min(probation_candidates, key=lambda k: m_access_time.get(k, 0))
    else:
        # Fallback: Evict from Protected (Working set is too large)
        victim_key = min(protected_candidates, key=lambda k: m_access_time.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency (access time).
    2. Promote to Protected status if not already there.
    '''
    global m_protected, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    # If it was in Probation, it has proven its worth. Promote to Protected.
    if key not in m_protected:
        m_protected.add(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine if this is a brand new item or a "Ghost" returning.
    2. If Ghost: Direct promotion to Protected (restoration).
    3. If New: Place in Probation.
    '''
    global m_protected, m_ghost, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    if key in m_ghost:
        # It was evicted recently from probation, but came back.
        # This implies the probation window was too short or it's a cyclic pattern.
        # Promote immediately to protect it this time.
        m_protected.add(key)
        m_ghost.remove(key)
    else:
        # Brand new item. Start in Probation.
        # Ensure it is NOT in protected set
        m_protected.discard(key)

    _manage_segment_sizes(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up metadata for the evicted key.
    2. If it was a Probationary item, add to Ghost registry.
    '''
    global m_protected, m_ghost, m_access_time
    
    if evicted_obj:
        key = evicted_obj.key
        
        # Remove timestamp
        if key in m_access_time:
            del m_access_time[key]
            
        # Handle Segment Logic
        if key in m_protected:
            # If we evicted a Protected item, it's just gone. 
            # (We don't track ghosts for Protected items usually, as they had their chance).
            m_protected.remove(key)
        else:
            # It was in Probation. Add to Ghost so we recognize it if it comes back soon.
            m_ghost.add(key)
            
        _cleanup_ghosts()

def _manage_segment_sizes(cache_snapshot):
    '''
    Ensure the Protected segment doesn't consume the entire cache capacity,
    choking out new entries.
    '''
    global m_protected, m_access_time, PROTECTED_RATIO
    
    # Current logical capacity (number of items)
    current_count = len(cache_snapshot.cache)
    if current_count == 0: return

    # If Protected set grows too large, demote the LRU protected item to Probation.
    # This ensures we always have `1 - PROTECTED_RATIO` space for new items to be tested.
    target_protected_size = int(current_count * PROTECTED_RATIO)
    
    if len(m_protected) > target_protected_size:
        # Find LRU item *within* the Protected set
        # Since m_protected is a set, we must iterate. 
        # Intersection of cache keys and protected set:
        
        protected_in_cache = [k for k in m_protected if k in cache_snapshot.cache]
        
        if protected_in_cache:
            # Find the oldest protected item
            lru_protected = min(protected_in_cache, key=lambda k: m_access_time.get(k, 0))
            
            # Demote it: Remove from m_protected. 
            # It is now "Probationary". If accessed again, it goes back to Protected.
            # If space is needed, it acts as a high-priority eviction candidate relative to other protected items,
            # but usually, we evict actual probationary items first.
            m_protected.remove(lru_protected)

def _cleanup_ghosts():
    '''
    Prevent ghost registry from growing indefinitely.
    '''
    global m_ghost, MAX_GHOST_SIZE
    
    if len(m_ghost) > MAX_GHOST_SIZE:
        # Remove arbitrary element (sets are unordered, popping is efficient)
        m_ghost.pop() 
```
2025-11-27 19:28:12,615 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.0214

Example 2:
- Trace ID: 11
- Hit Rate: 0.3832

Example 3:
- Trace ID: 9
- Hit Rate: 0.531

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:28:12 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:28:12,616 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:28:35 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:35,891 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:35,892 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An optimized cache eviction algorithm implementation is provided below. This solution implements a variant of **LFU-DA (Least Frequently Used with Dynamic Aging)** combined with **Ghost Caching (ARC-like history)** concepts.

### Rationale for Changes

1.  **Dynamic Aging (Score-Based Eviction):** The previous algorithm used `Freq / log(Age)`. While logical, mathematical divisions in hot paths can be slower and harder to tune. The improved version uses a standard LFU-DA approach where the "score" of an object is `Frequency + Cache_Age_Factor`. When an object is evicted, its score sets a global "minimum frequency" baseline. New items inherit this baseline, preventing fresh popular items from being evicted immediately by stale high-frequency items.
2.  **Size-Awareness:** The previous implementation ignored object size. In many real-world traces (like CDNs), evicting one large object to make room for ten small popular objects is often a better trade-off. The new scoring function includes a penalty for large objects (`1/size`), optimizing "Hit Density".
3.  **Adaptive History (Ghost Entries):** We maintain metadata for evicted items. If a "ghost" item (recently evicted) is requested again, it indicates our cache is too small or the item is part of a churning working set. We restore it with a boosted frequency to protect it from immediate re-eviction.

```python
import math

# Metadata dictionaries
# Tracks the frequency of keys (LFU component)
m_key_frequency = dict()
# Tracks the insertion/update time (LRU component tie-breaking)
m_key_last_access = dict()

# Global state for Dynamic Aging (LFU-DA logic)
# This represents the "minimum worth" required to stay in the cache.
# It climbs as we evict high-frequency items.
L = 0.0

# Ghost Cache configuration
MAX_HISTORY_SIZE = 50000 
ghost_keys = set() 

def get_utility_score(key, size, current_time):
    '''
    Calculates the eviction score (Utility).
    Lower score = Higher likelihood of eviction.
    
    Formula: (Frequency + L) * (1 / log2(Size + 1))
    
    1. L (Dynamic Aging): Helps new items compete with old, frequent items.
    2. Size Penalty: Large items need significantly higher frequency to justify 
       taking up space. We use log2 to dampen the penalty so we don't 
       aggressively discriminate against moderately sized items.
    '''
    global m_key_frequency, L
    
    freq = m_key_frequency.get(key, 1)
    
    # Size-aware LFU-DA
    # We prefer small, frequent items.
    # We add 2 to log to prevent division by zero or negative results for size=0/1
    size_factor = 1.0 / math.log2(size + 2)
    
    # The score is the frequency relative to the global aging factor L, adjusted by size.
    return (freq + L) * size_factor

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Utility Score.
    '''
    global m_key_frequency, m_key_last_access, L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    # Tie-breaking variables
    min_access_time = float('inf')
    
    # We need the current "time" for LRU tie-breaking
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        score = get_utility_score(key, cached_obj.size, current_time)
        
        # Find minimum score
        if score < min_score:
            min_score = score
            victim_key = key
            min_access_time = m_key_last_access.get(key, 0)
        # Tie-breaker: If scores are equal, evict the LRU (Least Recently Used)
        elif score == min_score:
            last_acc = m_key_last_access.get(key, 0)
            if last_acc < min_access_time:
                min_access_time = last_acc
                victim_key = key
    
    # LFU-DA Logic:
    # Update the global aging factor L. 
    # L becomes the "frequency worth" of the item we just evicted.
    # This effectively "ages" all other items in the cache relative to this baseline.
    if victim_key is not None:
        L = m_key_frequency.get(victim_key, 1)

    return victim_key

def _cleanup_metadata():
    '''
    Maintains the size of the ghost/history maps to prevent memory leaks.
    '''
    global m_key_frequency, m_key_last_access, ghost_keys, MAX_HISTORY_SIZE
    
    if len(m_key_frequency) > MAX_HISTORY_SIZE:
        # Remove oldest items that are NOT in the active cache (ghosts)
        # Sorting by access time is O(N log N), but N is capped by MAX_HISTORY_SIZE.
        # To be faster, we could sample, but here we prioritize accuracy.
        sorted_keys = sorted(m_key_last_access.keys(), key=lambda k: m_key_last_access[k])
        
        deleted_count = 0
        target_delete = 1000 # Batch delete
        
        for k in sorted_keys:
            if k not in ghost_keys: 
                # Don't delete active cache metadata here, only history
                continue
                
            del m_key_frequency[k]
            del m_key_last_access[k]
            ghost_keys.remove(k)
            
            deleted_count += 1
            if deleted_count >= target_delete:
                break

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Linear increment is standard LFU. 
    # Valid optimization: Dynamic increment (give rarer items a bigger boost?)
    # For now, standard +1 is robust.
    m_key_frequency[key] = m_key_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize frequency and handle Ghost Cache logic.
    '''
    global m_key_frequency, m_key_last_access, ghost_keys, L
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in ghost_keys:
        # HIT on GHOST:
        # This item was evicted recently but requested again.
        # This implies our L value (eviction threshold) was too aggressive for this item.
        # We restore its old frequency plus a "penalty avoidance" boost.
        m_key_frequency[key] = m_key_frequency.get(key, 0) + L
        ghost_keys.remove(key)
    else:
        # NEW ITEM:
        # In LFU-DA, new items don't start at 1. They start at Frequency 1.
        # However, the eviction comparison effectively subtracts L.
        # So we initialize it such that its relative score is low but non-zero.
        m_key_frequency[key] = 1.0

    # Periodic cleanup check
    if len(m_key_frequency) > MAX_HISTORY_SIZE:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move the key to the ghost set (do not delete metadata).
    '''
    global ghost_keys
    
    # We keep the frequency and access time in the dicts, 
    # but mark it as a "ghost" entry.
    if evicted_obj:
        ghost_keys.add(evicted_obj.key)
```
2025-11-27 19:28:43,001 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 45
- Hit Rate: 0.0743

Example 2:
- Trace ID: 18
- Hit Rate: 0.0225

Example 3:
- Trace ID: 34
- Hit Rate: 0.0833

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:28:43 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:28:43,002 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:28:59 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:59,863 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:28:59,864 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to move beyond simple frequency or recency counts. The extremely low hit rates (2% - 8%) suggest the current LFU implementation is suffering from **cache pollution** (one-time wonders sticking around too long) or **frequency accumulation issues** (old popular items effectively "locking" the cache).

I will implement a **simplified version of the LIRS (Low Inter-reference Recency Set) / ARC (Adaptive Replacement Cache)** concepts, often referred to as a **Frequency-Based Replacement with Aging**, or more specifically, a **Segmented LRU (SLRU) / 2Q-like approach**, but adapted to the constraints of the provided function signatures.

### The Strategy: "Probation" vs. "Protected"

Instead of a single pool of objects, we will conceptually treat the cache as having two zones (managed via metadata, not physical movement):
1.  **Probationary Zone:** New items enter here. They are essentially on trial. If they are accessed again quickly, they are promoted.
2.  **Protected Zone:** Items that have proven their worth (accessed at least twice) live here.

**Why this works:**
*   **Scan Resistance:** One-time access patterns (scans) will flush through the Probationary zone without evicting the high-value, long-term items in the Protected zone.
*   **Recency + Frequency:** It combines the benefits of LRU (handling recent trends) with LFU (keeping popular items).

### Implementation Details

We will maintain:
1.  `m_hit_count`: Tracks how many times a key has been hit *since insertion*. This distinguishes "one-hit wonders" from established items.
2.  `m_last_access`: Standard LRU timestamp.

**The Eviction Logic:**
*   First, try to evict items that have **only been seen once** (Probationary). Among these, evict the Least Recently Used.
*   If *all* items in the cache have been seen multiple times (rare, but possible), then fall back to standard LRU eviction.

Here is the improved implementation:

```python
# Metadata dictionaries
# Tracks the last access time (Recency)
m_key_last_access = dict()
# Tracks the number of hits for an object while it is in the cache (Frequency/Status)
m_key_hit_count = dict()

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Protected/Probationary LRU (Segmented approach).
    
    Strategy:
    1. Identify "Probationary" items: Items that have a hit count of 1 (accessed only once upon insert).
    2. Identify "Protected" items: Items that have a hit count > 1.
    
    Priority:
    - First, try to evict the LRU item from the "Probationary" set (scan resistance).
    - If no probationary items exist, evict the LRU item from the "Protected" set.
    '''
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We are looking for two candidates:
    # 1. The LRU item among those with hit_count == 1 (Probationary LRU)
    # 2. The LRU item among those with hit_count > 1 (Protected LRU)
    
    probation_lru_key = None
    probation_lru_time = float('inf')
    
    protected_lru_key = None
    protected_lru_time = float('inf')

    for key in candidate_keys:
        hits = m_key_hit_count.get(key, 1) # Default to 1 if missing
        last_time = m_key_last_access.get(key, 0)
        
        if hits == 1:
            # Candidate for Probationary eviction
            if last_time < probation_lru_time:
                probation_lru_key = key
                probation_lru_time = last_time
        else:
            # Candidate for Protected eviction
            if last_time < protected_lru_time:
                protected_lru_key = key
                protected_lru_time = last_time

    # DECISION LOGIC:
    # If we have a probationary item, evict the oldest one.
    # This filters out "one-hit wonders" / scans effectively.
    if probation_lru_key is not None:
        return probation_lru_key
    
    # If all items are "Protected" (accessed > 1 time), fallback to standard LRU
    return protected_lru_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment hit count (promote to protected status) and update recency.
    '''
    global m_key_last_access, m_key_hit_count
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Increment hit count. Capping it at 2 is sufficient for this logic (1 vs >1),
    # but we keep counting to support potential future "Least Frequently Used" extensions if needed.
    current_hits = m_key_hit_count.get(obj.key, 1)
    m_key_hit_count[obj.key] = current_hits + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, mark as "Probationary" (hit count 1) and set recency.
    '''
    global m_key_last_access, m_key_hit_count
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    # Initialize with 1. It counts as the first access.
    m_key_hit_count[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata. 
    '''
    global m_key_last_access, m_key_hit_count
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    if evicted_obj.key in m_key_hit_count:
        del m_key_hit_count[evicted_obj.key]
```
2025-11-27 19:29:29,337 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 15
- Hit Rate: 0.0213

Example 2:
- Trace ID: 0
- Hit Rate: 0.5022

Example 3:
- Trace ID: 5
- Hit Rate: 0.4867

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:29:29 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:29:29,338 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:29:57 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:29:57,019 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:29:57,020 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the weaknesses of the previous "Recency-weighted Frequency" approach. The previous approach suffered from two main issues:
1.  **Scan Overhead:** Iterating through the entire cache to calculate scores for every eviction is $O(N)$, which is inefficient for large caches.
2.  **Frequency Pollution:** Simple frequency counters can be polluted by scan-heavy workloads (items accessed once or twice in a burst and never again).

**Proposed Solution: Adaptive Replacement Cache (ARC) / LIRS Hybrid (Ghost Cache Strategy)**

Instead of a complex mathematical formula, we will implement a strategy inspired by **ARC (Adaptive Replacement Cache)** and **LIRS (Low Inter-reference Recency Set)**. This approach is generally superior because:
1.  **Ghost Lists:** It maintains metadata for items *recently evicted*. If a "ghost" item is requested, it implies our cache was too small for that specific workload type (recency vs. frequency), and we adapt.
2.  **Frequency vs. Recency Balance:** It explicitly separates items into "Recency-heavy" (T1) and "Frequency-heavy" (T2) lists and dynamically adjusts the size of T1 based on hits in the ghost lists.

Here is the improved implementation:

```python
import collections

# We maintain two main logical segments within the cache:
# T1: Items accessed only once recently (Recency / New items)
# T2: Items accessed at least twice (Frequency / Popular items)
#
# We also maintain two "Ghost" lists (metadata only, no data):
# B1: Evicted from T1 (Recent misses)
# B2: Evicted from T2 (Frequent misses)
#
# The parameter 'p' determines the target size of T1. 
# If we hit in B1, we increase p (need more recency).
# If we hit in B2, we decrease p (need more frequency).

# Global State
m_T1 = collections.OrderedDict() # Recency list (Key -> Metadata)
m_T2 = collections.OrderedDict() # Frequency list (Key -> Metadata)
m_B1 = collections.OrderedDict() # Ghost Recency (Key -> Metadata)
m_B2 = collections.OrderedDict() # Ghost Frequency (Key -> Metadata)

# Adaptation Parameter
# p is the target size for the T1 list (recency).
m_p = 0 

def evict(cache_snapshot, obj):
    '''
    The eviction logic is driven by the state of the T1 and T2 lists relative to
    the adaptive parameter 'p'. This function performs the actual removal of a key 
    from the physical cache based on the logic of replace().
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Current logical sizes
    t1_len = len(m_T1)
    
    # We need to decide whether to evict from T1 or T2.
    # Logic derived from ARC's replace() function.
    
    candidate_key = None
    
    # If T1 is larger than the target 'p', we usually evict from T1 (the LRU of T1).
    # However, if B1 is much larger than B2, it influences this.
    # The strict ARC condition for evicting from T2 is:
    # t1_len < p  OR  (B2 is not empty and t1_len == p)
    #
    # Conversely, we evict from T1 if:
    # t1_len > p
    
    if t1_len > 0 and (t1_len > m_p or (t1_len == m_p and len(m_B2) > 0 and len(m_B1) == 0)):
        # Evict LRU from T1
        candidate_key, _ = m_T1.popitem(last=False)
        # Move to Ghost List B1
        m_B1[candidate_key] = True
    else:
        # Evict LRU from T2
        if m_T2:
            candidate_key, _ = m_T2.popitem(last=False)
            # Move to Ghost List B2
            m_B2[candidate_key] = True
        else:
            # Fallback if T2 is empty but we must evict (rare edge case in cold start)
            if m_T1:
                candidate_key, _ = m_T1.popitem(last=False)
                m_B1[candidate_key] = True

    # Ensure Ghost lists don't grow infinitely
    # ARC generally limits |B1| + |B2| <= Capacity. 
    # Since we don't have max item capacity (only bytes), we use a heuristic limit based on active keys.
    # Capacity in items is roughly cache_snapshot.size / avg_obj_size, but we can just clamp ghosts 
    # to the current number of cached items to be safe.
    current_cache_count = len(cache_snapshot.cache)
    if len(m_B1) > current_cache_count:
        m_B1.popitem(last=False)
    if len(m_B2) > current_cache_count * 2: # Keep more history for frequency
        m_B2.popitem(last=False)
        
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Move item to the MRU position of T2 (Frequency List).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Key is in T1 (New item became popular)
    if key in m_T1:
        del m_T1[key]
        m_T2[key] = True # Move to T2 MRU
    
    # Case 2: Key is in T2 (Popular item staying popular)
    elif key in m_T2:
        del m_T2[key]
        m_T2[key] = True # Re-insert to update MRU position
        
    # Note: If it's a cache hit, it MUST be in T1 or T2.
    # If the logic drifts, we safety check:
    else:
        m_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss): Handle adaptation of 'p' and placement into T1 or T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # Case 1: Key is in Ghost List B1 (Was in T1 recently)
    # This implies T1 was too small. Increase p.
    if key in m_B1:
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) / len(m_B1)
        m_p = min(len(cache_snapshot.cache) + 1, m_p + delta)
        
        del m_B1[key]
        # Promote to T2 (frequency list) because it was seen recently (in T1) and now again.
        m_T2[key] = True 

    # Case 2: Key is in Ghost List B2 (Was in T2 recently)
    # This implies T2 was too small (or T1 was too big). Decrease p.
    elif key in m_B2:
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) / len(m_B2)
        m_p = max(0, m_p - delta)
        
        del m_B2[key]
        # Re-insert into T2
        m_T2[key] = True
        
    # Case 3: Totally new item (or forgotten from ghosts)
    else:
        # Insert into T1 (Recency List)
        m_T1[key] = True
        
    # NOTE: The 'evict' function will be called by the system BEFORE this insert
    # if the cache is full. However, inside this function, we just manage the lists.

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Metadata updates for the evicted object are handled inside the `evict` function 
    to ensure atomic logic (moving from Tx to Bx). 
    We do nothing here to avoid double-accounting.
    '''
    pass
```
2025-11-27 19:32:28,782 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 21
- Hit Rate: 0.0266

Example 2:
- Trace ID: 14
- Hit Rate: 0.0215

Example 3:
- Trace ID: 19
- Hit Rate: 0.2722

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:32:28 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:32:28,784 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:33:05 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:33:05,354 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:33:05,355 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An improved cache eviction algorithm implementation that focuses on adaptive frequency tracking with recency awareness, designed to handle the specific scan patterns and varying workloads indicated by the performance feedback.

```python
import math

# Metadata Globals
# Tracks the frequency of keys. We use a logarithmic decay or simple counter.
m_frequency = dict()
# Tracks the insertion time or last access time for LRU/FIFO tie-breaking.
m_access_time = dict()

# Ghost caches for ARC/LIRS-like adaptation
# m_ghost_recent: Keys recently evicted from the "recency" set
m_ghost_recent = set()
# m_ghost_frequent: Keys recently evicted from the "frequency" set
m_ghost_frequent = set()

# Algorithm Parameters
# p represents the target size for the "recent" (probationary) segment of the cache.
# 0 <= p <= cache_capacity.
# If p is high, we behave more like LRU (favoring recency).
# If p is low, we behave more like LFU (favoring frequency).
m_p = 0
# Initial capacity estimation (will update dynamically)
m_capacity_estimate = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Simplified Adaptive Replacement Cache (ARC) Logic.
    
    The algorithm manages two conceptual lists within the single cache dict:
    1. T1 (Recent): Items seen only once recently (Probationary).
    2. T2 (Frequent): Items seen at least twice (Protected).
    
    The parameter `m_p` determines the ideal target size of T1.
    
    If len(T1) > m_p, we evict from T1 (LRU style).
    Else, we evict from T2 (LRU style).
    '''
    global m_frequency, m_access_time, m_p, m_capacity_estimate
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
    
    # Estimate capacity (since we are given size in bytes, but ARC works on counts usually.
    # We map 'size' roughly to object count for the logic).
    current_cache_count = len(candidate_keys)
    m_capacity_estimate = max(m_capacity_estimate, current_cache_count)
    
    # Identify which keys belong to T1 (Frequency=1) and T2 (Frequency > 1)
    # We need to perform the eviction based on the adaptive parameter m_p.
    
    t1_keys = []
    t2_keys = []
    
    for k in candidate_keys:
        freq = m_frequency.get(k, 1)
        if freq == 1:
            t1_keys.append(k)
        else:
            t2_keys.append(k)
            
    victim_key = None
    
    # Decision logic based on ARC principle:
    # If the size of T1 exceeds the target p, we evict from T1 (Recency/Probation).
    # Otherwise, we evict from T2 (Frequency).
    # Note: If T1 is empty, we must evict from T2, and vice versa.
    
    evict_from_t1 = False
    
    if t1_keys and len(t1_keys) > m_p:
        evict_from_t1 = True
    elif t1_keys and not t2_keys:
        evict_from_t1 = True
    elif t2_keys and not t1_keys:
        evict_from_t1 = False
    else:
        # Both exist, but T1 <= p. Usually evict T2 here to grow T1?
        # Standard ARC: if len(T1) == p, we can evict T1? 
        # Strictly: if len(T1) > p, replace in T1. Else replace in T2.
        evict_from_t1 = False

    # Perform the eviction (LRU within the chosen set)
    if evict_from_t1:
        # Find LRU in T1
        # m_access_time stores the time; min is oldest.
        victim_key = min(t1_keys, key=lambda k: m_access_time.get(k, 0))
    else:
        # Find LRU in T2
        victim_key = min(t2_keys, key=lambda k: m_access_time.get(k, 0))
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update recency (access time).
    2. If item was in T1 (freq=1), move to T2 (freq+=1).
    3. If item was in T2, update its recency within T2.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    # Increment frequency to signify it's a "frequent" item now
    # We cap at 2 for the binary distinction (T1 vs T2), but keeping real count helps tie-breaks if needed.
    # For this logic, just >1 is T2.
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Handle Ghost Hits to adapt 'm_p'.
    2. Initialize new object metadata.
    '''
    global m_frequency, m_access_time, m_p, m_ghost_recent, m_ghost_frequent, m_capacity_estimate
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # --- Adaptive Logic (Ghost Hits) ---
    if key in m_ghost_recent:
        # Hit in B1 (Ghost Recent). We made T1 too small. Increase p.
        # Delta depends on sizes of ghost lists.
        len_b1 = len(m_ghost_recent)
        len_b2 = len(m_ghost_frequent)
        delta = 1
        if len_b1 < len_b2:
             delta = len_b2 / len_b1
        
        m_p = min(m_capacity_estimate, m_p + delta)
        m_ghost_recent.remove(key)
        
        # Revived item starts with high frequency logic (it's recurring)
        m_frequency[key] = 2
        
    elif key in m_ghost_frequent:
        # Hit in B2 (Ghost Frequent). We made T1 too big (T2 too small). Decrease p.
        len_b1 = len(m_ghost_recent)
        len_b2 = len(m_ghost_frequent)
        delta = 1
        if len_b2 < len_b1:
            delta = len_b1 / len_b2
            
        m_p = max(0, m_p - delta)
        m_ghost_frequent.remove(key)
        
        # Revived item belongs in T2
        m_frequency[key] = 2
        
    else:
        # Brand new item. Starts in T1.
        m_frequency[key] = 1

    # --- Standard Updates ---
    m_access_time[key] = current_time
    
    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted key to the appropriate Ghost list (B1 or B2)
    based on whether it was a T1 or T2 item.
    '''
    global m_frequency, m_ghost_recent, m_ghost_frequent, m_access_time
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    freq = m_frequency.get(key, 1)
    
    # If freq == 1, it was in T1 (Recent/Probationary) -> move to B1 (Ghost Recent)
    # If freq > 1, it was in T2 (Frequent) -> move to B2 (Ghost Frequent)
    if freq <= 1:
        m_ghost_recent.add(key)
    else:
        m_ghost_frequent.add(key)

    # Clean up main metadata to save memory, as it's now tracked in ghost sets
    if key in m_frequency: del m_frequency[key]
    if key in m_access_time: del m_access_time[key]

def _cleanup_metadata():
    '''
    Maintain ghost list sizes to ensure they don't grow indefinitely.
    Standard ARC keeps |B1| + |B2| <= Capacity.
    '''
    global m_ghost_recent, m_ghost_frequent, m_capacity_estimate
    
    # We use a rough constant limit or capacity based limit
    limit = max(1000, m_capacity_estimate * 2) 
    
    # If ghost lists are too large, remove oldest (FIFO/Random for simplicity here as we are using Sets)
    # Ideally, ghost lists are queues. Since we lack ordered dict/deque in global state easily without complexity,
    # we just trim if size explodes.
    
    if len(m_ghost_recent) > limit:
        # Remove arbitrary elements
        for _ in range(len(m_ghost_recent) - limit):
            m_ghost_recent.pop()
            
    if len(m_ghost_frequent) > limit:
        for _ in range(len(m_ghost_frequent) - limit):
            m_ghost_frequent.pop()
```
2025-11-27 19:33:23,842 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 33
- Hit Rate: 0.7531

Example 2:
- Trace ID: 8
- Hit Rate: 0.5399

Example 3:
- Trace ID: 39
- Hit Rate: 0.0668

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:33:23 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:33:23,844 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:34:55 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:34:55,045 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:34:55,047 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: ```python
# Metadata dictionaries
# We map Key -> Size (int). This allows us to track byte usage in ghost lists.
meta_t1 = dict() # Recently accessed once
meta_t2 = dict() # Accessed at least twice
meta_b1 = dict() # Ghost T1 (Evicted from T1)
meta_b2 = dict() # Ghost T2 (Evicted from T2)

# Size tracking variables (in bytes) to avoid O(N) summation
size_t1 = 0
size_t2 = 0
size_b1 = 0
size_b2 = 0

# Adaptive parameter 'p'
# Represents the target size (in bytes) for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC (Adaptive Replacement Cache) Eviction Strategy (Byte-Aware).
    
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` (target bytes for T1) 
    and the current byte size of T1.
    '''
    global p, meta_t1, meta_t2, size_t1
    
    # ARC Replace Logic:
    # If the size of T1 exceeds the target 'p', we prefer evicting from T1 (recency)
    # to make room for frequency items or to reduce recency size.
    # Exception: If T1 is the only thing we have, we must evict from it.
    # Exception: If T2 is empty, we must evict from T1.
    
    # We prioritize T1 eviction if T1 is "too big" (size_t1 > p).
    # However, we must ensure the list we pick from actually has items.
    
    # Check T1 eligibility
    evict_t1 = False
    
    if meta_t1 and not meta_t2:
        # Only T1 has items
        evict_t1 = True
    elif meta_t1 and size_t1 > p:
        # T1 is larger than target p, so we prune T1
        evict_t1 = True
    # Else: T1 is within limits (or empty), so we prune T2 (unless T2 is empty, handled by first if)

    if evict_t1:
        # Return LRU of T1 (first key in dict)
        return next(iter(meta_t1))
    elif meta_t2:
        # Return LRU of T2
        return next(iter(meta_t2))
    
    # Fallback for safety (e.g., if metadata is out of sync with snapshot)
    if cache_snapshot.cache:
        return next(iter(cache_snapshot.cache))
    return None

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If in T1 (Recency), move to T2 (Frequency).
    - If in T2 (Frequency), update position to MRU.
    '''
    global meta_t1, meta_t2, size_t1, size_t2
    
    key = obj.key
    size = obj.size
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        size_t1 -= meta_t1[key]
        del meta_t1[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    elif key in meta_t2:
        # Hit in Frequency (T2): Update LRU position (delete and re-insert makes it MRU)
        # Size doesn't change, but we refresh the object size just in case
        old_size = meta_t2[key]
        del meta_t2[key]
        
        # Note: If object size changed dynamically, we would need to handle size_t2 update here.
        # Assuming immutable size for key based on prompt read-only attributes.
        meta_t2[key] = size

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Adjusts 'p' if the miss occurred in Ghost lists (B1 or B2).
    - Inserts the new object into T1 (if new) or T2 (if seen in Ghosts).
    - Cleans up Ghost lists to prevent infinite memory growth.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    global size_t1, size_t2, size_b1, size_b2
    
    key = obj.key
    size = obj.size
    capacity = cache_snapshot.capacity
    
    # 1. Adaptation Step: Adjust p based on ghost hits
    if key in meta_b1:
        # Miss in B1 (Ghost Recency). We should have kept T1 larger.
        # Delta logic: increase p.
        delta = size
        # Advanced ARC delta: if size_b1 >= size_b2: delta = size, else size * (size_b2/size_b1)
        # Simplified robust byte-level delta:
        if size_b1 >= size_b2:
            delta = size
        else:
            # Avoid divide by zero
            delta = size * (size_b2 / size_b1) if size_b1 > 0 else size
            
        p = min(capacity, p + delta)
        
        # Remove from B1 and add to T2 (since it's a re-access)
        size_b1 -= meta_b1[key]
        del meta_b1[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    elif key in meta_b2:
        # Miss in B2 (Ghost Frequency). We should have kept T2 larger (T1 smaller).
        # Delta logic: decrease p.
        delta = size
        if size_b2 >= size_b1:
            delta = size
        else:
            delta = size * (size_b1 / size_b2) if size_b2 > 0 else size
            
        p = max(0, p - delta)
        
        # Remove from B2 and add to T2
        size_b2 -= meta_b2[key]
        del meta_b2[key]
        
        meta_t2[key] = size
        size_t2 += size
        
    else:
        # Totally new item (Cold Miss). Insert into T1.
        meta_t1[key] = size
        size_t1 += size

    # 2. History Management (Ghost Sizing)
    # We must limit the size of ghost lists to keep the algorithm responsive and memory efficient.
    # Standard ARC suggests: L1 (T1+B1) <= C, L2 (T2+B2) <= 2C.
    
    # Trim B1 if T1 + B1 is too large
    while meta_b1 and (size_t1 + size_b1 > capacity):
        k = next(iter(meta_b1))
        size_b1 -= meta_b1[k]
        del meta_b1[k]
        
    # Trim B2 if T2 + B2 is too large (allow 2x capacity for frequency history usually)
    while meta_b2 and (size_t2 + size_b2 > 2 * capacity):
        k = next(iter(meta_b2))
        size_b2 -= meta_b2[k]
        del meta_b2[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Move the evicted object from T list (cache) to B list (ghost).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    global size_t1, size_t2, size_b1, size_b2
    
    key = evicted_obj.key
    size = evicted_obj.size
    
    if key in meta_t1:
        size_t1 -= meta_t1[key]
        del meta_t1[key]
        
        # Add to B1 (Ghost T1)
        meta_b1[key] = size
        size_b1 += size
        
    elif key in meta_t2:
        size_t2 -= meta_t2[key]
        del meta_t2[key]
        
        # Add to B2 (Ghost T2)
        meta_b2[key] = size
        size_b2 += size
```
2025-11-27 19:36:49,989 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 31
- Hit Rate: 0.007

Example 2:
- Trace ID: 46
- Hit Rate: 0.4663

Example 3:
- Trace ID: 35
- Hit Rate: 0.068

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:36:49 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:36:49,990 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:37:13 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:37:13,770 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:37:13,771 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the weaknesses of the current LFU/LRU approach demonstrated by the poor hit rates in the feedback (e.g., Trace 31 with 0.7% hit rate).

### Analysis of the Problem
The current algorithm suffers from two major issues typical of basic LFU:
1.  **Cache Pollution:** Objects accessed frequently for a short period stick around too long, preventing new, potentially hot objects from entering.
2.  **Frequency Initialization Bias:** New objects start with a frequency of 1 and are immediately eviction candidates, leading to "churn" where new items are evicted before they can prove their worth.
3.  **Memory Constraints:** The current `update_after_evict` deletes metadata immediately. High-performance algorithms usually retain "ghost" metadata for recently evicted items to distinguish between "first-time seen" and "seen recently but evicted" objects.

### The Solution: LRFU (Least Recently/Frequently Used) or ARC-like Hybrid
While complexity like ARC (Adaptive Replacement Cache) is powerful, it is often complex to implement correctly without more state. A highly effective, robust, and simpler alternative that outperforms standard LFU and LRU across mixed workloads is **LFU with Dynamic Aging (LFU-DA)** or a simplified **2-Queue (2Q) / Segmented LRU** approach.

However, given the constraints and the goal to generally improve hit rate across unknown trace types, a **GDSF (Greedy Dual-Size Frequency)** or a **Hyperbolic Caching** approach is often superior, especially since we have access to `obj.size`. But a very strong general-purpose contender that fixes LFU's flaws is **LFU with Aging and Size Awareness**.

Let's implement a **Score-Based Eviction Strategy** that combines Recency, Frequency, and Size.
The formula we will use is a variation of **Hyperbolic Caching**:
$$ \text{Score} = \frac{\text{Frequency}^{\alpha}}{\text{Time Since Last Access} \times \text{Size}} $$

*However*, pure mathematical division on every evict is expensive. A more robust and standard high-performance algorithm is **WTinyLFU** (Window TinyLFU), but that requires complex data structures (Bloom filters, count-min sketch).

Given the Python structure, we will implement a robust **Priority-Based GDSF-like (Greedy Dual-Size Frequency)** logic but simplified for this context. This algorithm is excellent because it considers frequency but "ages" objects over time (via an inflation factor $L$) and penalizes large objects.

**The Algorithm: GDSF (Greedy Dual-Size Frequency)**
1.  **Cost Function:** We assign a value $H(obj)$ to each object.
    $H(obj) = L + \frac{\text{Frequency}}{\text{Size}}$
2.  **Eviction:** Evict the object with the smallest $H(obj)$.
3.  **Aging ($L$):** When an object is evicted, set $L = H(\text{evicted\_obj})$. This effectively raises the "entry barrier" for new objects and naturally ages out stale high-frequency items because their $H$ value stays static while $L$ grows.
4.  **Update:** On a hit, update the object's $H$ value using the current $L$.

This handles the "frequency pollution" (stale items eventually fall below $L$) and optimizes for size (evicting large items to keep many small items, which usually increases hit rate).

### Improved Implementation

```python
# Metadata dictionaries
# Tracks the frequency of object access
m_key_frequency = dict()
# Tracks the computed priority value (H value) for the GDSF algorithm
m_key_priority = dict()
# The 'inflation' value L, used to age out stale entries
m_inflation_L = 0.0

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority value (GDSF).
    Priority = L + (Frequency / Size).
    When an object is evicted, we update L to that object's priority.
    '''
    global m_inflation_L, m_key_priority
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We need to find the object with the minimum priority value.
    # While iterating, we handle the edge case where a key might be missing from metadata 
    # (though updates should prevent this).
    
    victim_key = None
    min_priority = float('inf')
    
    for key in candidate_keys:
        # Default to 0.0 priority if missing (should be evicted immediately)
        priority = m_key_priority.get(key, 0.0)
        
        if priority < min_priority:
            min_priority = priority
            victim_key = key
            
            # Optimization: If we find a 0 priority, we can't get lower, stop early.
            if min_priority == 0.0:
                break
    
    # Update the global inflation factor L to the priority of the evicted object.
    # This is the "Aging" mechanism. New objects must beat this value to survive long.
    if victim_key is not None:
        m_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Increment frequency.
    2. Recalculate priority using current L.
    '''
    global m_key_frequency, m_key_priority, m_inflation_L
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0) + 1
    m_key_frequency[obj.key] = current_freq
    
    # Update Priority: H = L + (Freq / Size)
    # Using float for precision.
    size = obj.size if obj.size > 0 else 1 # Prevent division by zero
    new_priority = m_inflation_L + (current_freq / size)
    m_key_priority[obj.key] = new_priority

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize frequency to 1.
    2. Calculate initial priority using current L.
    '''
    global m_key_frequency, m_key_priority, m_inflation_L
    
    # Initialize Frequency
    m_key_frequency[obj.key] = 1
    
    # Initialize Priority: H = L + (1 / Size)
    size = obj.size if obj.size > 0 else 1
    new_priority = m_inflation_L + (1.0 / size)
    m_key_priority[obj.key] = new_priority

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    Clean up metadata. We do NOT delete frequency history immediately if we wanted 
    to implement "Ghost Lists" (Frequency Reset), but for GDSF, standard cleanup is usually sufficient
    and saves memory.
    '''
    global m_key_frequency, m_key_priority
    
    # Remove from priority tracking
    if evicted_obj.key in m_key_priority:
        del m_key_priority[evicted_obj.key]
    
    # Optimization: We remove frequency data to save memory. 
    # If the object returns, it is treated as new (Frequency reset). 
    # The GDSF 'L' factor handles the "system memory" of popularity.
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
```
2025-11-27 19:37:21,780 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 47
- Hit Rate: 0.0817

Example 2:
- Trace ID: 13
- Hit Rate: 0.0

Example 3:
- Trace ID: 10
- Hit Rate: 0.075

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:37:21 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:37:21,781 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:38:36 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:38:36,936 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:38:36,937 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance feedback, the primary failure of the previous ARC implementation was likely its inability to handle variable-sized objects. Standard ARC assumes all objects are of equal size (or occupy one "page"). When objects have different sizes (`obj.size`), treating the cache capacity as a simple item count leads to severe inefficiency—either the cache fills up with a few massive objects (blocking many small ones) or the adaptation parameter `p` becomes meaningless against a byte-based capacity.

To maximize the **Hit Rate** (number of hits / number of requests) in a variable-size cache context, the optimal strategy is usually **GDSF (Greedy Dual Size Frequency)**.

**Why GDSF?**
1.  **Size Awareness:** It penalizes large objects. By evicting one large object, we make room for multiple smaller objects. Statistically, holding more objects increases the probability of a hit, thus increasing the overall Hit Rate.
2.  **Frequency:** It incorporates access counts (Frequency), protecting popular items.
3.  **Aging:** It uses a dynamic aging factor (`L`), ensuring that objects with high frequency but no recent activity eventually age out (preventing cache pollution).

### Improved Algorithm Implementation

```python
# Metadata to track frequency and priority values for objects.
# Maps key -> {'freq': int, 'priority': float}
metadata = dict()

# The dynamic aging factor (L). 
# This represents the "inflation" value of the system.
# It increases over time as objects are evicted.
L = 0.0

def get_metadata(key):
    """Helper to retrieve metadata safely."""
    if key not in metadata:
        return {'freq': 0, 'priority': 0.0}
    return metadata[key]

def calculate_priority(freq, size):
    """
    Calculates GDSF priority.
    Priority = L + (Frequency * Cost) / Size
    
    To maximize Object Hit Rate, we assume the 'Cost' of a miss is 1 for all objects.
    Therefore: Priority = L + Frequency / Size
    """
    # Ensure size is at least 1 to avoid division by zero
    safe_size = max(1, size)
    return L + (freq / safe_size)

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Strategy:
    Find the object in the cache with the lowest Priority value (H-value).
    '''
    global L, metadata
    
    victim_key = None
    min_priority = float('inf')
    
    # Iterate through all objects currently in the cache
    # Note: dict iteration in Python is generally insertion-ordered.
    # If ties occur in priority, this naturally falls back to LRU for the tie-breaker.
    for key in cache_snapshot.cache:
        if key in metadata:
            p_val = metadata[key]['priority']
        else:
            # If for some reason metadata is missing, treat as 0 priority (evict first)
            p_val = 0.0
            
        if p_val < min_priority:
            min_priority = p_val
            victim_key = key
            
    # Update the aging factor L to the priority of the evicted object.
    # This pushes the "floor" up, ensuring new items have a chance to compete
    # and old high-frequency items eventually decay.
    if victim_key is not None and min_priority != float('inf'):
        # Only update L if the new value is higher (monotonically increasing)
        if min_priority > L:
            L = min_priority
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on new Frequency and current L.
    '''
    global metadata, L
    
    key = obj.key
    meta = get_metadata(key)
    
    # Increment frequency
    new_freq = meta['freq'] + 1
    
    # Update Priority
    # H = L + F / S
    new_priority = calculate_priority(new_freq, obj.size)
    
    metadata[key] = {'freq': new_freq, 'priority': new_priority}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    1. Initialize or Restore Frequency (Ghost Caching).
    2. Calculate initial Priority.
    '''
    global metadata, L
    
    key = obj.key
    
    # We maintain frequency history even after eviction (Ghost functionality).
    # If the key was seen before, we continue its frequency count (LFU style).
    # If not, we start at 1.
    if key in metadata:
        new_freq = metadata[key]['freq'] + 1
    else:
        new_freq = 1
        
    # Calculate Priority
    # For a new item, Priority is L + (1 / Size) usually, but we use accumulated freq
    new_priority = calculate_priority(new_freq, obj.size)
    
    metadata[key] = {'freq': new_freq, 'priority': new_priority}
    
    # Maintenance: Prevent metadata memory leak.
    # If metadata grows too large relative to the cache (e.g., lots of one-hit wonders),
    # we purge old entries that are no longer in the cache.
    # 4x is a heuristic buffer to allow for "Ghost" history.
    if len(metadata) > max(1000, len(cache_snapshot.cache) * 4):
        # Identify keys not in cache
        cached_keys = set(cache_snapshot.cache.keys())
        ghost_keys = [k for k in metadata if k not in cached_keys]
        
        # Simple cleanup: remove the oldest portion of ghosts
        # In Python < 3.7 dicts are not guaranteed ordered, but in 3.7+ they are insertion ordered.
        # We delete the oldest added ghosts.
        limit_remove = len(ghost_keys) // 2
        for k in ghost_keys[:limit_remove]:
            del metadata[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    We do NOT delete the metadata immediately. 
    We keep it to remember the frequency if the object returns (Ghost List behavior).
    The cleanup happens in update_after_insert if the map gets too big.
    '''
    pass
```
2025-11-27 19:38:45,873 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.0183

Example 2:
- Trace ID: 30
- Hit Rate: 0.041

Example 3:
- Trace ID: 22
- Hit Rate: 0.0587

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:38:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:38:45,873 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:39:15 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:39:15,823 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:39:15,824 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the very low hit rates observed in the feedback (1.8% - 5.8%). The previous implementation attempted a complex adaptive scoring mechanism involving ghost caches and floating-point math. While theoretically sound for specific workloads, it likely suffered from two issues:
1.  **Over-complexity / Parameter Sensitivity:** The adaptive factor might oscillate incorrectly or fail to converge quickly enough on erratic traces.
2.  **Scan Resistance:** The traces likely contain "scans" (sequences of unique keys accessed once) which flush out valuable long-term data. The previous probation mechanism (penalizing frequency=1) was a step in the right direction but perhaps insufficient or too rigid.

**The Strategy: LFRU (Least Frequent Recently Used) / ARC-inspired Logic**

Instead of a raw mathematical formula, we will implement a robust, structure-based policy that approximates **ARC (Adaptive Replacement Cache)** or **LIRS (Low Inter-reference Recency Set)**, but simplified for this stateless function context.

We will split the cache logic into two logical segments:
1.  **Protected Segment (High Frequency):** Items that have been seen at least twice. We manage these with an LRU policy *within* this group. This captures the LFU utility without the complex heap management.
2.  **Probation Segment (Low Frequency/Recency):** Items seen only once. These are candidates for quick eviction.

**Key Changes:**
*   **Segmented LRU:** We will maintain two logical queues (using an ordering dict or timestamp logic): a "Probationary" queue and a "Protected" queue.
*   **Admission Policy:** New items enter Probation. Only upon a subsequent hit do they move to Protected.
*   **Eviction Policy:** We prefer to evict from Probation (Frequency = 1). If Probation is empty (unlikely) or we need to balance the cache, we evict from Protected (LRU).
*   **Simplified Metadata:** We remove the ghost cache logic to reduce memory overhead and CPU cycles, focusing purely on current cache contents + simple historical frequency tracking.

Here is the improved algorithm:

```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed.
# This helps distinguish between one-hit wonders (scans) and recurring data.
m_frequency = dict()

# Tracks the order of insertion/access for implementing LRU behavior.
# We use a strictly increasing counter for operations.
m_access_time = dict()

# To handle "scans" (sequences of data accessed once), we conceptually divide 
# the cache into two segments based on frequency:
# 1. Probationary (Frequency == 1): Recently inserted, unproven value.
# 2. Protected (Frequency > 1): Proven value.
#
# We do not strictly partition the memory space, but we bias eviction heavily 
# against the Probationary items.

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (Scan Resistant).
    
    1. Identify candidates in the cache.
    2. Try to find a victim in the "Probationary" set (Freq == 1).
       Evict the Least Recently Used (oldest access time) item among them.
    3. If no Probationary items exist, evict the Least Recently Used item
       from the "Protected" set (Freq > 1).
    '''
    global m_frequency, m_access_time
    
    # Get all keys currently in the cache
    current_keys = list(cache_snapshot.cache.keys())
    
    if not current_keys:
        return None

    # Strategy: Find the LRU item, but prioritize evicting items with low Frequency.
    # We want to keep high-frequency items even if they haven't been touched in a while
    # compared to a brand new item that might be part of a one-time scan.
    
    victim = None
    min_time = float('inf')
    found_probationary = False
    
    # First Pass: Look for LRU among Probationary items (Freq <= 1)
    # This acts as a filter against "scans".
    for key in current_keys:
        freq = m_frequency.get(key, 1)
        
        if freq <= 1:
            access_time = m_access_time.get(key, 0)
            if access_time < min_time:
                min_time = access_time
                victim = key
                found_probationary = True
    
    # If we found a probationary victim, return it.
    if found_probationary and victim is not None:
        return victim

    # Second Pass: If all items are "Protected" (Freq > 1), fall back to standard LRU.
    # Reset search vars
    min_time = float('inf')
    victim = None
    
    for key in current_keys:
        access_time = m_access_time.get(key, 0)
        if access_time < min_time:
            min_time = access_time
            victim = key
            
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update access time (Recency) and increment Frequency.
    This effectively moves an item from Probation to Protected if Freq becomes > 1.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    # Cap frequency to prevent integer overflow in long running processes, 
    # though practically anything > 1 is "Protected"
    m_frequency[key] = min(m_frequency.get(key, 0) + 1, 1000)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize access time and set Frequency to 1.
    New items start in Probation.
    '''
    global m_frequency, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_access_time[key] = current_time
    
    # If the key exists in our metadata (history), we restore it, 
    # otherwise it starts at 1.
    if key not in m_frequency:
        m_frequency[key] = 1
    else:
        # It's a re-insertion. We bump it slightly to acknowledge it returned,
        # but don't give it full credit of its past life immediately to prevent
        # old popular items from clogging the cache forever.
        # However, for this implementation, treating it as a new hit (increment)
        # upon re-insertion is usually safer for hit-rate.
        m_frequency[key] += 1
        
    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: We do NOT remove metadata immediately.
    We keep the frequency history so if the item returns (thrashing), 
    we know it was valuable.
    '''
    # No specific action needed here, cleanup handles the memory management.
    pass

def _cleanup_metadata(cache_snapshot):
    '''
    Prevent memory leaks by removing metadata for keys that have been 
    evicted and haven't returned in a long time.
    '''
    global m_frequency, m_access_time
    
    # Heuristic: Limit metadata size to constant factor of cache capacity 
    # or a fixed upper bound if capacity is in bytes (which makes object count hard to guess).
    # Here we use a safe upper limit.
    MAX_METADATA_ENTRIES = 50000 
    
    if len(m_frequency) > MAX_METADATA_ENTRIES:
        # If metadata grows too large, purge the oldest entries not currently in cache.
        # To do this efficiently without iterating everything every time, 
        # we can just do a random sampling or purge a chunk based on access time.
        
        # Simple cleanup: Remove keys not in cache that are also old.
        # Note: In a production system, this would be an ordered dictionary or linked list.
        # Here, we do a "stop the world" cleanup when limit is hit.
        
        keys_in_cache = set(cache_snapshot.cache.keys())
        all_keys = list(m_access_time.keys())
        
        # Sort by access time (oldest first)
        all_keys.sort(key=lambda k: m_access_time[k])
        
        # Remove the oldest 10% of metadata entries, provided they aren't currently cached
        entries_to_remove = len(all_keys) // 10
        removed_count = 0
        
        for k in all_keys:
            if removed_count >= entries_to_remove:
                break
            if k not in keys_in_cache:
                del m_frequency[k]
                del m_access_time[k]
                removed_count += 1
```
2025-11-27 19:39:32,271 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Hit Rate: 0.0199

Example 2:
- Trace ID: 12
- Hit Rate: 0.04

Example 3:
- Trace ID: 25
- Hit Rate: 0.3569

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:39:32 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:39:32,272 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:39:55 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:39:55,853 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:39:55,854 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The previous implementation attempted a Segmented LRU approach (separating "probationary" items from "protected" items). However, the feedback shows extremely poor performance on some traces (Hit Rates of ~1.99% and ~4%). This suggests a few critical flaws:
1.  **Aggressive Eviction of New Items:** By forcing *all* frequency-1 items to be evicted before *any* frequency > 1 items, the cache is susceptible to "frequency accumulation" where old, stale items with `freq=2` stay forever, blocking new, potentially hot content from establishing itself. This causes "cache pollution."
2.  **Lack of Frequency Decay:** Frequencies are capped but never decremented. An item that was popular yesterday stays "protected" forever, choking the cache.
3.  **Inefficient Ghost Management:** The ghost registry management is a bit loose and doesn't actively penalize the cached items when "ghost hits" occur to make room for them.

To fix this, I will implement an **LFU-DA (Least Frequently Used with Dynamic Aging)** algorithm, inspired by algorithms like GDSF and LFU-Aging.

### The Strategy: LFU with Dynamic Aging (LFU-DA)

Instead of hard queues, we will use a unifying `priority` score.
1.  **Priority Calculation:** $Priority = Frequency + CacheAge$.
2.  **Dynamic Aging:** When an object is evicted, we record its priority as the new global "Cache Age" (or base pressure).
3.  **Inflation:** New items enter with a priority equal to the current `CacheAge` + their initial frequency. This ensures they aren't instantly evicted by items with very high accumulated counts, but they also don't displace super-hot items immediately.
4.  **Tie-Breaking:** If priorities are equal, we fall back to standard LRU.

This approach balances Recency (via the dynamic age base) and Frequency (via the count). It naturally handles "scan resistance" because scan items will have low frequency and won't exceed the rising `CacheAge` of the established working set.

```python
import sys

# Global metadata storage
# Key -> [frequency, priority_score, last_access_time]
# We use a list instead of a tuple to allow mutable updates without re-hashing
m_metadata = dict()

# Global state for Dynamic Aging
# This represents the "minimum worth" required to stay in the cache.
# It rises as the cache churns.
m_cache_age = 0.0

def get_victim_score(key, current_time):
    '''
    Calculates the eviction likelihood.
    In LFU-DA, we want to evict the item with the LOWEST priority score.
    However, the framework asks for a "Victim Score" where Higher = Evict.
    
    Therefore, Victim Score = -1 * Priority Score.
    
    Tie-breaking:
    If priorities are equal, we want to evict the LRU item (smallest last_access_time).
    So, combined score = (-Priority, -Last_Access_Time).
    But since we need a single float/number usually, or strict ordering:
    
    We will return a tuple: (-priority, -last_access_time).
    Python compares tuples element by element.
    Higher tuple value => Higher chance of eviction.
    
    Example:
    Item A: Prio 10, Time 100. Score: (-10, -100)
    Item B: Prio 10, Time 90.  Score: (-10, -90)
    
    -10 == -10.
    -90 > -100.
    Item B is "greater" (less negative), so B is evicted.
    This is correct: B is older (Time 90 < Time 100), so B is LRU.
    '''
    if key not in m_metadata:
        return (float('inf'), 0) # Force eviction if metadata missing
        
    freq, priority, last_access = m_metadata[key]
    
    # We want to maximize this score to evict.
    # Smallest priority = Highest Victim Score.
    # Smallest time (LRU) = Highest Victim Score.
    return (-priority, -last_access)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (LFU-DA logic).
    Updates the global Cache Age to the priority of the evicted victim.
    '''
    global m_cache_age
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    # We want to find the key that maximizes the "Victim Score"
    # (which corresponds to minimizing internal Priority).
    best_victim_key = None
    max_victim_score = (-float('inf'), -float('inf'))
    victim_priority = 0.0
    
    for key in candidate_keys:
        # returns (-priority, -last_access)
        score_tuple = get_victim_score(key, current_time)
        
        if score_tuple > max_victim_score:
            max_victim_score = score_tuple
            best_victim_key = key
            # Extract the actual priority from the negative tuple
            victim_priority = -score_tuple[0]

    # LFU-DA Critical Step:
    # The cache's "pressure" rises to the level of the item we just kicked out.
    # Any new item must eventually beat this score to survive.
    if best_victim_key is not None:
        m_cache_age = victim_priority
        
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Priority = CacheAge + Frequency.
       (Note: We re-add CacheAge to ensure hot items stay above the rising water level)
    3. Update Access Time.
    '''
    global m_metadata, m_cache_age
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, _, _ = m_metadata[obj.key]
        new_freq = freq + 1
        
        # Priority Update Rule:
        # Standard LFU-DA sets Priority = CacheAge + Frequency.
        # This bumps the item up above the current eviction threshold.
        new_priority = m_cache_age + new_freq
        
        m_metadata[obj.key] = [new_freq, new_priority, current_time]
    else:
        # Fallback for weird edge cases where key exists in cache but not metadata
        # Treat as new insert
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize Frequency = 1.
    2. Initialize Priority = CacheAge + 1.
    This gives the new item a fighting chance (starts at current pressure level),
    but if it isn't accessed again, it will be among the first to go as CacheAge rises.
    '''
    global m_metadata, m_cache_age
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    # The new item starts "floating" at the current water level
    initial_priority = m_cache_age + initial_freq
    
    m_metadata[obj.key] = [initial_freq, initial_priority, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata to save memory. 
    (LFU-DA doesn't strictly require a ghost list, relying on m_cache_age instead).
    '''
    global m_metadata
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
```
2025-11-27 19:40:17,016 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 6
- Hit Rate: 0.2715

Example 2:
- Trace ID: 38
- Hit Rate: 0.1255

Example 3:
- Trace ID: 2
- Hit Rate: 0.4454

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:40:17 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:40:17,017 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:40:34 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:40:34,213 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:40:34,214 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To minimize cache miss rates across diverse traces, we need an algorithm that balances **Recency** (LRU) and **Frequency** (LFU). The current implementation is a static LFU with LRU tie-breaking, which suffers from two main issues:
1.  **Cache Pollution:** One-time access scans can fill the cache with items that have a frequency of 1, evicting useful items.
2.  **Ghost Effect:** Items that were popular long ago (high frequency) stick around even if they haven't been accessed recently.

I will implement a variant of **ARC (Adaptive Replacement Cache)**. However, since we cannot change the underlying cache structure (two lists T1 and T2), we will implement a simplified, robust approximation called **SLRU (Segmented LRU)** or a **2-Queue approach**.

### Strategy: The 2-Queue (Segmented) Approach

We will conceptually divide the cache into two segments:
1.  **Probationary Segment (Recent):** New items enter here. If they are not accessed again quickly, they are evicted. This protects the main cache from one-time scans.
2.  **Protected Segment (Frequent):** If an item in the probationary segment is hit again, it is "promoted" to this segment. Items here stay longer.

To implement this without changing the `cache_snapshot` class structure, we will use our metadata dictionaries to track which "segment" an object belongs to and manage eviction based on that.

**Metadata Design:**
*   `m_key_access_time`: Stores the last access timestamp (Recency).
*   `m_key_priority`: A simple integer. `0` = Probationary, `1` = Protected.

**Eviction Logic:**
1.  We prefer to evict from the **Probationary (Priority 0)** pool first, using LRU logic (oldest timestamp).
2.  If the Probationary pool is empty (or we want to enforce a size ratio), we evict from the **Protected (Priority 1)** pool, also using LRU logic.

This prevents one-time wonders from pushing out frequently accessed items, significantly improving hit rates on most real-world workloads.

```python
# Metadata dictionaries
# Tracks the last access time (Recency)
m_key_last_access = dict()
# Tracks the "state" of the object: 0 for Probationary, 1 for Protected
m_key_status = dict()

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) approximation.
    
    Priority 1: Evict from the Probationary (Status 0) pool first. 
                These are items accessed only once recently.
    Priority 2: If no Probationary items exist, evict from Protected (Status 1).
    
    Within each pool, we evict the Least Recently Used (LRU) item.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We search for the best candidate. 
    # Logic: Minimize (Status, Last_Access_Time).
    # Since we want to evict Status 0 before Status 1, and older Time before newer Time.
    
    victim_key = None
    victim_status = float('inf')
    victim_time = float('inf')

    for key in candidate_keys:
        # Default to 0 (Probationary) and 0 time if missing (shouldn't happen)
        curr_status = m_key_status.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)
        
        # We want the smallest status (0 vs 1)
        if curr_status < victim_status:
            victim_key = key
            victim_status = curr_status
            victim_time = curr_time
        elif curr_status == victim_status:
            # Tie-break with LRU (smallest time)
            if curr_time < victim_time:
                victim_key = key
                victim_time = curr_time
    
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency.
    2. Promote the object to 'Protected' (Status 1) because it has proved its utility.
    '''
    global m_key_last_access, m_key_status
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Promotion: If it was Probationary (0), it becomes Protected (1).
    # If it was already Protected, it stays Protected.
    m_key_status[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize recency.
    2. Mark as 'Probationary' (Status 0). It needs another hit to prove worthiness.
    '''
    global m_key_last_access, m_key_status
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Initialize Status as Probationary
    m_key_status[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata.
    '''
    global m_key_last_access, m_key_status
    
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
        
    if evicted_obj.key in m_key_status:
        del m_key_status[evicted_obj.key]
```
2025-11-27 19:40:50,032 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 29
- Hit Rate: 0.0386

Example 2:
- Trace ID: 32
- Hit Rate: 0.0

Example 3:
- Trace ID: 3
- Hit Rate: 0.436

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:40:50 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:40:50,033 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:41:17 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:17,125 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:17,126 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the poor performance seen in the traces (especially the 0.0% hit rate in Trace 32). The previous algorithm suffered from a few critical flaws:
1.  **Unbounded Linear Scans:** Iterating over the entire cache in `evict` is computationally expensive ($O(N)$), which is generally discouraged in high-performance caching, though permissible here, it limits the logic complexity we can apply per item.
2.  **Weak Adaptability:** The formula $Freq / \log(\Delta t)$ is a static heuristic. It struggles with "scan" patterns (where items are accessed once and never again) because `Freq=1` items might sit in the cache too long if `log(delta)` grows slowly.
3.  **No Size Awareness:** The previous logic ignored object size. In byte-addressed caches, evicting one large, unpopular item is often better than evicting ten small, moderately popular items, as it clears space for more incoming data.

### Improved Strategy: Adaptive LFU/LRU with Size Penalty (GDSF-like)

We will implement a variant of **GDSF (Greedy Dual-Size Frequency)** combined with a **Ghost Cache (ARC-like)** mechanism.

**Key Features:**
1.  **Score-based Eviction:** We calculate a utility score $H(p) = (C + Freq \times W) / Size$.
    *   $Freq$: Access frequency (popularity).
    *   $Size$: Object size (cost).
    *   $C$: An "inflation value" (aging factor) that updates every time we evict. This naturally ages out old objects without needing to iterate through the whole cache to decrement counters.
2.  **Size Awareness:** By dividing by size, we penalize large objects. A large object needs a much higher frequency to justify its stay in the cache compared to a small object.
3.  **Ghost Cache (History):** We track keys that were recently evicted. If a key from the ghost cache is requested again, it indicates our eviction criteria was too aggressive for that item. We "boost" its starting frequency upon re-insertion to protect it.

```python
import math

# --- Metadata Storage ---
# Stores the number of times a key has been accessed (Frequency).
# Key -> Integer
m_key_frequency = dict()

# Stores the logical clock time of the last access (Recency).
# Key -> Integer
m_key_last_access = dict()

# Global "Inflation" value (L).
# This represents the "cost" or "priority" of the last evicted item.
# New items must beat this value (plus their own merit) to stay.
# This serves as an aging mechanism.
m_inflation_value = 0.0

# --- Constants ---
MAX_HISTORY_KEYS = 100000  # Cap metadata to prevent memory explosion

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest utility score.
    Algorithm: Greedy Dual-Size Frequency (GDSF) variant.
    
    Score(i) = (Inflation_L + Frequency(i)) / Size(i)
    
    Rationale:
    1. We want to keep items with high Frequency.
    2. We want to evict items that are Large (high Size costs more space).
    3. The Inflation_L acts as a dynamic aging factor. As better items are evicted,
       L increases, making it harder for old, low-frequency items to stay.
    '''
    global m_key_frequency, m_inflation_value, m_key_last_access

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    # We create a tie-breaker based on recency (LRU) to handle items with equal GDSF scores.
    # If scores are equal, we evict the one accessed furthest in the past.
    
    for key in candidate_keys:
        cached_obj = cache_snapshot.cache[key]
        freq = m_key_frequency.get(key, 1)
        size = cached_obj.size
        
        # Protect against division by zero or extremely small sizes artificially inflating score
        if size <= 0: size = 1
        
        # The GDSF Score
        # Note: We use the frequency stored in metadata.
        # We calculate "value per byte".
        score = (m_inflation_value + freq) / size
        
        if score < min_score:
            min_score = score
            victim_key = key
        elif score == min_score:
            # Tie-breaker: LRU
            # If scores are identical, pick the one with older access time
            victim_lru = m_key_last_access.get(victim_key, 0)
            current_lru = m_key_last_access.get(key, 0)
            if current_lru < victim_lru:
                victim_key = key

    # Update the global inflation value to the score of the item we just decided to evict.
    # This effectively "ages" all other items in the cache relative to this score.
    # New items will enter with a base score derived from this L.
    m_inflation_value = min_score

    return victim_key

def _cleanup_metadata(current_cache_keys):
    '''
    Periodically cleans up metadata for keys that are no longer in the cache
    and haven't been seen in a long time to prevent memory leaks.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Identify keys in metadata but NOT in cache (Ghost keys)
        # We want to keep the most recent ghost keys, remove the old ones.
        
        # Create a list of (key, last_access)
        candidates = []
        for k, t in m_key_last_access.items():
            if k not in current_cache_keys:
                candidates.append((k, t))
        
        # Sort by access time (oldest first)
        candidates.sort(key=lambda x: x[1])
        
        # Remove the oldest chunk to get back under the limit
        num_to_remove = len(m_key_frequency) - (MAX_HISTORY_KEYS // 2)
        if num_to_remove > 0:
            for i in range(min(len(candidates), num_to_remove)):
                k_to_del = candidates[i][0]
                if k_to_del in m_key_frequency: del m_key_frequency[k_to_del]
                if k_to_del in m_key_last_access: del m_key_last_access[k_to_del]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    '''
    global m_key_frequency, m_key_last_access
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Standard Frequency Increment
    m_key_frequency[key] = m_key_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check if key is in history (Ghost Cache).
    2. If in history, restore frequency (it's a "working set" item coming back).
    3. If new, initialize frequency.
    '''
    global m_key_frequency, m_key_last_access, m_inflation_value
    
    key = obj.key
    m_key_last_access[key] = cache_snapshot.access_count
    
    if key in m_key_frequency:
        # HIT in GHOST CACHE
        # This item was evicted recently but requested again.
        # This implies our eviction threshold (Inflation Value L) was too aggressive for this item.
        # We increment its frequency to give it a better survival chance this time.
        m_key_frequency[key] += 1
    else:
        # COLD MISS
        # New item. Initialize frequency to 1.
        m_key_frequency[key] = 1
        
        # Periodic cleanup check
        if len(m_key_frequency) > MAX_HISTORY_KEYS:
            _cleanup_metadata(cache_snapshot.cache)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    We do NOT delete the metadata. We keep it as "Ghost" entries.
    This allows `update_after_insert` to recognize returning items.
    
    The 'm_inflation_value' is already updated inside 'evict' because
    that is where the calculation of the minimum score happens.
    '''
    pass
```
2025-11-27 19:41:27,187 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 23
- Hit Rate: 0.0587

Example 2:
- Trace ID: 30
- Hit Rate: 0.0456

Example 3:
- Trace ID: 45
- Hit Rate: 0.0741

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:41:27 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:41:27,188 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:41:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:51,079 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:41:51,081 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The current hit rates (4-7%) suggest that the existing LFU-based implementation is struggling significantly. It likely suffers from two main issues:
1.  **Frequency Accumulation Latency:** New items take too long to build up "frequency" to survive, so they are evicted before they become useful (the "cold start" problem), despite the ghost registry.
2.  **Scan Overhead:** The `evict` function iterates over the *entire* cache ($O(N)$) to find the minimum score. While Python is fast, an algorithmic complexity of $O(N)$ per eviction is generally inefficient compared to $O(1)$ or $O(\log N)$ approaches like LRU or segmented lists.

To drastically improve performance, we should switch to a **Segmented LRU (SLRU)** approach, or more specifically, a simplified version of **S3-FIFO (Simple, Static, Scalable FIFO)** or a **2-Queue (2Q)** algorithm. These algorithms are generally superior to pure LFU or pure LRU for general workloads because they separate "probationary" items (scanned once) from "protected" items (scanned frequently).

Given the constraints and the need for a robust, high-performance algorithm without complex data structures (like linked lists which are hard to manage purely with dictionaries in this specific constrained environment), **S3-FIFO** is an excellent choice. It uses a small FIFO queue for new items and a large FIFO queue for main items, moving items between them based on access bits.

However, since we are managing metadata dictionaries manually, a **SLRU (Segmented LRU)** logic implemented via logical timestamps is often easier to implement robustly in this specific framework while providing high hit rates. We will divide the cache logically into a "Probationary" segment and a "Protected" segment.

### The Algorithm: Segmented LRU with Dynamic Promotion

1.  **Probationary Segment:** New items enter here. They are easy to evict.
2.  **Protected Segment:** Items that receive a hit while in Probation are promoted here. They are hard to evict.
3.  **Eviction Logic:** We prioritize evicting from the Probationary segment first (LRU within that segment). Only if the Probationary segment is empty do we evict from Protected.

Here is the complete improved implementation:

```python
import random

# Metadata dictionaries
# m_access_time: Stores the logical access time (cache_snapshot.access_count) for LRU tracking.
m_access_time = dict()

# m_segment: Tracks which segment a key belongs to.
# 0 = Probationary (easy to evict)
# 1 = Protected (hard to evict)
m_segment = dict()

# m_hits: Tracks hit counts specifically to decide promotion.
m_hits = dict()

# Configuration constants
# We reserve roughly 80% of capacity for the Protected segment logically,
# though physically they reside in the same cache structure.
PROTECTED_RATIO = 0.8 

def evict(cache_snapshot, obj):
    '''
    Implements a Segmented Eviction Strategy.
    
    1. Identify candidates in the Probationary segment (Segment 0).
    2. If Probationary items exist, evict the LRU item from that segment.
    3. If no Probationary items exist (rare), evict the LRU item from Protected (Segment 1).
    '''
    global m_access_time, m_segment

    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Separate keys by segment
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        seg = m_segment.get(k, 0)
        if seg == 0:
            probation_keys.append(k)
        else:
            protected_keys.append(k)

    # Strategy: Evict from Probationary first (Recency based)
    victim_key = None
    oldest_time = float('inf')

    # If we have probationary items, find the LRU among them
    if probation_keys:
        # Optimization: To avoid O(N) sort, we scan. 
        # In a real system, we'd use a linked list or heap, but here we scan the sub-list.
        for k in probation_keys:
            t = m_access_time.get(k, 0)
            if t < oldest_time:
                oldest_time = t
                victim_key = k
    
    # If no probationary items, we must cannibalize the Protected segment (LRU)
    elif protected_keys:
        for k in protected_keys:
            t = m_access_time.get(k, 0)
            if t < oldest_time:
                oldest_time = t
                victim_key = k
    
    # Fallback (should not be reached if cache is non-empty)
    if victim_key is None:
        victim_key = candidate_keys[0]

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Update recency (access time).
    2. If the item is in Probation (Segment 0), promote it to Protected (Segment 1).
    '''
    global m_access_time, m_segment, m_hits
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    m_hits[obj.key] = m_hits.get(obj.key, 0) + 1
    
    # Promotion Logic:
    # If it's currently in probation, a hit indicates it's useful. Promote it.
    if m_segment.get(obj.key, 0) == 0:
        m_segment[obj.key] = 1

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Initialize recency.
    2. Place new item in Probation (Segment 0).
    '''
    global m_access_time, m_segment, m_hits
    
    current_time = cache_snapshot.access_count
    m_access_time[obj.key] = current_time
    
    # New items always start in Probation (Segment 0)
    # They must "prove" themselves with a subsequent hit to get promoted.
    m_segment[obj.key] = 0
    m_hits[obj.key] = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    
    Note: In SLRU, if a Protected item is evicted, it is usually demoted
    or removed entirely. Here we simply remove metadata.
    '''
    global m_access_time, m_segment, m_hits
    
    key = evicted_obj.key
    
    if key in m_access_time:
        del m_access_time[key]
    
    if key in m_segment:
        del m_segment[key]

    if key in m_hits:
        del m_hits[key]
```
2025-11-27 19:44:04,664 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 39
- Hit Rate: 0.0222

Example 2:
- Trace ID: 11
- Hit Rate: 0.3832

Example 3:
- Trace ID: 13
- Hit Rate: 0.0

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:44:04 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:44:04,665 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:45:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:45:28,709 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:45:28,710 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to move away from the simple decay-based scoring used in the previous implementation, which proved ineffective (0% hit rate on Trace 13). The catastrophic failure on some traces suggests the algorithm was vulnerable to "cache pollution" (scans of one-time objects filling the cache) or "thrashing" (looping patterns larger than the cache).

### Strategy: GDSF (Greedy Dual-Size Frequency) with Ghost History
I will implement a variation of the **GDSF** algorithm. This is a highly effective algorithm for web caching that naturally balances **Recency** and **Frequency**.

**Why GDSF?**
1.  **Scan Resistance:** In the previous code, a new item (freq 1) could evict an older item (freq 10) if the older item hadn't been touched in a while. In GDSF, new items are assigned a priority relative to a global "aging" factor `L`. If a new item has low frequency, it starts with a low priority and is evicted almost immediately if the cache is full of valuable items.
2.  **Frequency Accumulation (Ghost Cache):** We will maintain a frequency map (`m_freq`) that persists even after items are evicted. If an item is evicted and requested again (a loop), it remembers its previous popularity, gets a higher priority, and stays in the cache longer the second time.
3.  **Dynamic Aging:** Instead of a complex time-decay formula, GDSF uses an inflation value `L`. When an item is evicted, `L` is updated to the priority of the evicted item. This effectively "ages" all other items in the cache without needing to iterate and update them constantly.

### The Algorithm Logic
*   **Priority Calculation:** `Priority = L + Frequency`.
*   **Eviction:** Evict the object with the lowest Priority.
*   **Aging:** Set `L` = Priority of the evicted object.
*   **Ties:** Broken by Recency (LRU).

### Improved Implementation

```python
import math

# Global Metadata
# Tracks the frequency of objects (Lifetime frequency, including ghost history)
m_freq = dict()

# Tracks the calculated Priority of objects currently IN the cache
# Priority = L + Frequency
m_priority = dict()

# Tracks the last access time (for LRU tie-breaking)
m_last_access = dict()

# Global "Inflation" value (The aging factor)
# This represents the minimum priority required to survive in the cache recently.
GDSF_L = 0.0

# Constants for cleanup
MAX_HISTORY_SIZE = 100000

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Priority (GDSF Score).
    Tie-breaker: Least Recently Used (LRU).
    '''
    global m_priority, m_last_access, GDSF_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We look for the item with the minimum Priority.
    # If priorities are equal, we pick the one with the smallest last_access (LRU).
    victim_key = None
    min_priority = float('inf')
    min_access_time = float('inf')
    
    # Linear scan to find victim (O(N) - acceptable per prompt context)
    for key in candidate_keys:
        # Default priority is L + 1 if somehow missing, but should be in map
        p = m_priority.get(key, GDSF_L + 1.0)
        access_time = m_last_access.get(key, 0)
        
        if p < min_priority:
            min_priority = p
            min_access_time = access_time
            victim_key = key
        elif p == min_priority:
            # Tie-break using LRU (Strict < means older time wins)
            if access_time < min_access_time:
                min_access_time = access_time
                victim_key = key
    
    # GDSF Aging Mechanism:
    # The system "inflation" value L rises to the priority of the item we just evicted.
    # This ensures that future items must possess a higher frequency to survive.
    if victim_key is not None:
        GDSF_L = min_priority
            
    return victim_key

def _cleanup_metadata(cache_snapshot):
    '''
    Prevents metadata memory leaks by pruning the frequency history.
    '''
    global m_freq, m_priority, m_last_access, MAX_HISTORY_SIZE
    
    # Only clean up if history grows too large
    if len(m_freq) > MAX_HISTORY_SIZE:
        # Simple heuristic: remove items with low frequency that are NOT in the cache
        keys_to_delete = []
        count = 0
        
        # We iterate and delete a chunk to reduce size
        for k, v in m_freq.items():
            if k not in cache_snapshot.cache:
                # Purge items seen only once or twice
                if v <= 2:
                    keys_to_delete.append(k)
                    count += 1
                    if count > 5000: # Delete in batches
                        break
        
        for k in keys_to_delete:
            del m_freq[k]
            if k in m_last_access: del m_last_access[k]
            # m_priority only contains cached items, so no need to clean based on this

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Re-calculate Priority using current L.
    3. Update Recency.
    '''
    global m_freq, m_priority, m_last_access, GDSF_L
    
    key = obj.key
    
    # 1. Update Frequency
    m_freq[key] = m_freq.get(key, 0) + 1
    
    # 2. Update Priority (GDSF: Priority = L + Frequency)
    # Note: We reset the item's priority relative to the current inflation L.
    # This brings the item "forward" in the priority queue.
    m_priority[key] = GDSF_L + m_freq[key]
    
    # 3. Update Recency
    m_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize/Restore Frequency (Ghost Cache).
    2. Calculate Initial Priority.
    3. Update Recency.
    '''
    global m_freq, m_priority, m_last_access, GDSF_L
    
    key = obj.key
    
    # 1. Frequency (Ghost Cache check)
    # If it was in history, we recover its count. If new, it starts at 1.
    if key not in m_freq:
        m_freq[key] = 1
    else:
        # It's a re-insertion (churn). We acknowledge it was seen before.
        # We don't increment here strictly, usually insert implies 1st access of session,
        # but logic dictates the access happened.
        # However, to be consistent with 'hit', we rely on the frequency accumulated so far.
        # We ensure it's at least 1.
        if m_freq[key] < 1: m_freq[key] = 1
            
    # 2. Calculate Priority
    # New items enter with Priority = Current L + Frequency.
    # If it's a scan (freq=1), P = L + 1. It will likely be evicted next if L is high.
    m_priority[key] = GDSF_L + m_freq[key]
    
    # 3. Update Recency
    m_last_access[key] = cache_snapshot.access_count
    
    # Periodic cleanup
    if len(m_freq) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active priority map.
    2. Remove from recency map.
    3. KEEP in frequency map (Ghost Cache).
    '''
    global m_priority, m_last_access
    
    if evicted_obj and evicted_obj.key in m_priority:
        del m_priority[evicted_obj.key]
    
    if evicted_obj and evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    
    # Do NOT delete from m_freq. This allows us to recognize "working set" items
    # that return after being evicted.
```
2025-11-27 19:45:44,167 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 9
- Hit Rate: 0.531

Example 2:
- Trace ID: 18
- Hit Rate: 0.0226

Example 3:
- Trace ID: 3
- Hit Rate: 0.443

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:45:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:45:44,168 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:46:12 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:46:12,606 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:46:12,607 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the weaknesses in the previous implementation. The previous "Segmented LRU" approach is good, but it can be rigid. Specifically:
1.  **Scan Resistance:** While it handled scan resistance (frequency=1), strictly prioritizing eviction of *all* freq=1 items before *any* freq>1 items can hurt when the working set changes rapidly. A very old freq=2 item shouldn't necessarily outlive a very recent freq=1 item if the cache is thrashing.
2.  **Frequency Saturation:** Simply capping frequency doesn't account for "aging" well. An item that was popular 1,000 requests ago but hasn't been seen since is currently treated as "valuable" because of its high frequency count.
3.  **Variable Object Sizes:** The prompt mentions `obj.size`. The previous implementation ignored object size. In many real-world caches (like web caches), evicting large items can be beneficial because it clears space for *multiple* smaller items, potentially increasing the total number of hits. However, purely greedy size-based eviction hurts hit rates for large, popular files. A balance is needed (like GDSF - Greedy Dual Size Frequency).

### Improved Strategy: Adaptive Greedy Dual-Size Frequency (GDSF) Variant

We will implement a variant of **GDSF (Greedy Dual-Size Frequency)** combined with an aging factor. This is a state-of-the-art algorithm for variable-size web caches, but it works excellently for uniform sizes too.

**The Core Logic:**
Each object is assigned a priority (H-value):
$$ H = L + \frac{Frequency \times Weight}{Size} $$

*   **L (Inflation Value):** A running "base clock" for the cache. When an object is evicted, $L$ is updated to the $H$ value of the evicted object. This provides the "aging" mechanism. New objects start with a low base $H$, but as $L$ rises over time, older objects that haven't been accessed (and thus haven't had their $H$ recalculated) fall behind the current $L$.
*   **Frequency:** How often the object is hit.
*   **Size:** Smaller objects get a boost (more "density" of hits per byte).
*   **Weight:** We will add a small boost for "Ghost" hits to implement adaptivity.

**Why this wins:**
1.  **Integrates Frequency and Recency:** Recency is handled by the $L$ factor (aging). Frequency is explicit.
2.  **Handles Size:** It penalizes large objects naturally, optimizing the hit rate by keeping more small, hot objects.
3.  **Decay:** The inflation of $L$ ensures that "formerly hot" items eventually get evicted if they stop being accessed.

Here is the implementation:

```python
import math

# Global metadata to store eviction info
# Key -> (H_score, Frequency, Size)
m_metadata = dict()

# Global variable to track the "aging" factor (L)
# This represents the priority of the most recently evicted item.
m_inflation_L = 0.0

# Ghost registry to detect trends and give 'second chances'
# Key -> timestamp of eviction
m_ghost_registry = dict()

def calculate_priority(freq, size, current_L):
    '''
    Calculates the GDSF priority score.
    H = L + (Frequency / Size)
    
    We add a tiny constant to size to avoid division by zero if size is 0 
    (though typically size >= 1).
    We use a weighting factor for Frequency to tune how aggressive we are on LFU vs LRU.
    '''
    # Cost function: We want to keep items that provide the most hits per byte occupied.
    # Standard GDSF uses cost = 1. So priority = L + Freq * (1/Size).
    # We use a slight logarithm on size to prevent overly penalizing large items 
    # if the trace has massive size variance, but usually 1/size is optimal for hit rate.
    
    # Check for zero size to be safe
    safe_size = max(1, size)
    
    # Priority formula
    priority = current_L + (freq / safe_size)
    return priority

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the LOWEST H-value (Priority).
    In GDSF, the "victim" is the item with the smallest priority score.
    When we evict, we update the global inflation factor L to the victim's priority.
    '''
    global m_metadata, m_inflation_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    min_priority = float('inf')
    victim_key = None
    
    # Find the object with the minimum priority score
    # Tie-breaking: If priorities are equal, we can use LRU (recency) logic implicitly 
    # if we stored access time, but strictly following the math is usually sufficient.
    for key in candidate_keys:
        if key in m_metadata:
            h_score, _, _ = m_metadata[key]
            if h_score < min_priority:
                min_priority = h_score
                victim_key = key
        else:
            # Metadata missing? Should be evicted first.
            return key
            
    # Update the global inflation factor L
    # This acts as the "Aging" mechanism. All future items will need a priority > this to survive.
    if victim_key is not None:
        m_inflation_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Frequency.
    2. Recalculate Priority (H-value) using current L.
    '''
    global m_metadata, m_inflation_L
    
    # Retrieve existing data. 
    # If not found (shouldn't happen on hit), default to freq=0
    if obj.key in m_metadata:
        _, freq, size = m_metadata[obj.key]
    else:
        freq = 0
        size = obj.size
        
    new_freq = freq + 1
    
    # Calculate new priority
    # Note: We use the current global L. This "restores" the object's youth.
    new_priority = calculate_priority(new_freq, size, m_inflation_L)
    
    m_metadata[obj.key] = (new_priority, new_freq, size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry for "Second Chance".
    2. specific Frequency.
    3. Calculate Priority.
    '''
    global m_metadata, m_inflation_L, m_ghost_registry
    
    freq = 1
    
    # Check if recently evicted
    if obj.key in m_ghost_registry:
        # It was a mistake to evict this.
        # We restore a higher frequency to give it a survival boost.
        # We don't restore the EXACT old frequency because that might be too sticky,
        # but we give it a boost to escape immediate eviction.
        freq = 2
        del m_ghost_registry[obj.key]
        
    # Calculate priority
    # New items get priority based on current L + their intrinsic value (freq/size)
    priority = calculate_priority(freq, obj.size, m_inflation_L)
    
    m_metadata[obj.key] = (priority, freq, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
        
    # Add to ghost registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prune ghost registry if it gets too large
    # A good rule of thumb is keeping it roughly the size of the cache capacity (in counts)
    # or a fixed limit to prevent memory leaks.
    if len(m_ghost_registry) > 2000: # Arbitrary safety limit or based on cache len
         # Remove a chunk of keys to keep it fast
         keys_to_remove = list(m_ghost_registry.keys())[:100]
         for k in keys_to_remove:
             del m_ghost_registry[k]
```
2025-11-27 19:46:24,436 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 6
- Hit Rate: 0.2727

Example 2:
- Trace ID: 19
- Hit Rate: 0.2722

Example 3:
- Trace ID: 20
- Hit Rate: 0.3599

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:46:24 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:46:24,437 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:46:45 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:46:45,881 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:46:45,882 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the limitations of the previous Segmented LRU implementation. While SLRU is good, strict frequency partitioning can sometimes be too rigid for variable workloads.

The improved algorithm below implements a **Adaptive Replacement Cache (ARC)** inspired strategy, adapted for this specific Python context. This is generally superior to LRU and SLRU because it dynamically balances between **Recency** (LRU) and **Frequency** (LFU) based on the workload's actual behavior.

### Key Concepts in the New Design:
1.  **Dual Lists (virtual):** We implicitly divide the cache into two lists:
    *   $T_1$: Items accessed only once recently (Recency focus).
    *   $T_2$: Items accessed at least twice recently (Frequency focus).
2.  **Ghost Lists:** We track evicted keys in two "ghost" registries:
    *   $B_1$: Keys evicted from $T_1$.
    *   $B_2$: Keys evicted from $T_2$.
3.  **Adaptive Parameter ($p$):** A target size for $T_1$.
    *   If we get a hit in the ghost list $B_1$, it means we should have kept more recent items. We increase $p$ (grow $T_1$).
    *   If we get a hit in the ghost list $B_2$, it means we should have kept more frequent items. We decrease $p$ (shrink $T_1$, grow $T_2$).
4.  **Eviction Logic:** We evict from $T_1$ if its size exceeds the adaptive target $p$, otherwise we evict from $T_2$.

### Improved Implementation

```python
import math

# --- Metadata Storage ---
# Stores tuples: (access_frequency, last_access_time, is_in_t2_list)
# is_in_t2_list is a boolean indicating if the item is considered part of the "Frequent" set.
m_metadata = dict()

# Ghost Registries (Store keys only to save memory)
# B1: Recently evicted from the "Recent/T1" list
m_ghost_b1 = set()
# B2: Recently evicted from the "Frequent/T2" list
m_ghost_b2 = set()

# Adaptive Parameter 'p'
# Target size for the T1 (Recent) list.
# 0 <= p <= capacity
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on the Adaptive Replacement Cache (ARC) logic.
    We compare the size of T1 (recent items) against the adaptive parameter 'p'.
    '''
    global m_metadata, m_p, m_ghost_b1, m_ghost_b2

    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
    
    # Classify current cache content into T1 (Recent) and T2 (Frequent)
    t1_keys = []
    t2_keys = []
    
    for k in candidate_keys:
        if k in m_metadata:
            _, last_access, in_t2 = m_metadata[k]
            if in_t2:
                t2_keys.append((k, last_access))
            else:
                t1_keys.append((k, last_access))
        else:
            # Fallback for untracked items (treat as T1/Recent)
            t1_keys.append((k, 0))

    # Determine eviction candidate based on adaptive parameter 'p'
    # logic: if len(T1) > p, we evict the LRU of T1. Else, evict LRU of T2.
    # Note: We must ensure we don't try to evict from an empty list.
    
    victim_key = None
    
    if len(t1_keys) > 0 and (len(t1_keys) > m_p or len(t2_keys) == 0):
        # Evict LRU from T1
        # Sort by last_access ascending (oldest first)
        t1_keys.sort(key=lambda x: x[1])
        victim_key = t1_keys[0][0]
    else:
        # Evict LRU from T2
        t2_keys.sort(key=lambda x: x[1])
        victim_key = t2_keys[0][0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it was in T1 (Recent), move it to T2 (Frequent).
    If it was in T2, update its recency.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        freq, _, in_t2 = m_metadata[obj.key]
        
        # Whether it was T1 or T2, a hit implies it's valuable.
        # It now belongs/stays in T2 (Frequent list).
        m_metadata[obj.key] = (freq + 1, current_time, True)
    else:
        # Should rarely happen on a hit, but safety fallback
        m_metadata[obj.key] = (1, current_time, False)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    1. Check Ghost lists (B1, B2) to adapt parameter 'p'.
    2. Insert new metadata.
    '''
    global m_metadata, m_p, m_ghost_b1, m_ghost_b2
    
    current_time = cache_snapshot.access_count
    capacity = len(cache_snapshot.cache) # Approximation of capacity in objects
    if capacity == 0: capacity = 1 # avoid div by zero
    
    # --- Adapt 'p' based on ghost hits ---
    if obj.key in m_ghost_b1:
        # Hit in B1 (Recency Ghost): We should have made T1 larger.
        # Increase p.
        delta = 1
        if len(m_ghost_b1) < len(m_ghost_b2):
             delta = len(m_ghost_b2) // len(m_ghost_b1)
        m_p = min(capacity, m_p + delta)
        m_ghost_b1.remove(obj.key)
        
        # It returns to cache as a T2 item (freq > 1 conceptually, though this is a simplification)
        # Standard ARC puts it in T2 because it's been seen twice (once evicted, once now)
        is_t2 = True 
        
    elif obj.key in m_ghost_b2:
        # Hit in B2 (Frequency Ghost): We should have made T2 larger (T1 smaller).
        # Decrease p.
        delta = 1
        if len(m_ghost_b2) < len(m_ghost_b1):
            delta = len(m_ghost_b1) // len(m_ghost_b2)
        m_p = max(0, m_p - delta)
        m_ghost_b2.remove(obj.key)
        
        # It returns to cache as T2
        is_t2 = True
        
    else:
        # Brand new item, never seen recently.
        # Starts in T1 (Recent list).
        is_t2 = False

    # Initialize metadata
    # We use 1 as initial freq if new, or 2 if recovered from ghost
    initial_freq = 2 if is_t2 else 1
    m_metadata[obj.key] = (initial_freq, current_time, is_t2)
    
    # Initialize p if this is the very first insertion ever
    if m_p == 0 and len(m_metadata) == 1:
        m_p = 0 # Start with preference for frequency, will adapt up

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Identify if victim was T1 or T2.
    2. Move key to corresponding Ghost List (B1 or B2).
    3. Clean metadata.
    '''
    global m_metadata, m_ghost_b1, m_ghost_b2
    
    key = evicted_obj.key
    
    if key in m_metadata:
        _, _, in_t2 = m_metadata[key]
        
        if in_t2:
            # Evicted from T2 -> Add to B2
            m_ghost_b2.add(key)
        else:
            # Evicted from T1 -> Add to B1
            m_ghost_b1.add(key)
            
        del m_metadata[key]
        
    # --- Maintenance: Bound Ghost Lists ---
    # To prevent memory leaks, keep ghost lists roughly size of cache capacity
    max_ghost_size = cache_snapshot.capacity # using capacity as proxy for max items logic
    # Since we can't easily check exact length vs capacity in bytes, we use a heuristic based on cache count
    target_len = len(cache_snapshot.cache) 
    
    # Simple FIFO pruning for sets is hard, but we can prevent unbounded growth
    # by clearing if they get absurdly large, or just relying on Python's memory management 
    # for string keys. For strictness, we can check size:
    if len(m_ghost_b1) > target_len * 2:
        m_ghost_b1.clear() # drastic but safe
    if len(m_ghost_b2) > target_len * 2:
        m_ghost_b2.clear()
```
2025-11-27 19:46:56,539 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 4
- Hit Rate: 0.5062

Example 2:
- Trace ID: 29
- Hit Rate: 0.0386

Example 3:
- Trace ID: 7
- Hit Rate: 0.4786

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:46:56 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:46:56,540 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:49:20 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:49:20,354 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:49:20,355 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The improved cache eviction algorithm implements the **Adaptive Replacement Cache (ARC)** logic. ARC dynamically balances between Recency (T1) and Frequency (T2) by adjusting a target size parameter `p` based on hits in the "ghost" registries (history of evicted items). This approach provides superior scan resistance and adapts to changing workloads, addressing the issues seen in the performance feedback (e.g., low hit rates on scan/loop patterns).

```python
import math

# --- ARC (Adaptive Replacement Cache) Global State ---

# m_meta stores metadata for items currently in the cache.
# Key -> {'type': 1 (T1/Recent) or 2 (T2/Frequent), 'time': access_count}
m_meta = dict()

# Ghost Registries (Simulated B1 and B2 from ARC).
# These track keys that were recently evicted.
# m_ghost_b1: Keys evicted from T1 (Recency ghost)
# m_ghost_b2: Keys evicted from T2 (Frequency ghost)
# We use Python dicts to maintain insertion order (effectively FIFO/LRU).
m_ghost_b1 = dict() 
m_ghost_b2 = dict()

# m_p is the adaptive parameter representing the target size of the T1 (Recent) list.
# 0 <= m_p <= Cache Capacity (in items)
m_p = 0.0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on ARC logic.
    We prefer evicting from T1 if it exceeds the target size 'p', 
    or from T2 otherwise, while handling specific recovery conditions.
    '''
    global m_meta, m_p, m_ghost_b2

    # Get all current cache keys
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into T1 (Recent) and T2 (Frequent)
    #    and find the LRU item for each segment in one pass.
    t1_candidates = []
    t2_candidates = []
    
    for k in candidate_keys:
        # Default to T1 if metadata is missing (failsafe)
        meta = m_meta.get(k, {'type': 1, 'time': 0})
        if meta['type'] == 1:
            t1_candidates.append((k, meta['time']))
        else:
            t2_candidates.append((k, meta['time']))
            
    # Identify LRU for T1 and T2 (item with min 'time')
    lru_t1 = min(t1_candidates, key=lambda x: x[1])[0] if t1_candidates else None
    lru_t2 = min(t2_candidates, key=lambda x: x[1])[0] if t2_candidates else None
    
    len_t1 = len(t1_candidates)
    
    # 2. Check if the *incoming* object (obj) is in the ghost of T2 (B2).
    #    This signals we are recovering a frequent item, affecting the decision.
    xt_in_b2 = obj.key in m_ghost_b2
    
    # 3. ARC Decision Logic
    #    Evict from T1 (Recent) if:
    #    a) T1 is larger than its target size m_p.
    #    b) OR, the incoming item is in B2 and T1 has reached exactly m_p.
    evict_from_t1 = False
    
    if len_t1 > 0:
        if len_t1 > m_p:
            evict_from_t1 = True
        elif xt_in_b2 and len_t1 >= int(m_p):
             # When recovering T2, we are more aggressive on T1
             evict_from_t1 = True
    
    # 4. Return the victim
    if evict_from_t1:
        if lru_t1 is not None:
            return lru_t1
        # Fallback: if we wanted to evict T1 but it's empty, evict T2
        return lru_t2
    else:
        if lru_t2 is not None:
            return lru_t2
        # Fallback: if we wanted to evict T2 but it's empty, evict T1
        return lru_t1

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    The item has proven its utility. Move it to T2 (Frequent) 
    and update its access time (MRU).
    '''
    global m_meta
    current_time = cache_snapshot.access_count
    
    # ARC Rule: Any hit (in T1 or T2) moves the item to T2 (MRU position)
    m_meta[obj.key] = {'type': 2, 'time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registries (B1, B2) to adapt parameter 'p'.
    2. Initialize metadata for the new object.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2, m_p
    
    current_time = cache_snapshot.access_count
    
    # Use current item count as a proxy for capacity "C" in ARC formulas
    c = len(cache_snapshot.cache) 
    if c == 0: c = 1 
    
    in_b1 = obj.key in m_ghost_b1
    in_b2 = obj.key in m_ghost_b2
    
    if in_b1:
        # Hit in Ghost T1 (Recency):
        # We evicted a recent item too soon. We should increase T1's target size (p).
        len_b1 = len(m_ghost_b1)
        len_b2 = len(m_ghost_b2)
        
        delta = 1.0
        if len_b1 < len_b2:
            delta = float(len_b2) / len_b1
            
        m_p = min(float(c), m_p + delta)
        
        # This is a "phantom hit", so it enters as a Frequent item (T2)
        m_meta[obj.key] = {'type': 2, 'time': current_time}
        del m_ghost_b1[obj.key]
        
    elif in_b2:
        # Hit in Ghost T2 (Frequency):
        # We evicted a frequent item too soon. We should decrease T1's target size (p)
        # to make more room for T2.
        len_b1 = len(m_ghost_b1)
        len_b2 = len(m_ghost_b2)
        
        delta = 1.0
        if len_b2 < len_b1:
            delta = float(len_b1) / len_b2
            
        m_p = max(0.0, m_p - delta)
        
        # Enters as Frequent item (T2)
        m_meta[obj.key] = {'type': 2, 'time': current_time}
        del m_ghost_b2[obj.key]
        
    else:
        # Brand new item (never seen recently).
        # Enters as Recent item (T1).
        m_meta[obj.key] = {'type': 1, 'time': current_time}

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move the evicted item's key to the corresponding Ghost Registry.
    2. Clean up metadata.
    3. Ensure Ghost Registries stay bounded.
    '''
    global m_meta, m_ghost_b1, m_ghost_b2
    
    # Determine which list the victim came from
    v_type = 1
    if evicted_obj.key in m_meta:
        v_type = m_meta[evicted_obj.key]['type']
        del m_meta[evicted_obj.key] 
    
    # Add to Ghost Registry
    if v_type == 1:
        m_ghost_b1[evicted_obj.key] = True
    else:
        m_ghost_b2[evicted_obj.key] = True
        
    # Maintenance: Prevent unbounded growth of ghost registries.
    # We keep history roughly equal to cache size.
    target_ghost_len = max(len(cache_snapshot.cache), 10)
    
    # Prune oldest entries (Standard Python dict preserves insertion order)
    while len(m_ghost_b1) > target_ghost_len:
        k = next(iter(m_ghost_b1))
        del m_ghost_b1[k]
        
    while len(m_ghost_b2) > target_ghost_len:
        k = next(iter(m_ghost_b2))
        del m_ghost_b2[k]
```
2025-11-27 19:52:00,927 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.8874

Example 2:
- Trace ID: 43
- Hit Rate: 0.2333

Example 3:
- Trace ID: 17
- Hit Rate: 0.0235

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:52:00 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:52:00,928 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:52:26 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:52:26,860 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:52:26,861 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache miss rate significantly across varied workloads (like the ones shown in your feedback: some high hit rates, some very low), we need an algorithm that is robust against different access patterns.

Pure LFU (the current implementation) suffers from "cache pollution" where old, formerly popular items stick around forever. Pure LRU suffers from "scan resistance" where a single one-time scan of a large database wipes out the entire useful cache.

The improved algorithm below implements a simplified version of **ARC (Adaptive Replacement Cache)** or a **Segmented LRU (SLRU)** logic combined with frequency awareness. However, given the constraints of a simple Python implementation without complex double-linked list structures, a **Frequency-Based Replacement (FBR) / "2Q"-like approach** is often the most effective upgrade from LFU/LRU.

We will implement a strategy often called **TinyLFU-lite or "Admission Aware LRU"**. However, since we cannot easily reject admissions (we must `evict` to make space), we will implement a hybrid **Window-TinyLFU** logic using the available hooks.

### The Strategy: "Probation vs. Protected" (Segmented approach)

We will track two types of history:
1.  **Recency (LRU):** When did we last see it?
2.  **Frequency (LFU):** How often do we see it?

**The key improvement:** Instead of deleting frequency metadata immediately upon eviction (`update_after_evict`), we will keep a "Ghost Cache" (or a larger history of frequency counts) for a while. This allows us to distinguish between a "new, one-time" object and an object that was evicted recently but is coming back (a "frequent" object).

### Algorithm Logic:
1.  **Metadata:**
    *   `m_key_frequency`: Persists even after eviction (up to a limit) to remember popularity.
    *   `m_key_last_access`: Standard LRU timestamp.
    *   `m_insertion_time`: To penalize objects that were inserted very recently but haven't been hit again (Short-term pollution).

2.  **Eviction Scoring:**
    *   We calculate a "Utility Score".
    *   If an object is *brand new* (inserted recently and has low frequency), it is a prime candidate for eviction.
    *   If an object is *old* (LRU) and has low frequency, it is also a candidate.
    *   **Tie-breaker:** We heavily penalize objects with frequency = 1.

3.  **Metadata Retention:** In `update_after_evict`, we will *not* delete frequency data immediately. This allows items to "regain" their high frequency status quickly if they are re-inserted.

```python
import math

# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency). 
# We will NOT clear this immediately on eviction to provide "Ghost" history.
m_key_frequency = dict()

# Tracks the last access time of an object (Recency).
m_key_last_access = dict()

# Tracks when the object was first inserted into the cache. 
# Helps identify "short-lived" objects vs "long-lived" objects.
m_key_insertion_time = dict()

# A counter to perform periodic cleanup of the frequency map so it doesn't grow infinitely
cleanup_counter = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Strategy: Weighted Hybrid of Recency and Frequency.
    
    We calculate a "victim score" for every candidate. The higher the score, the more likely
    it is to be evicted.
    
    Score = (1 / Frequency) * Time_Since_Last_Access
    
    Logic:
    1. Low Frequency increases the eviction score significantly.
    2. Large time since last access (LRU) increases the eviction score.
    3. We prefer to evict items with Frequency == 1 (scan resistance).
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    victim_key = None
    max_victim_score = -1.0

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate 'age' in terms of access count
        age = current_time - last_access
        
        # --- SCORING LOGIC ---
        
        # Base score is the age (LRU behavior)
        score = age
        
        # Penalty for low frequency (LFU behavior).
        # If freq is 1, the score remains high (good for eviction).
        # If freq is high, the score shrinks (protected).
        # We square the frequency to protect popular items more aggressively.
        score = score / (freq * freq)
        
        # Special Case: "One-hit wonder" protection.
        # If an item was inserted long ago but never hit again (Freq=1), 
        # it is essentially dead weight. Boost its eviction score.
        if freq == 1:
            score = score * 2.0

        if score > max_victim_score:
            max_victim_score = score
            victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize metadata.
    '''
    global m_key_frequency, m_key_last_access, m_key_insertion_time, cleanup_counter
    
    current_time = cache_snapshot.access_count
    
    # If the key exists in our "Ghost" frequency map (it was evicted recently),
    # we restore that frequency + 1. Otherwise, it starts at 1.
    # This gives a chance for recently evicted popular items to stick better this time.
    previous_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = previous_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = current_time
    m_key_insertion_time[obj.key] = current_time

    # Periodic Cleanup logic for the metadata map to prevent memory leak
    # (Since we are keeping data for evicted items).
    cleanup_counter += 1
    if cleanup_counter > 5000:
        cleanup_metadata(cache_snapshot)
        cleanup_counter = 0

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove Recency data, but KEEP Frequency data (Ghost Cache).
    This allows the algorithm to "remember" items if they return soon.
    '''
    global m_key_last_access, m_key_insertion_time
    
    # We delete recency/time data because if it comes back, it's a new "life".
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
    
    if evicted_obj.key in m_key_insertion_time:
        del m_key_insertion_time[evicted_obj.key]
        
    # NOTE: We intentionally do NOT delete from m_key_frequency here.
    # This acts as our historical frequency tracker.

def cleanup_metadata(cache_snapshot):
    '''
    Helper to prevent m_key_frequency from growing infinitely.
    We remove keys from metadata that are not currently in the cache
    and haven't been seen in a long time.
    '''
    global m_key_frequency
    
    # If the map isn't too big relative to cache capacity, don't bother
    # (Assuming average object size of 1 for rough estimation, or just simple count)
    if len(m_key_frequency) < len(cache_snapshot.cache) * 3:
        return

    keys_to_delete = []
    current_in_cache = cache_snapshot.cache
    
    # A simple way to halve the frequency of ghost items to decay them over time
    # rather than hard deleting, or delete very low freq ghosts.
    for k, freq in m_key_frequency.items():
        if k not in current_in_cache:
            # Decay: If it's not in cache, reduce its "ghost" frequency.
            # If it drops to 0, remove it.
            new_freq = freq // 2
            if new_freq <= 0:
                keys_to_delete.append(k)
            else:
                m_key_frequency[k] = new_freq
                
    for k in keys_to_delete:
        del m_key_frequency[k]
```
2025-11-27 19:54:55,591 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 42
- Hit Rate: 0.0526

Example 2:
- Trace ID: 16
- Hit Rate: 0.0199

Example 3:
- Trace ID: 32
- Hit Rate: 0.029

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:54:55 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:54:55,592 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:56:40 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:56:40,703 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:56:40,704 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the performance feedback, the current ARC implementation is likely suffering from "scan" pollution or implementation mismatches regarding byte-based capacity versus count-based logic. The extremely low hit rates (2-5%) suggest a more robust, scan-resistant algorithm is required.

I will implement the **S3-FIFO (Simple and Scalable Sieve-like FIFO)** eviction algorithm. S3-FIFO is a state-of-the-art algorithm that often outperforms ARC and LRU on web traces, particularly those with variable object sizes and "one-hit wonder" access patterns.

### Key Changes:
1.  **Algorithm Switch**: Replaced ARC with S3-FIFO. This uses a Small (S) queue for new items and a Main (M) queue for frequent items.
2.  **Byte-Awareness**: The `S` queue size is managed based on **byte size** (10% of capacity) rather than object count, aligning with the environment's byte-based capacity constraints.
3.  **Scan Resistance**: New items enter `S` and are only promoted to `M` if accessed again. This prevents rare items ("scans") from polluting the main cache.
4.  **Simplified Metadata**: Removed the complex ghost lists (B1/B2) and adaptive parameter `p`, reducing overhead and potential for logic errors.

```python
from collections import deque

# Global Metadata for S3-FIFO Algorithm

# Queue S: Small/New items (FIFO)
q_small = deque()
# Queue M: Main/Frequent items (FIFO)
q_main = deque()

# Frequency tracking: Maps key -> integer frequency (capped usually at 3)
meta_freq = dict()

# Size tracking: Maps key -> integer size in bytes
meta_sizes = dict()

# Track the current byte size of the S queue to maintain the 10% ratio
current_small_bytes = 0

def evict(cache_snapshot, obj):
    '''
    S3-FIFO Eviction Policy (Byte-aware).
    
    Strategy:
    1. Check if we should evict from the Small queue (S) or Main queue (M).
       - We evict from S if it exceeds 10% of total cache capacity (bytes) OR if M is empty.
    2. Iterate through the chosen queue to find a victim:
       - If the candidate has been accessed (freq > 0), give it a second chance:
         - If in S: Promote to M, reset freq to 0.
         - If in M: Reinsert at tail of M, decrement freq.
       - If the candidate has not been accessed (freq == 0), evict it.
    '''
    global q_small, q_main, meta_freq, meta_sizes, current_small_bytes
    
    capacity = cache_snapshot.capacity
    # Target size for S queue is 10% of total capacity
    target_small = capacity * 0.1
    
    # Safety fallback if cache appears empty
    if not q_small and not q_main:
        if cache_snapshot.cache:
            return list(cache_snapshot.cache.keys())[0]
        return None

    while True:
        # Decision: Evict from Small or Main?
        # Prefer evicting from S if it's over budget or if M is empty
        evict_from_small = False
        if len(q_small) > 0:
            if current_small_bytes >= target_small or len(q_main) == 0:
                evict_from_small = True
        
        if evict_from_small:
            victim_key = q_small[0]
            
            # Lazy cleanup: ensure key is actually in cache (handles sync drifts)
            if victim_key not in cache_snapshot.cache:
                q_small.popleft()
                if victim_key in meta_sizes:
                    current_small_bytes -= meta_sizes[victim_key]
                    del meta_sizes[victim_key]
                if victim_key in meta_freq:
                    del meta_freq[victim_key]
                continue
                
            # S3-FIFO Logic for S
            freq = meta_freq.get(victim_key, 0)
            if freq > 0:
                # Promotion: Move from S to M
                q_small.popleft()
                
                # Update S-queue byte tracking
                size = meta_sizes.get(victim_key, 0)
                current_small_bytes -= size
                
                # Reset freq for M residency (standard S3-FIFO behavior)
                meta_freq[victim_key] = 0
                q_main.append(victim_key)
            else:
                # Victim found: Item in S with no hits
                q_small.popleft()
                
                # Adjust tracking immediately so loop logic remains consistent
                if victim_key in meta_sizes:
                    current_small_bytes -= meta_sizes[victim_key]
                
                # Return the key to be evicted
                return victim_key
        
        else:
            # Evict from Main
            if not q_main:
                # Fallback
                return list(cache_snapshot.cache.keys())[0]
                
            victim_key = q_main[0]
            
            if victim_key not in cache_snapshot.cache:
                q_main.popleft()
                if victim_key in meta_sizes: del meta_sizes[victim_key]
                if victim_key in meta_freq: del meta_freq[victim_key]
                continue
                
            # S3-FIFO Logic for M
            freq = meta_freq.get(victim_key, 0)
            if freq > 0:
                # Reinsert: Give second chance in M
                q_main.popleft()
                meta_freq[victim_key] -= 1 # Decay frequency
                q_main.append(victim_key)
            else:
                # Victim found: Item in M with expired frequency
                q_main.popleft()
                return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency on hit.
    Cap at 3 to distinguish "warm" vs "hot" without infinite growth.
    '''
    global meta_freq
    key = obj.key
    # Capped increment
    meta_freq[key] = min(meta_freq.get(key, 0) + 1, 3)

def update_after_insert(cache_snapshot, obj):
    '''
    Insert new objects into the Small queue (S).
    '''
    global q_small, meta_freq, meta_sizes, current_small_bytes
    
    key = obj.key
    # New items start with 0 frequency (need a subsequent hit to survive eviction)
    meta_freq[key] = 0
    meta_sizes[key] = obj.size
    
    q_small.append(key)
    current_small_bytes += obj.size

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Clean up metadata for the evicted object.
    '''
    global meta_freq, meta_sizes
    
    key = evicted_obj.key
    
    if key in meta_freq:
        del meta_freq[key]
    
    if key in meta_sizes:
        del meta_sizes[key]
        # Note: current_small_bytes was already adjusted inside evict() 
        # if the item was chosen from the S queue.
```
2025-11-27 19:56:54,579 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 31
- Hit Rate: 0.0266

Example 2:
- Trace ID: 36
- Hit Rate: 0.077

Example 3:
- Trace ID: 8
- Hit Rate: 0.5399

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:56:54 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:56:54,580 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m19:57:22 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 19:57:22,913 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 19:57:22,914 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is an optimized cache eviction algorithm.

### Analysis of the Previous Approach
The previous implementation attempted a **Segmented LRU (SLRU)** strategy with a "Ghost Registry" to mitigate scan resistance. While the logic was sound, there were likely a few issues leading to low hit rates in specific traces:
1.  **Strict Priority:** It evicted *all* frequency=1 items before touching *any* frequency>1 item. In workloads where the "working set" changes rapidly (shifts over time), old "frequent" items pollute the cache while new, potentially hot items are killed in the "probation" phase before they can prove themselves.
2.  **Inefficient Ghost Management:** The ghost registry management was somewhat rudimentary, potentially failing to capture the optimal cache size ratio between "recent" and "frequent" items.
3.  **Lack of Adaptability:** Real-world traces often oscillate between Recency-heavy (loops) and Frequency-heavy (hotspots) patterns. A static split (like implicit SLRU) often fails to adapt to these shifting phases.

### Improved Strategy: Adaptive Replacement Cache (ARC) Logic
To maximize hit rates across diverse traces, we will implement a strategy inspired by **ARC (Adaptive Replacement Cache)**. This is generally considered one of the highest-performing standard algorithms because it dynamically balances between **Recency** (LRU) and **Frequency** (LFU) based on which strategy is currently performing better.

**Key Concepts:**
1.  **Two Lists (T1 and T2):**
    *   `T1`: Contains items accessed only once recently (Recency focus).
    *   `T2`: Contains items accessed at least twice (Frequency focus).
2.  **Ghost Lists (B1 and B2):**
    *   `B1`: Evicted from `T1`. If we get a hit here, it means the cache is too small for "recent" items, so we should increase the target size of `T1`.
    *   `B2`: Evicted from `T2`. If we get a hit here, it means we need more space for "frequent" items, so we shrink the target size of `T1` (giving more to `T2`).
3.  **Adaptive Parameter (`p`):**
    *   A target size `p` determines how much of the cache capacity should be dedicated to `T1`. This `p` moves dynamically based on hits in the ghost lists.

### Implementation Details
Since we don't control the storage data structure directly (the system provides `cache_snapshot.cache` as a dict), we must maintain the ordering and list logic in our own metadata.

```python
import math

# Global Metadata
# Tracks the state of the ARC algorithm
# m_lists maps key -> location ('t1', 't2', 'b1', 'b2')
m_lists = dict()

# We need ordered structures to manage LRU behavior within the sets.
# Since we can't import complex structures, we use dictionaries as ordered maps 
# (standard in Python 3.7+), utilizing re-insertion to move to end.
m_t1 = dict() # Recent items (in cache)
m_t2 = dict() # Frequent items (in cache)
m_b1 = dict() # Ghosts of recent items (evicted history)
m_b2 = dict() # Ghosts of frequent items (evicted history)

# Adaptability parameter 'p' (target size for T1)
m_p = 0

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic:
    Decide whether to evict from T1 or T2 based on the target size 'p'
    and the current population of T1.
    '''
    global m_t1, m_t2, m_b1, m_b2, m_p
    
    # Safeties to ensure we don't crash if empty
    if not m_t1 and not m_t2:
        return None
        
    # We need to decide which victim to pick.
    # In ARC, the logic is usually handled during the *insertion* phase (replace()),
    # but here the system asks us to pick a victim explicitly.
    
    # We approximate the standard ARC replacement logic here:
    # If len(T1) > p, we usually prefer evicting from T1 (Recency).
    # Otherwise, we evict from T2 (Frequency).
    
    # Note: dict keys are ordered by insertion. 
    # The first key in the dict is the LRU.
    
    candidate = None
    
    # Logic:
    # If we have "too many" recent items (len(m_t1) > p), we shed from T1.
    # However, we must ensure T2 is touched if T1 is empty.
    if m_t1 and (len(m_t1) > m_p or not m_t2):
        # Evict LRU from T1
        candidate = next(iter(m_t1))
    else:
        # Evict LRU from T2
        candidate = next(iter(m_t2))
        
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in T1, move to T2 (it has been accessed twice now).
    2. If in T2, move to MRU of T2 (it is still popular).
    '''
    global m_lists, m_t1, m_t2
    
    key = obj.key
    
    # Locate the item
    location = m_lists.get(key)
    
    if location == 't1':
        # Promotion: Recent -> Frequent
        del m_t1[key]
        m_t2[key] = True # Add to MRU of T2
        m_lists[key] = 't2'
    elif location == 't2':
        # Maintenance: Re-insert to update MRU position
        del m_t2[key]
        m_t2[key] = True
        # m_lists remains 't2'

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    Check if it was in Ghost Lists (B1 or B2) to adjust 'p'.
    Add new item to T1.
    '''
    global m_lists, m_t1, m_t2, m_b1, m_b2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually works on count, here we treat 1 item = 1 unit.
    
    # 1. Adaptability Check (Ghost Hits)
    if key in m_b1:
        # Hit in Ghost Recent (B1).
        # We favored Frequency too much. Increase p (target size for T1).
        delta = 1
        if len(m_b1) < len(m_b2):
             delta = len(m_b2) / len(m_b1)
        m_p = min(capacity, m_p + delta)
        
        # Remove from ghost
        del m_b1[key]
        # Will be added to T2 (similar to a hit promotion) because it was known
        # But standard ARC adds to T2 if it was in B1/B2
        m_t2[key] = True
        m_lists[key] = 't2'
        
    elif key in m_b2:
        # Hit in Ghost Frequent (B2).
        # We favored Recency too much. Decrease p.
        delta = 1
        if len(m_b2) < len(m_b1):
            delta = len(m_b1) / len(m_b2)
        m_p = max(0, m_p - delta)
        
        # Remove from ghost
        del m_b2[key]
        # Restore to T2
        m_t2[key] = True
        m_lists[key] = 't2'
        
    else:
        # Total Miss (New item).
        # Add to MRU of T1
        m_t1[key] = True
        m_lists[key] = 't1'

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from T1/T2 to B1/B2 (Ghost lists).
    Maintain Ghost List sizes.
    '''
    global m_lists, m_t1, m_t2, m_b1, m_b2
    
    key = evicted_obj.key
    location = m_lists.get(key)
    capacity = cache_snapshot.capacity
    
    # 1. Clean up active lists
    if location == 't1':
        if key in m_t1: del m_t1[key]
        # Move to B1
        m_b1[key] = True
        m_lists[key] = 'b1'
    elif location == 't2':
        if key in m_t2: del m_t2[key]
        # Move to B2
        m_b2[key] = True
        m_lists[key] = 'b2'
    
    # 2. Maintain Ghost Registry Size
    # ARC typically enforces: len(T1)+len(B1) <= C and len(T2)+len(B2) <= 2C
    # Roughly, we just ensure ghosts don't grow infinitely.
    # A safe upper bound for ghosts is Capacity.
    
    while len(m_b1) > capacity:
        oldest = next(iter(m_b1))
        del m_b1[oldest]
        if m_lists.get(oldest) == 'b1':
            del m_lists[oldest]
            
    while len(m_b2) > capacity:
        oldest = next(iter(m_b2))
        del m_b2[oldest]
        if m_lists.get(oldest) == 'b2':
            del m_lists[oldest]
```
2025-11-27 19:59:20,327 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# T1: Recently accessed once (LRU keys) -> maps key to None (set behavior)
meta_t1 = dict()
# T2: Accessed at least twice (LRU keys) -> maps key to None
meta_t2 = dict()
# B1: Ghost list for T1 (evicted from T1) -> maps key to None
meta_b1 = dict()
# B2: Ghost list for T2 (evicted from T2) -> maps key to None
meta_b2 = dict()

# Adaptive parameter
# 'p' represents the target size for the T1 list.
p = 0

def evict(cache_snapshot, obj):
    '''
    ARC-style eviction strategy.
    Decides whether to evict from the T1 (Recency) set or the T2 (Frequency) set
    based on the current adaptation parameter `p` and the length of T1.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    # We need to pick a victim from cache_snapshot.cache.
    # The cache contains items in T1 U T2.
    # Logic:
    # If len(T1) > p, we prefer evicting from T1 (recency), moving it to B1.
    # Otherwise, we evict from T2 (frequency), moving it to B2.
    
    t1_keys_in_cache = [k for k in meta_t1 if k in cache_snapshot.cache]
    t2_keys_in_cache = [k for k in meta_t2 if k in cache_snapshot.cache]
    
    # If one list is empty, force eviction from the other
    if not t1_keys_in_cache and not t2_keys_in_cache:
        # Fallback (should rarely happen unless logic drifts): Random/First
        return list(cache_snapshot.cache.keys())[0]
    
    victim_key = None
    
    # Decision logic based on ARC "replace" subroutine
    # If len(t1) >= p, evict LRU of T1. Else evict LRU of T2.
    # Since Python dicts preserve insertion order (LRU order if we manage deletes/inserts),
    # the first key in the dict is the LRU.
    
    if t1_keys_in_cache and (len(t1_keys_in_cache) > p or not t2_keys_in_cache):
        # Evict from T1 (LRU is index 0)
        victim_key = t1_keys_in_cache[0]
    else:
        # Evict from T2 (LRU is index 0)
        victim_key = t2_keys_in_cache[0]
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If it's in T1, move to T2 (it has been accessed twice now).
    If it's in T2, move to MRU of T2.
    '''
    global meta_t1, meta_t2
    
    key = obj.key
    
    if key in meta_t1:
        # Promote from Recency (T1) to Frequency (T2)
        del meta_t1[key]
        meta_t2[key] = None
    elif key in meta_t2:
        # Re-insert to update LRU position (make it MRU)
        del meta_t2[key]
        meta_t2[key] = None
    else:
        # In rare sync issues or external modifications, ensure it's tracked.
        # Assume T2 if unknown but hit.
        meta_t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    This function handles the logic when a new item is added to the cache.
    Crucially, ARC checks if the miss occurred in B1 (Ghost T1) or B2 (Ghost T2)
    to adjust the parameter `p`.
    '''
    global p, meta_t1, meta_t2, meta_b1, meta_b2
    
    key = obj.key
    capacity = cache_snapshot.capacity # We treat capacity as object count mostly here
    
    # Case 1: Miss in B1 (It was in T1 recently, we should have made T1 larger)
    if key in meta_b1:
        # Adapt p
        delta = 1
        if len(meta_b1) >= len(meta_b2):
            delta = 1
        else:
            if len(meta_b1) > 0:
                delta = len(meta_b2) / len(meta_b1)
            else:
                delta = 1 # Safety
                
        p = min(capacity, p + delta)
        
        # Move from B1 to T2 (since it's a re-access, it qualifies for Frequency)
        del meta_b1[key]
        meta_t2[key] = None 
        # Note: logic typically moves B1->T2 because it's the second access
        
    # Case 2: Miss in B2 (It was in T2 recently, we should have made T2 larger / T1 smaller)
    elif key in meta_b2:
        # Adapt p
        delta = 1
        if len(meta_b2) >= len(meta_b1):
            delta = 1
        else:
            if len(meta_b2) > 0:
                delta = len(meta_b1) / len(meta_b2)
            else:
                delta = 1
        
        p = max(0, p - delta)
        
        # Move from B2 to T2
        del meta_b2[key]
        meta_t2[key] = None
        
    # Case 3: Totally new item
    else:
        # Insert into T1 (MRU position)
        meta_t1[key] = None

    # Enforce history size limits (Ghost lists shouldn't grow forever)
    # Standard ARC limits B1 + B2 roughly to capacity
    if len(meta_b1) > capacity:
        # Remove LRU of B1
        first = next(iter(meta_b1))
        del meta_b1[first]
        
    if len(meta_b2) > capacity * 2: # B2 can be larger in some implementations, but let's cap it
        first = next(iter(meta_b2))
        del meta_b2[first]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the victim from T list to B list (Ghost Cache).
    '''
    global meta_t1, meta_t2, meta_b1, meta_b2
    
    victim_key = evicted_obj.key
    
    # If evicted from T1, move to B1
    if victim_key in meta_t1:
        del meta_t1[victim_key]
        meta_b1[victim_key] = None
        
    # If evicted from T2, move to B2
    elif victim_key in meta_t2:
        del meta_t2[victim_key]
        meta_b2[victim_key] = None
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 33
- Hit Rate: 0.7531

Example 2:
- Trace ID: 37
- Hit Rate: 0.6404

Example 3:
- Trace ID: 44
- Hit Rate: 0.0933

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m19:59:20 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 19:59:20,328 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:00:36 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:00:36,145 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:00:36,146 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to implement the **Adaptive Replacement Cache (ARC)** logic more strictly. The previous implementation had loose management of the "Ghost" lists (B1 and B2) and disconnected the `evict` decision logic from the specific type of miss (B1 vs B2) that was occurring.

The key to solving Trace 44 (likely a scan pattern) and improving overall hit rates is to strictly adhere to the ARC `REPLACE` logic, specifically ensuring that items seen only once (scans) are quickly flushed through T1/B1 without polluting the frequency-based T2 list, while adapting the parameter `p` to balance Recency (T1) and Frequency (T2) dynamically.

### Key Changes:
1.  **Strict State Management**: Using Python `dict`s as Ordered Sets to enforce strict LRU (Least Recently Used) order.
2.  **Look-ahead Eviction**: In the `evict` function, we peek at the incoming `obj.key`. If this key is found in our ghost lists (B1 or B2), we simulate the adaptation of `p` locally to make the correct eviction decision *before* the insertion actually happens.
3.  **Correct Placement**: Items found in Ghost lists (B1/B2) are promoted to **T2** (Frequency) upon insertion, not T1. Only truly new items go to T1.
4.  **Ghost List Bounding**: Strictly limiting the size of ghost lists to prevent memory bloat and stale history.

```python
# Metadata Global State
# T1: Recent Cache (LRU is 0, MRU is -1)
t1 = dict()
# T2: Frequent Cache (LRU is 0, MRU is -1)
t2 = dict()
# B1: Ghost Recent (Evicted from T1)
b1 = dict()
# B2: Ghost Frequent (Evicted from T2)
b2 = dict()
# p: Target size for T1 (Adaptation parameter)
p = 0

def evict(cache_snapshot, obj):
    '''
    Determines the victim to evict.
    Implements the ARC REPLACE(x) logic.
    We must look ahead at `obj.key` to see if it is in B1 or B2 
    to decide whether to prefer evicting from T1 or T2.
    '''
    global t1, t2, b1, b2, p

    incoming_key = obj.key
    
    # We use len(cache) as the working set capacity 'C' for ARC calculations.
    # In byte-based capacity scenarios, this adapts to the count of items currently fitting.
    c = len(cache_snapshot.cache) 
    if c == 0: 
        # Should not happen if cache is full, but safety first
        return None

    # Determine eviction target based on ARC logic
    # We must simulate the adaptation of 'p' to make the correct decision 
    # corresponding to the state we will be in after `update_after_insert` runs.
    
    # Logic derived from ARC "REPLACE" subroutine:
    evict_from_t1 = False
    
    if t1 and not t2:
        evict_from_t1 = True
    elif t2 and not t1:
        evict_from_t1 = False
    else:
        # Both lists have items, check ARC condition
        # If incoming key is in B2, we would decrement p (prefer T2), 
        # making the condition (len(t1) > p) more likely to be true.
        
        # Calculate adaptation delta locally for decision making
        if incoming_key in b2:
            delta = 1
            if len(b2) > 0 and len(b1) > 0:
                 delta = len(b1) / len(b2)
            # If hit in B2, target p decreases. 
            # The condition for evicting T1 is lenient:
            # REPLACE Logic: if (len(t1) > p) OR (key in B2 and len(t1) == p) -> evict T1
            # We use a temporary p for this check or explicit logic:
            if len(t1) > p or (len(t1) == int(p)): # int(p) handles float precision
                evict_from_t1 = True
        else:
            # If incoming key is in B1 (p increases) or New:
            # Standard check: if len(t1) > p -> evict T1
            if len(t1) > p:
                evict_from_t1 = True
    
    # Perform selection (keys are insertion ordered, iter(d) gives first/LRU)
    if evict_from_t1:
        # Return LRU of T1
        return next(iter(t1))
    else:
        # Return LRU of T2
        return next(iter(t2))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - If in T1, move to T2 (promotion).
    - If in T2, move to MRU of T2.
    '''
    global t1, t2, b1, b2
    key = obj.key

    # If it is in T1, it has been accessed a second time -> Promote to T2
    if key in t1:
        del t1[key]
        t2[key] = None
    # If it is in T2, update LRU position
    elif key in t2:
        del t2[key]
        t2[key] = None
    # If it is in ghosts (rare race condition or data sync issue), revive to T2
    elif key in b1:
        del b1[key]
        t2[key] = None
    elif key in b2:
        del b2[key]
        t2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Handle Ghost hits (B1/B2) to adapt p.
    - Insert new object into correct list (T1 if new, T2 if from Ghost).
    - Enforce bounds on Ghost lists.
    '''
    global t1, t2, b1, b2, p

    key = obj.key
    # Capacity in terms of count (approximate)
    c = len(cache_snapshot.cache) 

    # Case 1: Ghost Hit in B1 (Recency Miss) -> Increase p (Enlarge T1)
    if key in b1:
        delta = 1
        if len(b1) >= len(b2):
            delta = 1
        else:
            # Safety check for divide by zero
            delta = len(b2) / len(b1)
        
        p = min(float(c), p + delta)
        
        # Remove from Ghost and Add to Frequency (T2)
        del b1[key]
        t2[key] = None

    # Case 2: Ghost Hit in B2 (Frequency Miss) -> Decrease p (Shrink T1, Enlarge T2)
    elif key in b2:
        delta = 1
        if len(b2) >= len(b1):
            delta = 1
        else:
            delta = len(b1) / len(b2)
            
        p = max(0.0, p - delta)
        
        # Remove from Ghost and Add to Frequency (T2)
        del b2[key]
        t2[key] = None

    # Case 3: Totally New Item -> Add to Recency (T1)
    else:
        t1[key] = None

    # Maintenance: Trim Ghost Lists
    # ARC generally suggests keeping ghost history roughly equal to capacity
    # If ghosts grow too large, memory overhead increases without utility.
    if len(b1) > c:
        del b1[next(iter(b1))] # Remove LRU of B1
    
    if len(b2) > c:
        del b2[next(iter(b2))] # Remove LRU of B2

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    - Move evicted object key to the corresponding Ghost list.
    '''
    global t1, t2, b1, b2
    
    victim_key = evicted_obj.key

    # If evicted from T1, add to B1
    if victim_key in t1:
        del t1[victim_key]
        b1[victim_key] = None
    # If evicted from T2, add to B2
    elif victim_key in t2:
        del t2[victim_key]
        b2[victim_key] = None
```
2025-11-27 20:00:46,450 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
# Tracks how many times an object has been accessed (Frequency)
m_key_frequency = dict()
# Tracks the last access time of an object (Recency)
m_key_last_access = dict()

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest frequency (LFU).
    Tie-breaking: If frequencies are equal, evict the Least Recently Used (LRU) among them.
    '''
    candid_obj_key = None
    
    # We want to find the key with min frequency, and then min last_access among those.
    # Since we need to inspect the cache content, we iterate over keys in the snapshot.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Initialize best candidate with the first item
    best_key = candidate_keys[0]
    best_freq = m_key_frequency.get(best_key, 0)
    best_time = m_key_last_access.get(best_key, 0)

    for key in candidate_keys[1:]:
        curr_freq = m_key_frequency.get(key, 0)
        curr_time = m_key_last_access.get(key, 0)

        # Primary Sort Key: Frequency (Lower is better for eviction)
        if curr_freq < best_freq:
            best_key = key
            best_freq = curr_freq
            best_time = curr_time
        # Secondary Sort Key: Recency (Older/Lower timestamp is better for eviction)
        elif curr_freq == best_freq:
            if curr_time < best_time:
                best_key = key
                best_freq = curr_freq
                best_time = curr_time
                
    return best_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit, increment frequency and update last access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    
    # Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert, initialize frequency and access time.
    '''
    global m_key_frequency, m_key_last_access
    
    # Initialize Frequency (usually 1 for the first access/insert)
    m_key_frequency[obj.key] = 1
    
    # Initialize Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction, remove metadata for the evicted object to save memory.
    '''
    global m_key_frequency, m_key_last_access
    
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 1
- Hit Rate: 0.4393

Example 2:
- Trace ID: 22
- Hit Rate: 0.0587

Example 3:
- Trace ID: 40
- Hit Rate: 0.0635

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:00:46 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:00:46,451 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:01:09 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:09,979 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:10,011 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the weaknesses of the current LFU/LRU hybrid implementation. Pure LFU suffers from "cache pollution" where an item accessed frequently in the past but never again stays in the cache too long. Pure LRU suffers when scans (reading many items once) wipe out the "hot" frequent items.

A highly effective algorithm that balances recency and frequency without the heavy overhead of complex structures is **TinyLFU** (or a simplified variant like **W-TinyLFU** or **SLRU** - Segmented LRU). Given the constraints and the provided interface, a **Segmented LRU (SLRU)** approach is a robust choice. It divides the cache into a "probationary" segment (for new items) and a "protected" segment (for frequently accessed items).

However, a slightly more sophisticated but easy-to-implement strategy that often outperforms SLRU on mixed traces is **LFU-DA (Least Frequently Used with Dynamic Aging)** or a frequency-based approach that decays over time.

Given the constraint to stick to Python dictionaries and metadata, a variation of **GDSF (Greedy Dual-Size Frequency)** or a **Hyperbolic Caching** approach is often very powerful for web traces. But a very reliable, high-performing algorithm for general traces (like the ones hinted at in your examples with low hit rates) is **S3-FIFO** or **2Q**.

Let's implement a **simplified 2Q-like algorithm with Frequency Awareness**.
Instead of just one list, we will track:
1.  **Ghost Cache (History):** Metadata for items recently evicted. If we see a "ghost" item again, it means it's a recurring pattern, and we should promote it aggressively.
2.  **Frequency + Recency Score:** We will use a scoring function. A robust function is `Score = Frequency / (CurrentTime - LastAccessTime)`. However, `Log(Frequency) / (CurrentTime - LastAccessTime)` is often better.

**The Strategy: "LeDe" (Least Decayed)**
We will implement an algorithm that calculates a "decayed frequency" or "utility" value.
*   **Metric:** `Utility = Frequency * (1 / (CurrentTime - LastAccessTime + 1))`. This is essentially "Frequency per unit of time since last access". It naturally favors recent items (small denominator) and frequent items (large numerator).
*   **Ghost Entries:** When we evict, we keep the metadata for a while. If an item returns and has "ghost" metadata, we restore its old frequency count (perhaps slightly penalized) rather than starting at 1. This captures long-term popularity.

**Why this is better:**
*   **Hit Rate:** It handles "scans" (new items have low frequency, so low utility, evicted quickly) and "hot items" (high frequency keeps utility high even as time passes).
*   **Memory:** We only need `m_key_frequency` and `m_key_last_access`.

Here is the implementation using a **Hyperbolic/Decay-based scoring** mechanism, which generally outperforms standard LFU/LRU on mixed workloads.

```python
import math

# Metadata dictionaries
# Tracks the accumulated frequency of an object
m_key_frequency = dict()
# Tracks the last access time (logical clock) of an object
m_key_last_access = dict()
# Tracks the time an object was inserted (to calculate retention duration)
m_key_insertion_time = dict()

def get_utility_score(key, current_time):
    '''
    Calculates a utility score based on Hyperbolic Caching principles.
    Score = (Frequency) / (Time Since Last Access + 1)
    
    This balances Recency and Frequency naturally.
    - Recently accessed items have a small denominator -> High Score.
    - Frequently accessed items have a large numerator -> High Score.
    - Old, infrequent items have small numerator, large denominator -> Low Score (Eviction targets).
    '''
    freq = m_key_frequency.get(key, 1)
    last_access = m_key_last_access.get(key, 0)
    
    # +1 prevents division by zero and normalizes immediate reuse
    duration_since_access = (current_time - last_access) + 1
    
    # We use a slight modification where frequency is somewhat dampened to prevent 
    # historical heavy hitters from staying forever if they stop being accessed.
    # Score = Freq / (Delta_Time)
    return freq / duration_since_access

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest Utility Score.
    Algorithm: Hyperbolic Caching / Least Decayed Frequency
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to minimize O(N) scans, but given the constraints, we must scan the keys.
    # We look for the item with the MINIMUM score.
    
    current_time = cache_snapshot.access_count
    
    # Initialize best candidate (lowest score) with the first item
    victim_key = candidate_keys[0]
    min_score = get_utility_score(victim_key, current_time)

    # Optimization: Check a sample of keys if the cache is huge, 
    # but strictly following the prompt, we iterate to find the absolute best victim.
    for key in candidate_keys[1:]:
        score = get_utility_score(key, current_time)
        
        if score < min_score:
            min_score = score
            victim_key = key
        # Tie-breaker: If scores are identical, evict the one inserted earliest (FIFO fallback)
        elif score == min_score:
            if m_key_insertion_time.get(key, 0) < m_key_insertion_time.get(victim_key, 0):
                victim_key = key
                
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Increment frequency and update recency.
    '''
    global m_key_frequency, m_key_last_access
    
    current_freq = m_key_frequency.get(obj.key, 0)
    m_key_frequency[obj.key] = current_freq + 1
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    If the object was seen recently (in metadata but not in cache), we restore some frequency.
    Otherwise, start fresh.
    '''
    global m_key_frequency, m_key_last_access, m_key_insertion_time
    
    # "Ghost" Cache Logic:
    # If the key exists in frequency dict but not in the cache (implied by this being an insert),
    # it means it was evicted previously. We treat it as a "returning hero".
    if obj.key in m_key_frequency:
        # We boost it, but not fully, to prevent stale items from bouncing back too hard.
        # However, for pure hit-rate maximization, resuming frequency is usually best.
        m_key_frequency[obj.key] += 1
    else:
        m_key_frequency[obj.key] = 1
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    m_key_insertion_time[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    CRITICAL CHANGE: We do NOT delete the metadata immediately.
    We keep the frequency data. This allows the algorithm to "remember" items 
    that are evicted but requested again shortly after (solving the "Scan" vs "Working Set" problem).
    
    To prevent memory leaks in a real system, we would cap the size of m_key_frequency.
    Here, to maximize hit rate within the logic constraints, we retain history.
    '''
    global m_key_last_access, m_key_insertion_time
    
    # We remove insertion time as it's only relevant for the current residence in cache
    if evicted_obj.key in m_key_insertion_time:
        del m_key_insertion_time[evicted_obj.key]
    
    # We remove last_access to ensure if it comes back, delta_time calculation is fresh
    if evicted_obj.key in m_key_last_access:
         del m_key_last_access[evicted_obj.key]

    # NOTE: We intentionally keep m_key_frequency[evicted_obj.key].
    # This acts as our "Ghost Cache".
```
2025-11-27 20:01:20,811 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 35
- Hit Rate: 0.0653

Example 2:
- Trace ID: 34
- Hit Rate: 0.0746

Example 3:
- Trace ID: 25
- Hit Rate: 0.3952

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:01:20 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:01:20,812 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:01:47 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:47,197 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:01:47,198 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to move away from simple scalar scoring (which can be computationally expensive to iterate over for every eviction and sometimes fails to capture complex patterns) and implement a variation of **ARC (Adaptive Replacement Cache)** or a segmented LRU approach (like **SLRU** or **2Q**).

Given the extremely low hit rates in the feedback (0.0653, 0.0746), the traces likely contain "scans" (large sequences of one-time access items) which wash out valuable high-frequency items. The previous adaptive scoring tried to handle this but required iterating over the entire cache ($O(N)$), which is inefficient, and likely didn't protect the "Frequency" component strongly enough.

### The Solution: Segmented LRU (SLRU) with Probation
I will implement a **Segmented LRU** strategy. This divides the cache into two segments:
1.  **Probationary Segment:** For items accessed only once recently. New items land here.
2.  **Protected Segment:** For items accessed at least twice. Items are promoted here from Probation.

This structure inherently resists "scans" because scanning a long list of unique keys only churns the Probationary Segment, leaving the Protected Segment (the high-value items) untouched.

### Implementation Details

1.  **Metadata:**
    *   We need to track which segment a key belongs to.
    *   We need ordered structures (Python `OrderedDict` or just manipulating `dict` insertion order since Python 3.7+) to manage LRU behavior efficiently ($O(1)$) instead of scanning timestamps ($O(N)$).

2.  **Logic:**
    *   **New Item:** Insert into *Probation*. If Probation is full, evict the LRU from Probation.
    *   **Hit on Probation Item:** Promote to *Protected*.
    *   **Hit on Protected Item:** Move to MRU (Most Recently Used) position of *Protected*.
    *   **Eviction:**
        *   Preferentially evict from *Probation* (the one-hit wonders).
        *   Only evict from *Protected* if it exceeds its allocated capacity, demoting the victim back to *Probation* (giving it one last chance) or evicting strictly if the whole cache is stressed.

3.  **Dynamic Sizing:**
    *   To be truly robust, the ratio of Probation vs. Protected capacity isn't fixed. However, given the constraints of the function signatures, we can simulate this logic by maintaining logical queues within the single `cache_snapshot.cache`.

### Revised Algorithm Code

```python
from collections import OrderedDict

# Metadata Globals
# We effectively maintain two logical queues using dictionaries to track state.
# 1. probation_set: Keys that have been accessed roughly once recently.
# 2. protected_set: Keys that have been accessed >= 2 times.
m_probation = set()
m_protected = set()

# We maintain explicit LRU tracking using OrderedDicts for O(1) operations.
# While the main cache holds the objects, these hold the order.
m_lru_probation = OrderedDict()
m_lru_protected = OrderedDict()

# Configuration
# This ratio determines the maximum size of the protected segment relative to the cache size.
# 80% protected / 20% probation is a standard starting point for scan resistance.
PROTECTED_RATIO = 0.8

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU) equivalent.
    
    We prefer to evict from the Probationary segment (items seen once).
    If the Probationary segment is empty (unlikely), we evict from Protected.
    '''
    global m_lru_probation, m_lru_protected
    
    # 1. Try to evict from Probation (Scan resistance)
    # The first item in an OrderedDict is the LRU.
    if m_lru_probation:
        victim_key, _ = m_lru_probation.popitem(last=False)
        return victim_key
        
    # 2. If Probation is empty, evict from Protected (fallback)
    if m_lru_protected:
        victim_key, _ = m_lru_protected.popitem(last=False)
        return victim_key
        
    # Fallback if metadata is desynced (should not happen in normal flow)
    return next(iter(cache_snapshot.cache))

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. If in Probation, promote to Protected.
    2. If in Protected, update LRU position (move to end).
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected, PROTECTED_RATIO
    
    key = obj.key
    
    if key in m_probation:
        # Promotion: Probation -> Protected
        m_probation.remove(key)
        if key in m_lru_probation:
            del m_lru_probation[key]
            
        m_protected.add(key)
        m_lru_protected[key] = True # Add to MRU of protected
        
        # Enforce Protected Segment Size Limit
        # If Protected grows too big, demote the LRU of Protected back to Probation
        # This keeps the "elite" list exclusive.
        target_protected_size = int(cache_snapshot.capacity * PROTECTED_RATIO)
        if len(m_protected) > target_protected_size:
            demoted_key, _ = m_lru_protected.popitem(last=False) # Pop LRU
            m_protected.remove(demoted_key)
            m_probation.add(demoted_key)
            m_lru_probation[demoted_key] = True # Add to MRU of probation
            
    elif key in m_protected:
        # Hit on Protected: Just update recency
        m_lru_protected.move_to_end(key)
    else:
        # Edge case: Hit on item not in metadata (e.g., initial state or desync), treat as probation hit
        m_protected.add(key)
        m_lru_protected[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    New items always go to Probation.
    '''
    global m_probation, m_lru_probation
    
    key = obj.key
    
    # Add to Probationary segment
    m_probation.add(key)
    m_lru_probation[key] = True # Add to MRU
    
    _sync_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Cleanup metadata.
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected
    
    if evicted_obj:
        key = evicted_obj.key
        
        # Remove from whichever set it was in
        if key in m_probation:
            m_probation.remove(key)
        if key in m_protected:
            m_protected.remove(key)
            
        # Remove from LRU trackers if present (might have been removed in evict())
        if key in m_lru_probation:
            del m_lru_probation[key]
        if key in m_lru_protected:
            del m_lru_protected[key]

def _sync_metadata(cache_snapshot):
    '''
    Ensure our metadata doesn't hold keys that the cache dropped silently
    or grew stale.
    '''
    global m_probation, m_protected, m_lru_probation, m_lru_protected
    
    # Simple check to ensure we don't leak memory indefinitely if the simulation
    # framework does something unexpected with the cache dictionary.
    if len(m_lru_probation) + len(m_lru_protected) > cache_snapshot.capacity + 10:
        
        # Rebuild based on actual cache content if we drifted significantly
        current_keys = set(cache_snapshot.cache.keys())
        
        # Prune Probation
        probation_keys = list(m_lru_probation.keys())
        for k in probation_keys:
            if k not in current_keys:
                del m_lru_probation[k]
                m_probation.discard(k)
                
        # Prune Protected
        protected_keys = list(m_lru_protected.keys())
        for k in protected_keys:
            if k not in current_keys:
                del m_lru_protected[k]
                m_protected.discard(k)
```
2025-11-27 20:02:16,751 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 2
- Hit Rate: 0.4873

Example 2:
- Trace ID: 46
- Hit Rate: 0.4663

Example 3:
- Trace ID: 24
- Hit Rate: 0.2885

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:02:16 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:02:16,752 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:02:46 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:02:46,151 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:02:46,152 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Optimizing for cache miss rates across diverse traces often requires an algorithm that can handle "scan resistance" (not letting one-time access items pollute the cache) while retaining popular items (frequency) and recent items (recency).

The previous implementation attempted a mathematical hybrid but likely suffered from O(N) complexity in the `evict` function (scanning all keys to calculate scores) and potentially erratic tuning of the adaptive factor.

The improved algorithm below implements a variant of **ARC (Adaptive Replacement Cache)** logic but simplified for this context. It separates the cache into two logical lists (simulated via metadata):
1.  **L1 (Recency):** Captures items seen once recently.
2.  **L2 (Frequency):** Captures items seen at least twice.

This structure allows the algorithm to dynamically tune the size of the Recency list vs. the Frequency list based on "Ghost" hits, which is far more robust than a mathematical scoring function.

```python
# Metadata Globals
# T1: Main Cache for recent items (seen once). Maps key -> object (though we only track keys here for logic)
# T2: Main Cache for frequent items (seen >1 times). Maps key -> object
# B1: Ghost Cache for T1 (recently evicted from T1). Maps key -> None
# B2: Ghost Cache for T2 (recently evicted from T2). Maps key -> None
# Note: In the actual `cache_snapshot.cache`, both T1 and T2 items reside mixed. 
# We track their logical state here.

m_T1 = set()
m_T2 = set()
m_B1 = set()
m_B2 = set()

# P: Target size for T1. 
# If P increases, we favor Recency. If P decreases, we favor Frequency.
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Replacement (similar to ARC).
    
    We decide whether to evict from the T1 (Recency) set or the T2 (Frequency) set 
    based on the current cache state relative to the target parameter `p`.
    
    Logic:
    1. If len(T1) > p, we usually evict from T1 (LRU behavior for recent items).
    2. Otherwise, we evict from T2 (LRU behavior for frequent items).
    
    Crucially, "evicting" here just means picking the victim. We must select the 
    LRU item from the chosen logical set (T1 or T2).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Need access times to determine LRU within the sets
    # We rely on the order in cache_snapshot.cache not being guaranteed, 
    # so we need to track access order or infer it. 
    # To keep it efficient without maintaining a separate sorted list, 
    # we will scan the cache once to find the LRU candidates for T1 and T2.
    # In a production ARC, T1 and T2 are Doubly Linked Lists. 
    # Here we approximate by iterating the cache keys and tracking access times.
    
    # Note: We need a way to track LRU order. Since we can't change the object class,
    # we will maintain a separate `m_access_time` dict updated on every hit/insert.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    # We need the global access time map to find LRU
    global m_access_time
    
    # Identify the LRU item in T1 and the LRU item in T2
    lru_t1_key = None
    lru_t1_time = float('inf')
    
    lru_t2_key = None
    lru_t2_time = float('inf')
    
    # Also track a global LRU just in case metadata is out of sync
    global_lru_key = None
    global_lru_time = float('inf')

    # Iterate once to find candidates
    for key in candidate_keys:
        acc_time = m_access_time.get(key, 0)
        
        if acc_time < global_lru_time:
            global_lru_time = acc_time
            global_lru_key = key
            
        if key in m_T1:
            if acc_time < lru_t1_time:
                lru_t1_time = acc_time
                lru_t1_key = key
        elif key in m_T2:
            if acc_time < lru_t2_time:
                lru_t2_time = acc_time
                lru_t2_key = key
        # If key is in neither (shouldn't happen often), it falls to global backup

    # ARC Replacement Logic Decision
    # If len(T1) exceeds the target p, we evict from T1 to make room.
    # Otherwise, we evict from T2.
    
    current_t1_size = len([k for k in m_T1 if k in cache_snapshot.cache])
    
    # Determine Victim
    if current_t1_size > 0 and (current_t1_size > m_p or (lru_t2_key is None and current_t1_size > 0)):
        # Evict LRU from T1
        victim = lru_t1_key
    else:
        # Evict LRU from T2
        victim = lru_t2_key

    # Fallback if logic fails (e.g. T1 empty but p tells us to evict T1, or sync issues)
    if victim is None:
        victim = global_lru_key
        
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    - Update access time.
    - If item was in T1 (Recency), move it to T2 (Frequency).
    - If item was in T2, update it to MRU position (by access time).
    '''
    global m_T1, m_T2, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_access_time[key] = current_time
    
    if key in m_T1:
        m_T1.remove(key)
        m_T2.add(key)
    # If in T2, it stays in T2, just gets new time

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Miss):
    - Determine if this is a Ghost Hit (B1 or B2).
    - Adapt parameter `p`.
    - Insert into T2 if it was in B2 (it's a returning frequent item).
    - Insert into T1 otherwise (it's a new item).
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p, m_access_time
    
    key = obj.key
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: In object count usually, but here sizes are bytes.
    # We will assume capacity roughly correlates to count for logic adaptation.
    # Using a fixed approximation for capacity count helps the math.
    approx_count_capacity = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100
    
    m_access_time[key] = current_time

    # Case 1: Key is in B1 (Ghost Recency) -> We should have made T1 larger
    if key in m_B1:
        delta = 1
        if len(m_B1) >= len(m_B2) and len(m_B2) > 0:
             delta = 1
        elif len(m_B2) > 0:
             delta = len(m_B1) / len(m_B2)
             
        m_p = min(approx_count_capacity, m_p + delta)
        
        # Move from Ghost to Real Frequency list
        m_B1.remove(key)
        m_T2.add(key) # Promoted to frequent because it came back

    # Case 2: Key is in B2 (Ghost Frequency) -> We should have kept T2 larger (T1 smaller)
    elif key in m_B2:
        delta = 1
        if len(m_B2) >= len(m_B1) and len(m_B1) > 0:
             delta = 1
        elif len(m_B1) > 0:
             delta = len(m_B2) / len(m_B1)
             
        m_p = max(0, m_p - delta)
        
        # Move from Ghost to Real Frequency list
        m_B2.remove(key)
        m_T2.add(key) 

    # Case 3: Totally new item
    else:
        # Add to T1 (Recency list)
        m_T1.add(key)
    
    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    - Move from T1 -> B1 (Ghost Recency)
    - Or T2 -> B2 (Ghost Frequency)
    - Remove from Access Time map
    '''
    global m_T1, m_T2, m_B1, m_B2, m_access_time
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    
    # Remove from access time
    if key in m_access_time:
        del m_access_time[key]
        
    if key in m_T1:
        m_T1.remove(key)
        m_B1.add(key)
    elif key in m_T2:
        m_T2.remove(key)
        m_B2.add(key)
    
    # If for some reason it's in neither (shouldn't happen), we just ignore

# Helper global for access times (needed for LRU approximation within sets)
m_access_time = dict()

def _cleanup_metadata():
    '''
    Keep ghost lists bounded to prevent memory leaks.
    ARC typically suggests |L1| + |L2| <= 2 * c.
    '''
    global m_B1, m_B2
    
    # We use a loose upper bound to keep memory predictable
    MAX_GHOST = 5000 
    
    if len(m_B1) > MAX_GHOST:
        # Remove arbitrary element (FIFO ideal, but set doesn't support, we just need to bound size)
        # Using pop on set is random but O(1)
        for _ in range(100):
            if m_B1: m_B1.pop()
            
    if len(m_B2) > MAX_GHOST:
        for _ in range(100):
            if m_B2: m_B2.pop()
```
2025-11-27 20:03:26,633 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 28
- Hit Rate: 0.0692

Example 2:
- Trace ID: 14
- Hit Rate: 0.0212

Example 3:
- Trace ID: 15
- Hit Rate: 0.021

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:03:26 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:03:26,634 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:03:51 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:03:51,642 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:03:51,643 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An analysis of the current implementation and performance results suggests the previous algorithm is too rigid in its segmentation. While Segmented LRU (SLRU) is powerful, the hard separation between "Frequency 1" and "Frequency > 1" (probation vs. protected) can be detrimental if the working set size changes rapidly or if "one-hit wonders" are actually part of a cyclic scan that repeats just outside the cache window.

The extremely low hit rates (e.g., 0.021) suggest the cache is struggling to identify the true working set, likely due to:
1.  **Over-eviction of new items:** The "Frequency 1" penalty is so high ($10^{12}$) that new items are evicted almost immediately if the cache is full, even if they might have become hot shortly after.
2.  **Lack of Frequency granularity:** Treating everything with `freq > 1` as essentially equal (via LRU) ignores the difference between an item hit twice and an item hit 100 times.
3.  **Static Policies:** The policy doesn't adapt to the workload type (Recency-heavy vs. Frequency-heavy).

### Optimization Strategy: Adaptive Replacement Cache (ARC) / LIRS Hybrid approach

To fix this, we will implement a simplified **LFU-LRU Hybrid with Aging**.

Instead of a strict SLRU, we will use a scoring mechanism that balances **Recency** (how long ago was it used?) and **Frequency** (how popular is it?), but crucially, we will add **Aging**. Pure LFU fails because old heavy hitters stick around forever. Pure LRU fails because it can't handle scans.

**The New Logic (Hyperbolic Caching / LRFU concept):**
We will calculate a score for every item: `Score = Frequency / (CurrentTime - LastAccessTime)`.
*   Items used recently have a small denominator -> High Score.
*   Items used frequently have a large numerator -> High Score.
*   Items that are old and infrequent have low scores -> Eviction Candidates.

We will also maintain the **Ghost Registry** but use it to dynamically adjust an "admission confidence" rather than just a binary "skip probation" flag.

**Key Changes:**
1.  **Metadata:** Track `access_count`, `last_access_time`.
2.  **Scoring (`get_victim_score`):** Use a combined weight. To make it efficient and effective, we will invert the logic slightly for the victim score: `VictimScore = (CurrentTime - LastAccessTime) * Weight`. Lower frequency means higher weight (easier to evict).
3.  **Eviction:** Evict the item that maximizes the "Staleness weighted by inverse popularity".
4.  **Aging:** To prevent frequency counts from growing indefinitely (pollution), we will periodically halve the frequency counts or cap them.

Here is the improved implementation:

```python
import math

# Metadata dictionary
# Maps key -> [frequency, last_access_time]
# We use a list so it's mutable in place if needed, though tuple replacement is fine.
m_access_info = dict()

# Ghost registry (Shadow Cache)
# Maps key -> removal_time
# Used to give a boost to items that were recently evicted but requested again (handling 'churn' or 'thrashing').
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculates a victim score based on Recency and Frequency.
    Higher Score = Better candidate for eviction (Less valuable).
    
    Formula: Generalized LRFU (Least Recently/Frequently Used) approach.
    Value = Frequency * (1 / (TimeSinceAccess + Epsilon))
    
    Victim Score (Inverse of Value) = (TimeSinceAccess) / Frequency
    
    This effectively means:
    - An item accessed 1000 ticks ago with freq 1 has score 1000.
    - An item accessed 1000 ticks ago with freq 10 has score 100. (Harder to evict).
    '''
    if key not in m_access_info:
        return float('inf')
        
    freq, last_access = m_access_info[key]
    
    staleness = current_time - last_access
    
    # We apply a logarithmic dampener to the frequency. 
    # This prevents an item with 10,000 hits from becoming virtually un-evictable 
    # compared to an item with 5,000 hits. We care more about 1 vs 2 than 100 vs 101.
    # We add 1 to freq to avoid division by zero if logic somehow sets freq to 0.
    weighted_freq = math.log2(freq + 1) + 1
    
    # Victim Score = How stale it is / How popular it is.
    # High Staleness + Low Frequency = High Victim Score = Evict.
    return staleness / weighted_freq

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score (Oldest and Least Frequent).
    '''
    # Optimization: If the cache is very large, iterating all keys is slow.
    # However, given the constraints of the standard definition where we don't control the 
    # cache data structure (it's a dict), we must scan.
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    max_score = -1.0
    best_victim_key = None
    
    # We sample a subset if the cache is massive to keep eviction O(1)-ish, 
    # but strictly following the prompt to minimize miss rate, we scan all 
    # to find the absolute best victim.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with aging/capping).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    freq, _ = m_access_info.get(obj.key, (0, current_time))
    
    # Simple aging mechanism:
    # If frequency gets too high, it becomes "cache pollution".
    # We cap it. A cap of 20-50 allows distinct "tiers" of heat without infinite locking.
    new_freq = min(freq + 1, 50)
    
    m_access_info[obj.key] = [new_freq, current_time]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Adaptive Component:
    # If this item was recently evicted (it's in the ghost registry), it implies our 
    # cache was too small or we evicted it prematurely. 
    # We grant it a "Frequency Boost" so it survives longer this time.
    if obj.key in m_ghost_registry:
        # Boost frequency to simulate that it has been "remembered"
        # We give it a head start of 5 (arbitrary heuristic based on trace analysis)
        initial_freq = 5
        del m_ghost_registry[obj.key]
    
    m_access_info[obj.key] = [initial_freq, current_time]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
    
    # 2. Add to Ghost Registry
    # This marks that we saw this item recently.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # If the ghost registry grows too large, it consumes memory.
    # We keep it roughly the size of the cache capacity to balance memory vs adaptability.
    # Since Python 3.7+, dicts maintain insertion order. We can treat it as a FIFO queue easily.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest inserted item (the first key in the dict)
        oldest_key = next(iter(m_ghost_registry))
        del m_ghost_registry[oldest_key]
```
2025-11-27 20:04:23,818 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 10
- Hit Rate: 0.075

Example 2:
- Trace ID: 21
- Hit Rate: 0.0262

Example 3:
- Trace ID: 12
- Hit Rate: 0.04

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:04:23 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:04:23,819 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:05:02 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:02,390 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:02,391 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to address the extremely low hit rates observed in the feedback (2.6% - 7.5%). These low rates suggest that the previous "Segmented LRU" attempt was too simplistic or rigidly tuned, likely failing to adapt to changing workload patterns (frequency vs. recency bias).

The improved algorithm below implements a logic inspired by **ARC (Adaptive Replacement Cache)**. ARC is superior because it dynamically balances between **Recency** (LRU) and **Frequency** (LFU) based on the workload's behavior. It maintains two lists (T1 for recent items, T2 for frequent items) and two "ghost" lists (B1 for evicted recent, B2 for evicted frequent). By observing hits in the ghost lists, it adaptively shifts the cache capacity to favor either recency or frequency.

### Key Improvements:
1.  **Dynamic Adaptation (`p` parameter):** Instead of a fixed rule (like "always evict freq=1 first"), we track a target size `p` for the recency list (T1). If we see hits in the recency ghost list (B1), we increase `p` (we need more room for recent items). If we see hits in the frequency ghost list (B2), we decrease `p` (we need more room for frequent items).
2.  **Strict Separation:** We explicitly track which logical list (T1, T2, B1, B2) a key belongs to.
3.  **Scan Resistance:** One-time access items (T1) compete for space with frequent items (T2) based on the adaptive parameter `p`, preventing scans from flushing out useful high-frequency items, but allowing recent items to survive if the workload shifts to strictly recency-based patterns.

```python
import math

# Metadata management
# We need to maintain the state of the ARC (Adaptive Replacement Cache) algorithm.
# Since we cannot create a class instance to hold state between calls, we use global dictionaries.

# ARC divides the cache into two lists:
# T1: Recently accessed items (Recency) - currently in cache
# T2: Frequently accessed items (Frequency) - currently in cache
# B1: Ghost list for T1 (Evicted Recency history) - keys only
# B2: Ghost list for T2 (Evicted Frequency history) - keys only

# Globals to store state
meta_lists = {
    'T1': set(), # Keys in T1
    'T2': set(), # Keys in T2
    'B1': set(), # Keys in B1
    'B2': set()  # Keys in B2
}

# LRU tracking: We need to know the order of elements in T1, T2, B1, B2.
# Since Python dicts preserve insertion order (3.7+), we can use dicts mapping key -> None/Time
# to simulate an ordered set or LRU queue.
lru_T1 = dict()
lru_T2 = dict()
lru_B1 = dict()
lru_B2 = dict()

# Target size for T1 list.
# 0 <= p <= capacity
# This adapts: if we hit B1, p increases (favor recency). If we hit B2, p decreases (favor frequency).
p = 0.0 

def access_lru(lru_dict, key):
    '''Helper to refresh a key in an LRU dict (move to end).'''
    if key in lru_dict:
        del lru_dict[key]
    lru_dict[key] = None

def pop_lru(lru_dict):
    '''Helper to pop the Least Recently Used item (first item).'''
    if not lru_dict:
        return None
    key = next(iter(lru_dict))
    del lru_dict[key]
    return key

def evict(cache_snapshot, obj):
    '''
    ARC Eviction Logic.
    Decides which item to evict from the cache (T1 or T2) to make room.
    The decision relies on the adaptive parameter `p`.
    '''
    global p, lru_T1, lru_T2, meta_lists

    # IMPORTANT: The "obj" passed here is the new object we are trying to insert.
    # However, evict() is called when the physical cache is full.
    # We must pick a victim from the keys present in cache_snapshot.cache.
    
    # In ARC, T1 U T2 constitutes the cache content.
    # Rule: Replace from T1 if |T1| > p, else replace from T2.
    
    t1_size = len(lru_T1)
    
    # We also check if the incoming item `obj` is in B2.
    # If the new item is in B2, the replacement logic is slightly stricter on T1.
    is_in_B2 = obj.key in meta_lists['B2']
    
    candidate = None
    
    # Logic derived from ARC "REPLACE" subroutine
    if (t1_size > p) or (is_in_B2 and t1_size == p):
        # Evict LRU from T1
        if lru_T1:
            candidate = next(iter(lru_T1))
        else:
            # Fallback if T1 is empty (shouldn't happen given conditions, but safety first)
            candidate = next(iter(lru_T2))
    else:
        # Evict LRU from T2
        if lru_T2:
            candidate = next(iter(lru_T2))
        else:
            # Fallback
            candidate = next(iter(lru_T1))
            
    return candidate

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If item is in T1 or T2, it's a hit.
    Move it to MRU of T2 (since it has been referenced again, it has frequency).
    '''
    global lru_T1, lru_T2, meta_lists
    
    key = obj.key
    
    # Case 1: Hit in T1.
    # Move x from T1 to MRU of T2.
    if key in meta_lists['T1']:
        meta_lists['T1'].remove(key)
        if key in lru_T1: del lru_T1[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 2: Hit in T2.
    # Move x to MRU of T2.
    elif key in meta_lists['T2']:
        access_lru(lru_T2, key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    This happens when we insert a new object.
    It might be a "real" miss (new key) or a "ghost" hit (key in B1 or B2).
    '''
    global p, lru_T1, lru_T2, lru_B1, lru_B2, meta_lists

    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually defined by count, here we approximate with capacity count if possible, but problem says capacity is Bytes. 
    # Provided "cache is full when total number of cached objects reaches its capacity" implies capacity is count-based in this specific challenge context or we treat it as such.
    # If capacity is bytes, we can't implement pure ARC easily. However, looking at the context: "The cache is full when the total number of cached objects reaches its capacity."
    # This implies Capacity is NUMBER OF OBJECTS.
    c = capacity 
    
    # Case 1: x in B1 (Ghost Hit on Recency)
    # We missed in cache, but remembered it was recently evicted from T1.
    # Adaptation: Increase p (target size of T1).
    if key in meta_lists['B1']:
        # Adaptation delta
        b1_size = len(lru_B1)
        b2_size = len(lru_B2)
        delta = 1 if b1_size >= b2_size else b2_size / b1_size
        p = min(c, p + delta)
        
        # Move from B1 to T2 (It has been accessed twice now)
        meta_lists['B1'].remove(key)
        if key in lru_B1: del lru_B1[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 2: x in B2 (Ghost Hit on Frequency)
    # We missed in cache, but remembered it was recently evicted from T2.
    # Adaptation: Decrease p (target size of T1, favors T2).
    elif key in meta_lists['B2']:
        # Adaptation delta
        b1_size = len(lru_B1)
        b2_size = len(lru_B2)
        delta = 1 if b2_size >= b1_size else b1_size / b2_size
        p = max(0, p - delta)
        
        # Move from B2 to T2
        meta_lists['B2'].remove(key)
        if key in lru_B2: del lru_B2[key]
        
        meta_lists['T2'].add(key)
        access_lru(lru_T2, key)
        
    # Case 3: x is totally new (not in T1, T2, B1, B2)
    else:
        # Check Ghost List sizes (L1 = T1+B1, L2 = T2+B2)
        # ARC maintains |L1| + |L2| <= 2*c
        
        # If |T1| + |B1| == c:
        if len(lru_T1) + len(lru_B1) == c:
            if len(lru_T1) < c:
                # Discard LRU of B1
                discard = pop_lru(lru_B1)
                if discard in meta_lists['B1']: meta_lists['B1'].remove(discard)
            else:
                # Discard LRU of T1 (handled by evict usually, but just in case of ghost cleaning)
                # In standard ARC, `replace` is called before this.
                # Here, we clean B1 to make room if needed conceptually.
                discard = pop_lru(lru_B1)
                if discard and discard in meta_lists['B1']: meta_lists['B1'].remove(discard)

        # If |T1| + |B1| < c and |T1| + |B1| + |T2| + |B2| >= c:
        elif (len(lru_T1) + len(lru_B1) < c) and ((len(lru_T1) + len(lru_B1) + len(lru_T2) + len(lru_B2)) >= 2 * c):
            if (len(lru_T1) + len(lru_B1) + len(lru_T2) + len(lru_B2)) == 2 * c:
                # Discard LRU of B2
                discard = pop_lru(lru_B2)
                if discard in meta_lists['B2']: meta_lists['B2'].remove(discard)
        
        # Put x in T1 (MRU)
        meta_lists['T1'].add(key)
        access_lru(lru_T1, key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    The `evict` function selected a victim. Now we must move that victim
    to the appropriate Ghost List (B1 or B2).
    '''
    global lru_T1, lru_T2, lru_B1, lru_B2, meta_lists
    
    victim_key = evicted_obj.key
    
    # If victim was in T1, move to B1
    if victim_key in meta_lists['T1']:
        meta_lists['T1'].remove(victim_key)
        if victim_key in lru_T1: del lru_T1[victim_key]
        
        meta_lists['B1'].add(victim_key)
        access_lru(lru_B1, victim_key)
        
    # If victim was in T2, move to B2
    elif victim_key in meta_lists['T2']:
        meta_lists['T2'].remove(victim_key)
        if victim_key in lru_T2: del lru_T2[victim_key]
        
        meta_lists['B2'].add(victim_key)
        access_lru(lru_B2, victim_key)
        
    # Sanity check: Ensure ghost lists don't grow infinitely.
    # In strict ARC, |T1|+|B1| <= c and |T2|+|B2| <= 2c, but simpler bound is often enough.
    c = cache_snapshot.capacity
    if len(lru_B1) > c:
        k = pop_lru(lru_B1)
        if k in meta_lists['B1']: meta_lists['B1'].remove(k)
    if len(lru_B2) > c:
        k = pop_lru(lru_B2)
        if k in meta_lists['B2']: meta_lists['B2'].remove(k)
```
2025-11-27 20:05:33,229 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 38
- Hit Rate: 0.1255

Example 2:
- Trace ID: 0
- Hit Rate: 0.5093

Example 3:
- Trace ID: 5
- Hit Rate: 0.493

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:05:33 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:05:33,230 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:05:58 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:58,666 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:05:58,667 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Optimizing a cache eviction algorithm requires balancing "recency" (how recently an item was used) and "frequency" (how often an item is used). The previous implementation (SLRU with Ghost Cache) is a strong foundation, but it can be improved by refining how we score victims and adapting to changing workloads dynamically.

The following solution implements an **Adaptive Replacement Cache (ARC)-inspired** logic simplified for this interface. It maintains two distinct segments implicitly via a scoring mechanism:
1.  **Recency (T1)**: Items accessed only once recently.
2.  **Frequency (T2)**: Items accessed at least twice.

We improve upon the previous version by:
1.  **Refining the Victim Score**: Instead of a hard threshold between frequency 1 and >1, we use a hybrid score that weighs recency heavily but gives a "frequency boost" that decays over time. This prevents high-frequency items from polluting the cache forever if they stop being accessed.
2.  **Dynamic Ghost Sizing**: We allow the ghost registry to grow slightly larger than the cache to capture "longer" cyclic patterns, which helps us rescue items that are part of a working set larger than the cache capacity.
3.  **Liner Aging**: We introduce a slight aging factor to frequency to prevent "cache pollution" where an item accumulates a count of 10 and stays forever despite not being touched for millions of cycles.

### Improved Cache Eviction Algorithm

```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```
2025-11-27 20:08:42,906 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.0723

Example 2:
- Trace ID: 26
- Hit Rate: 0.7895

Example 3:
- Trace ID: 47
- Hit Rate: 0.0817

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:08:42 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:08:42,907 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:09:56 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:09:56,945 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:09:56,946 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the weaknesses of the previous implementation:
1.  **Scan Resistance:** The previous algorithm allowed low-frequency items to pollute the cache if the "recency bonus" wasn't calibrated perfectly.
2.  **Size Awareness:** The previous algorithm ignored object size. In byte-constrained caches, evicting one large object can make room for many small, popular objects, significantly increasing the Hit Rate.
3.  **Dynamic Aging:** The explicit "periodic aging" in the previous code is clunky. We need a continuous aging mechanism.

### The Solution: Greedy Dual-Size Frequency (GDSF)
We will implement the **GDSF** algorithm. This is a state-of-the-art algorithm for variable-size web/object caching.

**Concept:**
Each object is assigned a priority value $H$.
$$H = L + \frac{Frequency}{Size}$$
*   **$L$ (Inflation Value):** A global running value that represents the "aging" of the cache. It starts at 0.
*   **$Frequency$:** How many times the object has been accessed.
*   **$Size$:** The size of the object.

**How it works:**
1.  **Eviction:** When space is needed, the object with the **lowest** $H$ value is evicted.
2.  **Aging:** Upon eviction of an object with priority $H_{min}$, we update $L = H_{min}$. This effectively "raises the water level," making older objects (whose $H$ was calculated with a smaller $L$) more likely to be evicted unless they are accessed again.
3.  **Restoration:** When an object is hit or re-inserted, its priority is recalculated using the *current* $L$, bringing it to the "top" of the recency stack.
4.  **Ghost Registry:** We store the frequency of evicted items so if they return (thrashing or cyclic patterns), they resume with their previous popularity rather than starting at 0.

### Improved Implementation

```python
import math

# Metadata Globals
# Tracks the computed GDSF priority for active cache items.
m_priority = dict()

# Tracks the raw frequency count. 
# We separate this because Priority changes with L, but Frequency accumulates.
m_frequency = dict()

# Ghost Registry: Stores frequencies of recently evicted items to handle cyclic patterns/thrashing.
m_ghost_freq = dict()

# The "Inflation Value" (L) for GDSF.
# This represents the priority of the most recently evicted item.
m_L = 0.0

# Configuration
# Max ghost entries to prevent memory leaks in metadata
MAX_GHOST_ENTRIES = 5000 

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction Policy.
    Finds the item with the lowest Priority score.
    Updates the global aging factor L to the priority of the evicted item.
    '''
    global m_priority, m_L
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # We need to find the key with the minimum priority.
    # While iterating is O(N), for typical simulation cache sizes this is efficient enough.
    # (A production system would use a Min-Heap).
    
    victim_key = None
    min_priority = float('inf')
    
    # Tie-breaking: 
    # If priorities are equal, the order of iteration (insertion order in modern Python) 
    # usually acts as a secondary FIFO tie-breaker, which is acceptable.
    for key in candidate_keys:
        p = m_priority.get(key, 0.0)
        if p < min_priority:
            min_priority = p
            victim_key = key
            
    # GDSF Critical Step:
    # Update L to the priority of the item leaving the cache.
    # This "ages" all remaining items relative to new insertions.
    if victim_key is not None:
        m_L = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a hit:
    1. Increment Frequency.
    2. Recalculate Priority using CURRENT L and new Frequency.
       This restores the item's Recency and boosts its Frequency factor.
    '''
    global m_priority, m_frequency, m_L
    
    key = obj.key
    
    # 1. Update Frequency
    current_freq = m_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_frequency[key] = new_freq
    
    # 2. Update Priority
    # Priority = L + (Frequency / Size)
    # Note: We use the CURRENT global L. This brings the item to the "front" of time.
    # Size penalty: Large items need much higher frequency to stay in cache.
    size = obj.size if obj.size > 0 else 1
    m_priority[key] = m_L + (new_freq / size)

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    1. Check Ghost Registry for past frequency (warm start).
    2. Calculate initial Priority based on current L.
    '''
    global m_priority, m_frequency, m_ghost_freq, m_L
    
    key = obj.key
    
    # 1. Frequency Restoration (Ghost Logic)
    if key in m_ghost_freq:
        # It was here recently, restore its frequency + 1 for the current access
        freq = m_ghost_freq[key] + 1
        # Remove from ghost since it's now in main cache
        del m_ghost_freq[key]
    else:
        # Cold start
        freq = 1
        
    m_frequency[key] = freq
    
    # 2. Calculate Priority
    # New items start "fresh" at the current L water-mark.
    size = obj.size if obj.size > 0 else 1
    m_priority[key] = m_L + (freq / size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    1. Save frequency to Ghost Registry.
    2. Clean up active metadata.
    '''
    global m_priority, m_frequency, m_ghost_freq
    
    key = evicted_obj.key
    
    # 1. Move Frequency to Ghost
    if key in m_frequency:
        m_ghost_freq[key] = m_frequency[key]
        del m_frequency[key]
        
    # 2. Remove Priority
    if key in m_priority:
        del m_priority[key]
        
    # 3. Maintain Ghost Size
    # Prevent metadata explosion. Since Python 3.7+, dicts preserve insertion order.
    # We remove the oldest inserted ghost item if we exceed capacity.
    if len(m_ghost_freq) > MAX_GHOST_ENTRIES:
        # next(iter(dict)) gets the first key (oldest inserted) efficiently
        oldest_ghost_key = next(iter(m_ghost_freq))
        del m_ghost_freq[oldest_ghost_key]
```
2025-11-27 20:10:08,134 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 18
- Hit Rate: 0.0229

Example 2:
- Trace ID: 9
- Hit Rate: 0.531

Example 3:
- Trace ID: 31
- Hit Rate: 0.0001

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:10:08 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:10:08,135 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:10:37 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:10:37,635 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:10:37,636 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the poor performance observed in the feedback, particularly the extremely low hit rates in Examples 1 and 3. These results strongly suggest that the previous algorithm was vulnerable to "cache pollution" from scan-heavy workloads (sequences of one-time access items) which flush out valuable, frequently accessed items.

The improved algorithm implements a **Frequency-Aware Least Recently Used (Re-insertion) Policy**, often similar to algorithms like **ARC (Adaptive Replacement Cache)** or **LIRS**, but simplified for this context into a **Segmented LRU with Admission Control**.

### Key Improvements:
1.  **Segmented Cache (Probation vs. Protected):** Instead of a single scoring formula, we conceptually split the cache into a "Probationary" segment (items seen once) and a "Protected" segment (items seen multiple times).
2.  **Scan Resistance:** Items enter the Probationary segment first. They are only promoted to the Protected segment if they are accessed again while in the cache. This prevents one-time scans from evicting high-frequency "hot" items.
3.  **Ghost Cache for Tuning:** We maintain a history of recently evicted keys ("ghosts"). If a ghost key is accessed, it indicates our cache was too small or we evicted the wrong thing. We use this signal to dynamically adjust the capacity allocated to the Protected segment.
4.  **Recency within Frequency:** Within both segments, we use LRU (Least Recently Used) to decide eviction. This is generally more robust than complex math formulas for scoring.

Here is the complete improved implementation:

```python
import collections

# Metadata Globals

# 1. Main Cache Lists (simulated via OrderedDict/dict logic)
# Tracks items that have been seen only once recently (Probationary/L1)
m_probationary = set()
# Tracks items that have been seen at least twice (Protected/L2)
m_protected = set()

# 2. Recency Tracking
# Map: Key -> Last Access Time. Used to determine LRU within sets.
m_last_access = dict()

# 3. Ghost Lists (History of evicted keys)
# Tracks keys recently evicted from Probationary (B1)
m_ghost_probationary = collections.deque(maxlen=10000)
m_ghost_probationary_set = set()
# Tracks keys recently evicted from Protected (B2)
m_ghost_protected = collections.deque(maxlen=10000)
m_ghost_protected_set = set()

# 4. Adaptive Parameter
# Target size for the Protected segment (p).
# 0 <= p <= Capacity. 
# If p is high, we act like LFU (prefer keeping freq items).
# If p is low, we act like LRU (prefer keeping recent items).
m_target_protected_size = 0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Segmented LRU.
    
    We decide which segment (Probationary or Protected) to evict from based on 
    the current size of the Protected segment relative to the target `m_target_protected_size`.
    '''
    global m_probationary, m_protected, m_last_access, m_target_protected_size
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # Calculate current sizes based on keys actually in the cache snapshot
    # (We filter our metadata sets to ensure consistency with the physical cache)
    active_probationary = [k for k in m_probationary if k in cache_snapshot.cache]
    active_protected = [k for k in m_protected if k in cache_snapshot.cache]
    
    victim_key = None
    
    # Decision Logic:
    # If the Protected set is exceeding its target size, we must prune it to make room.
    # Otherwise, we default to pruning the Probationary set (Scan resistance).
    # Logic derived from ARC (Adaptive Replacement Cache) principles.
    
    len_p = len(active_protected)
    len_b1 = len(m_ghost_probationary_set)
    len_b2 = len(m_ghost_protected_set)
    
    # We evict from Protected if:
    # 1. It is strictly larger than its target.
    # 2. OR (Optimization) It contains data, Probationary is empty, and we need space.
    evict_from_protected = False
    
    if len_p > m_target_protected_size:
        evict_from_protected = True
    elif not active_probationary:
        # If probation is empty, we have no choice but to drop a protected item
        evict_from_protected = True
    
    # Perform Eviction
    if evict_from_protected and active_protected:
        # LRU eviction from Protected Segment
        victim_key = min(active_protected, key=lambda k: m_last_access.get(k, 0))
    elif active_probationary:
        # LRU eviction from Probationary Segment
        victim_key = min(active_probationary, key=lambda k: m_last_access.get(k, 0))
    else:
        # Fallback (should rarely reach here unless metadata is out of sync)
        victim_key = min(candidate_keys, key=lambda k: m_last_access.get(k, 0))
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: 
    1. Update Recency.
    2. If item was in Probationary, promote to Protected.
    '''
    global m_probationary, m_protected, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    m_last_access[key] = current_time
    
    # Promotion: If it hits, it has proven its worth. Move to Protected.
    if key in m_probationary:
        m_probationary.remove(key)
        m_protected.add(key)
    elif key not in m_protected:
        # Edge case: It's in cache but not in our sets (sync issue), treat as protected
        m_protected.add(key)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: 
    1. Check Ghost hits to adapt `m_target_protected_size`.
    2. Add new item to Probationary list.
    '''
    global m_probationary, m_protected, m_last_access
    global m_ghost_probationary_set, m_ghost_protected_set
    global m_ghost_probationary, m_ghost_protected
    global m_target_protected_size
    
    key = obj.key
    current_time = cache_snapshot.access_count
    capacity = cache_snapshot.capacity # Note: In object count logic, this is count, but here it's bytes.
    # Approximation for adaptation logic: we treat count as capacity proxy for the target ratio.
    # Ideally we'd use 'capacity' directly, but since `m_target_protected_size` is a counter, 
    # we scale it relative to current number of items.
    current_count_capacity = len(cache_snapshot.cache) 
    
    m_last_access[key] = current_time
    
    # --- Adaptive Tuning (ARC Logic) ---
    
    # Case A: Ghost Hit on Probationary History (B1)
    # Means we evicted a 'scan' item too early. We should have had a smaller Protected area
    # and a larger Probationary area.
    if key in m_ghost_probationary_set:
        delta = 1
        b1_len = len(m_ghost_probationary_set)
        b2_len = len(m_ghost_protected_set)
        
        if b1_len < b2_len:
            delta = b2_len // b1_len
            
        # Increasing target for Protected? No, hitting B1 means we needed more room in L1 (Probationary).
        # Wait, ARC logic: Hitting B1 means we just missed it in L1. 
        # Actually, hitting B1 implies the probation list was too small, so we should shrink Protected
        # to grow Probation. But standard ARC increases P here?
        # Correction: If we hit in B1, it means the item was recently evicted from L1. 
        # If we had a larger L1 (and thus smaller L2/Protected), it might have stayed.
        # So we actually want to favor Recency (L1) -> Decrease P?
        # Standard ARC: If x in B1, p = min(c, p + max(1, |B2|/|B1|)). 
        # Increasing P essentially reserves more space for the "Protected" items, 
        # but in this implementation context, let's stick to the intuition:
        # If we see ghosts of one-timers, we might want to let them live longer -> increase L1 -> Decrease Target P.
        # HOWEVER, the standard ARC formula increases P on B1 hit. Why? 
        # Because B1 represents "recent" misses. If we miss recent things, we need to cache more recent things.
        # But `m_target_protected_size` is the size of L2.
        # Let's use a simplified heuristic:
        # If hit in Ghost Probationary -> It was a recent item we tossed. Bias toward Recency (Probationary).
        # Decrease Target Protected Size.
        m_target_protected_size = max(0, m_target_protected_size - 1)
        
        # Clean up ghost
        m_ghost_probationary_set.remove(key)
        # Remove from deque is O(N), expensive, so we just leave it in deque 
        # and rely on set for lookup. Lazy deletion happens in _cleanup.

    # Case B: Ghost Hit on Protected History (B2)
    # Means we evicted a 'hot' item. We need a larger Protected area.
    elif key in m_ghost_protected_set:
        m_target_protected_size = min(current_count_capacity, m_target_protected_size + 1)
        
        # Clean up ghost
        m_ghost_protected_set.remove(key)

    # --- Placement ---
    
    # New items always start in Probationary (L1)
    # Exception: If it was in Ghost Protected, it was a "hot" item that came back.
    # We restore it directly to Protected to protect it from the scan filter.
    if key in m_ghost_protected_set: # (Check again or use flag if logic separated)
        # Logic handled above removes it from set, so we can't check set again easily 
        # without a flag. Let's simplify: 
        # New items go to Probationary. If it gets hit again, it moves to Protected.
        m_probationary.add(key)
    else:
        m_probationary.add(key)

    _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move the victim key to the appropriate Ghost list.
    '''
    global m_probationary, m_protected, m_last_access
    global m_ghost_probationary, m_ghost_probationary_set
    global m_ghost_protected, m_ghost_protected_set
    
    if not evicted_obj:
        return

    key = evicted_obj.key
    
    # Determine where it came from
    if key in m_protected:
        m_protected.remove(key)
        # Add to Ghost Protected (B2)
        m_ghost_protected.append(key)
        m_ghost_protected_set.add(key)
    elif key in m_probationary:
        m_probationary.remove(key)
        # Add to Ghost Probationary (B1)
        m_ghost_probationary.append(key)
        m_ghost_probationary_set.add(key)
    else:
        # Fallback: assume probationary
        m_ghost_probationary.append(key)
        m_ghost_probationary_set.add(key)

    # Clean up main memory dict
    if key in m_last_access:
        del m_last_access[key]

def _cleanup_metadata():
    '''
    Keep ghost lists bounded to prevent memory leaks.
    '''
    global m_ghost_probationary, m_ghost_probationary_set
    global m_ghost_protected, m_ghost_protected_set
    
    # Limit size of ghost lists (e.g., to 2000 items each)
    limit = 2000
    
    while len(m_ghost_probationary) > limit:
        k = m_ghost_probationary.popleft()
        if k in m_ghost_probationary_set:
            m_ghost_probationary_set.remove(k)
            
    while len(m_ghost_protected) > limit:
        k = m_ghost_protected.popleft()
        if k in m_ghost_protected_set:
            m_ghost_protected_set.remove(k)
```
2025-11-27 20:10:47,193 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 34
- Hit Rate: 0.0789

Example 2:
- Trace ID: 47
- Hit Rate: 0.0392

Example 3:
- Trace ID: 20
- Hit Rate: 0.3884

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:10:47 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:10:47,194 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:11:16 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:11:16,270 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:11:16,271 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we need to move away from a purely formulaic approach that attempts to balance Frequency and Recency manually (which often fails on edge cases like scans or loops) and adopt an adaptive strategy.

The solution below implements an algorithm inspired by **ARC (Adaptive Replacement Cache)**. ARC is superior because it dynamically tunes the balance between Recency (LRU) and Frequency (LFU) based on the workload's behavior.

### Key Improvements:
1.  **Dual Queues (T1 and T2):**
    *   **T1 (Recency):** Captures items seen recently but only once. This handles "new" items.
    *   **T2 (Frequency):** Captures items seen at least twice. This handles "popular" items.
2.  **Ghost Caches (B1 and B2):** We track keys evicted from T1 (into B1) and T2 (into B2). This history allows the algorithm to learn. If we get a hit in B1, we know the Recency cache was too small (we evicted something we shouldn't have), so we increase the size target for T1. If we hit B2, we increase the size target for T2.
3.  **Adaptive Parameter (p):** This variable dynamically shifts the target size of T1 vs T2.
4.  **Removal of Magic Constants:** The previous implementation relied on arbitrary decay factors (`0.95`) and thresholds (`5000`). This approach uses the cache capacity itself to regulate behavior, making it robust across different trace sizes.

```python
import math

# Metadata Logic: Adaptive Replacement Cache (ARC) Strategy
# ARC maintains two lists of active pages:
#   T1: Pages accessed only once recently (Recency focus)
#   T2: Pages accessed at least twice recently (Frequency focus)
# And two lists of history (Ghost Caches):
#   B1: History of pages evicted from T1
#   B2: History of pages evicted from T2

# Global State
m_T1 = set()    # Active cache: Recently seen once
m_T2 = set()    # Active cache: Frequently seen
m_B1 = set()    # Ghost cache: Evicted from T1
m_B2 = set()    # Ghost cache: Evicted from T2

# Tracks insertion order for LRU behavior within the sets
# We use Python dicts as ordered sets (Python 3.7+ preserves insertion order)
m_LRU_T1 = dict() 
m_LRU_T2 = dict()
m_LRU_B1 = dict()
m_LRU_B2 = dict()

# Adaptation Parameter 'p'
# 0 <= p <= capacity
# Target size for T1 is p. Target size for T2 is (capacity - p).
m_p = 0

def _replace(cache_snapshot, p):
    """
    Subroutine to decide which page to evict based on the target size 'p'.
    """
    global m_T1, m_T2, m_B1, m_B2, m_LRU_T1, m_LRU_T2, m_LRU_B1, m_LRU_B2
    
    # Check if T1 is non-empty and exceeds its target or implicit condition
    t1_size = len(m_T1)
    
    # We evict from T1 if it has items and is over the target 'p',
    # OR if T1 is just larger than p and we need space.
    # Specifically, if len(T1) > p, we evict the LRU of T1 to B1.
    if m_T1 and (t1_size > p or (m_B2 and t1_size == p)):
        # Evict LRU from T1
        victim_key = next(iter(m_LRU_T1))
        del m_LRU_T1[victim_key]
        m_T1.remove(victim_key)
        
        # Move to B1 (Ghost Recency)
        m_B1.add(victim_key)
        m_LRU_B1[victim_key] = True
        return victim_key
    else:
        # Evict LRU from T2 to B2
        if m_LRU_T2:
            victim_key = next(iter(m_LRU_T2))
            del m_LRU_T2[victim_key]
            m_T2.remove(victim_key)
            
            # Move to B2 (Ghost Frequency)
            m_B2.add(victim_key)
            m_LRU_B2[victim_key] = True
            return victim_key
            
    # Fallback (should not happen if cache is populated)
    return None

def evict(cache_snapshot, obj):
    """
    Determines the victim key to evict.
    """
    global m_p
    # Note: The 'obj' passed here is the NEW object trying to enter, 
    # but we don't need its details to decide WHO to kill, only the state of ARC.
    
    # In a standard cache interaction, evict is called when cache is full.
    # We delegate the choice to the _replace logic which respects the parameter 'p'.
    victim_key = _replace(cache_snapshot, m_p)
    return victim_key

def update_after_hit(cache_snapshot, obj):
    """
    Handles a cache hit.
    Effect: Promotes items to MRU positions and moves items from T1 to T2.
    """
    global m_T1, m_T2, m_LRU_T1, m_LRU_T2, m_p
    
    key = obj.key
    
    # If it is in T1 (Recency List), move it to T2 (Frequency List)
    if key in m_T1:
        m_T1.remove(key)
        if key in m_LRU_T1: del m_LRU_T1[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True # Add to MRU end
    
    # If it is in T2, update its LRU position (move to MRU end)
    elif key in m_T2:
        if key in m_LRU_T2: del m_LRU_T2[key]
        m_LRU_T2[key] = True

def update_after_insert(cache_snapshot, obj):
    """
    Handles inserting a new object into the cache.
    This logic includes the adaptive tuning of 'p' when hitting ghost caches.
    """
    global m_T1, m_T2, m_B1, m_B2, m_LRU_T1, m_LRU_T2, m_LRU_B1, m_LRU_B2, m_p
    
    key = obj.key
    capacity = cache_snapshot.capacity # Note: ARC usually defines cap by count, here we approximate with slots or size.
    # Assuming the simulation capacity is count-based roughly, or we normalize. 
    # For strict object counts, we can use len(cache). 
    # However, since we track T1/T2 independently of the physical cache snapshot sometimes,
    # we treat the 'cache' size as the authority.
    
    # Case 1: Key is in Ghost Cache B1 (Was in T1 recently, but evicted)
    # Interpretation: We should have made T1 larger.
    if key in m_B1:
        # Adaptation: Increase p
        delta = 1
        if len(m_B1) < len(m_B2):
            delta = len(m_B2) // len(m_B1)
        m_p = min(capacity, m_p + delta)
        
        # Move from B1 history to T2 (It has now been seen twice: once T1->B1, now B1->T2)
        m_B1.remove(key)
        if key in m_LRU_B1: del m_LRU_B1[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True
        return

    # Case 2: Key is in Ghost Cache B2 (Was in T2 recently, but evicted)
    # Interpretation: We should have made T2 larger (T1 smaller).
    if key in m_B2:
        # Adaptation: Decrease p
        delta = 1
        if len(m_B2) < len(m_B1):
            delta = len(m_B1) // len(m_B2)
        m_p = max(0, m_p - delta)
        
        # Move from B2 history to T2 (It is a frequent item returning)
        m_B2.remove(key)
        if key in m_LRU_B2: del m_LRU_B2[key]
        
        m_T2.add(key)
        m_LRU_T2[key] = True
        return

    # Case 3: Completely new item (not in T1, T2, B1, B2)
    # Insert into T1 (Recency List)
    
    # Cleanup history if it gets too large (usually 2x cache size is standard ARC implementation)
    total_l1 = len(m_T1) + len(m_B1)
    total_l2 = len(m_T2) + len(m_B2)
    
    if total_l1 >= capacity:
        if len(m_B1) > 0:
            del_key = next(iter(m_LRU_B1))
            del m_LRU_B1[del_key]
            m_B1.remove(del_key)
    
    if (total_l1 + total_l2) >= (2 * capacity):
         if len(m_B2) > 0:
            del_key = next(iter(m_LRU_B2))
            del m_LRU_B2[del_key]
            m_B2.remove(del_key)

    # Finally add the new item to T1
    m_T1.add(key)
    m_LRU_T1[key] = True

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    Post-eviction cleanup.
    """
    # The actual logic of moving items to B1/B2 happens inside `evict` (via _replace)
    # or implicitly during insert/updates.
    # However, we must ensure consistency between the Physical Cache and our Metadata.
    
    # In a standard simulation flow:
    # 1. evict() is called -> we chose a victim, updated our T/B lists, returned key.
    # 2. Simulator removes that key from cache.
    # 3. update_after_evict() is called.
    
    # Since we did the metadata move in `evict`, we don't strictly need to do anything here,
    # unless the simulator evicted something DIFFERENT than what we suggested (unlikely).
    pass
```
2025-11-27 20:13:21,100 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 26
- Hit Rate: 0.857

Example 2:
- Trace ID: 29
- Hit Rate: 0.0364

Example 3:
- Trace ID: 13
- Hit Rate: 0.0

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:13:21 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:13:21,101 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:13:49 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:13:49,016 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:13:49,017 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An improved cache eviction algorithm designed to minimize miss rates by combining recency, frequency, and size-awareness with a dynamic feedback loop.

### Analysis of Previous Performance
The previous algorithm attempted a hybrid scoring approach but failed significantly on specific traces (Trace 13 and 29), likely due to:
1.  **Ghost Cache Management**: The mechanism for "ghosts" was likely too aggressive or memory-intensive, and the adaptive factor might have oscillated too wildly.
2.  **Scan Resistance**: While it penalized frequency-1 items, it didn't strictly separate them. In scan-heavy workloads (like Trace 29/13), new items flush out the entire cache before they can prove their utility. This is the classic "LIRS" or "ARC" problem.
3.  **Lack of Size Awareness**: The context mentions `obj.size`. In many caching scenarios (especially web/CDN), evicting large items can be beneficial because it frees up space for *multiple* smaller items, potentially increasing the total hit rate.

### Strategy for Improvement: Segmented LRU with Size Awareness (SLRU-S)
To address the failures, this implementation moves to a **Segmented LRU (SLRU)** inspired design with an added **Size Factor**.

1.  **Probation vs. Protected Segments**:
    *   **Probationary Segment**: New items enter here. They are the first candidates for eviction. This acts as a filter for one-time "scan" accesses.
    *   **Protected Segment**: Items that get a hit *while* in the probationary segment are promoted here. These are "proven" popular items.
2.  **Size Penalty**:
    *   Larger objects consume more of the limited capacity. We penalize large objects slightly to prefer retaining 10 small popular objects over 1 large popular object (density-based caching).
3.  **Recency within Segments**:
    *   Within both segments, we track Recency (Last Access Time).
4.  **Fallback**:
    *   If the Protected segment grows too large, we demote the LRU item from Protected back to Probation.
    *   Eviction happens strictly from the Probation segment's LRU end (unless empty).

This structure naturally handles scan traces (scans churn through Probation without touching Protected) and adapts to frequency (frequent items move to Protected).

```python
import math

# Metadata Globals
# Tracks the last access time (Recency) for all keys
m_last_access = dict()
# Tracks the frequency of access for all keys
m_frequency = dict()
# Tracks which segment a key belongs to. 
# True = Protected (High Frequency/Utility), False = Probation (New/Low Utility)
m_protected = dict()

# Algorithm Parameters
# Maximum percentage of cache count dedicated to the "Protected" segment.
# 80% ensures we keep popular items but leave 20% buffer for new items to prove themselves.
PROTECTED_RATIO = 0.8
# Tunable parameter for size penalty. Higher = stronger bias against large items.
SIZE_PENALTY_FACTOR = 0.5

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Size Awareness.
    
    1. We divide the cache into two logical segments: "Probation" and "Protected".
    2. New items start in Probation.
    3. Items are promoted to Protected if accessed again while in Probation.
    4. We prefer evicting from Probation. 
    5. If Probation is empty (unlikely) or we need to balance, we demote from Protected.
    
    Scoring Metric for Eviction (within Probation):
    Score = Recency_Age * (Size ^ SIZE_PENALTY_FACTOR)
    Higher Score = Older and/or Larger = Better Victim.
    '''
    global m_last_access, m_protected, m_frequency
    
    current_time = cache_snapshot.access_count
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None
    
    # 1. Identify candidates in Probation vs Protected
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        if m_protected.get(k, False):
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    # 2. Dynamic Balancing: If Protected segment is too big, force a demotion first.
    # This prevents the cache from becoming static with old "popular" items.
    capacity_count = len(candidate_keys)
    target_protected_size = int(capacity_count * PROTECTED_RATIO)
    
    # If we have too many protected items, we pick the LRU from Protected to evict/demote.
    # However, standard SLRU just demotes. Here, since we MUST evict to make space,
    # we treat the LRU of Protected as a valid eviction candidate only if Probation is empty.
    # Otherwise, we evict from Probation.
    
    pool_to_evict_from = probation_keys
    
    if not pool_to_evict_from:
        # Fallback: If probation is empty, we must kill a protected item.
        pool_to_evict_from = protected_keys
        
    # 3. Select Victim from the chosen pool
    best_victim = None
    max_eviction_score = -1.0
    
    for k in pool_to_evict_from:
        last_acc = m_last_access.get(k, 0)
        age = (current_time - last_acc)
        
        # Retrieve object size
        item_size = 1
        cached_obj = cache_snapshot.cache.get(k)
        if cached_obj:
            item_size = cached_obj.size
            
        # Calculate Eviction Score
        # We want to evict items that are OLD (high age) and LARGE (high size).
        # Log of size helps dampen the impact of extremely large files, preventing
        # the algorithm from becoming purely "Smallest Object First".
        size_metric = math.log(item_size + 1) # +1 avoids log(1)=0 if size=1
        
        # Score = Age * (Size factor)
        # Standard LRU is just 'Age'. 
        # By multiplying by size metric, we say "A slightly younger large object 
        # is a better victim than a very old tiny object."
        score = age * (size_metric ** SIZE_PENALTY_FACTOR)
        
        if score > max_eviction_score:
            max_eviction_score = score
            best_victim = k
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Recency.
    2. Increment Frequency.
    3. Promote to Protected segment if not already there.
    '''
    global m_last_access, m_frequency, m_protected
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    # Promotion Logic (SLRU)
    # If it was in cache and got a hit, it has proven utility.
    m_protected[key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Place in Probation segment.
    '''
    global m_last_access, m_frequency, m_protected
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = 1 # First time seen
    
    # New items always start in Probation (False)
    # This filters out "scan" traffic (items seen once and never again).
    m_protected[key] = False

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up metadata for the evicted object to free memory.
    '''
    global m_last_access, m_frequency, m_protected
    
    if evicted_obj:
        key = evicted_obj.key
        if key in m_last_access:
            del m_last_access[key]
        if key in m_frequency:
            del m_frequency[key]
        if key in m_protected:
            del m_protected[key]
```
2025-11-27 20:14:10,108 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 3
- Hit Rate: 0.4431

Example 2:
- Trace ID: 2
- Hit Rate: 0.4934

Example 3:
- Trace ID: 32
- Hit Rate: 0.03

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:14:10 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:14:10,109 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:15:19 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:15:19,945 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:15:19,946 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is the improved cache eviction algorithm. It implements a **Segmented LRU (SLRU)** strategy with a **Ghost Registry**. This approach is significantly more robust than a simple score-based system because it physically separates "Probationary" items (newly inserted, potentially scans) from "Protected" items (frequently accessed).

This structure specifically targets the issues in the feedback:
1.  **Trace 32 (Scans):** Scanning patterns will flow through the "Probation" segment and be evicted quickly without disturbing the "Protected" segment, preventing the 3% hit rate scenario.
2.  **Hit Rate Optimization:** High-frequency items are promoted to the "Protected" segment, keeping them in the cache longer.
3.  **Ghost Registry:** It detects if the "Probation" segment is too small by catching items that are re-accessed shortly after eviction.

```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'last_access': int, 
#   'segment': int (0 = Probation/Amateur, 1 = Protected/Elite)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items to detect false negatives (premature eviction).
m_ghost_registry = dict()

# Configuration
# The portion of the cache capacity reserved for "Protected" items.
# 0.8 means up to 80% of the cache bytes can be held by Protected items.
# If Protected grows larger, we force eviction from it to make room for new blood.
PROTECTED_CAPACITY_RATIO = 0.8
# Max items in ghost registry relative to current object count
GHOST_RATIO = 1.0

def evict(cache_snapshot, obj):
    '''
    Implements Segmented LRU Eviction.
    
    Logic:
    1. The cache is conceptually divided into two segments: Probation (0) and Protected (1).
    2. We calculate the total size currently occupied by Protected items.
    3. IF Protected segment is overflowing (>80% capacity):
       - We MUST evict from the Protected segment (LRU policy within that segment) to demote.
    4. ELSE (Normal case):
       - We evict from the Probation segment (LRU policy within that segment).
       - If Probation is empty, fallback to Protected.
    '''
    global m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Analyze current state of segments
    protected_size = 0
    protected_candidates = []
    probation_candidates = []
    
    # Single pass to categorize candidates and calculate sizes
    for key in candidate_keys:
        # Retrieve metadata, default to Probation if missing (fail-safe)
        meta = m_access_info.get(key, {'last_access': 0, 'segment': 0})
        obj_in_cache = cache_snapshot.cache[key]
        
        if meta['segment'] == 1:
            protected_size += obj_in_cache.size
            protected_candidates.append((key, meta['last_access']))
        else:
            probation_candidates.append((key, meta['last_access']))

    # 2. Determine which segment to victimize
    target_protected_capacity = cache_snapshot.capacity * PROTECTED_CAPACITY_RATIO
    
    victim_key = None
    
    # If Protected segment is too big, we prune it (effectively demoting/evicting from Elite)
    if protected_size > target_protected_capacity and protected_candidates:
        # Find LRU in Protected
        # Min by last_access
        victim_key = min(protected_candidates, key=lambda x: x[1])[0]
    else:
        # Otherwise, we prefer to evict from Probation
        if probation_candidates:
            # Find LRU in Probation
            victim_key = min(probation_candidates, key=lambda x: x[1])[0]
        elif protected_candidates:
            # Fallback: Cache is 100% protected items, evict LRU from there
            victim_key = min(protected_candidates, key=lambda x: x[1])[0]
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time (LRU logic).
    2. Promotion: If item was in Probation, move to Protected.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        # Promotion: A hit proves utility. Welcome to the Elite segment.
        meta['segment'] = 1 
        m_access_info[obj.key] = meta
    else:
        # Reconstruction if metadata missing
        m_access_info[obj.key] = {
            'last_access': current_time,
            'segment': 1 # Assume protected if it survived to get a hit
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost: Insert as "Protected" (Resurrected).
    3. Else: Insert as "Probation" (Newcomer).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # Determine Initial Segment
    # If it was in ghost, we made a mistake evicting it recently. 
    # Promote immediately to prevent "Thrashing".
    initial_segment = 1 if is_ghost else 0
    
    m_access_info[obj.key] = {
        'last_access': current_time,
        'segment': initial_segment
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Clean up active metadata.
    2. Add to Ghost Registry (to catch false negatives).
    3. Prune Ghost Registry.
    '''
    global m_access_info, m_ghost_registry
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    # We only care THAT it existed, not when (FIFO removal is fine).
    m_ghost_registry[evicted_obj.key] = True
    
    # 3. Maintain Ghost Registry Size
    # Keep ghost registry roughly size of cache to detect immediate loops.
    # Python 3.7+ dicts preserve insertion order, so this acts as a FIFO queue.
    max_ghost_count = max(len(cache_snapshot.cache), 10) # Minimum safety buffer
    
    if len(m_ghost_registry) > max_ghost_count:
        # Remove oldest inserted key (first key in dict)
        try:
            oldest = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest]
        except StopIteration:
            pass
```
2025-11-27 20:15:35,194 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 39
- Hit Rate: 0.1436

Example 2:
- Trace ID: 23
- Hit Rate: 0.0587

Example 3:
- Trace ID: 46
- Hit Rate: 0.4663

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:15:35 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:15:35,195 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:16:11 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:16:11,272 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:16:11,273 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache hit rate significantly across diverse workloads (like the ones shown in the feedback: scan-heavy, recency-heavy, or frequency-heavy), we need a more robust adaptive algorithm.

The previous implementation attempted a manual "Segmented LRU" with a Ghost cache. However, the static threshold (Frequency=1 vs >1) and the lack of dynamic resizing between the "recent" and "frequent" segments make it rigid. It likely fails when the workload shifts from requiring recency (LRU) to frequency (LFU) or vice versa.

### The Solution: Adaptive Replacement Cache (ARC)

I will implement a variant of **ARC (Adaptive Replacement Cache)**. ARC is generally considered superior to LRU, LFU, and static Segmented LRU because it dynamically tunes itself based on the workload.

**Core Concept:**
ARC maintains two lists (logically):
1.  **L1 (Recency):** Items accessed only once recently. (Captures new, potentially scan-based traffic).
2.  **L2 (Frequency):** Items accessed at least twice. (Captures popular, frequently accessed items).

Crucially, ARC also maintains "Ghost" lists (evicted keys) for both L1 and L2 (let's call them B1 and B2).
*   If we get a hit in the ghost list B1, it means we should have kept the item in L1 longer. **We increase the size of L1.**
*   If we get a hit in the ghost list B2, it means we should have kept the item in L2 longer. **We decrease the size of L1 (giving more space to L2).**

### Implementation Details

Since I cannot modify the Cache class itself, I will maintain the ARC logic entirely within the metadata structures (`m_access_info` and global tracking variables).

**Data Structures:**
*   `T1`: Active keys in the Recency list (subset of `cache_snapshot.cache`).
*   `T2`: Active keys in the Frequency list (subset of `cache_snapshot.cache`).
*   `B1`: Ghost keys evicted from T1 (keys only).
*   `B2`: Ghost keys evicted from T2 (keys only).
*   `p`: The target size of the T1 list (the "adaptation parameter").

**Logic Flow:**
*   **Insert:** Determine if the key is in B1 or B2 to adjust `p`. Add to T1 or T2 accordingly.
*   **Hit:** Move item to MRU of T2.
*   **Evict:** Use the `replace` logic of ARC to decide whether to evict from T1 or T2 based on the current size of T1 relative to `p`.

Here is the improved algorithm:

```python
import collections

# --- Global Metadata ---

# Ideally, we would use a class, but we must stick to the global variable constraints of the interface.
# We will simulate the ARC (Adaptive Replacement Cache) lists using OrderedDicts for O(1) LRU behavior.

# T1: Active Recency List (Key -> None). Order: LRU to MRU.
m_T1 = collections.OrderedDict()

# T2: Active Frequency List (Key -> None). Order: LRU to MRU.
m_T2 = collections.OrderedDict()

# B1: Ghost Recency List (Key -> None). Keys evicted from T1. Order: LRU to MRU.
m_B1 = collections.OrderedDict()

# B2: Ghost Frequency List (Key -> None). Keys evicted from T2. Order: LRU to MRU.
m_B2 = collections.OrderedDict()

# p: Target size for T1 (The adaptation parameter). 
# Range: [0, capacity]. Starts at 0 (or capacity/2, but 0 is safe dynamic start).
m_p = 0

def evict(cache_snapshot, obj):
    '''
    Decides which object to evict based on the ARC (Adaptive Replacement Cache) logic.
    This function implements the 'REPLACE' logic of ARC.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    # Safety check
    if not cache_snapshot.cache:
        return None
        
    candidate_key = None
    
    # According to ARC "REPLACE" logic:
    # We evict from T1 if T1 is strictly larger than the target p.
    # OR, if T1 has items and (T1 is exactly p but the incoming item is in B2). 
    # (Note: The B2 check is handled implicitly by how we manage 'p' in update_after_insert/hit,
    # but strictly speaking, REPLACE evicts from T1 if len(T1) > p).
    
    t1_len = len(m_T1)
    
    # Logic:
    # If len(T1) >= 1 and ((incoming item is in B2 and len(T1) == p) OR (len(T1) > p)):
    # Then delete LRU of T1 and move to B1.
    # Else: delete LRU of T2 and move to B2.
    
    # Since 'evict' is called *before* we insert the new item (usually), 
    # we need to be careful. The standard framework asks us to return a key to evict.
    # We rely on m_p which was adjusted during the *miss* processing if we had access to the missed key 
    # in update_after_insert, but here we only see the snapshot.
    # However, strictly strictly:
    
    if t1_len > 0 and t1_len > m_p:
        # Evict LRU from T1
        candidate_key = next(iter(m_T1))
    else:
        # Evict LRU from T2
        # Fallback: if T2 is empty (should rarely happen if T1 <= p), try T1
        if m_T2:
            candidate_key = next(iter(m_T2))
        elif m_T1:
            candidate_key = next(iter(m_T1))
            
    return candidate_key

def update_after_hit(cache_snapshot, obj):
    '''
    ARC Logic on Hit (Case 1):
    x is in T1 or T2. Move x to MRU of T2.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    
    # If in T1, remove and move to T2 MRU
    if key in m_T1:
        del m_T1[key]
        m_T2[key] = None
    # If in T2, move to MRU
    elif key in m_T2:
        m_T2.move_to_end(key)
    else:
        # Edge case: Metadata out of sync with cache (e.g. restart), re-add to T2
        m_T2[key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    ARC Logic on Miss (Insert):
    Cases handled here:
    1. x in B1 (Ghost Recency): Adaptation (Increase p), Move from B1 to T2.
    2. x in B2 (Ghost Frequency): Adaptation (Decrease p), Move from B2 to T2.
    3. x not in anything: New item. Add to T1.
    
    Note: The actual eviction from Cache happens via the `evict` function, 
    but the Metadata lists (T1, T2, B1, B2) must be managed here to reflect the *intent*.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    key = obj.key
    c = cache_snapshot.capacity # Capacity C
    
    # --- Case 2: x in B1 (Miss in Cache, Hit in History L1) ---
    if key in m_B1:
        # Adaptation: Increase p
        # Delta = 1 if |B1| >= |B2|, else |B2| / |B1|
        len_b1 = len(m_B1)
        len_b2 = len(m_B2)
        delta = 1
        if len_b1 < len_b2:
            delta = len_b2 / len_b1
        
        m_p = min(c, m_p + delta)
        
        # Promote: Move from B1 to T2 (MRU)
        del m_B1[key]
        m_T2[key] = None
        
    # --- Case 3: x in B2 (Miss in Cache, Hit in History L2) ---
    elif key in m_B2:
        # Adaptation: Decrease p
        # Delta = 1 if |B2| >= |B1|, else |B1| / |B2|
        len_b1 = len(m_B1)
        len_b2 = len(m_B2)
        delta = 1
        if len_b2 < len_b1:
            delta = len_b1 / len_b2
            
        m_p = max(0, m_p - delta)
        
        # Promote: Move from B2 to T2 (MRU)
        del m_B2[key]
        m_T2[key] = None
        
    # --- Case 4: x not in (T1 u B1 u T2 u B2) ---
    else:
        # Completely new item.
        # Case A: L1 (T1+B1) has size c.
        # If we are just filling up, T1 just grows.
        # But logically for ARC, a new item goes to MRU of T1.
        m_T1[key] = None

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    The `evict` function selected a victim from Cache.
    Now we must update the Metadata lists (T1/T2 -> B1/B2) to reflect that eviction.
    '''
    global m_T1, m_T2, m_B1, m_B2, m_p
    
    victim_key = evicted_obj.key
    c = cache_snapshot.capacity
    
    # We need to ensure the victim is moved from T (Active) to B (Ghost).
    
    # If victim was in T1, move to B1
    if victim_key in m_T1:
        del m_T1[victim_key]
        m_B1[victim_key] = None
        # B1 Restriction: Maintain |T1| + |B1| <= C (roughly, or 2C in some papers, C is safer for strict memory)
        # Standard ARC allows directory size to be 2C. Let's limit B1 to Capacity to be safe.
        if len(m_B1) > c:
            m_B1.popitem(last=False) # Remove oldest
            
    # If victim was in T2, move to B2
    elif victim_key in m_T2:
        del m_T2[victim_key]
        m_B2[victim_key] = None
        # B2 Restriction: Maintain |T2| + |B2| <= 2C.
        if len(m_B2) > c:
             m_B2.popitem(last=False) # Remove oldest
             
    # Fallback cleanup (shouldn't happen if logic is tight)
    if victim_key in m_access_info_fallback:
         del m_access_info_fallback[victim_key]

# Fallback dictionary just in case we need to track something outside T1/T2/B1/B2
m_access_info_fallback = {}
```
2025-11-27 20:16:49,281 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata Globals
# Tracks the number of times a key has been accessed (Frequency)
m_frequency = dict()
# Tracks the last access time (Recency)
m_last_access = dict()
# Ghost cache: Tracks keys recently evicted to adapt the algorithm
m_ghost_hits = set()

# Algorithm Parameters
# This factor balances Recency vs Frequency importance. 
# It adapts dynamically based on ghost hits.
# Higher = bias toward Recency (LRU), Lower = bias toward Frequency (LFU)
ADAPTIVE_FACTOR = 0.5 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Adaptive Hybrid Scoring.
    
    We calculate a generic 'utility' score. Lower score = better candidate for eviction.
    Score = (Frequency ^ (1 - ADAPTIVE_FACTOR)) / (Time_Since_Access ^ ADAPTIVE_FACTOR)
    
    However, to handle the "Scan" traces (low hit rates in examples), we add a strict 
    penalty for items with Frequency=1 (Probationary items).
    '''
    global m_frequency, m_last_access, ADAPTIVE_FACTOR
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    victim_key = None
    min_score = float('inf')
    
    # We prioritize evicting "Probationary" items (seen only once) if they are old.
    # This prevents one-time scans from flushing out useful high-frequency items.
    
    for key in candidate_keys:
        freq = m_frequency.get(key, 1)
        last_access = m_last_access.get(key, 0)
        age = (current_time - last_access) + 1 # +1 to avoid division by zero
        
        # Base Score calculation
        # If ADAPTIVE_FACTOR is high (0.8), Age dominates (LRU-like).
        # If ADAPTIVE_FACTOR is low (0.2), Freq dominates (LFU-like).
        score = (freq ** (1.0 - ADAPTIVE_FACTOR)) / (age ** ADAPTIVE_FACTOR)
        
        # Penalize items seen only once significantly to improve scan resistance.
        if freq == 1:
            score = score / 10.0
            
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update frequency and recency.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    m_frequency[key] = m_frequency.get(key, 0) + 1

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Handle Ghost hits to tune ADAPTIVE_FACTOR.
    '''
    global m_frequency, m_last_access, m_ghost_hits, ADAPTIVE_FACTOR
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    # Initialize metadata
    m_last_access[key] = current_time
    
    # Check if this inserted key was recently evicted (Ghost Hit)
    if key in m_ghost_hits:
        # We evicted it, but now it's back. That was a mistake.
        # Check previous frequency to know *why* we shouldn't have evicted it.
        prev_freq = m_frequency.get(key, 0)
        
        if prev_freq > 1:
            # It was a popular item we evicted. We need to be more LFU-like (protect frequency).
            # Decrease Adaptive Factor (shift towards Frequency)
            ADAPTIVE_FACTOR = max(0.1, ADAPTIVE_FACTOR - 0.1)
        else:
            # It was a recent item (low freq) we evicted. We need to be more LRU-like.
            # Increase Adaptive Factor (shift towards Recency)
            ADAPTIVE_FACTOR = min(0.9, ADAPTIVE_FACTOR + 0.1)
            
        # Remove from ghost since it's now real
        m_ghost_hits.remove(key)
        
        # Restore/Boost frequency
        m_frequency[key] = m_frequency.get(key, 0) + 1
    else:
        # Brand new item
        m_frequency[key] = 1

    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: Move key to Ghost Cache.
    '''
    global m_ghost_hits
    
    if evicted_obj:
        m_ghost_hits.add(evicted_obj.key)
        # Note: We do NOT delete frequency/last_access here immediately, 
        # so we can remember stats if it returns (Ghost hit).

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion.
    '''
    global m_frequency, m_last_access, m_ghost_hits
    
    # Soft limit on metadata size (e.g., 2x capacity or fixed limit)
    # Using a fixed large number here as provided in context constraints often allow it,
    # but cleaning up ghosts is essential.
    MAX_META = 20000 
    
    if len(m_ghost_hits) > MAX_META:
        # Clear out old ghosts
        # In a real system we'd use a FIFO queue for ghosts, here we clear a chunk
        # by converting to list and slicing to save CPU cycles on sorting
        kill_list = list(m_ghost_hits)[:5000]
        for k in kill_list:
            m_ghost_hits.remove(k)
            # Once it leaves ghost cache, we can forget its history to save RAM
            if k not in cache_snapshot.cache:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 14
- Hit Rate: 0.0215

Example 2:
- Trace ID: 15
- Hit Rate: 0.0213

Example 3:
- Trace ID: 24
- Hit Rate: 0.2885

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:16:49 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:16:49,282 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:17:58 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:17:58,652 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:17:58,653 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is the improved cache eviction algorithm.

This solution implements a **Frequency-Aware Segmented LRU (SLRU)** strategy with **Historical Frequency Retention**.

### The Strategy
1.  **Segmentation:** The cache is logically divided into two segments based on object frequency:
    *   **Probationary Segment:** Items with a frequency of 1 (newly inserted or "scan" items).
    *   **Protected Segment:** Items with a frequency > 1 (items that have proven their worth).
2.  **Scan Resistance:** "Scan" operations (accessing many items once) often destroy cache performance by flushing out useful data. By prioritizing the eviction of **Probationary** items, we ensure that one-time-use objects pass through the cache quickly without displacing high-frequency "Protected" items.
3.  **Dynamic Capacity Management:** To prevent "Protected" items from filling 100% of the cache (which would starve new content), we enforce a soft limit (e.g., 80% of the keys). If the Protected segment grows too large, we force eviction from it to make room for new candidates.
4.  **Frequency Memory (Ghosting):** When an object is evicted, we do *not* immediately delete its frequency data. If it returns later (a "Ghost Hit"), it resumes with its previous frequency. This allows items with long inter-arrival times to quickly re-enter the "Protected" segment.
5.  **Aging:** To handle workload changes (where popular items become unpopular), we periodically "decay" (halve) the frequency counts.

```python
import math

# --- Metadata Globals ---

# Tracks the number of times a key has been accessed.
# We maintain this even after eviction to recognize returning items.
m_frequency = dict()

# Tracks the timestamp of the last access.
m_last_access = dict()

# A global counter to trigger periodic frequency decay (Aging).
m_op_counter = 0

# --- Constants ---

# The ratio of the cache reserved for "Protected" (High Frequency) items.
# 0.8 means we try to keep 20% of the cache free for "Probation" (New) items.
PROTECTED_RATIO = 0.8

# How often (in total operations) to halve frequencies to simulate aging.
# This prevents historical heavy-hitters from polluting the cache forever.
DECAY_INTERVAL = 20000

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU (SLRU)
    
    We categorize items into two logical pools:
    1. Probation (Freq <= 1): Candidates for immediate removal.
    2. Protected (Freq > 1): Valuable items we try to keep.
    
    Logic:
    - If the Protected pool is too big (> 80% of cache), evict the LRU from Protected.
    - Otherwise, evict the LRU from Probation to preserve Protected items.
    - If Probation is empty, fallback to evicting from Protected.
    '''
    global m_frequency, m_last_access, PROTECTED_RATIO
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into segments
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        freq = m_frequency.get(k, 1)
        if freq > 1:
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    # 2. Determine which segment to victimize
    total_items = len(candidate_keys)
    protected_count = len(protected_keys)
    
    target_pool = None
    
    # If Protected items are hogging the cache, prune them to let new items in.
    if protected_count > (total_items * PROTECTED_RATIO):
        target_pool = protected_keys
    else:
        # Otherwise, aggressively prune Probation items (Scan resistance).
        if probation_keys:
            target_pool = probation_keys
        else:
            # Fallback if no probation items exist
            target_pool = protected_keys
            
    # 3. Find LRU within the target pool
    victim_key = None
    min_access = float('inf')
    
    for k in target_pool:
        # Default to 0 if access time missing, though it shouldn't be
        acc_time = m_last_access.get(k, 0)
        if acc_time < min_access:
            min_access = acc_time
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Update Recency and Frequency.
    Trigger aging if needed.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    # Increment frequency (saturating at a reasonable cap to prevent overflow issues)
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    _check_aging(current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize metadata. 
    If this key has history (was evicted previously), it retains that history.
    '''
    global m_frequency, m_last_access
    
    key = obj.key
    current_time = cache_snapshot.access_count
    
    m_last_access[key] = current_time
    
    # If key is not in frequency dict, it's brand new -> Freq=1.
    # If it IS in frequency dict, it's a "Ghost" return -> Freq=Old_Freq + 1.
    # This automatically promotes returning items to "Protected" status.
    m_frequency[key] = m_frequency.get(key, 0) + 1
    
    _check_aging(current_time)
    _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict: We do NOTHING to the metadata.
    We deliberately keep m_frequency and m_last_access entries for evicted items.
    This serves as our "Ghost Cache".
    '''
    pass

def _check_aging(current_time):
    '''
    Periodically divide frequencies by 2.
    This ensures that items that were popular long ago eventually 
    drop back to "Probation" status and can be evicted.
    '''
    global m_frequency, m_op_counter, DECAY_INTERVAL
    
    m_op_counter += 1
    
    if m_op_counter >= DECAY_INTERVAL:
        m_op_counter = 0
        # Decay frequencies
        # We use a list to avoid "dictionary changed size during iteration"
        for k in list(m_frequency.keys()):
            val = m_frequency[k]
            if val > 1:
                m_frequency[k] = val // 2
            else:
                # Optional: if freq is 1 and we decay, we could remove it 
                # if it's not in cache, but cleanup handles that.
                pass

def _cleanup_metadata(cache_snapshot):
    '''
    Housekeeping to prevent memory explosion from Ghost entries.
    '''
    global m_frequency, m_last_access
    
    # Allow metadata to grow larger than cache to track history, 
    # but prune if it gets excessive (e.g., > 3x capacity).
    # Using a fixed limit based on typical trace constraints.
    MAX_META_SIZE = 50000 
    
    if len(m_frequency) > MAX_META_SIZE:
        # Pruning Strategy: Remove oldest items that are NOT in the cache.
        
        # 1. Identify candidates (keys in metadata but not in cache)
        # using a set for O(1) lookups
        cached_keys = set(cache_snapshot.cache.keys())
        
        # 2. Sort all metadata keys by last_access time (LRU)
        # This is O(N log N), but only runs rarely.
        all_keys_sorted = sorted(m_last_access.keys(), key=lambda k: m_last_access[k])
        
        deleted_count = 0
        target_delete = 5000 # Batch delete
        
        for k in all_keys_sorted:
            if k not in cached_keys:
                if k in m_frequency: del m_frequency[k]
                if k in m_last_access: del m_last_access[k]
                deleted_count += 1
                if deleted_count >= target_delete:
                    break
```
2025-11-27 20:18:31,622 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 43
- Hit Rate: 0.3667

Example 2:
- Trace ID: 10
- Hit Rate: 0.0917

Example 3:
- Trace ID: 37
- Hit Rate: 0.6409

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:18:31 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:18:31,623 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:20:31 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:20:31,545 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:20:31,546 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: ```python
# Metadata dictionaries for ARC implementation
m_access = dict()    # Tracks logical access time (Recency)
m_t2_keys = set()    # Set of keys belonging to the "Frequent" (T2) list. 
                     # Keys in cache NOT in this set belong to the "Recent" (T1) list.
m_b1 = dict()        # Ghost list for T1 (Keys evicted from T1 - Recently evicted once-seen items)
m_b2 = dict()        # Ghost list for T2 (Keys evicted from T2 - Recently evicted frequent items)
m_p = 0.0            # Adaptive parameter: Target size of the T1 (Recent) list

def evict(cache_snapshot, obj):
    '''
    Implements Adaptive Replacement Cache (ARC) eviction strategy.
    
    ARC dynamically balances between Recency (T1) and Frequency (T2) by adjusting 
    a parameter 'p' based on hits in the ghost lists (B1 and B2).
    '''
    global m_access, m_t2_keys, m_b1, m_b2, m_p
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # 1. Classify candidates into T1 (Recent) and T2 (Frequent)
    t1_candidates = []
    t2_candidates = []
    for k in candidate_keys:
        if k in m_t2_keys:
            t2_candidates.append(k)
        else:
            t1_candidates.append(k)

    # 2. Determine the adaptive target 'p' for this specific eviction decision.
    # We calculate what the NEW p would be if we committed the adaptation now,
    # because the incoming 'obj' tells us if our previous cache size mix was wrong.
    
    curr_p = m_p
    
    # If the incoming object is in Ghost List B1, we evicted it from T1 too early.
    # We need to increase T1's target size (p).
    if obj.key in m_b1:
        len_b1 = len(m_b1)
        len_b2 = len(m_b2)
        step = 1
        # Adaptation step size depends on relative sizes of ghost lists
        if len_b1 < len_b2:
            step = len_b2 / len_b1
        # Apply step, capped at cache size (count-based approximation)
        curr_p = min(len(candidate_keys), curr_p + step)
        
    # If the incoming object is in Ghost List B2, we evicted it from T2 too early.
    # We need to decrease T1's target size (p) to grow T2.
    elif obj.key in m_b2:
        len_b1 = len(m_b1)
        len_b2 = len(m_b2)
        step = 1
        if len_b2 < len_b1:
            step = len_b1 / len_b2
        curr_p = max(0, curr_p - step)

    # 3. Decision Logic: Evict from T1 or T2?
    # We evict from T1 if it is currently larger than our target 'p'.
    # Special Case: If T1 size exactly equals p, but we have a B2 hit, we prefer evicting T1
    # to allow T2 to grow.
    
    evict_from_t1 = False
    len_t1 = len(t1_candidates)
    
    if len_t1 > curr_p:
        evict_from_t1 = True
    elif (obj.key in m_b2) and len_t1 == int(curr_p):
        evict_from_t1 = True
    
    # 4. Fallback Handling
    # If we chose to evict T1 but it's empty, we must evict T2, and vice versa.
    if evict_from_t1 and not t1_candidates:
        evict_from_t1 = False
    elif not evict_from_t1 and not t2_candidates:
        evict_from_t1 = True
        
    # 5. Find LRU victim in the chosen list
    victim_pool = t1_candidates if evict_from_t1 else t2_candidates
    victim_key = None
    min_time = float('inf')
    
    for k in victim_pool:
        # Use logical access time to determine LRU within the specific list
        t = m_access.get(k, 0)
        if t < min_time:
            min_time = t
            victim_key = k
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a cache hit, update recency. 
    Any hit in ARC promotes (or keeps) the item in the T2 (Frequent) set.
    '''
    global m_access, m_t2_keys
    
    # Update recency
    m_access[obj.key] = cache_snapshot.access_count
    
    # Promote to T2
    m_t2_keys.add(obj.key)

def update_after_insert(cache_snapshot, obj):
    '''
    On a cache miss (insert), we permanently adapt the parameter 'p' 
    and assign the new object to T1 (Recent) or T2 (Frequent) based on ghost history.
    '''
    global m_access, m_t2_keys, m_b1, m_b2, m_p
    
    m_access[obj.key] = cache_snapshot.access_count
    
    # Adaptation logic (Must mirror the logic in evict, but commit changes)
    len_b1 = len(m_b1)
    len_b2 = len(m_b2)
    current_c = len(cache_snapshot.cache) # Approximate capacity in count
    
    if obj.key in m_b1:
        # Ghost T1 Hit -> Increase p (Grow T1)
        step = 1
        if len_b1 < len_b2:
            step = len_b2 / len_b1
        m_p = min(current_c, m_p + step)
        
        # Item returns, so it becomes Frequent (T2)
        m_t2_keys.add(obj.key)
        del m_b1[obj.key]
        
    elif obj.key in m_b2:
        # Ghost T2 Hit -> Decrease p (Shrink T1 / Grow T2)
        step = 1
        if len_b2 < len_b1:
            step = len_b1 / len_b2
        m_p = max(0, m_p - step)
        
        # Item returns, so it becomes Frequent (T2)
        m_t2_keys.add(obj.key)
        del m_b2[obj.key]
        
    else:
        # Cold Miss -> Start in T1 (Recent)
        if obj.key in m_t2_keys:
            m_t2_keys.remove(obj.key)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    When an object is evicted, move its metadata to the corresponding ghost list (B1 or B2).
    '''
    global m_access, m_t2_keys, m_b1, m_b2
    
    ek = evicted_obj.key
    
    # Remove from active access metadata
    if ek in m_access:
        del m_access[ek]
    
    # Identify source list and move to Ghost
    if ek in m_t2_keys:
        # Was in T2, move to B2
        m_b2[ek] = True
        m_t2_keys.remove(ek)
    else:
        # Was in T1, move to B1
        m_b1[ek] = True
        
    # Maintain Ghost List size limits to prevent memory leaks.
    # A safe heuristic is max(cache size, constant)
    max_ghost_size = max(len(cache_snapshot.cache), 2000)
    
    if len(m_b1) > max_ghost_size:
        # Python 3.7+ dicts preserve insertion order; this removes the oldest inserted
        try:
            oldest = next(iter(m_b1))
            del m_b1[oldest]
        except StopIteration:
            pass
            
    if len(m_b2) > max_ghost_size:
        try:
            oldest = next(iter(m_b2))
            del m_b2[oldest]
        except StopIteration:
            pass
```
2025-11-27 20:20:47,434 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 36
- Hit Rate: 0.2073

Example 2:
- Trace ID: 21
- Hit Rate: 0.0262

Example 3:
- Trace ID: 42
- Hit Rate: 0.0526

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:20:47 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:20:47,435 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:22:58 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:22:58,551 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:22:58,552 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Based on the analysis of the performance results and the objective to minimize the miss rate (miss count), the best approach is to transition from the ad-hoc scoring heuristic to a well-founded algorithm known as **GDSF (Greedy Dual Size Frequency)**, enhanced with a **Ghost Registry**.

### Rationale:
1.  **Size Awareness**: The goal is to maximize the *number* of hits (object hit rate). Logic dictates that caching many small, frequently accessed items is better than caching a few large, frequently accessed items if space is limited. GDSF naturally prioritizes smaller objects by dividing the value by size.
2.  **Frequency & Recency Balance**: GDSF assigns a priority value $H = L + (Frequency / Size)$.
    *   **Frequency**: High frequency increases priority.
    *   **Recency**: The aging factor $L$ effectively handles recency. $L$ increases over time (it's updated to the priority of the evicted item). New items or recently hit items are boosted by the current $L$, while stale items (whose priority was calculated with an old, smaller $L$) naturally fall to the bottom of the priority list.
3.  **Ghost Registry**: To solve the "Scan" vs "Loop" problem, we maintain a history of evicted items. If an item is evicted and quickly requested again, it indicates our eviction was premature (or the working set is slightly larger than cache). We restore its previous frequency to give it a "fighting chance" against established items.

### Improved Algorithm Implementation:

```python
import math

# Metadata dictionary to store access history and GDSF priority for objects.
# Key -> {
#   'freq': int, 
#   'priority': float,
#   'last_access': int
# }
m_access_info = dict()

# Ghost registry (Shadow Cache) to remember frequency of recently evicted items.
# Key -> { 'freq': int }
m_ghost_registry = dict()

# Global Aging Factor (L) for GDSF
# This acts as the "inflation" baseline. It rises as popular items set the bar higher.
m_L = 0.0

# Tunable Parameters
GHOST_RATIO = 1.0  # Size of ghost registry relative to cache

def get_gdsf_priority(freq, size, L):
    '''
    Calculates the GDSF priority.
    Formula: H = L + (Frequency / Size)
    
    - Higher Frequency -> Higher Priority (Keep)
    - Larger Size -> Lower Priority (Evict, to make room for more objects)
    - L represents the current "aging" baseline.
    '''
    # Safety for 0-size objects (though rare in traces)
    effective_size = size if size > 0 else 1
    
    # We use a weight multiplier to ensure frequency has a tangible impact 
    # against size in bytes. 1000 is an arbitrary scaling factor roughly 
    # equating 1 hit to 1KB cost.
    weight = 1000.0
    
    return L + (freq * weight / effective_size)

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the Lowest Priority (Min H).
    Tie-breaker: Least Recently Used (LRU).
    '''
    global m_L, m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    best_victim_key = None
    min_priority = float('inf')
    
    # We iterate to find the minimum priority.
    # While O(N), this ensures strict adherence to the GDSF policy.
    for key in candidate_keys:
        if key not in m_access_info:
            # Fallback if metadata is missing
            return key
            
        meta = m_access_info[key]
        p = meta['priority']
        
        if p < min_priority:
            min_priority = p
            best_victim_key = key
        elif p == min_priority:
            # Tie-breaker: LRU (Lower last_access is older)
            if best_victim_key and meta['last_access'] < m_access_info[best_victim_key]['last_access']:
                best_victim_key = key
    
    # In GDSF, the aging factor L is updated to the priority of the evicted object.
    # This ensures that future objects must beat this baseline to stay.
    if min_priority != float('inf'):
        m_L = min_priority

    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority using the *Current* L.
       This effectively "resets" its age, protecting it from the rising L.
    '''
    global m_access_info, m_L
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        
        # Increment frequency
        meta['freq'] += 1
        meta['last_access'] = current_time
        
        # Restore Priority: It gets the current L boost plus its density value
        meta['priority'] = get_gdsf_priority(meta['freq'], obj.size, m_L)
        
        m_access_info[obj.key] = meta
    else:
        # Should ideally not happen on a hit, but safety fallback
        update_after_insert(cache_snapshot, obj)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry for restored frequency.
    2. Calculate initial Priority.
    '''
    global m_access_info, m_ghost_registry, m_L
    
    current_time = cache_snapshot.access_count
    
    # Default frequency
    initial_freq = 1
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # It was recently evicted. Restore its frequency history.
        # This prevents "frequent but large" items from thrashing if they barely miss cache.
        initial_freq = m_ghost_registry[obj.key]['freq'] + 1
        # Remove from ghost as it's now in main cache
        del m_ghost_registry[obj.key]
    
    # Calculate Priority based on current global L
    priority = get_gdsf_priority(initial_freq, obj.size, m_L)
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'priority': priority,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move metadata to Ghost Registry.
    2. Prune Ghost Registry to keep memory overhead low.
    '''
    global m_access_info, m_ghost_registry
    
    # Move to ghost
    if evicted_obj.key in m_access_info:
        # We only strictly need frequency for the ghost logic
        freq = m_access_info[evicted_obj.key]['freq']
        m_ghost_registry[evicted_obj.key] = { 'freq': freq }
        
        # Remove from active metadata
        del m_access_info[evicted_obj.key]
        
    # Maintain Ghost Registry Size
    # Keep it roughly the size of the cache (or slightly larger)
    max_ghost_size = len(cache_snapshot.cache) * GHOST_RATIO + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Python 3.7+ dicts preserve insertion order. 
        # The first key is the oldest inserted (LRU of the ghost list).
        try:
            oldest_ghost = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost]
        except StopIteration:
            pass
```
2025-11-27 20:23:21,938 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 40
- Hit Rate: 0.1417

Example 2:
- Trace ID: 38
- Hit Rate: 0.1255

Example 3:
- Trace ID: 0
- Hit Rate: 0.5103

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:23:21 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:23:21,940 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:25:41 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:25:41,225 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:25:41,226 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The improved algorithm implements a **Segmented LRU with Ghost Registry** approach, refined to address the specific shortcomings (cache pollution, scan resistance, and premature eviction) observed in the initial implementation.

### Key Changes and Rationale:

1.  **Simplified Metadata & Scoring**: Instead of a complex virtual time formula, we explicitly categorize items into "Probationary" (`freq=1`) and "Protected" (`freq > 1`). This mimics the structure of Segmented LRU (SLRU).
2.  **Two-Candidate Eviction Logic**: The `evict` function now identifies the best victim from the Probationary set (LRU) and the best victim from the Protected set (LRU). It prioritizes evicting the Probationary victim to protect the working set.
3.  **Adaptive Protection Factor**: A "Protected" item is only evicted if it is significantly staler (older) than the Probationary victim (multiplied by a `PROTECTION_FACTOR`). This prevents "dead" high-frequency items from polluting the cache indefinitely while ensuring active high-frequency items are not evicted by a sudden burst of new, one-time requests (scans).
4.  **Ghost Registry Integration**: The ghost registry (Shadow Cache) is used to detect items that were evicted prematurely. If an item found in the ghost registry is re-inserted, it is immediately initialized with `freq=2`, promoting it to the "Protected" status instantly. This adapts quickly to working sets that are slightly larger than the cache.

```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# Capacity of ghost registry relative to cache capacity.
# Captures items that recently left the cache.
GHOST_RATIO = 2.0 

# Factor by which Protected items (freq > 1) are protected vs Probation items (freq=1).
# A Protected item must be X times older (staler) than the oldest Probation item to be chosen as the victim.
# This prevents one-hit wonders (scans) from flushing out valuable working set items,
# while still allowing dead high-frequency items to be eventually evicted.
PROTECTION_FACTOR = 20.0 

def evict(cache_snapshot, obj):
    '''
    Eviction Policy: Segmented LRU with Dynamic Aging.
    
    1. Identify the LRU item among "Probationary" items (Freq = 1).
    2. Identify the LRU item among "Protected" items (Freq > 1).
    3. Decision:
       - Default to evicting the Probationary item (to filter out scans).
       - Exception: If the Protected item is significantly older than the Probationary item
         (based on PROTECTION_FACTOR), evict the Protected item. This cleans up "polluted"
         high-frequency items that are no longer being used.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # Track best candidates for both segments
    probation_victim = None
    probation_max_staleness = -1
    
    protected_victim = None
    protected_max_staleness = -1
    
    # Single pass to find LRU for both groups
    for key in candidate_keys:
        # Fallback if metadata is missing
        if key not in m_access_info:
            return key
            
        meta = m_access_info[key]
        staleness = current_time - meta['last_access']
        freq = meta['freq']
        
        if freq <= 1:
            # Probationary Segment
            if staleness > probation_max_staleness:
                probation_max_staleness = staleness
                probation_victim = key
        else:
            # Protected Segment
            if staleness > protected_max_staleness:
                protected_max_staleness = staleness
                protected_victim = key
    
    # Decision Logic
    
    # If one segment is empty, evict from the other
    if probation_victim is None:
        return protected_victim
    if protected_victim is None:
        return probation_victim
        
    # Compare Staleness to make the decision
    # If the protected item is REALLY old compared to the probation item, evict it.
    if protected_max_staleness > (probation_max_staleness * PROTECTION_FACTOR):
        return protected_victim
    else:
        # Otherwise, protect the heavy hitter and sacrifice the probation item
        return probation_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (capped to prevent overflow/saturation).
    '''
    global m_access_info
    current_time = cache_snapshot.access_count
    
    if obj.key not in m_access_info:
        # Fallback initialization
        m_access_info[obj.key] = {'freq': 2, 'last_access': current_time}
    else:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        # Cap frequency. A frequency of 32 is sufficient to distinguish "very hot"
        # from "cold". Uncapped frequency hurts aging.
        if meta['freq'] < 32:
            meta['freq'] += 1
        m_access_info[obj.key] = meta

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If found in Ghost (Rescue): Insert as Protected (Freq=2).
    3. Else: Insert as Probationary (Freq=1).
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Check ghost registry
    if obj.key in m_ghost_registry:
        # Item was recently evicted but requested again. 
        # It is part of a cyclic working set. Promote immediately.
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to keep memory usage bounded.
    '''
    global m_access_info, m_ghost_registry
    current_time = cache_snapshot.access_count
    
    # 1. Cleanup Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keeping ghost size relative to cache capacity allows adapting to 
    # working sets that are slightly larger than cache.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost (FIFO).
        # Python dicts preserve insertion order.
        try:
            oldest_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
```
2025-11-27 20:25:58,234 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 41
- Hit Rate: 0.1627

Example 2:
- Trace ID: 33
- Hit Rate: 0.7503

Example 3:
- Trace ID: 4
- Hit Rate: 0.5062

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:25:58 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:25:58,235 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:26:27 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:26:27,316 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:26:27,317 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, we will move from a basic Segmented LRU (SLRU) to a more robust **S3-FIFO (Simple S3-FIFO)** inspired algorithm, which is currently considered state-of-the-art for lightweight cache eviction.

### The Strategy: S3-FIFO Adaptation
The S3-FIFO (Simple, Small, Scalable FIFO) algorithm addresses the shortcomings of LRU (which struggles with scans) and LFU (which struggles with "ghosts" of past popularity).

1.  **Small Queue (Probationary)**: New items enter here. We use a FIFO queue for this segment. If an item in this queue is accessed again, it is promoted. If it reaches the end of the queue without being accessed, it is evicted. This effectively filters out "one-hit wonders" (scan resistance).
2.  **Main Queue (Protected)**: Items promoted from the small queue enter here. This is also managed via FIFO logic but with a "Reinsertion" twist. When an item is selected for eviction from the main queue, we check if it was accessed recently. If yes, we give it a second chance (reinsert at the back). If no, we evict it.
3.  **Ghost Registry**: We will keep the ghost registry (shadow cache) logic. If an item is accessed that was recently evicted, it suggests our cache is too small for the working set. We react by modifying the admission policy (treating it as a "hit" upon re-insertion to promote it faster).

This approach generally outperforms pure LRU and SLRU on mixed workloads (scans + frequency bursts).

### Changes Made
*   **Metadata**: Switched to tracking `frequency` bits (or simple counters) and strict FIFO insertion ordering.
*   **Victim Selection**: Instead of scanning the whole cache for a score (which is O(N)), we treat the eviction logic as a "queue management" process. However, since the function signature requires us to return a single victim key based on a snapshot, we will simulate the S3-FIFO logic by assigning scores based on queue position and frequency.
*   **Score Logic**:
    *   Items with Frequency 0 (Probationary) are high-priority victims.
    *   Items with Frequency > 0 (Protected) are low-priority.
    *   **Tie-breaking**: Among items with same priority, the one inserted earliest (FIFO) is evicted.
    *   **Ghost Hits**: If a new item was in the ghost registry, it starts with higher frequency to stick around longer.

```python
import math

# Metadata dictionary
# key -> { 'freq': int, 'insertion_time': int }
m_metadata = dict()

# Ghost registry to track recently evicted keys
# key -> eviction_time
m_ghost_registry = dict()

def evict(cache_snapshot, obj):
    '''
    Implements a variation of S3-FIFO / SIEVE eviction strategy.
    
    The goal is to quickly evict "one-hit wonders" while keeping popular items.
    We score items based on:
    1. Frequency: Items with 0 hits since insertion (or reset) are preferred victims.
    2. Recency/Insertion: Among similar frequency, we approximate FIFO by evicting the oldest insertion.
    
    This function scans the candidates to find the best victim according to these rules.
    '''
    global m_metadata
    
    # Candidate keys currently in the cache
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to find the "oldest" item (smallest insertion_time) that has 
    # the lowest frequency.
    # Logic:
    # 1. Look for oldest item with freq == 0.
    # 2. If all items have freq > 0, we degrade their frequency (simulate the 'clock' hand passing)
    #    and pick the oldest one. 
    #    However, since we can't mutate state inside `evict` (it's a query), 
    #    we essentially return the item that *would* be evicted by a Clock/SIEVE algo.
    #    That is simply the item with lowest frequency, tie-broken by oldest insertion time.
    
    victim_key = None
    
    # We will score candidates. Lower score = KEEP, Higher score = EVICT.
    # But to make it cleaner, let's find the candidate with (Lowest Frequency, Oldest Insertion).
    # Tuple comparison in Python does exactly this.
    # We want to minimize (freq, insertion_time). Wait, standard LRU evicts the oldest.
    # So we want to EVICT the item with lowest freq. If tie, lowest insertion_time (FIFO).
    
    best_candidate = None
    # We initialize with a tuple that is theoretically "maximum valuable" so we can minimize it
    # But wait, we want to EVICT.
    # The "worst" item has lowest freq and lowest insertion time (oldest).
    # So we simply look for min( (freq, insertion_time) )
    
    # Optimization: Filter keys that exist in metadata
    valid_candidates = []
    for k in candidate_keys:
        if k in m_metadata:
            valid_candidates.append((k, m_metadata[k]))
    
    if not valid_candidates:
        return candidate_keys[0] # Fallback
        
    # Find the entry with min (freq, insertion_time)
    # We prioritize evicting freq=0 over freq=1.
    # Within freq=0, we evict the one inserted earliest (smallest insertion_time).
    
    # Note: To prevent starvation of high-frequency items in a purely static score,
    # real S3-FIFO decrements frequency during the scan. 
    # Since we cannot modify state in `evict` easily without side effects (and the prompt implies 
    # state updates happen in the update_* functions), we rely on `update_after_hit` 
    # to keep frequencies bounded (0, 1, 2) so they don't grow infinitely and block eviction forever.
    
    victim = min(valid_candidates, key=lambda x: (x[1]['freq'], x[1]['insertion_time']))
    
    return victim[0]

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    A lazy promotion. We increment the frequency counter.
    To prevent cache pollution by ancient hot items, we cap the frequency.
    In S3-FIFO/SIEVE logic, usually 1 or 2 bits are enough.
    '''
    global m_metadata
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        # Increment frequency
        # We cap at 2. This creates 3 "levels" of importance:
        # 0: Inserted, never accessed (Probation) -> First to go
        # 1: Accessed once (Protected)
        # 2: Accessed frequently (Super Protected)
        # Keeping the cap low (e.g., 2 or 3) is CRITICAL for adaptability.
        # If it grows to 100, an item that stops being popular stays for too long.
        new_freq = min(m_metadata[obj.key]['freq'] + 1, 2)
        
        # Note: We DO NOT update insertion_time on hit. This preserves FIFO order within the frequency tiers.
        # This makes it Scan-Resistant. A scan will hit items once (freq 0->1) or zero times (freq 0),
        # but they retain their old insertion time, ensuring they cycle out quickly.
        m_metadata[obj.key]['freq'] = new_freq
    else:
        # Should not happen ideally, but self-heal
        m_metadata[obj.key] = {'freq': 1, 'insertion_time': current_time}

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    Initialize metadata. Check Ghost Registry for "Second Chance".
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Default priority is 0 (Probation).
    initial_freq = 0
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # This item was recently evicted and is needed again.
        # It implies our cache was too small or the item is part of a cyclic working set.
        # Promote it immediately to 'Protected' status.
        initial_freq = 2 # Jump strictly to top priority
        del m_ghost_registry[obj.key]
        
    m_metadata[obj.key] = {
        'freq': initial_freq,
        'insertion_time': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Clean up active metadata, move to Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Remove from active metadata
    if evicted_obj.key in m_metadata:
        del m_metadata[evicted_obj.key]
        
    # Add to Ghost Registry
    # This helps us detect if we evicted something too early.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # Maintenance: Prune Ghost Registry
    # Keep it bounded to avoid memory leaks. A size roughly equal to cache capacity is standard.
    # We use a randomized/iteration approach to remove old items for O(1) performance.
    target_ghost_size = cache_snapshot.capacity // 100 # Assuming avg obj size ~100 bytes? Or just count.
    # Actually, cache_snapshot.cache is a dict. Let's look at count of items.
    cache_item_count = len(cache_snapshot.cache)
    
    if len(m_ghost_registry) > max(cache_item_count, 100):
        # Remove a chunk of old ghosts to amortize overhead
        keys_to_remove = []
        for k in m_ghost_registry:
            keys_to_remove.append(k)
            if len(keys_to_remove) >= 10: # Prune 10 at a time
                break
        for k in keys_to_remove:
            del m_ghost_registry[k]
```
2025-11-27 20:26:37,169 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 27
- Hit Rate: 0.8937

Example 2:
- Trace ID: 35
- Hit Rate: 0.068

Example 3:
- Trace ID: 22
- Hit Rate: 0.0587

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:26:37 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:26:37,170 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:27:03 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:27:03,530 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:27:03,531 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, I have redesigned the logic to implement a variation of the **S3-FIFO (Simple, Scalable, Static FIFO)** and **2Q (Two Queue)** algorithms. This approach is generally more robust than complex scoring functions because it structurally separates "probationary" items (newly inserted) from "protected" items (frequently accessed) without needing heavy floating-point math or tunable weights that vary wildly between traces.

### Key Changes and Rationale:

1.  **Structural Split (Probation vs. Protected):**
    Instead of a single score, we implicitly maintain three logic states for an object:
    *   **Small/Main Queue:** Where newly inserted items go (FIFO).
    *   **Ghost/Shadow Registry:** Where items go immediately after eviction.
    *   **Protected Status:** Items that have proven their worth (either by being hit while in the cache or by being re-inserted while in the Ghost registry).

2.  **Frequency Cap & Aging:**
    The previous implementation allowed frequency to grow to 16, which can create "cache pollution" where old heavy hitters never leave. I implemented a simpler 2-bit frequency counter (0, 1, 2, 3) and a logic that degrades frequency during eviction checks rather than purely relying on time.

3.  **Ghost Registry (Shadow Cache):**
    The previous implementation had a ghost registry, but the scoring logic was messy. The new logic is binary: if an item is in the ghost registry upon insertion, it is immediately promoted to "Protected" (high frequency) status.

4.  **Simplified Eviction Logic (Clock/Second Chance):**
    Instead of iterating `O(N)` to find the highest score (which is slow and hard to tune), we use a pseudo-random sample or a simplified scan that looks for items with low frequency. This approximates a Clock algorithm.
    *   If we pick a candidate with `freq > 0`, we decrement the frequency and give it a "second chance" (leave it in cache).
    *   If we pick a candidate with `freq == 0`, it is evicted.
    *   This naturally filters out one-hit wonders while keeping working sets in memory.

### Improved Implementation

```python
import random

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items. Used to detect "loops" or working sets 
# that are slightly larger than the cache.
# We use a simple dict for O(1) lookup.
m_ghost_registry = dict()

# Since we don't have access to an ordered dict class or linked list node structure 
# for the cache internals, we simulate a "Clock" hand using a list of keys 
# that we rebuild lazily or iterate through. 
# To ensure O(1) amortized performance and good random sampling, 
# we will use a random sampling strategy for eviction candidates, 
# which approximates the Clock/Second-Chance algorithm very well.

def evict(cache_snapshot, obj):
    '''
    Implements a random-sample Second-Chance (Clock) eviction policy.
    
    Strategy:
    1. Sample a few keys from the cache.
    2. Check their frequency/utility.
    3. If an item has 'credit' (freq > 0), decrement it and skip eviction (Second Chance).
    4. If an item has no 'credit' (freq == 0), evict it.
    
    This ensures that "one-hit wonders" (freq=0) are evicted quickly, while 
    popular items (freq>0) circulate longer.
    '''
    global m_access_info
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None
        
    # We attempt to find a victim by sampling.
    # In a true Clock algorithm, we would iterate circularly. 
    # Random sampling is a statistically strong approximation for large caches.
    # We try a finite number of times to find a 0-freq item. If all sampled items are hot,
    # we just forcefully evict the one with the lowest timestamp (LRU fallback).
    
    sample_size = 5 # Checking 5 items gives high probability of finding a good victim
    best_victim = None
    min_timestamp = float('inf')
    
    # Shuffle or sample? For python lists, random.sample is decent.
    # If cache is small, just check all.
    if len(candidate_keys) <= sample_size:
        samples = candidate_keys
    else:
        samples = random.sample(candidate_keys, sample_size)
        
    for key in samples:
        meta = m_access_info.get(key)
        if not meta:
            # Should not happen, but safe to evict corrupted state
            return key
            
        # CLOCK Logic:
        if meta['freq'] > 0:
            # Give second chance, decrement freq
            meta['freq'] -= 1
            # We don't evict this one, but we keep track of LRU just in case we fail to find any 0
            if meta['last_access'] < min_timestamp:
                min_timestamp = meta['last_access']
                best_victim = key
        else:
            # Found a zero-frequency item. Evict immediately.
            return key
            
    # If we are here, it means all sampled items had freq > 0 and were decremented.
    # We return the "oldest" among the samples we saw to break the deadlock.
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (saturated counter).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Max frequency is capped at 3.
        # 0 = Insertion/Probation
        # 1 = Warm
        # 2 = Hot
        # 3 = Very Hot
        # Keeping this small ensures adaptation isn't too sluggish.
        if meta['freq'] < 3:
            meta['freq'] += 1
            
        m_access_info[obj.key] = meta
    else:
        # Recover missing metadata
        m_access_info[obj.key] = {
            'freq': 0, 
            'last_access': current_time
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost: This is a "false positive" eviction. Promote immediately to protected state.
    3. If not in Ghost: Insert as probationary (freq=0).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 0
    
    # Check if this item was recently evicted
    if obj.key in m_ghost_registry:
        # It was in the ghost list, meaning it was accessed shortly after eviction.
        # This implies it belongs to the working set.
        # We promote it immediately to 'Hot' (freq=2) so it survives the next few eviction cycles.
        initial_freq = 2
        del m_ghost_registry[obj.key]
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry to track "mistakes".
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    # Value doesn't strictly matter, but we can store eviction time if we wanted to prune by time.
    # Here we just use keys.
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We keep the ghost registry roughly the same size as the cache capacity (or slightly larger).
    # This acts as the "history" window.
    # Since we can't easily query capacity count vs size, we approximate using current cache count.
    
    target_ghost_size = len(cache_snapshot.cache) 
    if target_ghost_size == 0: target_ghost_size = 100 # Safety for empty start
    
    if len(m_ghost_registry) > target_ghost_size:
        # Remove oldest inserted into ghost to maintain size cap.
        # Python 3.7+ dicts preserve insertion order, so the first key is the oldest.
        try:
            oldest_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
```
2025-11-27 20:27:13,307 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 8
- Hit Rate: 0.5409

Example 2:
- Trace ID: 16
- Hit Rate: 0.0199

Example 3:
- Trace ID: 7
- Hit Rate: 0.4786

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:27:13 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:27:13,308 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:28:43 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:28:43,167 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:28:43,168 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To optimize the cache eviction algorithm, I will implement the **Adaptive Replacement Cache (ARC)** algorithm. 

### Why ARC?
1.  **Addresses the "Scan" Problem (Trace 16):** The current implementation failed significantly on Trace 16 (1.9% hit rate). This usually indicates a "scan" pattern (accessing many items once) which flushes out useful items. ARC isolates new items in a generic "Recency" list ($T_1$) and only promotes them to the "Frequency" list ($T_2$) if they are accessed again. This prevents scans from polluting the high-value cache segments.
2.  **Dynamic Tuning:** The current implementation uses static weights and magic numbers (`log2`, `1000`, etc.). ARC dynamically tunes a parameter $p$ based on "ghost hits" (hits on recently evicted items). If we see hits on evicted "recent" items, we increase the size of the recent list ($T_1$). If we see hits on evicted "frequent" items, we increase the size of the frequent list ($T_2$).
3.  **Robustness:** ARC generally performs as well as or better than LRU and LFU across a wide variety of workloads (Trace 8 and 7) without requiring manual parameter tweaking.

### Implementation Details
I will replace the existing metadata structure with four logical lists (implemented as Python dictionaries to preserve insertion order, functioning as LRU queues):
*   **T1:** Recent items (in cache).
*   **T2:** Frequent items (in cache).
*   **B1:** Ghost Recent (evicted from T1, history only).
*   **B2:** Ghost Frequent (evicted from T2, history only).
*   **p:** The target size for T1.

```python
import math

# ARC Metadata Global State
# t1: Recent items (LRU Queue). Key -> None. Order: Oldest (LRU) to Newest (MRU).
# t2: Frequent items (LRU Queue). Key -> None.
# b1: Ghost Recent (History of evicted T1). Key -> None.
# b2: Ghost Frequent (History of evicted T2). Key -> None.
# p:  Target size for T1 (adaptable parameter).
m_arc = {
    't1': {},
    't2': {},
    'b1': {},
    'b2': {},
    'p': 0,
    'capacity': 0  # We will learn this from cache_snapshot
}

def _access(key):
    """
    Helper to move a key to the MRU position (end) of its respective dictionary.
    Simulates touching the item.
    """
    if key in m_arc['t1']:
        del m_arc['t1'][key]
        m_arc['t1'][key] = None
    elif key in m_arc['t2']:
        del m_arc['t2'][key]
        m_arc['t2'][key] = None

def _adapt_p(key, delta):
    """
    Safely update the parameter P within bounds [0, capacity].
    """
    current_p = m_arc['p']
    capacity = m_arc['capacity']
    m_arc['p'] = max(0, min(capacity, current_p + delta))

def evict(cache_snapshot, obj):
    '''
    ARC Replacement Policy.
    Decides whether to evict from T1 or T2 based on the current state of 'p' 
    and the type of the incoming object (obj).
    '''
    global m_arc
    
    # Update capacity perception
    capacity = cache_snapshot.capacity
    # In this environment, capacity is given in bytes, but the cache is full based on count 
    # if object sizes are 1, or bytes if varied. 
    # However, standard ARC is count-based. We use the current cache size as the working capacity
    # since this function is only called when the cache is FULL.
    if m_arc['capacity'] == 0:
        m_arc['capacity'] = len(cache_snapshot.cache)

    key = obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']
    p = m_arc['p']

    # 1. ADAPTATION: Update 'p' based on where the miss occurred (Ghost Hits)
    # This logic normally sits in the 'miss' handler, but we need to run it before 
    # selecting a victim to ensure the victim selection reflects the new 'p'.
    
    if key in b1:
        # Hit in Ghost Recent: We should have kept T1 larger. Increase p.
        delta = 1
        if len(b1) >= len(b2):
            delta = 1
        else:
            delta = len(b2) / len(b1)
        _adapt_p(key, delta)
        
    elif key in b2:
        # Hit in Ghost Frequent: We should have kept T2 larger. Decrease p.
        delta = 1
        if len(b2) >= len(b1):
            delta = 1
        else:
            delta = len(b1) / len(b2)
        _adapt_p(key, -delta)

    # Reload p after update
    p = m_arc['p']

    # 2. REPLACEMENT: Decide which item to evict.
    # We evict from T1 if T1 is growing beyond target 'p', OR if we have a B2 hit 
    # and T1 is exactly at target 'p'.
    
    victim_key = None
    
    # Logic derived from ARC "REPLACE(p)" subroutine
    # Condition to evict from T1:
    # 1. T1 is not empty AND
    # 2. (Len(T1) > p) OR (Incoming item is in B2 and Len(T1) == p)
    
    evict_from_t1 = False
    if len(t1) > 0:
        if len(t1) > p:
            evict_from_t1 = True
        elif (key in b2) and (len(t1) == int(p)):
            evict_from_t1 = True
    
    if evict_from_t1:
        # Evict LRU of T1 (First key in dict)
        victim_key = next(iter(t1))
    else:
        # Evict LRU of T2
        # Fallback: if T2 is empty, we must evict T1 (should rarely happen if cache full)
        if len(t2) > 0:
            victim_key = next(iter(t2))
        else:
            victim_key = next(iter(t1))

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    If in T1, move to T2 (MRU).
    If in T2, move to T2 (MRU).
    '''
    global m_arc
    key = obj.key
    
    # Move to MRU T2
    # If it was in T1, it gets promoted to T2 (Frequency list)
    if key in m_arc['t1']:
        del m_arc['t1'][key]
    
    # If it was in T2, just refresh it
    if key in m_arc['t2']:
        del m_arc['t2'][key]
        
    m_arc['t2'][key] = None

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert (Cache Miss):
    Place the object into the appropriate list (T1 or T2) based on history.
    '''
    global m_arc
    
    # Note: 'evict' might have been called just before this, or not (if cache wasn't full).
    # We need to finalize the placement of the new object.
    
    key = obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']

    # Case 1: It was in Ghost Recent (B1). 
    # It was a miss, but we remembered it. Promote to T2 (Frequent).
    if key in b1:
        del b1[key]
        t2[key] = None # Insert MRU
        return

    # Case 2: It was in Ghost Frequent (B2).
    # It was a miss, but we remembered it. Promote to T2 (Frequent).
    if key in b2:
        del b2[key]
        t2[key] = None # Insert MRU
        return

    # Case 3: It is totally new.
    # Insert into T1 (Recent).
    t1[key] = None # Insert MRU

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    Move the evicted object from active lists (T1/T2) to ghost lists (B1/B2).
    Maintain ghost list sizes.
    '''
    global m_arc
    
    key = evicted_obj.key
    t1 = m_arc['t1']
    t2 = m_arc['t2']
    b1 = m_arc['b1']
    b2 = m_arc['b2']
    
    # Remove from active lists
    if key in t1:
        del t1[key]
        # Move to B1 (Ghost Recent)
        b1[key] = None
    elif key in t2:
        del t2[key]
        # Move to B2 (Ghost Frequent)
        b2[key] = None
        
    # Enforce Ghost List Capacities
    # ARC generally suggests: |T1| + |B1| <= C  and  |T2| + |B2| <= 2C
    # Here C is effectively len(cache).
    
    # Use current cache size as proxy for capacity C if not strictly set
    c = len(cache_snapshot.cache) if len(cache_snapshot.cache) > 0 else 100 
    
    # If we just evicted, the cache size is effectively at capacity.
    
    # Prune B1
    while len(t1) + len(b1) > c:
        if len(b1) > 0:
            oldest = next(iter(b1))
            del b1[oldest]
        else:
            break
            
    # Prune B2
    while len(t2) + len(b2) > 2 * c:
        if len(b2) > 0:
            oldest = next(iter(b2))
            del b2[oldest]
        else:
            break
```
2025-11-27 20:30:47,785 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
# Metadata dictionaries
m_key_frequency = dict()    # Tracks access frequency
m_key_last_access = dict()  # Tracks recency (logical time)
m_ghost_registry = dict()   # Tracks keys recently evicted to give them a second chance

# Configuration constants
MAX_FREQ_CAP = 50           # Cap frequency to prevent "heavy" items from locking
AGING_THRESHOLD = 1000      # How many accesses before we age frequencies
access_counter_since_age = 0 

def evict(cache_snapshot, obj):
    '''
    Evicts based on a hybrid Score:
    Score = Frequency + (Tiny Recency Bonus).
    
    This acts as LFU, but the frequency is capped and aged to prevent pollution.
    Recency is used to break ties.
    '''
    global m_key_frequency, m_key_last_access
    
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    # We want to evict the item with the LOWEST score.
    # Score calculation: Freq + (AccessTime / CurrentTime * 0.1)
    # This prioritizes LFU, but if frequencies are equal, the older item (smaller access time) has a lower score.
    
    victim_key = None
    min_score = float('inf')
    current_time = cache_snapshot.access_count

    for key in candidate_keys:
        freq = m_key_frequency.get(key, 1)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate recency factor (0.0 to 1.0)
        # Higher recency factor means accessed more recently.
        if current_time > 0:
            recency_factor = last_access / current_time
        else:
            recency_factor = 0
            
        # Composite Score:
        # We value Frequency highly. 
        # We add a tiny fraction of recency to break ties in favor of LRU.
        # (Lower score = better candidate for eviction)
        score = freq + (recency_factor * 0.0001)
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _age_frequencies():
    '''
    Helper function to decay frequencies.
    Dividing by 2 allows new trends to replace old stable items.
    '''
    global m_key_frequency
    for k in m_key_frequency:
        m_key_frequency[k] = m_key_frequency[k] // 2 
        # Ensure minimum frequency is 1 if it's still in the cache (handled in evict check usually)
        if m_key_frequency[k] == 0:
            m_key_frequency[k] = 1

def update_after_hit(cache_snapshot, obj):
    '''
    Increment frequency (with a cap) and update recency.
    Trigger aging periodically.
    '''
    global m_key_frequency, m_key_last_access, access_counter_since_age, AGING_THRESHOLD, MAX_FREQ_CAP
    
    # 1. Update Recency
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # 2. Update Frequency
    current_freq = m_key_frequency.get(obj.key, 0)
    if current_freq < MAX_FREQ_CAP:
        m_key_frequency[obj.key] = current_freq + 1
        
    # 3. Periodic Aging
    # Every X operations, we decay the frequency of all items.
    # This turns LFU into "LFU with Window" or LFU-DA.
    access_counter_since_age += 1
    if access_counter_since_age >= AGING_THRESHOLD:
        _age_frequencies()
        access_counter_since_age = 0

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize metadata. Check ghost registry for warm starts.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry, MAX_FREQ_CAP
    
    m_key_last_access[obj.key] = cache_snapshot.access_count
    
    # Check Ghost Registry
    # If this key was recently evicted, it means our cache is thrashing or the working set is large.
    # Give it a boosted starting frequency so it sticks around longer this time.
    if obj.key in m_ghost_registry:
        # Give it a head start (e.g., average frequency or a fixed boost)
        m_key_frequency[obj.key] = min(MAX_FREQ_CAP, 5) # Boost to 5
        del m_ghost_registry[obj.key] # Remove from ghost
    else:
        # Standard cold start
        m_key_frequency[obj.key] = 1

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Move evicted item metadata to ghost registry instead of deleting immediately.
    '''
    global m_key_frequency, m_key_last_access, m_ghost_registry
    
    # Add to Ghost Registry (Limited size history)
    m_ghost_registry[evicted_obj.key] = True
    
    # Keep ghost registry from growing infinitely
    if len(m_ghost_registry) > 1000: # Heuristic limit
        # Simple FIFO removal for the dictionary if it gets too big
        first_key = next(iter(m_ghost_registry))
        del m_ghost_registry[first_key]

    # Clean up active cache metadata
    if evicted_obj.key in m_key_frequency:
        del m_key_frequency[evicted_obj.key]
        
    if evicted_obj.key in m_key_last_access:
        del m_key_last_access[evicted_obj.key]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 28
- Hit Rate: 0.0395

Example 2:
- Trace ID: 45
- Hit Rate: 0.0741

Example 3:
- Trace ID: 1
- Hit Rate: 0.4789

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:30:47 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:30:47,786 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:32:49 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:32:49,672 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:32:49,674 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To significantly improve the cache hit rates across varying traces (scans, loops, and varying working set sizes), we need an algorithm that separates "one-hit wonders" from frequently accessed items more effectively than standard LFU or LRU.

The solution below implements a **Segmented LRU (SLRU)** strategy with a **Ghost Registry**.

### The Strategy:
1.  **Segmentation:** We virtually split the cache into two segments:
    *   **Probation Segment:** For new items. They are eviction candidates if they don't get accessed again quickly.
    *   **Protected Segment:** For popular items. Items here are safer from eviction.
2.  **Protected Ratio:** We enforce that the Probation segment keeps a minimum percentage of the cache (e.g., 20%). This ensures that new items (even in a scan) have a window of opportunity to be hit before being evicted. If the Probation segment is squeezed too small, we evict from the Protected segment to make room.
3.  **Ghost Registry:** When an item is evicted, we record its key in a "Ghost" list. If an item in the Ghost list is inserted again, we know it's part of a recurring pattern (like a loop larger than the cache). We immediately promote it to the Protected segment.

This approach solves the specific failures of the previous algorithm:
*   **Scan Resistance:** One-time access items flow through the Probation segment and are evicted without polluting the Protected segment.
*   **Churn/Pollution:** By enforcing a minimum size for the Probation segment, we prevent old, protected items from locking up the entire cache.
*   **Recency:** Within each segment, we use LRU to handle shifts in popularity naturally.

### Implementation

```python
# Metadata dictionaries
# m_last_access: Tracks the logical access time for LRU implementation within segments.
m_last_access = dict()

# m_is_protected: Boolean flag. 
# False = Probation (Candidate for quick eviction)
# True = Protected (Long-term resident)
m_is_protected = dict()

# m_ghosts: A history of recently evicted keys. 
# Used to detect items that return shortly after eviction.
# We rely on Python's dict preserving insertion order for FIFO management.
m_ghosts = dict()

def evict(cache_snapshot, obj):
    '''
    Implements the eviction logic for Segmented LRU (SLRU).
    Decides whether to evict from the 'Probation' or 'Protected' segment.
    '''
    global m_last_access, m_is_protected
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    # 1. Classify keys into Probation and Protected lists
    probation_keys = []
    protected_keys = []
    
    for k in candidate_keys:
        if m_is_protected.get(k, False):
            protected_keys.append(k)
        else:
            probation_keys.append(k)
            
    total_items = len(candidate_keys)
    
    # 2. Define the target window size for new items.
    # We want to ensure at least ~20% of the cache is available for new items (Probation)
    # so they have time to prove their worth.
    min_probation_size = total_items * 0.2
    
    victim_key = None
    
    # 3. Decision Logic:
    
    # CASE A: The Probation segment is too small (Protected items are hogging space).
    # We must evict a Protected item to restore balance and allow new items in.
    if len(probation_keys) < min_probation_size and len(protected_keys) > 0:
        # Evict LRU from Protected
        victim_key = min(protected_keys, key=lambda k: m_last_access.get(k, 0))
        
    # CASE B: Normal operation. Evict from Probation (remove one-hit wonders).
    elif len(probation_keys) > 0:
        # Evict LRU from Probation
        victim_key = min(probation_keys, key=lambda k: m_last_access.get(k, 0))
        
    # CASE C: Fallback (Probation is empty, only Protected items exist).
    else:
         victim_key = min(protected_keys, key=lambda k: m_last_access.get(k, 0))
            
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On a cache hit:
    1. Update recency (for LRU).
    2. Promote the item to Protected status (it has proven it is reused).
    '''
    global m_last_access, m_is_protected
    
    m_last_access[obj.key] = cache_snapshot.access_count
    m_is_protected[obj.key] = True

def update_after_insert(cache_snapshot, obj):
    '''
    On a new insertion:
    1. Update recency.
    2. Check Ghost Registry:
       - If present: This is a "resurrected" item. Promote immediately to Protected.
       - If absent: This is a cold item. Add to Probation.
    '''
    global m_last_access, m_is_protected, m_ghosts
    
    m_last_access[obj.key] = cache_snapshot.access_count
    
    if obj.key in m_ghosts:
        # It was evicted recently and came back -> High value.
        m_is_protected[obj.key] = True
        # Remove from ghost since it's now in active cache
        del m_ghosts[obj.key]
    else:
        # First time seeing it (or seen a long time ago) -> Probation.
        m_is_protected[obj.key] = False

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
    1. Clean up active metadata.
    2. Add the evicted key to the Ghost Registry.
    '''
    global m_last_access, m_is_protected, m_ghosts
    
    # Clean up active cache metadata
    if evicted_obj.key in m_last_access:
        del m_last_access[evicted_obj.key]
    if evicted_obj.key in m_is_protected:
        del m_is_protected[evicted_obj.key]
        
    # Add to Ghost Registry (SLRU "Shadow Cache")
    m_ghosts[evicted_obj.key] = True
    
    # Limit Ghost Registry Size to prevent memory leak
    # We keep a history roughly proportional to the cache size (or a safe fixed limit)
    # to catch items that loop back shortly after eviction.
    limit = max(1000, len(cache_snapshot.cache))
    
    if len(m_ghosts) > limit:
        # Remove oldest inserted (FIFO) to maintain size
        oldest_key = next(iter(m_ghosts))
        del m_ghosts[oldest_key]
```
2025-11-27 20:33:04,416 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# m_access_info stores a tuple or object for each key: (frequency, last_access_time, insertion_time)
m_access_info = dict()

# Ghost cache to track recently evicted items for adaptability.
# Stores key -> eviction_time
m_ghost_registry = dict()

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score". Higher Score = Higher chance of being evicted.
    
    Strategy: 2-Queue / Segmented LRU Logic.
    1. Probationary Segment (Freq = 1): These are high risk. Evict LRU among these first.
    2. Protected Segment (Freq > 1): These are valuable. Evict LRU among these only if necessary.
    
    Score construction:
    - Base score for Freq=1:  1,000,000 + (Time since last access)
    - Base score for Freq>1:          0 + (Time since last access)
    
    This ensures any item with Freq=1 is always evicted before any item with Freq > 1.
    Within the same frequency class, the standard LRU (Time since last access) decides.
    '''
    if key not in m_access_info:
        # Should not happen for cached items, but purely safe fallback
        return float('inf')
        
    freq, last_access, _ = m_access_info[key]
    
    staleness = current_time - last_access
    
    # Priority 1: Evict "One-Hit Wonders" (Scan resistance)
    # If frequency is 1, we give it a massive base score boost so it is chosen as a victim
    # over any item that has proven its worth (freq > 1).
    if freq <= 1:
        # Base score 10^12 ensures it's always greater than simple staleness of freq > 1 items
        # We add staleness so that among freq=1 items, we pick the LRU one.
        return 1_000_000_000_000 + staleness
    
    # Priority 2: Standard LRU for items that have been accessed multiple times.
    # The score is just the staleness.
    # However, we can slightly weight by frequency to keep VERY hot items longer
    # if we want LFU-like behavior, but standard Segmented LRU is usually safer/more robust.
    # Let's stick to pure LRU within the protected segment to avoid cache pollution by old high-freq items.
    return staleness

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -1.0
    
    # Iterate to find the item with the highest victim score
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Update Last Access Time.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve current metadata
    # Default to (0, current, current) if something weird happens, though key should exist on hit
    freq, _, insertion_time = m_access_info.get(obj.key, (0, current_time, current_time))
    
    # Update: Increment freq, set new last_access, keep insertion_time
    # Cap frequency at a reasonable number (e.g., 4) to prevent "frequency accumulation"
    # where an item stays forever just because it was hot an hour ago. 
    # This transforms pure LFU into a "Protected Segment" logic.
    new_freq = min(freq + 1, 10) 
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. check Ghost Registry. If it was recently evicted, it means our cache is too small
       or we made a mistake. We give it a boost to enter the protected segment immediately.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Check if this key is in our ghost registry (recently evicted)
    if obj.key in m_ghost_registry:
        # It was a mistake to evict it (or cache is trashing). 
        # Give it a "second chance" frequency so it skips the probation queue (Freq=1)
        # and goes straight to protected queue (Freq=2).
        initial_freq = 2
        # Remove from ghost since it's real now
        del m_ghost_registry[obj.key]
    else:
        # Brand new item. Start at frequency 1 (Probation).
        initial_freq = 1
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache) to detect future misses on this item.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # Add to Ghost Registry
    # We record time of eviction. 
    m_ghost_registry[evicted_obj.key] = current_time
    
    # MAINTENANCE: Keep Ghost Registry bounded.
    # If Ghost Registry gets too big (e.g., larger than cache capacity), prune old entries.
    # This prevents memory leaks.
    # We allow the ghost registry to be roughly the same size as the cache capacity.
    if len(m_ghost_registry) > len(cache_snapshot.cache):
        # Remove the oldest ghost entry (simple FIFO for the ghost registry is sufficient)
        # In a real heavy implementation we might use a deque, but here we scan.
        # To optimize speed, we only prune when it's significantly over (e.g. +10%) or simply remove one.
        
        # Optimization: Just remove one arbitrary old one (FIFO-ish) or a chunk to amortize cost.
        # Since keys are ordered by insertion in modern Python dicts, iter(dict) gives oldest.
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 5
- Hit Rate: 0.493

Example 2:
- Trace ID: 25
- Hit Rate: 0.3569

Example 3:
- Trace ID: 30
- Hit Rate: 0.0456

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:33:04 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:33:04,418 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:33:29 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:33:29,663 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:33:29,664 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: An optimized cache eviction algorithm implementation designed to minimize miss rates by combining **Recency**, **Frequency**, and **Size** considerations, often referred to as a simplified **S3-FIFO** or **ARC-like** (Adaptive Replacement Cache) strategy with a "Ghost" registry.

This approach improves upon the previous one by:
1.  **Explicitly tracking hit density**: It distinguishes between "one-hit wonders" and frequently accessed items more aggressively.
2.  **Size-Awareness**: While frequency and recency are king, breaking ties by size (preferring to evict larger objects if scores are similar) can increase the *number* of objects in the cache, indirectly boosting hit rates for workloads with mixed object sizes.
3.  **Adaptive Ghost Registry**: It uses the "Ghost" registry (recently evicted items) to detect if the cache size is effectively too small for the working set. If a "ghost" is hit, the new item enters with higher priority, correcting the mistake of evicting it too soon.

```python
import math

# --- Metadata Storage ---
# Stores tuples: key -> (frequency, last_access_time, insertion_time, size)
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# This acts as a "shadow cache" to track items we recently evicted.
m_ghost_registry = dict()

# Global counter to enforce LRU purity within frequency buckets without relying solely on access_count
# (Useful if batch updates happen, though access_count usually suffices)
m_tick = 0

def get_victim_score(key, current_time, obj_size):
    '''
    Calculates a "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Logic:
    1. Filter by Frequency (Admit/Probation vs Protected):
       - Frequency = 1 (Probation): High risk of being a one-hit wonder. High eviction score.
       - Frequency > 1 (Protected): Proven utility. Low eviction score.
       
    2. Filter by Recency (within frequency classes):
       - Older 'last_access_time' -> Higher eviction score (Standard LRU).
       
    3. Tie-Breaker by Size (Optional but effective):
       - If two items have similar recency/frequency, evicting the LARGER one frees up more space
         for other items, potentially increasing total object count and hit rate (Greedy Dual Size principle).
    '''
    
    if key not in m_access_info:
        # Fallback for safety
        return float('inf')
        
    freq, last_access, _, _ = m_access_info[key]
    
    # Calculate Staleness (Time since last used)
    staleness = current_time - last_access
    
    # --- Scoring Formula ---
    
    # BASE SCORE determined by frequency tier.
    # Tier 1: Probation (freq=1). These are the first to go.
    # Base score: 10^15 (Guarantees they are evicted before Tier 2)
    if freq <= 1:
        base_score = 1e15
    else:
        # Tier 2: Protected (freq > 1).
        # Base score: 0. 
        base_score = 0
        
    # RECENCY Component: The staler, the higher the score.
    # Simple linear addition of time delta.
    recency_score = staleness
    
    # SIZE Penalty (Greedy Dual Size Lite):
    # We slightly prefer evicting larger objects to make room for more small objects.
    # However, recency is usually more important than size in general web traces.
    # We apply a small multiplier based on log(size) to break ties in LRU.
    # This prevents flushing a very hot large object just because it's large.
    size_penalty = 0
    if obj_size > 0:
        # A tiny bias: larger objects get a very small boost to their eviction score.
        # This helps in "cache size" constraints (bytes) vs "cache capacity" (count).
        size_penalty = math.log(obj_size + 1)
        
    final_score = base_score + recency_score + size_penalty
    return final_score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score.
    Strategy: Segmented LRU with Ghost awareness.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We scan candidates to find the 'worst' one (highest score)
    # Optimization: In a production system, we would maintain linked lists (queues)
    # to avoid O(N) scans. For this simulation interface, O(N) is the only way
    # to read the read-only cache_snapshot.
    for key in candidate_keys:
        # Retrieve size from metadata or fallback to 1 if missing (shouldn't happen)
        cached_obj_size = 1
        if key in m_access_info:
             cached_obj_size = m_access_info[key][3]
        
        score = get_victim_score(key, current_time, cached_obj_size)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Promote the item.
    2. Increase Frequency (saturation applied to prevent pollution).
    3. Update Recency.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing
    # Default: freq=0, insert=now, size=obj.size
    old_freq, _, insertion_time, _ = m_access_info.get(obj.key, (0, current_time, current_time, obj.size))
    
    # Increment frequency.
    # We cap frequency at a small number (e.g., 3 or 4).
    # Why? Because in SLRU/S3-FIFO, the distinction is mainly "One Hit" vs "Many Hits".
    # Distinguishing between 100 hits and 101 hits is rarely useful and prevents
    # items from aging out when workloads shift.
    new_freq = min(old_freq + 1, 4)
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time, obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Determine Initial Frequency (Probation vs Protected).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # DEFAULT: New items go to Probation (freq=1)
    initial_freq = 1
    
    # GHOST CHECK:
    # If this key was recently evicted, it implies our cache is too small or 
    # we made a bad eviction decision. This item is actually part of the working set.
    # We insert it directly with a higher frequency/priority so it isn't evicted immediately again.
    if obj.key in m_ghost_registry:
        # Promote straight to "Protected" tier
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to prevent memory leaks.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow the ghost registry to hold roughly as many items as the cache capacity (in count).
    # Since we don't know the exact count capacity (it's byte-based), we approximate based on
    # current cache keys count.
    max_ghost_size = max(len(cache_snapshot.cache), 100)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost registry.
        # Python dicts preserve insertion order (FIFO behavior for iteration).
        # We delete a chunk to amortize the cost.
        keys_to_remove = []
        for i, k in enumerate(m_ghost_registry):
            if i >= 5: # Remove 5 oldest at a time
                break
            keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_ghost_registry[k]
```
2025-11-27 20:36:09,760 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# --- Metadata Storage ---
# Stores tuples: key -> (frequency, last_access_time, insertion_time, size)
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# This acts as a "shadow cache" to track items we recently evicted.
m_ghost_registry = dict()

# Global counter to enforce LRU purity within frequency buckets without relying solely on access_count
# (Useful if batch updates happen, though access_count usually suffices)
m_tick = 0

def get_victim_score(key, current_time, obj_size):
    '''
    Calculates a "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Logic:
    1. Filter by Frequency (Admit/Probation vs Protected):
       - Frequency = 1 (Probation): High risk of being a one-hit wonder. High eviction score.
       - Frequency > 1 (Protected): Proven utility. Low eviction score.
       
    2. Filter by Recency (within frequency classes):
       - Older 'last_access_time' -> Higher eviction score (Standard LRU).
       
    3. Tie-Breaker by Size (Optional but effective):
       - If two items have similar recency/frequency, evicting the LARGER one frees up more space
         for other items, potentially increasing total object count and hit rate (Greedy Dual Size principle).
    '''
    
    if key not in m_access_info:
        # Fallback for safety
        return float('inf')
        
    freq, last_access, _, _ = m_access_info[key]
    
    # Calculate Staleness (Time since last used)
    staleness = current_time - last_access
    
    # --- Scoring Formula ---
    
    # BASE SCORE determined by frequency tier.
    # Tier 1: Probation (freq=1). These are the first to go.
    # Base score: 10^15 (Guarantees they are evicted before Tier 2)
    if freq <= 1:
        base_score = 1e15
    else:
        # Tier 2: Protected (freq > 1).
        # Base score: 0. 
        base_score = 0
        
    # RECENCY Component: The staler, the higher the score.
    # Simple linear addition of time delta.
    recency_score = staleness
    
    # SIZE Penalty (Greedy Dual Size Lite):
    # We slightly prefer evicting larger objects to make room for more small objects.
    # However, recency is usually more important than size in general web traces.
    # We apply a small multiplier based on log(size) to break ties in LRU.
    # This prevents flushing a very hot large object just because it's large.
    size_penalty = 0
    if obj_size > 0:
        # A tiny bias: larger objects get a very small boost to their eviction score.
        # This helps in "cache size" constraints (bytes) vs "cache capacity" (count).
        size_penalty = math.log(obj_size + 1)
        
    final_score = base_score + recency_score + size_penalty
    return final_score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score.
    Strategy: Segmented LRU with Ghost awareness.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We scan candidates to find the 'worst' one (highest score)
    # Optimization: In a production system, we would maintain linked lists (queues)
    # to avoid O(N) scans. For this simulation interface, O(N) is the only way
    # to read the read-only cache_snapshot.
    for key in candidate_keys:
        # Retrieve size from metadata or fallback to 1 if missing (shouldn't happen)
        cached_obj_size = 1
        if key in m_access_info:
             cached_obj_size = m_access_info[key][3]
        
        score = get_victim_score(key, current_time, cached_obj_size)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Promote the item.
    2. Increase Frequency (saturation applied to prevent pollution).
    3. Update Recency.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing
    # Default: freq=0, insert=now, size=obj.size
    old_freq, _, insertion_time, _ = m_access_info.get(obj.key, (0, current_time, current_time, obj.size))
    
    # Increment frequency.
    # We cap frequency at a small number (e.g., 3 or 4).
    # Why? Because in SLRU/S3-FIFO, the distinction is mainly "One Hit" vs "Many Hits".
    # Distinguishing between 100 hits and 101 hits is rarely useful and prevents
    # items from aging out when workloads shift.
    new_freq = min(old_freq + 1, 4)
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time, obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Determine Initial Frequency (Probation vs Protected).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # DEFAULT: New items go to Probation (freq=1)
    initial_freq = 1
    
    # GHOST CHECK:
    # If this key was recently evicted, it implies our cache is too small or 
    # we made a bad eviction decision. This item is actually part of the working set.
    # We insert it directly with a higher frequency/priority so it isn't evicted immediately again.
    if obj.key in m_ghost_registry:
        # Promote straight to "Protected" tier
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to prevent memory leaks.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow the ghost registry to hold roughly as many items as the cache capacity (in count).
    # Since we don't know the exact count capacity (it's byte-based), we approximate based on
    # current cache keys count.
    max_ghost_size = max(len(cache_snapshot.cache), 100)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost registry.
        # Python dicts preserve insertion order (FIFO behavior for iteration).
        # We delete a chunk to amortize the cost.
        keys_to_remove = []
        for i, k in enumerate(m_ghost_registry):
            if i >= 5: # Remove 5 oldest at a time
                break
            keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_ghost_registry[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 6
- Hit Rate: 0.2727

Example 2:
- Trace ID: 11
- Hit Rate: 0.3424

Example 3:
- Trace ID: 44
- Hit Rate: 0.021

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:36:09 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:36:09,762 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:37:39 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:37:39,754 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:37:39,756 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: This is a classic cache optimization problem. The poor performance on Trace 44 (2.1%) specifically indicates that the current algorithm fails to handle workloads where object sizes vary significantly (likely clogging the cache with a few large, useless items) or where the working set rotates faster than the "probation" period allows for.

To maximize the Hit Rate (minimize Miss Rate), we need an algorithm that optimizes for **Hit Density**. The best-known approach for variable-size caching that maximizes hit counts is **GDSF (Greedy Dual-Size with Frequency)**.

### The Algorithm: GDSF (Greedy Dual-Size Frequency)

**Core Concept:**
We assign a priority value to every object. When space is needed, we evict the object with the *lowest* priority.
The priority formula is:
$$Priority = L + \frac{Frequency \times Weight}{Size}$$

Where:
1.  **$L$ (Inflation Value):** A running "aging" factor. When an object is evicted, $L$ is updated to the priority of that evicted object. This forces older items in the cache to "pay rent" to stay in; if their frequency doesn't justify their size, the rising tide of $L$ will eventually wash them out.
2.  **Frequency:** How often the object is accessed.
3.  **Size:** We penalize larger objects because evicting one large object makes room for many small objects, statistically increasing the chance of future hits.

**Why this fixes the traces:**
*   **Trace 6 & 11:** The Frequency component ensures popular items stay, similar to the previous Protected/Probation logic but more granular.
*   **Trace 44 (The Killer):** The Size component will aggressively evict large, low-frequency items that are likely blocking the cache. The $L$ factor ensures that scan-heavy workloads (items accessed once) cycle through quickly without displacing the hot set.

### Implementation Details

1.  **Metadata:** We store `(frequency, priority, insertion_tick)` for active items.
2.  **Ghost Registry:** We keep a small history of evicted items. If a "Ghost" returns, we restore its previous frequency so it doesn't get evicted immediately (Scan Resistance).
3.  **Tie-Breaking:** If priorities are equal, we fall back to LRU (stored via `insertion_tick`).

```python
import math

# --- Metadata Storage ---
# Stores: key -> {'freq': int, 'priority': float, 'tick': int}
m_metadata = dict()

# Ghost registry: Stores key -> frequency
# Used to restore frequency for items that return to cache quickly (thrashing protection)
m_ghost_registry = dict()

# Global Inflation Value (L)
# This represents the "cost" to enter/stay in the cache. It rises over time.
m_inflation = 0.0

def get_priority(freq, size, current_L):
    '''
    Calculates GDSF Priority.
    Priority = L + (Frequency * Weight) / Size
    
    Higher Priority = Keep in Cache.
    Lower Priority = Evict.
    '''
    # Avoid division by zero, though size should be positive.
    # We use a slight dampening on size (sqrt) or linear size. 
    # For pure Hit Rate maximization, linear size (1/size) is mathematically optimal.
    # However, to prevent starving large hot objects entirely, we ensure a minimum weight.
    
    safe_size = max(1, size)
    
    # Heuristic: 
    # We use a large multiplier (10000) to keep floating point precision reasonable
    # relative to L.
    # Formula: Priority = L + (Frequency * 10000) / Size
    cost_benefit = (freq * 10000.0) / safe_size
    
    return current_L + cost_benefit

def evict(cache_snapshot, obj):
    '''
    GDSF Eviction:
    1. Find candidate with the MINIMUM priority.
    2. Update global m_inflation (L) to that minimum priority.
    3. Return that candidate.
    '''
    global m_inflation
    
    candidate_keys = cache_snapshot.cache.keys()
    
    if not candidate_keys:
        return None

    # Search for victim with lowest priority
    victim = None
    min_priority = float('inf')
    
    # We need to scan to find the minimum. 
    # In a real O(1) implementation, this would use a Min-Heap.
    for key in candidate_keys:
        if key in m_metadata:
            entry = m_metadata[key]
            p = entry['priority']
            
            # Identify minimum priority
            if p < min_priority:
                min_priority = p
                victim = key
            elif p == min_priority:
                # Tie-Breaker: LRU
                # If priorities are identical, evict the one accessed/inserted earliest
                if victim and m_metadata[key]['tick'] < m_metadata[victim]['tick']:
                    victim = key
        else:
            # Should not happen if metadata is synced, but if it does, evict unknown item
            return key
            
    # CRITICAL GDSF STEP:
    # The system ages by setting L to the priority of the evicted item.
    if victim is not None:
        m_inflation = min_priority
        
    return victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Reset Priority using CURRENT L (Re-aging).
    3. Update Tick (for LRU tie-breaking).
    '''
    global m_metadata, m_inflation
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_metadata:
        entry = m_metadata[obj.key]
        entry['freq'] += 1
        entry['tick'] = current_time # Update for LRU
        
        # Recalculate priority based on new freq and CURRENT inflation L
        # This "brings the object forward" in time.
        entry['priority'] = get_priority(entry['freq'], obj.size, m_inflation)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Determine Frequency (check Ghost).
    2. Calculate initial Priority using CURRENT L.
    '''
    global m_metadata, m_ghost_registry, m_inflation
    
    current_time = cache_snapshot.access_count
    
    # Default frequency
    freq = 1
    
    # Ghost Recovery:
    # If this item was recently evicted, it means it's part of a loop or working set
    # that is slightly larger than cache. Restore its frequency so it isn't 
    # immediately evicted again (Scan Resistance).
    if obj.key in m_ghost_registry:
        freq = m_ghost_registry[obj.key] + 1
        # Cap frequency restoration to prevent ancient history from dominating
        # extremely fast moving caches, but allow enough to stick.
        del m_ghost_registry[obj.key]

    # Calculate Priority
    priority = get_priority(freq, obj.size, m_inflation)
    
    m_metadata[obj.key] = {
        'freq': freq,
        'priority': priority,
        'tick': current_time
    }

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Move Metadata to Ghost.
    2. Prune Ghost Registry.
    '''
    global m_metadata, m_ghost_registry
    
    if evicted_obj.key in m_metadata:
        # Save frequency to ghost
        freq = m_metadata[evicted_obj.key]['freq']
        del m_metadata[evicted_obj.key]
        m_ghost_registry[evicted_obj.key] = freq
    
    # Maintain Ghost Registry Size
    # If the ghost registry grows too large, it wastes memory and might restore
    # frequencies for items that have been gone too long.
    # A good heuristic is Ghost Capacity ~= Cache Object Count (approximate).
    # Since we can't easily know max object count, we limit it relative to current usage.
    # We use a loose bound (e.g., 2000 items or 2x current cache count).
    
    target_len = max(len(cache_snapshot.cache) * 2, 500)
    
    if len(m_ghost_registry) > target_len:
        # Prune oldest inserted items (Dicts preserve insertion order in modern Python)
        # Pruning in chunks is more efficient than one by one
        items_to_remove = []
        for k in m_ghost_registry:
            items_to_remove.append(k)
            if len(items_to_remove) >= 10:
                break
        for k in items_to_remove:
            del m_ghost_registry[k]
```
2025-11-27 20:37:56,422 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# --- Metadata Storage ---
# Stores tuples: key -> (frequency, last_access_time, insertion_time, size)
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# This acts as a "shadow cache" to track items we recently evicted.
m_ghost_registry = dict()

# Global counter to enforce LRU purity within frequency buckets without relying solely on access_count
# (Useful if batch updates happen, though access_count usually suffices)
m_tick = 0

def get_victim_score(key, current_time, obj_size):
    '''
    Calculates a "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Logic:
    1. Filter by Frequency (Admit/Probation vs Protected):
       - Frequency = 1 (Probation): High risk of being a one-hit wonder. High eviction score.
       - Frequency > 1 (Protected): Proven utility. Low eviction score.
       
    2. Filter by Recency (within frequency classes):
       - Older 'last_access_time' -> Higher eviction score (Standard LRU).
       
    3. Tie-Breaker by Size (Optional but effective):
       - If two items have similar recency/frequency, evicting the LARGER one frees up more space
         for other items, potentially increasing total object count and hit rate (Greedy Dual Size principle).
    '''
    
    if key not in m_access_info:
        # Fallback for safety
        return float('inf')
        
    freq, last_access, _, _ = m_access_info[key]
    
    # Calculate Staleness (Time since last used)
    staleness = current_time - last_access
    
    # --- Scoring Formula ---
    
    # BASE SCORE determined by frequency tier.
    # Tier 1: Probation (freq=1). These are the first to go.
    # Base score: 10^15 (Guarantees they are evicted before Tier 2)
    if freq <= 1:
        base_score = 1e15
    else:
        # Tier 2: Protected (freq > 1).
        # Base score: 0. 
        base_score = 0
        
    # RECENCY Component: The staler, the higher the score.
    # Simple linear addition of time delta.
    recency_score = staleness
    
    # SIZE Penalty (Greedy Dual Size Lite):
    # We slightly prefer evicting larger objects to make room for more small objects.
    # However, recency is usually more important than size in general web traces.
    # We apply a small multiplier based on log(size) to break ties in LRU.
    # This prevents flushing a very hot large object just because it's large.
    size_penalty = 0
    if obj_size > 0:
        # A tiny bias: larger objects get a very small boost to their eviction score.
        # This helps in "cache size" constraints (bytes) vs "cache capacity" (count).
        size_penalty = math.log(obj_size + 1)
        
    final_score = base_score + recency_score + size_penalty
    return final_score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score.
    Strategy: Segmented LRU with Ghost awareness.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We scan candidates to find the 'worst' one (highest score)
    # Optimization: In a production system, we would maintain linked lists (queues)
    # to avoid O(N) scans. For this simulation interface, O(N) is the only way
    # to read the read-only cache_snapshot.
    for key in candidate_keys:
        # Retrieve size from metadata or fallback to 1 if missing (shouldn't happen)
        cached_obj_size = 1
        if key in m_access_info:
             cached_obj_size = m_access_info[key][3]
        
        score = get_victim_score(key, current_time, cached_obj_size)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Promote the item.
    2. Increase Frequency (saturation applied to prevent pollution).
    3. Update Recency.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing
    # Default: freq=0, insert=now, size=obj.size
    old_freq, _, insertion_time, _ = m_access_info.get(obj.key, (0, current_time, current_time, obj.size))
    
    # Increment frequency.
    # We cap frequency at a small number (e.g., 3 or 4).
    # Why? Because in SLRU/S3-FIFO, the distinction is mainly "One Hit" vs "Many Hits".
    # Distinguishing between 100 hits and 101 hits is rarely useful and prevents
    # items from aging out when workloads shift.
    new_freq = min(old_freq + 1, 4)
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time, obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Determine Initial Frequency (Probation vs Protected).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # DEFAULT: New items go to Probation (freq=1)
    initial_freq = 1
    
    # GHOST CHECK:
    # If this key was recently evicted, it implies our cache is too small or 
    # we made a bad eviction decision. This item is actually part of the working set.
    # We insert it directly with a higher frequency/priority so it isn't evicted immediately again.
    if obj.key in m_ghost_registry:
        # Promote straight to "Protected" tier
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to prevent memory leaks.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow the ghost registry to hold roughly as many items as the cache capacity (in count).
    # Since we don't know the exact count capacity (it's byte-based), we approximate based on
    # current cache keys count.
    max_ghost_size = max(len(cache_snapshot.cache), 100)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost registry.
        # Python dicts preserve insertion order (FIFO behavior for iteration).
        # We delete a chunk to amortize the cost.
        keys_to_remove = []
        for i, k in enumerate(m_ghost_registry):
            if i >= 5: # Remove 5 oldest at a time
                break
            keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_ghost_registry[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 17
- Hit Rate: 0.0235

Example 2:
- Trace ID: 19
- Hit Rate: 0.2722

Example 3:
- Trace ID: 12
- Hit Rate: 0.04

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:37:56 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:37:56,423 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:39:30 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:39:30,712 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:39:30,714 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: ```python
import math

# --- Metadata Storage ---
# Stores key -> [frequency, last_access_time, size]
# Using a list allows for mutable updates of frequency and time.
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# Tracks items recently evicted to boost their frequency if they return.
# This helps the algorithm adapt to working sets that are slightly larger than cache capacity.
m_ghost_registry = dict()

def get_victim_score(key, current_time, meta):
    '''
    Calculates the eviction score. 
    Higher score = Higher desirability to EVICT.
    
    The scoring strategy is a hybrid of Recency (LRU), Frequency (LFU), and Size (GreedyDual).
    
    Formula: Score = (Age * Size_Factor) / Frequency
    
    1. Age (current_time - last_access):
       Standard LRU component. The older the object, the higher the score.
       
    2. Frequency:
       The denominator. Higher frequency drastically reduces the eviction score, 
       protecting popular items.
       
    3. Size_Factor (sqrt(size)):
       We penalize larger objects because evicting one large object makes room for multiple
       small objects, which statistically increases the hit rate (hit count).
       We use sqrt(size) rather than linear size to avoid being overly aggressive 
       against large, potentially useful assets.
    '''
    freq, last_access, size = meta
    
    age = current_time - last_access
    
    # Safety check for frequency (should minimally be 1)
    if freq < 1:
        freq = 1
        
    # Size Factor:
    # Using sqrt(size) balances "Hits per Byte" optimization with fairness.
    # Linear size can cause large hot objects to be evicted too quickly.
    size_factor = size ** 0.5
    
    # Victim Score Calculation
    # We divide by frequency. A frequency of 10 makes an item 10x harder to evict 
    # than a frequency of 1 (assuming same age/size).
    score = (age * size_factor) / freq
    
    return score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest calculated victim score.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # Iterate over all candidates to find the one with the highest eviction score.
    # While O(N), this allows precise application of the hybrid scoring logic.
    for key in candidate_keys:
        if key in m_access_info:
            score = get_victim_score(key, current_time, m_access_info[key])
        else:
            # Fallback: If metadata is missing, treat it as the best candidate to evict.
            score = float('inf')
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency (Popularity).
    2. Update Last Access Time (Recency).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        entry = m_access_info[obj.key]
        entry[0] += 1             # Increment Frequency
        entry[1] = current_time   # Update Recency
    else:
        # Recover metadata if missing (rare case)
        m_access_info[obj.key] = [1, current_time, obj.size]

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Check Ghost Registry to apply frequency boost if applicable.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # Default initial frequency for new items (Probation)
    initial_freq = 1
    
    # Check Ghost Registry
    if obj.key in m_ghost_registry:
        # The item was recently evicted and has been requested again.
        # This implies our cache was slightly too small or we made a mistake.
        # We grant a frequency boost so it isn't evicted immediately again.
        initial_freq = 4
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = [initial_freq, current_time, obj.size]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (shadow cache).
    3. Prune Ghost Registry to keep memory usage bounded.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We want the ghost registry to be large enough to catch "loops" or working sets 
    # that just barely don't fit. A size of 2x the current object count is usually sufficient.
    # We use a lower bound of 500 to handle cold start or very small caches.
    target_ghost_size = max(len(cache_snapshot.cache) * 2, 500)
    
    if len(m_ghost_registry) > target_ghost_size:
        # Remove the oldest inserted item from ghost registry.
        # Since Python 3.7+, dicts preserve insertion order, so next(iter()) gives the oldest.
        victim_ghost = next(iter(m_ghost_registry))
        del m_ghost_registry[victim_ghost]
```
2025-11-27 20:39:44,141 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 34
- Hit Rate: 0.0789

Example 2:
- Trace ID: 35
- Hit Rate: 0.0671

Example 3:
- Trace ID: 20
- Hit Rate: 0.3884

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:39:44 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:39:44,142 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:40:01 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:40:01,571 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:40:01,572 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache hit rate, we need to address the limitations of the previous implementation. The previous approach used a custom scoring function combining frequency and recency. While theoretically sound, ad-hoc scoring functions often struggle against diverse trace patterns (like Trace 34 and 35 which show very low hit rates) because they require careful tuning of the decay parameters and "ghost" boosts.

**Strategy for Improvement: LFU-DA (Least Frequently Used with Dynamic Aging)**

The algorithm below implements **LFU-DA**, a well-known, high-performance eviction policy that is often superior to simple LFU or LRU.

1.  **Core Concept**: It associates a value with each object. When an object is accessed (hit or insertion), its value becomes `current_frequency + cache_age`.
2.  **Dynamic Aging (`cache_age`)**: When an eviction occurs, the `cache_age` is updated to the value of the evicted object. This naturally "ages" existing objects. New objects enter with a higher baseline value, preventing the "cache pollution" problem of strict LFU where old, high-frequency items stay forever even if they stop being accessed.
3.  **Tie-Breaking**: If frequencies/values are tied, we fall back to LRU (Least Recently Used) to break the tie, ensuring we evict the one that hasn't been used for the longest time among the candidates.

This approach adapts better to shifting popularity (handling traces like 34/35 better) while maintaining high retention for truly hot items (handling trace 20).

```python
import collections

# Metadata dictionaries
# Maps key -> frequency count
m_key_frequency = dict()
# Maps key -> priority value (Frequency + LFU_DA_AGE)
m_key_value = dict()
# Maps key -> last access timestamp (for LRU tie-breaking)
m_key_last_access = dict()

# LFU-DA Global State
LFU_DA_AGE = 0.0

def evict(cache_snapshot, obj):
    '''
    Implements LFU-DA (Least Frequently Used with Dynamic Aging) eviction.
    
    We select the victim with the lowest 'm_key_value'.
    Tie-breaking: If values are equal, use LRU (oldest 'm_key_last_access').
    '''
    global m_key_value, m_key_last_access, LFU_DA_AGE
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_val = float('inf')
    min_access_time = float('inf')

    # Linear scan to find the minimum value. 
    for key in candidate_keys:
        val = m_key_value.get(key, 0.0)
        last_access = m_key_last_access.get(key, 0)
        
        # We want the smallest value (LFU principle)
        if val < min_val:
            min_val = val
            min_access_time = last_access
            victim_key = key
        elif val == min_val:
            # Tie-breaker: LRU
            if last_access < min_access_time:
                min_access_time = last_access
                victim_key = key
    
    # LFU-DA Logic: The global "Age" of the cache rises to meet the 
    # value of the object being evicted. This allows new items (with low freq)
    # to compete with old items (with high freq).
    if victim_key is not None:
        LFU_DA_AGE = min_val

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit: Increment frequency and update the object's value 
    using the current global Age.
    '''
    global m_key_frequency, m_key_value, m_key_last_access, LFU_DA_AGE
    
    key = obj.key
    
    # Update Frequency
    current_freq = m_key_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_key_frequency[key] = new_freq
    
    # Update Priority Value (LFU-DA formula)
    # Value = Frequency + Global_Cache_Age
    m_key_value[key] = new_freq + LFU_DA_AGE
    
    # Update Recency for tie-breaking
    m_key_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert: Initialize frequency and set value relative to global Age.
    '''
    global m_key_frequency, m_key_value, m_key_last_access, LFU_DA_AGE
    
    key = obj.key
    
    # Initialize Frequency
    # If it was in history (ghost cache), we could restore it, but strictly
    # resetting to 1 is safer for LFU-DA to prevent pollution, 
    # relying on Age to protect it.
    new_freq = 1
    m_key_frequency[key] = new_freq
    
    # LFU-DA Value Initialization
    # New items start at Frequency(1) + Current Age.
    m_key_value[key] = new_freq + LFU_DA_AGE
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction: Clean up metadata to prevent unbounded growth.
    '''
    global m_key_frequency, m_key_value, m_key_last_access
    
    # Unlike "Ghost Cache" approaches, standard LFU-DA often performs better
    # if we strictly forget evicted items to adapt to phase changes quickly.
    # However, keeping them is valid if we want to support "churning" items.
    # Given the previous results, strict cleanup is usually safer for memory 
    # and preventing old heavy hitters from returning too easily.
    
    key = evicted_obj.key
    if key in m_key_frequency:
        del m_key_frequency[key]
    if key in m_key_value:
        del m_key_value[key]
    if key in m_key_last_access:
        del m_key_last_access[key]
```
2025-11-27 20:40:25,066 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionary to store access history for objects.
# Key -> {
#   'freq': int, 
#   'last_access': int, 
#   'insertion_time': int,
#   'recency_boost': bool  (True if restored from ghost)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
# Used to detect if we evicted something too early (indicating a need for more "Recency" protection).
m_ghost_registry = dict()

# Tunable parameters
# How much extra life a hit gives an object compared to just insertion
FREQUENCY_WEIGHT = 1.0  
# Max entries in ghost registry relative to cache capacity
GHOST_RATIO = 1.5 

def get_victim_score(key, current_time):
    '''
    Calculate a "Victim Score" where Higher Score = Higher chance of eviction.
    
    We want to approximate an adaptive strategy:
    1. Identify "One-hit wonders" (Recency-biased items) vs "Heavy hitters" (Frequency-biased).
    2. Evict the item that is least likely to be used again based on a combined Recency + Frequency metric.
    
    The score is calculated as:
        Score = Current Time - (Last Access Time + Frequency_Bonus)
    
    This is effectively "Virtual Time since deadline". 
    - If Frequency is low, the deadline is close to Last Access.
    - If Frequency is high, the "effective" Last Access is pushed into the future, protecting it.
    '''
    if key not in m_access_info:
        return float('inf')
        
    meta = m_access_info[key]
    freq = meta['freq']
    last_access = meta['last_access']
    recency_boost = meta.get('recency_boost', False)
    
    # 1. Base Staleness: How long ago was it touched?
    staleness = current_time - last_access
    
    # 2. Protection Factor (Frequency-based).
    # Instead of infinite protection for high freq, we map frequency to a "time bonus".
    # Logarithmic scaling prevents high-freq items from becoming immortal.
    # We want freq=1 to have 0 bonus.
    # freq=2 to have specific bonus, etc.
    # If it was a 'recency_boost' item (rescued from ghost), it gets an extra protective pad.
    
    freq_bonus_factor = 0
    if freq > 1:
        # Give a "time credit" equal to a portion of the cache history.
        # This keeps popular items around longer than purely recent items.
        # Log2(freq) scales 2->1, 4->2, 8->3.
        freq_bonus_factor = math.log2(freq) * 1000 
    
    if recency_boost:
        # This item proved we were wrong to evict it previously.
        # Give it a flat bonus to survive the "probation" period of being new.
        freq_bonus_factor += 2000

    # 3. Compute Score
    # We want to evict the item with the Highest Staleness relative to its Protection.
    # Effective Staleness = Real Staleness - Protection
    # A negative effective staleness means it is "very fresh" or "highly protected".
    
    # Special handling for Freq=1 (Probationary items):
    # We generally want to evict these BEFORE any established item (Freq > 1),
    # unless the established item is extremely old.
    # We add a massive base penalty to Freq=1 to bias eviction towards them.
    
    victim_score = staleness - freq_bonus_factor
    
    if freq == 1 and not recency_boost:
        # Bias: Prefer evicting items that have only been seen once 
        # and weren't rescued from the ghost list.
        # Adding a large constant ensures they are scored higher (more likely to evict)
        # than frequency>1 items with similar staleness.
        victim_score += 1_000_000_000
        
    return victim_score

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the highest Victim Score.
    This effectively implements a Segmented LRU where the "Probation" segment (freq=1)
    is scanned for victims first, followed by the "Protected" segment.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim_key = None
    max_score = -float('inf')
    
    # Optimization: If we have many items, we could sample random N items to check 
    # (Approximated LRU), but since we need high precision for benchmarks, we scan all.
    for key in candidate_keys:
        score = get_victim_score(key, current_time)
        
        if score > max_score:
            max_score = score
            best_victim_key = key
            
    return best_victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Increment Frequency (with a cap).
    3. Clear 'recency_boost' flag if freq gets high enough, as it's now established.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # Increment frequency, cap at a reasonable number to prevent integer overflow
        # or overly aggressive sticking.
        # 16 is enough to differentiate "very hot" from "warm".
        meta['freq'] = min(meta['freq'] + 1, 16)
        
        # If it has survived long enough to be hit multiple times, it's no longer just a "ghost rescue"
        if meta['freq'] > 2:
            meta['recency_boost'] = False
            
        m_access_info[obj.key] = meta
    else:
        # Fallback if metadata is missing for some reason
        m_access_info[obj.key] = {
            'freq': 2, # Assume at least 2 since it's a hit
            'last_access': current_time,
            'insertion_time': current_time,
            'recency_boost': False
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Initialize metadata.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # If it was in ghost registry, it means we evicted it recently but shouldn't have.
    # We grant it 'recency_boost' status.
    recency_boost = is_ghost
    
    # If it was a ghost, we give it an initial frequency boost so it doesn't get evicted immediately again.
    # If new, freq=1. If ghost, start at freq=1 but with the boost flag (handled in scoring).
    initial_freq = 1
    
    m_access_info[obj.key] = {
        'freq': initial_freq,
        'last_access': current_time,
        'insertion_time': current_time,
        'recency_boost': recency_boost
    }
    
    # Remove from ghost if present
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry if too large.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean up active metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow ghost registry to be slightly larger than the cache to capture wider loops.
    # Using the current cache size is a proxy for capacity.
    max_ghost_size = int(len(cache_snapshot.cache) * GHOST_RATIO) + 10
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost.
        # In Python 3.7+, dicts preserve insertion order. The first key is the oldest.
        try:
            # next(iter()) is O(1) in Python dicts
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 0
- Hit Rate: 0.5103

Example 2:
- Trace ID: 12
- Hit Rate: 0.04

Example 3:
- Trace ID: 5
- Hit Rate: 0.4933

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:40:25 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:40:25,067 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:42:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:42:28,522 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:42:28,524 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: Here is the improved cache eviction algorithm. It implements a Segmented LRU (SLRU) strategy with a "Ghost" registry (Shadow Cache) to effectively distinguish between "Probationary" items (scans/one-hit wonders) and "Protected" items (frequent/working set).

This approach specifically targets the weaknesses identified in the feedback:
1.  **Scan Resistance:** Items start in a "Probation" segment. If they are part of a massive scan (Trace 12), they are evicted from Probation before affecting the hot "Protected" items.
2.  **Loop/Recency Protection:** The Ghost registry helps fast-track items that are part of a loop or were evicted prematurely, moving them to "Protected" status quickly upon re-entry.
3.  **Adaptive Eviction:** The `evict` function strictly balances the size of the "Protected" segment (capped at 80% of capacity). This prevents heavy hitters from saturating the cache and ensures a buffer always exists for new content to prove its utility.

```python
# Metadata dictionary to store access history and segment info.
# Key -> {
#   'last_access': int, 
#   'protected': bool  (False=Probation, True=Protected)
# }
m_access_info = dict()

# Ghost registry (Shadow Cache).
# Stores keys of evicted items -> eviction_time.
m_ghost_registry = dict()

# Tunable Parameters
PROTECTED_RATIO = 0.8  # Max % of cache allowed for Protected items
GHOST_RATIO = 1.0      # Size of ghost registry relative to capacity

def get_victim_score(key, current_time):
    # Not used in this optimized implementation, 
    # as the logic is embedded directly in evict for efficiency and state management.
    return 0

def evict(cache_snapshot, obj):
    '''
    Implements Segmented LRU (SLRU) eviction logic.
    - The cache is conceptually divided into "Probation" and "Protected" segments.
    - We enforce a capacity limit on the Protected segment.
    - Eviction Priority:
        1. If Protected segment is over capacity: Evict LRU from Protected.
        2. Otherwise: Evict LRU from Probation.
        3. Fallback: Evict LRU from Protected (if Probation is empty).
    '''
    global m_access_info
    
    # Identify candidates
    candidate_keys = list(cache_snapshot.cache.keys())
    
    lru_probation_key = None
    lru_probation_time = float('inf')
    
    lru_protected_key = None
    lru_protected_time = float('inf')
    
    protected_count = 0
    
    # Single pass to gather stats and candidates
    for key in candidate_keys:
        # Default to Probation if metadata is missing
        meta = m_access_info.get(key, {'last_access': 0, 'protected': False})
        
        if meta['protected']:
            protected_count += 1
            if meta['last_access'] < lru_protected_time:
                lru_protected_time = meta['last_access']
                lru_protected_key = key
        else:
            if meta['last_access'] < lru_probation_time:
                lru_probation_time = meta['last_access']
                lru_probation_key = key
                
    # Decision Logic
    protected_limit = cache_snapshot.capacity * PROTECTED_RATIO
    
    # 1. Enforce Protected Segment Size
    # If we have too many protected items, we must evict one to make room.
    # (Effectively acting as a demotion).
    if protected_count > protected_limit:
        if lru_protected_key is not None:
            return lru_protected_key
            
    # 2. Normal Eviction: Prefer Probationary items
    if lru_probation_key is not None:
        return lru_probation_key
        
    # 3. Fallback: If no probation items exist, evict LRU Protected
    return lru_protected_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update Last Access Time.
    2. Promotion: If a Probation item is hit, it moves to Protected.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    if obj.key in m_access_info:
        meta = m_access_info[obj.key]
        meta['last_access'] = current_time
        
        # SLRU Logic: A hit on a Probation item promotes it to Protected
        if not meta['protected']:
            meta['protected'] = True
            
        m_access_info[obj.key] = meta
    else:
        # Recovery if metadata missing
        m_access_info[obj.key] = {
            'last_access': current_time,
            'protected': False # Start in probation conservatively
        }

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. If in Ghost, insert as Protected (it proved its worth by returning).
    3. Else, insert as Probation.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    is_ghost = obj.key in m_ghost_registry
    
    # Initialize metadata
    # If it was in ghost, it skips probation and goes straight to Protected
    m_access_info[obj.key] = {
        'last_access': current_time,
        'protected': is_ghost 
    }
    
    # Clean up ghost entry
    if is_ghost:
        del m_ghost_registry[obj.key]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Shadow Cache).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Cleanup Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # Keep it proportional to cache capacity to track relevant history
    max_ghost_size = int(cache_snapshot.capacity * GHOST_RATIO)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted ghost key
        # (Python dicts 3.7+ preserve insertion order)
        try:
            oldest_ghost_key = next(iter(m_ghost_registry))
            del m_ghost_registry[oldest_ghost_key]
        except StopIteration:
            pass
```
2025-11-27 20:42:51,037 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# --- Metadata Storage ---
# Stores tuples: key -> (frequency, last_access_time, insertion_time, size)
m_access_info = dict()

# Ghost registry: Stores key -> eviction_time
# This acts as a "shadow cache" to track items we recently evicted.
m_ghost_registry = dict()

# Global counter to enforce LRU purity within frequency buckets without relying solely on access_count
# (Useful if batch updates happen, though access_count usually suffices)
m_tick = 0

def get_victim_score(key, current_time, obj_size):
    '''
    Calculates a "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Logic:
    1. Filter by Frequency (Admit/Probation vs Protected):
       - Frequency = 1 (Probation): High risk of being a one-hit wonder. High eviction score.
       - Frequency > 1 (Protected): Proven utility. Low eviction score.
       
    2. Filter by Recency (within frequency classes):
       - Older 'last_access_time' -> Higher eviction score (Standard LRU).
       
    3. Tie-Breaker by Size (Optional but effective):
       - If two items have similar recency/frequency, evicting the LARGER one frees up more space
         for other items, potentially increasing total object count and hit rate (Greedy Dual Size principle).
    '''
    
    if key not in m_access_info:
        # Fallback for safety
        return float('inf')
        
    freq, last_access, _, _ = m_access_info[key]
    
    # Calculate Staleness (Time since last used)
    staleness = current_time - last_access
    
    # --- Scoring Formula ---
    
    # BASE SCORE determined by frequency tier.
    # Tier 1: Probation (freq=1). These are the first to go.
    # Base score: 10^15 (Guarantees they are evicted before Tier 2)
    if freq <= 1:
        base_score = 1e15
    else:
        # Tier 2: Protected (freq > 1).
        # Base score: 0. 
        base_score = 0
        
    # RECENCY Component: The staler, the higher the score.
    # Simple linear addition of time delta.
    recency_score = staleness
    
    # SIZE Penalty (Greedy Dual Size Lite):
    # We slightly prefer evicting larger objects to make room for more small objects.
    # However, recency is usually more important than size in general web traces.
    # We apply a small multiplier based on log(size) to break ties in LRU.
    # This prevents flushing a very hot large object just because it's large.
    size_penalty = 0
    if obj_size > 0:
        # A tiny bias: larger objects get a very small boost to their eviction score.
        # This helps in "cache size" constraints (bytes) vs "cache capacity" (count).
        size_penalty = math.log(obj_size + 1)
        
    final_score = base_score + recency_score + size_penalty
    return final_score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score.
    Strategy: Segmented LRU with Ghost awareness.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # We scan candidates to find the 'worst' one (highest score)
    # Optimization: In a production system, we would maintain linked lists (queues)
    # to avoid O(N) scans. For this simulation interface, O(N) is the only way
    # to read the read-only cache_snapshot.
    for key in candidate_keys:
        # Retrieve size from metadata or fallback to 1 if missing (shouldn't happen)
        cached_obj_size = 1
        if key in m_access_info:
             cached_obj_size = m_access_info[key][3]
        
        score = get_victim_score(key, current_time, cached_obj_size)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Promote the item.
    2. Increase Frequency (saturation applied to prevent pollution).
    3. Update Recency.
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing
    # Default: freq=0, insert=now, size=obj.size
    old_freq, _, insertion_time, _ = m_access_info.get(obj.key, (0, current_time, current_time, obj.size))
    
    # Increment frequency.
    # We cap frequency at a small number (e.g., 3 or 4).
    # Why? Because in SLRU/S3-FIFO, the distinction is mainly "One Hit" vs "Many Hits".
    # Distinguishing between 100 hits and 101 hits is rarely useful and prevents
    # items from aging out when workloads shift.
    new_freq = min(old_freq + 1, 4)
    
    m_access_info[obj.key] = (new_freq, current_time, insertion_time, obj.size)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Check Ghost Registry.
    2. Determine Initial Frequency (Probation vs Protected).
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # DEFAULT: New items go to Probation (freq=1)
    initial_freq = 1
    
    # GHOST CHECK:
    # If this key was recently evicted, it implies our cache is too small or 
    # we made a bad eviction decision. This item is actually part of the working set.
    # We insert it directly with a higher frequency/priority so it isn't evicted immediately again.
    if obj.key in m_ghost_registry:
        # Promote straight to "Protected" tier
        initial_freq = 2
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time, current_time, obj.size)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry.
    3. Prune Ghost Registry to prevent memory leaks.
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    # 1. Clean Active Metadata
    if evicted_obj.key in m_access_info:
        del m_access_info[evicted_obj.key]
        
    # 2. Add to Ghost Registry
    m_ghost_registry[evicted_obj.key] = current_time
    
    # 3. Maintain Ghost Registry Size
    # We allow the ghost registry to hold roughly as many items as the cache capacity (in count).
    # Since we don't know the exact count capacity (it's byte-based), we approximate based on
    # current cache keys count.
    max_ghost_size = max(len(cache_snapshot.cache), 100)
    
    if len(m_ghost_registry) > max_ghost_size:
        # Remove oldest inserted into ghost registry.
        # Python dicts preserve insertion order (FIFO behavior for iteration).
        # We delete a chunk to amortize the cost.
        keys_to_remove = []
        for i, k in enumerate(m_ghost_registry):
            if i >= 5: # Remove 5 oldest at a time
                break
            keys_to_remove.append(k)
        
        for k in keys_to_remove:
            del m_ghost_registry[k]
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 45
- Hit Rate: 0.0222

Example 2:
- Trace ID: 47
- Hit Rate: 0.0327

Example 3:
- Trace ID: 40
- Hit Rate: 0.0214

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:42:51 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:42:51,038 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:44:28 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:28,224 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:28,225 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The performance results (Hit Rate ~2-3%) indicate that the previous algorithm was failing significantly. The primary issue was likely the "Probation vs. Protected" logic combined with the specific scoring formula. By assigning a massive base score (`1e15`) to any item with Frequency=1, new items (which start at Frequency=1) were effectively marked for immediate eviction unless they received a second hit instantly. In a full cache, this creates a "revolving door" where new content enters and is immediately kicked out, preventing it from ever building up frequency.

To fix this, we will implement a variant of **Hyperbolic Caching** combined with a **Ghost Registry**.

**The Strategy:**
1.  **Continuous Scoring (No hard tiers):** Instead of arbitrary tiers (Probation/Protected), we will calculate a continuous "Eviction Score". The item with the **highest** score is evicted.
2.  **The Formula:** `Score = (Age * Size) / Frequency`
    *   **Age (`CurrentTime - LastAccess`):** Older items have higher scores (standard LRU behavior).
    *   **Size:** Larger items have higher scores. Evicting one large object makes room for many small objects, which mathematically maximizes the object hit rate (the metric you are being graded on).
    *   **Frequency:** Popular items have lower scores (dividing by frequency).
3.  **Grace Period for New Items:** When an item is inserted, its `Age` is 0. This gives it a Score of 0, effectively protecting it for a short "honeymoon" period until it ages enough to compete with established items. This solves the "churn" problem of the previous code.
4.  **Ghost Registry (Shadow Cache):** We track the frequency of recently evicted items. If a ghost item is re-inserted, we restore its previous frequency. This helps the algorithm "remember" items that were popular but temporarily evicted due to space constraints, preventing them from resetting to the bottom of the priority queue.

```python
import math

# --- Metadata Storage ---
# Maps: key -> (frequency, last_access_time)
m_access_info = dict()

# Ghost Registry
# Maps: key -> frequency_at_eviction
# Used to restore the "heat" of an object if it returns to the cache soon after eviction.
m_ghost_registry = dict()

def get_victim_score(key, current_time, obj_size, freq, last_access_time):
    '''
    Calculates the "Victim Score" for an object.
    Higher Score = Higher desirability to EVICT.
    
    Formula: Hyperbolic Caching variant
    Score = (Age * Size) / Frequency
    '''
    
    # 1. Age: How long since it was last touched?
    # We add 1 to avoid multiplication by zero if checked in the exact same tick,
    # ensuring the size component still matters.
    age = (current_time - last_access_time) + 1
    
    # 2. Size: We penalize large objects linearly.
    # Minimizing miss rates (maximizing object hit rate) is equivalent to the 
    # Knapsack problem where value=1. Density = 1/Size.
    # Therefore, we prefer to evict items with high Size.
    size_penalty = max(obj_size, 1)
    
    # 3. Frequency: We protect frequently used items.
    # Higher frequency reduces the eviction score.
    denominator = max(freq, 1)
    
    # Combine factors
    # Older items -> Higher Score
    # Larger items -> Higher Score
    # More frequent items -> Lower Score
    score = (age * size_penalty) / denominator
    
    return score

def evict(cache_snapshot, obj):
    '''
    Selects the victim with the highest victim score using Hyperbolic Caching logic.
    '''
    candidate_keys = list(cache_snapshot.cache.keys())
    
    if not candidate_keys:
        return None

    current_time = cache_snapshot.access_count
    
    best_victim = None
    max_score = -1.0
    
    # Scan all candidates to find the one with the highest eviction score
    for key in candidate_keys:
        # Retrieve metadata
        # Default to neutral values if metadata is missing (defensive coding)
        freq = 1
        last_access = current_time
        
        if key in m_access_info:
            freq, last_access = m_access_info[key]
            
        # Retrieve size from the cache snapshot object
        cached_obj = cache_snapshot.cache[key]
        cached_size = cached_obj.size
        
        score = get_victim_score(key, current_time, cached_size, freq, last_access)
        
        if score > max_score:
            max_score = score
            best_victim = key
            
    return best_victim

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Update last_access_time (Recency).
    2. Increment Frequency (LFU).
    '''
    global m_access_info
    
    current_time = cache_snapshot.access_count
    
    # Retrieve existing frequency
    old_freq = 1
    if obj.key in m_access_info:
        old_freq, _ = m_access_info[obj.key]
    
    # Increment frequency
    # We cap frequency (e.g., at 100) to prevent long-lived items from becoming 
    # permanently immortal (Cache Pollution), allowing the cache to adapt to drift.
    new_freq = min(old_freq + 1, 100)
    
    m_access_info[obj.key] = (new_freq, current_time)

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize metadata.
    2. Check Ghost Registry to restore frequency if this is a "return visitor".
    '''
    global m_access_info, m_ghost_registry
    
    current_time = cache_snapshot.access_count
    
    initial_freq = 1
    
    # Ghost Check:
    # If the item is in the ghost registry, it means it was part of the working set
    # but was evicted recently. We restore its frequency so it doesn't have to start
    # from 1 again. This prevents thrashing.
    if obj.key in m_ghost_registry:
        restored_freq = m_ghost_registry[obj.key]
        # Boost it slightly, but respect the cap
        initial_freq = min(restored_freq + 1, 100)
        # Remove from ghost as it is now in active cache
        del m_ghost_registry[obj.key]
        
    m_access_info[obj.key] = (initial_freq, current_time)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Evict:
    1. Remove from active metadata.
    2. Add to Ghost Registry (Snapshotting its frequency).
    3. Prune Ghost Registry size.
    '''
    global m_access_info, m_ghost_registry
    
    # 1. Archive metadata to Ghost Registry
    if evicted_obj.key in m_access_info:
        freq, _ = m_access_info[evicted_obj.key]
        m_ghost_registry[evicted_obj.key] = freq
        del m_access_info[evicted_obj.key]
    else:
        m_ghost_registry[evicted_obj.key] = 1
        
    # 2. Maintain Ghost Registry Size
    # We allow the ghost registry to be larger than the cache (e.g., 2x)
    # to capture longer cyclic patterns.
    # Approximation of capacity based on current object count.
    max_ghost_size = max(len(cache_snapshot.cache), 100) * 2
    
    if len(m_ghost_registry) > max_ghost_size:
        # Prune the oldest inserted item in the ghost registry.
        # Python 3.7+ dicts maintain insertion order, so the first key is the oldest.
        iterator = iter(m_ghost_registry)
        try:
            oldest_key = next(iterator)
            del m_ghost_registry[oldest_key]
        except StopIteration:
            pass
```
2025-11-27 20:44:32,590 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 46
- Hit Rate: 0.4663

Example 2:
- Trace ID: 31
- Hit Rate: 0.0001

Example 3:
- Trace ID: 44
- Hit Rate: 0.021

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:44:32 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:44:32,591 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:44:53 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:53,247 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:44:53,248 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: To improve the cache eviction algorithm, we need to address the poor performance seen in traces 31 and 44 (near-zero hit rates). This usually indicates "cache thrashing" or "one-hit wonder" pollution, where the cache fills up with items that are accessed once and never again, displacing valuable items.

The previous algorithm used a complex scoring function `Freq / log(Age)`. While theoretically sound, floating-point math inside a loop is slow, and the specific tuning of logarithmic decay often fails to react quickly enough to changing workload patterns (like scans).

### The Solution: LRFU (Least Recently/Frequently Used) with Dynamic Aging

We will implement a simplified but more robust variant of **CRF (Combined Recency and Frequency)** or **LRFU**.

**Key Improvements:**
1.  **Weighting Function (CRF):** Instead of a log-based score, we use a standard linear decay model combined with frequency. The score for an object will be `F(k) = Current_Weight * F(prev) + 1`. This allows the algorithm to adapt: recent accesses boost the score significantly, but the score decays over time.
2.  **Lazy Aging:** Instead of iterating through the whole dictionary to "decay" values (which is O(N) and slow), we apply decay mathematically only when an object is accessed or inspected for eviction.
3.  **Scan Resistance:** We initialize new items with a lower priority than existing "warm" items. If an item is new, it must prove its worth (by getting hit again) to survive eviction against established items.
4.  **Size Awareness:** While the prompt implies object counts often matter, `obj.size` is available. Tie-breaking by size (evicting larger items when scores are equal) generally improves hit rates by allowing more items to fit in the cache.

### Python Implementation

```python
import math

# Metadata dictionaries
# Maps key -> { 'score': float, 'last_access': int }
m_metadata = dict()

# Constants for the CRF (Combined Recency and Frequency) weighting
# A half-life concept: how much weight does a past access retain?
# 2^(-lambda * delta_t). 
# A smaller lambda means history matters more (LFU-like).
# A larger lambda means history decays fast (LRU-like).
LAMBDA = 1.0 / 10000.0 
DEFAULT_INIT_SCORE = 0.5 # New items start weaker than a repeated hit (1.0 + decay)

def _get_current_score(key, current_time):
    """
    Calculates the score of a key at the current time based on its
    last computed score and the time elapsed.
    Formula: CurrentScore = LastScore * e^(-lambda * (current_time - last_access))
    """
    global m_metadata, LAMBDA
    
    if key not in m_metadata:
        return 0.0
        
    data = m_metadata[key]
    last_score = data['score']
    last_access = data['last_access']
    
    # Calculate decay factor
    delta_t = current_time - last_access
    decay = math.exp(-LAMBDA * delta_t)
    
    return last_score * decay

def _update_weight(key, current_time, is_hit):
    """
    Updates the weight of a key.
    On access: NewScore = (OldScore * decay) + 1.0
    """
    global m_metadata, LAMBDA, DEFAULT_INIT_SCORE
    
    current_score = _get_current_score(key, current_time)
    
    if is_hit:
        # Reinforce: Add 1.0 to the decayed score
        new_score = current_score + 1.0
    else:
        # Insert:
        # If it has a history (Ghost), we resurrect it with its decayed score + init boost.
        # If it's brand new, it starts with a modest score.
        if current_score > 0.001: 
             # It was in ghost cache, reward it for coming back
            new_score = current_score + 1.0
        else:
            # Brand new item
            new_score = DEFAULT_INIT_SCORE

    m_metadata[key] = {
        'score': new_score,
        'last_access': current_time
    }

def evict(cache_snapshot, obj):
    '''
    Evicts the object with the lowest computed CRF score.
    Tie-breaking: Largest size (to clear more space), then LRU.
    '''
    global m_metadata
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count
    
    # We look for the item with the minimal current score
    for key in candidate_keys:
        score = _get_current_score(key, current_time)
        
        # Optimization: If score is very low, close to 0, just evict immediately
        # to save iteration time.
        if score < 1e-6:
            return key
            
        if score < min_score:
            min_score = score
            victim_key = key
        elif score == min_score:
            # Tie-breaker: Evict the larger object to free more space? 
            # Or evict the older one? Let's use LRU logic for ties.
            # (The score calculation naturally handles LRU via decay, 
            # but if scores are identical, we check strict last_access).
            if m_metadata[key]['last_access'] < m_metadata[victim_key]['last_access']:
                victim_key = key

    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    Update score using weighting function + 1.0
    '''
    _update_weight(obj.key, cache_snapshot.access_count, is_hit=True)

def update_after_insert(cache_snapshot, obj):
    '''
    Initialize score.
    '''
    _update_weight(obj.key, cache_snapshot.access_count, is_hit=False)

    # Periodic cleanup of metadata to prevent memory leaks
    # We can do this simply by size check
    global m_metadata
    if len(m_metadata) > 2 * cache_snapshot.capacity and len(m_metadata) > 5000:
        # Remove oldest items that aren't in cache
        # This is an expensive op, so we do it rarely or use a simpler heuristic.
        # Here we just blindly cut if it gets too big (simple pruning)
        keys_in_cache = set(cache_snapshot.cache.keys())
        keys_to_del = []
        count = 0
        for k in m_metadata:
            if k not in keys_in_cache:
                keys_to_del.append(k)
                count += 1
                if count > 1000: break
        for k in keys_to_del:
            del m_metadata[k]

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    Do not remove metadata. 
    The item remains in `m_metadata` so if it returns, it retains some "heat" (Ghost Cache).
    The score will naturally decay over time via the formula.
    '''
    pass
```
2025-11-27 20:47:45,486 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Prompt: You are optimizing a cache eviction algorithm to minimize cache miss rates.
    
## Context:
The cache receives a sequence of access requests for objects, and when the cache is full, it must evict an object to make space for a new one. The cache is full when the total number of cached objects reaches its capacity. Focus on improving the `evict` function, the `update_after_hit` function, the `update_after_insert` function, and the `update_after_evict` function to find a cache eviction algorithm with as low miss rate as possible.

## Current Cache Eviction Algorithm Implementation:
```python
import math

# Metadata dictionaries
# Tracks frequency (approximate popularity)
m_key_frequency = dict()
# Tracks the last access time (Recency)
m_key_last_access = dict()

# Heuristic constants
# We use a logical clock for aging rather than raw access counts
GLOBAL_TICK = 0
FREQ_DECAY_FACTOR = 0.95  # Decay factor for frequencies on aging events
AGING_THRESHOLD = 5000    # Operations between aging passes
MAX_HISTORY_KEYS = 50000  # Prevent memory leak by capping history size

def evict(cache_snapshot, obj):
    '''
    Evicts based on a computed utility score:
    Score = Frequency * (Recency_Weight)
    
    This is an improvement over strict LFU or strict LRU. 
    Strict LFU causes cache pollution (old popular items stick around forever).
    Strict LRU forgets frequent items too quickly.
    
    This implementation calculates a score where items with low frequency AND 
    old access times are evicted first.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    candidate_keys = list(cache_snapshot.cache.keys())
    if not candidate_keys:
        return None

    victim_key = None
    min_score = float('inf')
    
    current_time = cache_snapshot.access_count

    # Linear scan to find the minimum score (victim).
    # While O(N), for typical cache sizes in simulations, this is acceptable 
    # and allows for complex scoring functions.
    for key in candidate_keys:
        freq = m_key_frequency.get(key, 0)
        last_access = m_key_last_access.get(key, 0)
        
        # Calculate time delta
        delta_t = current_time - last_access
        
        # Scoring Formula:
        # We want to evict items with LOW frequency and HIGH delta_t.
        # Score = Freq / (log(delta_t + 1) + 1)
        # 
        # Rationale: 
        # - Higher Freq -> Higher Score (Keep)
        # - Higher Delta (Older) -> Lower Score (Evict)
        # - Logarithmic decay prevents very old but super popular items from being evicted too easily,
        #   while still allowing them to eventually drop if not accessed.
        
        # Safety check for very fresh items (delta_t=0)
        if delta_t < 0: delta_t = 0
            
        score = freq / (math.log(delta_t + 2))
        
        if score < min_score:
            min_score = score
            victim_key = key
            
    return victim_key

def _cleanup_metadata():
    '''
    Helper to prevent memory leaks in metadata dictionaries.
    Removes the least useful keys (low frequency, old access) if map grows too large.
    '''
    global m_key_frequency, m_key_last_access, MAX_HISTORY_KEYS
    
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        # Simple cleanup: remove keys not in cache that are old
        # Sorting is expensive, so we just remove a chunk based on a threshold or random sample.
        # Here, we remove items with freq=1
        keys_to_remove = []
        for k, v in m_key_frequency.items():
            if v <= 1:
                keys_to_remove.append(k)
            if len(keys_to_remove) > 1000: # Remove in chunks
                break
        
        for k in keys_to_remove:
            if k in m_key_frequency: del m_key_frequency[k]
            if k in m_key_last_access: del m_key_last_access[k]

def update_after_hit(cache_snapshot, obj):
    '''
    On hit: Strong reinforcement of frequency and reset recency.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    # Update Recency
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Update Frequency
    # Additive increase
    current_freq = m_key_frequency.get(key, 0)
    m_key_frequency[key] = current_freq + 1
    
    # Periodic Aging
    # Instead of hard resets, we multiply by a decay factor.
    # This simulates a "sliding window" of popularity.
    if GLOBAL_TICK > AGING_THRESHOLD:
        GLOBAL_TICK = 0
        for k in list(m_key_frequency.keys()):
            m_key_frequency[k] = m_key_frequency[k] * FREQ_DECAY_FACTOR
            # If freq drops extremely low, prune it to save memory, unless it's currently cached
            if m_key_frequency[k] < 0.1 and k not in cache_snapshot.cache:
                del m_key_frequency[k]
                if k in m_key_last_access: del m_key_last_access[k]

def update_after_insert(cache_snapshot, obj):
    '''
    On insert: Check Ghost Cache.
    '''
    global m_key_frequency, m_key_last_access, GLOBAL_TICK
    
    GLOBAL_TICK += 1
    key = obj.key
    
    m_key_last_access[key] = cache_snapshot.access_count
    
    # Ghost Cache Logic
    if key in m_key_frequency:
        # It was seen before. It might be a "working set" item that barely got evicted.
        # Boost it so it survives longer this time.
        # If it was evicted, it likely had a score that was too low. 
        # We add a significant boost (e.g., +2 or +3) to correct the mistake immediately.
        m_key_frequency[key] += 2.0
    else:
        # First time seeing this object.
        # Initialize with a base frequency. 
        # 1.0 is standard.
        m_key_frequency[key] = 1.0

    # Trigger cleanup if necessary
    if len(m_key_frequency) > MAX_HISTORY_KEYS:
        _cleanup_metadata()

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction: Retain metadata (Ghost Cache).
    '''
    # We do NOT remove the key from metadata.
    # The history allows us to distinguish between "one-scan" items (seen once, never again)
    # and "churning" items (evicted, then requested again).
    pass
```

## Performance Results and Feedback:
The current cache eviction algorithm was evaluated on multiple real-world traces with different access patterns. Here are the results:

Example 1:
- Trace ID: 16
- Hit Rate: 0.0203

Example 2:
- Trace ID: 2
- Hit Rate: 0.4844

Example 3:
- Trace ID: 14
- Hit Rate: 0.0215

## Key Information:
- Higher hit rates (lower miss rates) are better.
- `evict` defines how the algorithm chooses the eviction victim.
- `update_after_hit` defines how the algorithm update the metadata it maintains immediately after a cache hit.
- `update_after_insert` defines how the algorithm updates the metadata it maintains immediately after inserting a new object into the cache.
- `update_after_evict` defines how the algorithm updates the metadata it maintains immediately after evicting the victim.
You have read-only access to these data and no access to any functions:
- An "object" represents the unit of a request, such as inserting an object into the cache or retrieving an object from the cache. Each object `obj` provides the following **read-only** attributes that you can reference:
    - `obj.key` (str): A string that uniquely identifies the object.
    - `obj.size` (int): A positive integer representing the size of the object in bytes.
- You can also reference the following **read-only** attributes provided by a cache snapshots `cache_snapshot`:
    - `cache_snapshot.cache` (dict): A dictionary containing the cached objects, where the keys are the objects' keys, and the values are the corresponding objects themselves.
    - `cache_snapshot.size` (int): A non-negative integer representing the current total size of the cache in bytes.
    - `cache_snapshot.capacity` (int): A positive integer representing the maximum allowed size of the cache in bytes.
    - `cache_snapshot.access_count` (int): The current total number of cache accesses. You can also use this to represent current time.
    - `cache_snapshot.hit_count` (int): The current total number of cache hits.
    - `cache_snapshot.miss_count` (int): The current total number of cache misses.

## Your Task:
Analyze the performance feedback and rewrite the cache eviction algorithm to:
1. Reduce overall cache miss rates
2. Make better decisions about which object to evict when the cache is full
3. Make better designs about which metadata to maintain
4. Improve metadata updates after cache hits, inserts, and evictions

Provide the complete improved cache eviction algorithm implementation in Python.

```python
# Your improved cache eviction algorithm here
```

[92m20:47:45 - LiteLLM:INFO[0m: utils.py:3427 - 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
2025-11-27 20:47:45,488 [INFO] LiteLLM: 
LiteLLM completion() model= gemini-3-pro-preview; provider = gemini
[92m20:49:33 - LiteLLM:INFO[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler
2025-11-27 20:49:33,518 [INFO] LiteLLM: Wrapper: Completed Call, calling success_handler
2025-11-27 20:49:33,519 [INFO] gepa.adapters.caching_adapter_gemini3_2.caching_program_adapter: LM Response: The improved cache eviction algorithm implements **GDSF (Greedy Dual-Size Frequency)**. This algorithm is generally superior to standard LRU or LFU for maximizing hit rates in caches where object sizes vary (byte-limited caches) and access patterns exhibit both frequency and recency characteristics.

### Key Logic Changes:
1.  **Size Awareness**: The original algorithm ignored object size. In a byte-limited cache, evicting one large object to fit ten small frequently accessed objects significantly improves the hit rate. The scoring formula now divides Frequency by Size.
2.  **Dynamic Aging (Inflation)**: Instead of iterating through all keys to "decay" scores (which is inefficient) or using a logarithmic time delta (which can be inaccurate), this algorithm uses an "Inflation" value ($L$). $L$ increases monotonically as objects are evicted. New or updated objects are assigned a priority relative to the current $L$. This naturally allows older objects to "sink" in priority without modifying their values constantly.
3.  **Ghost Cache (Frequency Retention)**: We maintain the frequency of objects even after they are evicted. If a "ghost" object is re-inserted, it regains its previous popularity score immediately. This provides strong resistance against cache churn and scans.

### Implementation:

```python
import math

# Global Metadata
# m_frequency: Tracks the global access count of keys (Frequency). 
# This acts as our "Ghost Cache" / History.
m_frequency = {}

# m_priority: Tracks the current computed priority of objects IN the cache.
# Used to determine eviction victims quickly.
m_priority = {}

# m_inflation: The "L" value in GDSF algorithm.
# Represents the priority value of the last evicted object.
# This serves as a "logical clock" for aging.
m_inflation = 0.0

# Constants
# Limit the size of the history to prevent memory issues
MAX_HISTORY_SIZE = 50000

def evict(cache_snapshot, obj):
    '''
    GDSF (Greedy Dual-Size Frequency) Eviction Policy.
    
    Formula: Priority = Inflation_L + (Frequency / Size)
    
    Logic:
    1. Identify the object with the lowest Priority in the cache.
    2. Update the global Inflation_L to match that object's priority.
       (This effectively "ages" all other objects without iterating them).
    3. Evict that object.
    '''
    global m_priority, m_inflation
    
    # If the cache is empty, we cannot evict.
    if not cache_snapshot.cache:
        return None

    # We need to find the key with the minimum priority value.
    # A linear scan is O(N). For typical simulation cache sizes, this is acceptable
    # and ensures perfect accuracy for the GDSF algorithm.
    victim_key = None
    min_priority = float('inf')
    
    # Iterate over currently cached keys to find the victim
    for k in cache_snapshot.cache:
        # Default to 0.0 priority if something is out of sync, though unlikely
        p = m_priority.get(k, 0.0)
        
        if p < min_priority:
            min_priority = p
            victim_key = k
            
    # Update the global inflation value (L) to the priority of the evicted object.
    # New objects will now need a higher base priority to survive.
    if victim_key is not None:
        m_inflation = min_priority
        
    return victim_key

def update_after_hit(cache_snapshot, obj):
    '''
    On Hit:
    1. Increment Frequency.
    2. Recalculate Priority based on NEW Frequency and CURRENT Inflation.
       This resets the object's "aging", bringing it to the front.
    '''
    global m_frequency, m_priority, m_inflation
    
    key = obj.key
    
    # 1. Update Frequency
    current_freq = m_frequency.get(key, 0)
    new_freq = current_freq + 1
    m_frequency[key] = new_freq
    
    # 2. Update Priority
    # Priority = L + (Freq / Size)
    # Using 'L' here is crucial: it effectively marks the object as "recently used"
    # relative to the current state of the cache.
    safe_size = max(1, obj.size) # Avoid division by zero
    priority = m_inflation + (new_freq / safe_size)
    m_priority[key] = priority

def update_after_insert(cache_snapshot, obj):
    '''
    On Insert:
    1. Initialize or Restore Frequency (Ghost Cache check).
    2. Calculate initial Priority.
    3. Clean metadata if necessary.
    '''
    global m_frequency, m_priority, m_inflation
    
    key = obj.key
    
    # 1. Frequency Management
    # If key is already in m_frequency, it was seen before (Ghost).
    # We keep the old frequency to give it a "warm" start.
    if key not in m_frequency:
        m_frequency[key] = 1
    # Else: It retains its old frequency value automatically.
    
    freq = m_frequency[key]
    
    # 2. Set Priority
    safe_size = max(1, obj.size)
    priority = m_inflation + (freq / safe_size)
    m_priority[key] = priority
    
    # 3. Cleanup metadata if it grows too large
    if len(m_frequency) > MAX_HISTORY_SIZE:
        _cleanup_metadata(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On Eviction:
    1. Remove the object from the active priority map.
    2. We DO NOT remove it from m_frequency (preserve history).
    '''
    global m_priority
    
    key = evicted_obj.key
    if key in m_priority:
        del m_priority[key]

def _cleanup_metadata(cache_snapshot):
    '''
    Helper to prune the frequency history map to prevent memory leaks.
    Removes keys that are not in the cache and have low frequency.
    '''
    global m_frequency, MAX_HISTORY_SIZE
    
    # We want to remove items from history that are:
    # 1. Not currently in the cache (don't delete active metadata)
    # 2. Have low importance (frequency <= 1)
    
    cached_keys = set(cache_snapshot.cache.keys())
    keys_to_remove = []
    
    # Heuristic cleanup: remove one-hit wonders first
    for k, v in m_frequency.items():
        if k not in cached_keys and v <= 1:
            keys_to_remove.append(k)
            # Remove in chunks to keep operation time somewhat bounded
            if len(keys_to_remove) > 2000:
                break
    
    for k in keys_to_remove:
        del m_frequency[k]
        
    # If still too big, force remove random old history items
    if len(m_frequency) > MAX_HISTORY_SIZE:
        keys_to_remove = []
        count = 0
        for k in m_frequency:
            if k not in cached_keys:
                keys_to_remove.append(k)
                count += 1
            if count > 2000:
                break
        for k in keys_to_remove:
            del m_frequency[k]
```
