GEPA artifacts will be saved to: /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/gepa_results
Dataset sizes -> train/val: 48 samples
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpky5knjdq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv_t_eem6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp85a00w3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_ueeaky9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq6njsxrj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5vg3y_63.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpshnwxf37.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq0pfd97i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpntdaye05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu32rxegh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp87j73k1_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxk4ohfcp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkh3key3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp36lfrm58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfv3098gd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpezr6k162.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwalb9lbm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5r6cb3aq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo2g5mkai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpavcf_d_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg_gb39wc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppianh5a9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmxo563lg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7yy1pf1q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmporjg_kcb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpecigsj5a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwnlrcw7_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiilln6lr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdfxcvo8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps2dm4_t3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpedl0kjav.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsinfdp29.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv3rizs1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaxthkwwk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9cnyohut.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5ohpwcm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpozlg0w1a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt_unqj71.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo7id53ow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp85te7erp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn736vc18.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8zibdbai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwv7p378t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp479jxc7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5ktv67zn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnsmtk7mn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_h18_qv4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpad484m4m.pickle

Iteration 0: Base program full valset score: 0.2066171041666667
Iteration 1: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7yg68xn2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7unhxwhj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiidl7l51.pickle

Iteration 1: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with aging and lightweight recency tie-break
# - Size-aware and frequency-aware
# - Global aging via L ensures old items lose priority over time
# - On each hit, items accumulate priority proportional to 1/size

# Global metadata
m_key_H = dict()           # key -> priority H
m_key_freq = dict()        # key -> observed frequency
m_key_last_access = dict() # key -> last access timestamp
m_L = 0.0                  # global aging value (last evicted H)

def _unit_value_per_size(obj):
    # Basic "value" per access; here 1 access unit, normalized by size
    # Large objects get smaller increments; small objects get larger increments.
    return 1.0 / float(max(1, obj.size))

def evict(cache_snapshot, obj):
    '''
    Choose the victim with the smallest GDSF priority H.
    Tie-breakers:
      - older last access first
      - if still tied, evict the larger object (frees more space sooner)
    '''
    global m_key_H, m_key_last_access, m_L

    # Defensive defaults if metadata missing
    def key_priority(k):
        H = m_key_H.get(k, m_L)
        last = m_key_last_access.get(k, 0)
        sz = cache_snapshot.cache[k].size
        # We minimize by (H, last_access, -size)
        return (H, last, -sz)

    # Select the key with minimum tuple; O(n) scan over current cache contents
    candid_obj_key = min(cache_snapshot.cache.keys(), key=key_priority)
    return candid_obj_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
      - Increment frequency
      - Increase priority by value/size (GDSF-F additive update)
      - Update last access time
    '''
    global m_key_H, m_key_freq, m_key_last_access, m_L

    inc = _unit_value_per_size(obj)
    # If missing (shouldn't be), initialize conservatively
    if obj.key not in m_key_H:
        # Treat as if it had been inserted earlier with baseline L
        m_key_H[obj.key] = m_L + inc
        m_key_freq[obj.key] = 1
    else:
        m_key_H[obj.key] += inc
        m_key_freq[obj.key] = m_key_freq.get(obj.key, 0) + 1

    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert (after a miss):
      - Initialize frequency to 1 (for this access that triggered the insert)
      - Initialize priority to L + value/size
      - Record last access time
    '''
    global m_key_H, m_key_freq, m_key_last_access, m_L

    base = _unit_value_per_size(obj)
    m_key_H[obj.key] = m_L + base
    m_key_freq[obj.key] = 1
    m_key_last_access[obj.key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    On eviction:
      - Update global aging L to the evicted object's H
      - Remove all metadata for the evicted object
    '''
    global m_key_H, m_key_freq, m_key_last_access, m_L

    # Get the priority of the evicted item; default to current L if missing
    H_evicted = m_key_H.pop(evicted_obj.key, m_L)
    m_key_freq.pop(evicted_obj.key, None)
    m_key_last_access.pop(evicted_obj.key, None)

    # Age the system: future insertions/hits get a baseline above this
    if H_evicted > m_L:
        m_L = H_evicted
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptg7i163q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpptpsxuz2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppfzaz99j.pickle

Iteration 1: New subsample score 0.485896 is not better than old score 0.48805599999999993, skipping
Iteration 2: Selected program 0 score: 0.2066171041666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx8rq9w4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn9iyi_e4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu7e0m92c.pickle

Iteration 2: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) cache eviction policy
# - Size-aware: favors keeping smaller objects (higher value per byte).
# - Frequency-aware: promotes frequently accessed objects.
# - Recency-aware via global aging (L): older priorities effectively decay over time.

# Metadata stores
m_priority = dict()      # key -> float priority (H)
m_freq = dict()          # key -> int frequency
m_last_access = dict()   # key -> last access time (for tie-breaking)
m_age_L = 0.0            # global aging factor (monotonically non-decreasing)

def _priority_for(key, obj=None):
    """
    Helper to safely fetch an object's priority.
    If metadata is missing, fall back to a conservative default (age baseline).
    """
    if key in m_priority:
        return m_priority[key]
    # Default priority if metadata missing: at least the aging baseline.
    # We avoid changing state here to keep evict side-effect free.
    # If 'obj' is provided, we could use L + 1/size, but we keep just L for safety.
    return m_age_L

def evict(cache_snapshot, obj):
    '''
    Choose the eviction victim using GDSF:
    - Evict the object with the smallest priority H.
    - Tie-break by:
        1) Oldest last access (LRU among equals)
        2) Larger size (protect small objects)
        3) Lexicographical key to ensure deterministic choice
    '''
    if not cache_snapshot.cache:
        return None

    candid_key = None
    candid_pri = None
    candid_last = None
    candid_size = None

    for k, v in cache_snapshot.cache.items():
        pri = _priority_for(k, v)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if candid_key is None:
            candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
            continue

        # Primary: lower priority first
        if pri < candid_pri:
            candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
            continue

        if pri == candid_pri:
            # Secondary: older last access first (smaller last_access)
            if la < candid_last:
                candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
                continue
            if la == candid_last:
                # Tertiary: evict larger object to protect cache space for many small items
                if sz > candid_size:
                    candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
                    continue
                if sz == candid_size:
                    # Deterministic final tie-breaker
                    if k < candid_key:
                        candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz

    return candid_key

def update_after_hit(cache_snapshot, obj):
    '''
    On hit:
    - Increment frequency.
    - Increase priority by 1/size (GDSF update).
    - Update last access time.
    '''
    key = obj.key
    sz = max(1, int(obj.size))

    # Initialize if missing (robustness)
    if key not in m_freq:
        m_freq[key] = 0
    if key not in m_priority:
        # If missing, start from the current baseline to avoid unfair advantage
        m_priority[key] = m_age_L

    m_freq[key] += 1
    # GDSF increment: add normalized benefit of this hit
    m_priority[key] += 1.0 / float(sz)
    m_last_access[key] = cache_snapshot.access_count

def update_after_insert(cache_snapshot, obj):
    '''
    On insert:
    - Initialize frequency to 1 (first reference).
    - Set priority: H = L + 1/size
      This gives small objects higher initial priority and ties them to current aging baseline.
    - Record last access time.
    '''
    key = obj.key
    sz = max(1, int(obj.size))

    # Initialize metadata
    m_freq[key] = 1
    m_priority[key] = m_age_L + 1.0 / float(sz)
    m_last_access[key] = cache_snapshot.access_count

def update_after_evict(cache_snapshot, obj, evicted_obj):
    '''
    After eviction:
    - Raise the global aging baseline L to the evicted object's priority.
      This effectively "ages" all remaining items relative to that threshold.
    - Remove evicted object's metadata.
    '''
    global m_age_L
    ekey = evicted_obj.key

    # Fetch evicted priority safely
    evicted_priority = m_priority.get(ekey, m_age_L)
    # Aging step: L becomes the priority of the evicted item (monotonic non-decreasing)
    if evicted_priority > m_age_L:
        m_age_L = evicted_priority

    # Clean metadata
    if ekey in m_priority:
        m_priority.pop(ekey, None)
    if ekey in m_freq:
        m_freq.pop(ekey, None)
    if ekey in m_last_access:
        m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiazqu851.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbuw07j1j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf7qmrojk.pickle

Iteration 2: New subsample score 0.05231 is better than old score 0.051461. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr635uvzk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1phrigam.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9nud12ze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpul3uyrp_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpza8jimvf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu89vklej.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgq_7q4nt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbj_nxieo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqytzwdrn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdj9mpbin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsexk4rum.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphzasign0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5d66ofkm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0z9252_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr6bt4ejf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdao_o5z_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt_ap8fxa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpplvuzdcm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi_h8g_kt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv75lbsss.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6qokl_r3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc2nbiusx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe1q_1nl2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps8d5mk9o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnbs05eab.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuznsuccx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe0e5vowl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6guetzvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5xsoj26c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0rhv8f54.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8ob2bcnb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp15dzggh2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0r7d3ljd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsk8b97cg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpebemv8b4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8sg3ar2v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbckrpudg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp4dfg_d6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsxk8o_2p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptqtkoxbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfr3tqf5e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptt9hbg7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvosiase4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd70g92mf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvcp5uzm8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmmxe9hy5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps6k_53ez.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8dzmqx1r.pickle

Iteration 2: New program is on the linear pareto front
Iteration 2: Full valset score for new program: 0.2117635416666667
Iteration 2: Full train_val score for new program: 0.2117635416666667
Iteration 2: Individual valset scores for new program: [0.450764, 0.41583, 0.431307, 0.38677, 0.442798, 0.43843, 0.261962, 0.498624, 0.535216, 0.531017, 0.066667, 0.296625, 0.023893, 0.0, 0.019255, 0.01916, 0.018581, 0.022069, 0.021235, 0.266728, 0.333333, 0.025282, 0.057382, 0.057382, 0.269778, 0.24496, 0.757696, 0.873524, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.746256, 0.072368, 0.062665, 0.009162, 0.611025, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.05, 0.020965, 0.021879, 0.441718, 0.03268]
Iteration 2: New valset pareto front scores: [0.450764, 0.41799, 0.431307, 0.387303, 0.442798, 0.43843, 0.261962, 0.498624, 0.535216, 0.531017, 0.066667, 0.296625, 0.023893, 0.0, 0.019255, 0.01916, 0.018581, 0.022069, 0.021235, 0.266728, 0.333333, 0.025282, 0.057382, 0.057382, 0.269778, 0.25, 0.757696, 0.873524, 0.020066, 0.036364, 0.038724, 9.6e-05, 3.6e-05, 0.746256, 0.072368, 0.062665, 0.009162, 0.611025, 0.125461, 0.021902, 0.02077, 0.023482, 0.042763, 0.05, 0.020965, 0.021879, 0.441718, 0.03268]
Iteration 2: Full valset pareto front score: 0.21192464583333337
Iteration 2: Updated valset pareto front programs: [{1}, {0}, {1}, {0}, {1}, {1}, {1}, {0, 1}, {1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {1}, {0, 1}, {0, 1}, {1}, {0}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {1}, {1}, {0, 1}, {1}, {1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {0, 1}, {1}, {0, 1}]
Iteration 2: Best valset aggregate score so far: 0.2117635416666667
Iteration 2: Best program as per aggregate score on train_val: 1
Iteration 2: Best program as per aggregate score on valset: 1
Iteration 2: Best score on valset: 0.2117635416666667
Iteration 2: Best score on train_val: 0.2117635416666667
Iteration 2: Linear pareto front program index: 1
Iteration 2: New program candidate index: 1
Iteration 3: Selected program 1 score: 0.2117635416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqreuphxx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz5hqrlxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiuolapum.pickle

Iteration 3: Proposed new text for program: # Hybrid Size-aware Decayed-Frequency with Segmented Recency (SLRU-DF)
# - Frequency with exponential time decay (LRFU-style): rewards repeated hits, forgets stale.
# - Size-aware: frequency gains are normalized by object size (per byte value).
# - Segmented recency (probation/protected SLRU):
#     * New items enter probation.
#     * First hit promotes to protected.
#     * Evictions prefer probation; protected only evicted when needed.
# - Adaptive protected share: dynamically adjusts how much of the cache should be protected,
#   reacting to access/eviction patterns to balance between recency (probation) and frequency (protected).

# Metadata
m_score = dict()        # key -> float, decayed frequency score (at last_access timestamp)
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> 0 probation, 1 protected

# Global controls
m_num_protected = 0     # current count of protected items resident in cache
m_p_target = 0.5        # target fraction of cache to be protected (adaptive)

# Tunables (conservative defaults)
NEW_ITEM_BOOST = 0.5    # initial admission score multiplier (per-size). <1 prevents single-touch pollution
HALFLIFE_MIN = 512      # minimum half-life in accesses
HALFLIFE_PER_ITEM = 8   # additional half-life per resident item (scales decay with cache size)
ETA_UP = 0.02           # step to increase protected target on probation hits/promotions
ETA_DOWN = 0.01         # step to decrease protected target on protected hits
ETA_EVICT_PROBATION = 0.01  # adjust target when evicting from probation
ETA_EVICT_PROTECTED = 0.01  # adjust target when evicting from protected


def _half_life(cache_snapshot):
    """
    Compute a dynamic half-life (in accesses) used for exponential decay.
    Larger caches decay more slowly to keep learned frequency longer.
    """
    n = max(1, len(cache_snapshot.cache))
    return max(HALFLIFE_MIN, HALFLIFE_PER_ITEM * n)


def _effective_score(key, obj, now, half_life):
    """
    Lazily decay a key's score to 'now' without mutating metadata.
    score_now = stored_score * 0.5 ** ((now - last_access) / half_life)
    """
    base = m_score.get(key, 0.0)
    last = m_last_access.get(key, now)
    dt = now - last
    if dt <= 0:
        return base
    # Exponential decay with given half-life
    decay = pow(0.5, float(dt) / float(half_life))
    return base * decay


def _enforce_protected_budget(cache_snapshot):
    """
    Ensure the number of protected items does not exceed the adaptive target.
    If over budget, demote the weakest protected items back to probation.
    """
    global m_num_protected
    total = max(0, len(cache_snapshot.cache))
    if total == 0:
        m_num_protected = 0
        return

    target = int(m_p_target * total)

    # Quick sanity clamp
    if m_num_protected < 0 or m_num_protected > total:
        # Recount to recover from any inconsistencies
        m_num_protected = sum(1 for k in cache_snapshot.cache.keys() if m_seg.get(k, 0) == 1)

    # Demote weakest protected items if above target
    while m_num_protected > target and m_num_protected > 0:
        now = cache_snapshot.access_count
        hl = _half_life(cache_snapshot)

        weakest_key = None
        weakest_score = None
        weakest_last = None
        weakest_size = None

        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k, 0) != 1:
                continue
            eff = _effective_score(k, v, now, hl)
            la = m_last_access.get(k, -1)
            sz = getattr(v, "size", 1)

            if weakest_key is None:
                weakest_key, weakest_score, weakest_last, weakest_size = k, eff, la, sz
                continue

            if eff < weakest_score:
                weakest_key, weakest_score, weakest_last, weakest_size = k, eff, la, sz
                continue

            if eff == weakest_score:
                # Older first
                if la < weakest_last:
                    weakest_key, weakest_score, weakest_last, weakest_size = k, eff, la, sz
                    continue
                if la == weakest_last:
                    # Larger first
                    if sz > weakest_size:
                        weakest_key, weakest_score, weakest_last, weakest_size = k, eff, la, sz
                        continue
                    if sz == weakest_size:
                        if k < weakest_key:
                            weakest_key, weakest_score, weakest_last, weakest_size = k, eff, la, sz

        if weakest_key is None:
            # No protected keys found; fix counter and exit
            m_num_protected = 0
            break

        # Demote
        m_seg[weakest_key] = 0
        m_num_protected -= 1


def evict(cache_snapshot, obj):
    """
    Evict victim with the smallest effective score, preferring probation segment.
    Tie-breakers:
      1) older last access (LRU among equals)
      2) larger size (free more space and protect small objs)
      3) lexicographical key (determinism)
    """
    if not cache_snapshot.cache:
        return None

    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    # First, try to evict from probation
    cand_key = None
    cand_score = None
    cand_last = None
    cand_size = None

    found_probation = False
    for k, v in cache_snapshot.cache.items():
        seg = m_seg.get(k, 0)
        if seg != 0:
            continue
        found_probation = True

        eff = _effective_score(k, v, now, hl)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if cand_key is None:
            cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
            continue

        if eff < cand_score:
            cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
            continue

        if eff == cand_score:
            if la < cand_last:
                cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
                continue
            if la == cand_last:
                if sz > cand_size:
                    cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
                    continue
                if sz == cand_size and k < cand_key:
                    cand_key, cand_score, cand_last, cand_size = k, eff, la, sz

    if found_probation:
        return cand_key

    # If no probation items (all protected), evict weakest protected
    for k, v in cache_snapshot.cache.items():
        seg = m_seg.get(k, 0)
        if seg != 1:
            continue

        eff = _effective_score(k, v, now, hl)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if cand_key is None:
            cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
            continue

        if eff < cand_score:
            cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
            continue

        if eff == cand_score:
            if la < cand_last:
                cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
                continue
            if la == cand_last:
                if sz > cand_size:
                    cand_key, cand_score, cand_last, cand_size = k, eff, la, sz
                    continue
                if sz == cand_size and k < cand_key:
                    cand_key, cand_score, cand_last, cand_size = k, eff, la, sz

    return cand_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Lazily decay and then add per-size increment to score.
    - Update last access timestamp.
    - Promote probation -> protected on first hit.
    - Adapt protected target p based on where hits occur.
    - Optionally demote weakest protected if above budget.
    """
    global m_num_protected, m_p_target

    key = obj.key
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)
    sz = max(1, int(getattr(obj, "size", 1)))

    # Decay then add normalized gain
    base = _effective_score(key, obj, now, hl)
    m_score[key] = base + 1.0 / float(sz)
    m_last_access[key] = now

    seg = m_seg.get(key, 0)
    if seg == 0:
        # Promote to protected on first hit
        m_seg[key] = 1
        m_num_protected += 1
        # More probation hits suggest more long-lived items; increase protected share
        m_p_target = min(0.9, m_p_target + ETA_UP)
        # If we exceed budget, demote weakest protected
        _enforce_protected_budget(cache_snapshot)
    else:
        # Many protected hits -> we may give more room to probation to adapt to shifts
        m_p_target = max(0.1, m_p_target - ETA_DOWN)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - New items start in probation with small per-size admission score.
    - Initialize last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count
    sz = max(1, int(getattr(obj, "size", 1)))

    # Initialize only if missing to avoid overwriting on reinsert anomalies
    m_last_access[key] = now
    # Start with a dampened score to reduce cache pollution by one-timers
    m_score[key] = NEW_ITEM_BOOST / float(sz)
    m_seg[key] = 0  # probation (will be promoted on first hit)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Adjust adaptive protected share based on which segment was evicted.
    - Clean metadata of evicted object.
    - Enforce protected budget (may demote weakest protected if needed).
    """
    global m_num_protected, m_p_target

    ekey = evicted_obj.key
    evicted_seg = m_seg.get(ekey, 0)

    # Adjust target: frequent probation evictions -> give more room to probation (shrink protected)
    if evicted_seg == 0:
        m_p_target = max(0.1, m_p_target - ETA_EVICT_PROBATION)
    else:
        m_p_target = min(0.9, m_p_target + ETA_EVICT_PROTECTED)

    # Update protected count if needed
    if evicted_seg == 1:
        m_num_protected = max(0, m_num_protected - 1)

    # Clean metadata
    m_score.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)

    # Enforce budget (in case target shrank)
    _enforce_protected_budget(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp600o_b_t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzaxmit0k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps6dtvxtf.pickle

Iteration 3: New subsample score 1.439461 is better than old score 1.356968. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6k95xj3g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8m2ax8pr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr3zw2g2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptem0wd18.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa_2r2cy9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2zu3ssms.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsmu_3ev_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu55nyb28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppapy_rue.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvf0byj4b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdm53svmr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp0f0iy0v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6j28d6qt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0bseyies.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0vyx14j2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpajw9_is5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnejpvx6p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplptr2bbn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3vqforfx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp57u50opz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2if3sm74.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnp4dirbp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv0wt8zf2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppk4ir0vi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcsmbf1lg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppzr3oel2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuybo8y22.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwi7gc6s7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjnf_90z0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5nvzc38i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfkd2w9rg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsc5mmh8f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3zbct9ke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0vmd3rnt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7vsipoj4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppt1lnmvy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwhcfdc1d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7e5k0w67.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpww5dxu4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0mnu7qy9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz01h1tem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6rntofdg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqtw0crlw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmputv_v10e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7mlq_i7e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpckz4lm7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9un0i_dl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwtyek047.pickle

Iteration 3: New program is on the linear pareto front
Iteration 3: Full valset score for new program: 0.22998966666666665
Iteration 3: Full train_val score for new program: 0.22998966666666665
Iteration 3: Individual valset scores for new program: [0.470889, 0.446533, 0.457003, 0.397084, 0.472857, 0.459472, 0.271531, 0.462053, 0.536646, 0.531017, 0.075, 0.340142, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.325915, 0.25504, 0.703078, 0.886873, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.754084, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 3: New valset pareto front scores: [0.470889, 0.446533, 0.457003, 0.397084, 0.472857, 0.459472, 0.271531, 0.498624, 0.536646, 0.531017, 0.075, 0.340142, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.325915, 0.25504, 0.757696, 0.886873, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.754084, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 3: Full valset pareto front score: 0.2318894375
Iteration 3: Updated valset pareto front programs: [{2}, {2}, {2}, {2}, {2}, {2}, {2}, {0, 1}, {2}, {0, 1, 2}, {2}, {2}, {2}, {0, 1, 2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {1}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {1, 2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {2}]
Iteration 3: Best valset aggregate score so far: 0.22998966666666665
Iteration 3: Best program as per aggregate score on train_val: 2
Iteration 3: Best program as per aggregate score on valset: 2
Iteration 3: Best score on valset: 0.22998966666666665
Iteration 3: Best score on train_val: 0.22998966666666665
Iteration 3: Linear pareto front program index: 2
Iteration 3: New program candidate index: 2
Iteration 4: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa0vmzzr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9pn2gov7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpejmpg9yp.pickle

Iteration 4: Proposed new text for program: # GreedyDual-Size with TinyLFU-style initialization (GDSF-TLFU)
# - Eviction priority H(k) = L + freq(k) / size(k)^alpha
#   * L is a global "clock" set to the priority of the last evicted item.
#   * freq(k) is a small, per-key frequency (number of hits in-cache), aged implicitly by L.
#   * size-aware via exponent alpha (default 1.0), optimizing hits per byte.
# - On hit, the item is "rejuvenated": H := L + freq/size^alpha.
# - On insert, initial freq is estimated by a TinyLFU-like Count-Min Sketch
#   and dampened by a small admission threshold to curb one-hit pollution.
# - Scan-resistant: L grows on each eviction, ensuring stale items eventually fall out even if once-hot.
# - Size-aware and deterministic tie-breakers.

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()     # key -> float, eviction priority H
m_freq = dict()         # key -> small int, in-cache frequency (hits since (re)admission)
m_last_access = dict()  # key -> int, last access (for LRU tie-break)
m_L = 0.0               # global clock (GreedyDual)
m_inited = False        # sketch initialization guard

# TinyLFU sketch (Count-Min Sketch) for admission/initialization
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096     # power-of-two strongly preferred
m_sketch = None         # list of SKETCH_DEPTH arrays of length SKETCH_WIDTH
m_sketch_ops = 0        # how many increments since last decay
SKETCH_DECAY_EVERY = 20000  # periodically halve counts to keep sketch fresh

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 1.0          # size penalty exponent in [0.5, 1.0]; 1.0 = classic GDSF
NEW_ADMIT_THRESHOLD = 1 # subtract this from estimated freq for new items (TinyLFU doorkeeper)
NEW_ITEM_BIAS = 0.0     # extra boost for new items after thresholding
FREQ_CAP = 255          # cap per-key in-cache frequency to bound priorities
EPS = 1e-12             # numeric epsilon for ties

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    # Initialize fixed-size sketch. Width kept modest to control overhead.
    # Optionally could scale with cache capacity; 4096 works well across traces.
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    # Derive SKETCH_DEPTH distinct indices using 64-bit mixing
    h = hash(key)
    # 64-bit golden ratio constant for bit spreading
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    # Python ints are unbounded; ensure positive and map to width
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        # Avoid overflow; counters are small
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        # Periodically age the sketch to keep it representative of recent history
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    # size^alpha
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _current_priority(key, obj):
    # Fallback priority if metadata missing: treat as just-admitted at baseline L
    return m_priority.get(key, m_L)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict the item with minimal priority H.
    Tie-breakers (deterministic):
      1) older last access (LRU among equals)
      2) larger size (free more space; protects small objs)
      3) lexicographic key
    """
    if not cache_snapshot.cache:
        return None

    weakest_key = None
    weakest_pri = None
    weakest_la = None
    weakest_size = None

    for k, v in cache_snapshot.cache.items():
        pri = _current_priority(k, v)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if weakest_key is None:
            weakest_key = k
            weakest_pri = pri
            weakest_la = la
            weakest_size = sz
            continue

        # Primary: smallest priority
        if pri < weakest_pri - EPS:
            weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
            continue

        if abs(pri - weakest_pri) <= EPS:
            # Secondary: older last access
            if la < weakest_la:
                weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
                continue
            if la == weakest_la:
                # Tertiary: larger size (free space)
                if sz > weakest_size:
                    weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
                    continue
                if sz == weakest_size and k < weakest_key:
                    weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz

    return weakest_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Increase per-key in-cache frequency (capped).
      - Rejuvenate priority: H = L + freq / size^alpha
      - Update last access timestamp.
    """
    global m_L

    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    f = m_freq.get(key, 0)
    if f < FREQ_CAP:
        f += 1
    m_freq[key] = f

    # Re-anchor to current L; GreedyDual ensures aging vs concurrent items
    m_priority[key] = float(m_L) + float(f) / denom
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch (to account for this access).
      - Estimate initial frequency via sketch; apply a small admission threshold.
      - Initialize priority: H = L + (NEW_ITEM_BIAS + max(0, est - threshold)) / size^alpha
      - Initialize last access and per-key freq accordingly.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    est = _sketch_estimate(key)
    # Apply a small doorkeeper threshold to keep cold one-timers near baseline L
    init_f = max(0, int(est) - int(NEW_ADMIT_THRESHOLD))
    # Bound the starting freq to the same cap
    init_f = min(init_f, FREQ_CAP)

    # New item bias (optional, usually 0.0)
    init_score = float(NEW_ITEM_BIAS) + float(init_f)

    m_freq[key] = init_f
    m_priority[key] = float(m_L) + (init_score / denom)
    m_last_access[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Advance global clock to the evicted item's priority (GreedyDual invariant).
      - Clean all metadata of the evicted key.
      - Periodically age the TinyLFU sketch (handled during add ops).
    """
    global m_L

    ekey = evicted_obj.key
    # Advance L to evicted priority to maintain GD invariants
    ev_pri = m_priority.get(ekey, m_L)
    if ev_pri > m_L:
        m_L = float(ev_pri)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_63iqry6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpegvujfmz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfxbsbnrg.pickle

Iteration 4: New subsample score 1.3210439999999999 is better than old score 1.2994700000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf6vi24_8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnswx554d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5c5a3v_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3s0wgffl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp816vv63l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx36v890g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3hmbiqrx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoxe1x5ca.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvny6yt44.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpczs1d51k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjbqjiwo8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp6f2fpcs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3ugmkvt2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe69mvxc4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp85jefrna.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4o2kg0og.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbnns2edt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsiu3gcvk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcrnkdj3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpruqifs20.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp34cr6h8p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptkbftpn2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4e2yxg0o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfnnf56ng.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4070wodg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp16kb1gv6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9onpf5nj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpum_1ue5p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi7zaglxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfw1r2nd9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplkbv79sy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu967dcej.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiohp61j7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwqadvjto.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpytonk21n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46a6q2ux.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg16ic91f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8e_5351p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4t231c58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe32i8ljm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4gqjf_by.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmky5cipr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphgo2hk21.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpptewns9e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7fq20dc1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy1q5eq2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptpe3394f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1oh7oywv.pickle

Iteration 4: New program is on the linear pareto front
Iteration 4: Full valset score for new program: 0.23035914583333325
Iteration 4: Full train_val score for new program: 0.23035914583333325
Iteration 4: Individual valset scores for new program: [0.472019, 0.449101, 0.460033, 0.417445, 0.465957, 0.457685, 0.270335, 0.497641, 0.538792, 0.531017, 0.083333, 0.361012, 0.023893, 0.0, 0.020246, 0.020147, 0.018985, 0.022631, 0.021797, 0.272227, 0.357915, 0.025184, 0.057382, 0.057382, 0.269781, 0.366935, 0.832175, 0.890467, 0.020365, 0.040909, 0.041002, 9.6e-05, 3.6e-05, 0.745916, 0.076754, 0.067079, 0.00931, 0.641937, 0.125461, 0.022179, 0.021381, 0.023637, 0.042763, 0.366667, 0.021194, 0.023562, 0.466258, 0.039216]
Iteration 4: New valset pareto front scores: [0.472019, 0.449101, 0.460033, 0.417445, 0.472857, 0.459472, 0.271531, 0.498624, 0.538792, 0.531017, 0.083333, 0.361012, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.325915, 0.366935, 0.832175, 0.890467, 0.039832, 0.040909, 0.045558, 0.007003, 0.020672, 0.754084, 0.083333, 0.067961, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 4: Full valset pareto front score: 0.24005070833333328
Iteration 4: Updated valset pareto front programs: [{3}, {3}, {3}, {3}, {2}, {2}, {2}, {0, 1}, {3}, {0, 1, 2, 3}, {3}, {3}, {2}, {0, 1, 2, 3}, {2}, {2}, {2}, {2}, {2}, {2, 3}, {2}, {2}, {2}, {2}, {2}, {3}, {3}, {3}, {2}, {3}, {2}, {2}, {2}, {2}, {2}, {2}, {2}, {3}, {1, 2, 3}, {2}, {2}, {2}, {2}, {3}, {2}, {2}, {2, 3}, {2}]
Iteration 4: Best valset aggregate score so far: 0.23035914583333325
Iteration 4: Best program as per aggregate score on train_val: 3
Iteration 4: Best program as per aggregate score on valset: 3
Iteration 4: Best score on valset: 0.23035914583333325
Iteration 4: Best score on train_val: 0.23035914583333325
Iteration 4: Linear pareto front program index: 3
Iteration 4: New program candidate index: 3
Iteration 5: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpub_s7301.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplwmyp6e8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp00_gmllc.pickle

Iteration 5: Proposed new text for program: # GreedyDual Size-Frequency (GDSF) with soft size normalization
# - Prioritizes objects by H = L + freq / norm_size(size)
#   * L is a global inflation (age) that increases to the priority of each evicted object.
#   * freq is the number of references while resident (updated on hit).
#   * norm_size(size) = (size + SIZE_SMOOTH)^ALPHA softens the dominance of tiny objects.
# - Eviction always removes the lowest-H object (ties: older, then larger, then lexicographic).
# - Insert initializes with a small starting freq to reduce one-hit pollution.
# - On hit, we increase freq and reset the priority using current L (making it "young" again).
# This is a simple, efficient, and robust policy for byte-caching with variable-size objects.

# Metadata
m_H = dict()            # key -> float, current priority H
m_freq = dict()         # key -> float, resident frequency
m_last_access = dict()  # key -> int, last access time (access_count)

# Global inflation (age)
g_L = 0.0

# Tunables
ALPHA = 0.75            # soft size normalization exponent in (0,1]; smaller -> less bias vs large objects
SIZE_SMOOTH = 2048.0    # softens tiny objects' advantage; effective size is (size + SIZE_SMOOTH)^ALPHA
INIT_FREQ = 0.5         # admission frequency for new items (reduces one-hit pollution)
HIT_INC = 1.0           # increment on hit to the resident frequency


def _norm_size(obj):
    sz = float(max(1, int(getattr(obj, "size", 1))))
    return (sz + SIZE_SMOOTH) ** ALPHA


def _current_priority(freq, obj, L):
    return L + (float(freq) / _norm_size(obj))


def evict(cache_snapshot, obj):
    """
    Choose victim with the smallest priority H.
    Tie-breakers:
      1) older last access (LRU among equals)
      2) larger size (free more space)
      3) lexicographic key (determinism)
    """
    if not cache_snapshot.cache:
        return None

    cand_key = None
    cand_H = None
    cand_last = None
    cand_size = None

    for k, v in cache_snapshot.cache.items():
        H = m_H.get(k)
        if H is None:
            # Fallback if metadata missing: treat as freshly admitted with minimal priority
            H = _current_priority(INIT_FREQ, v, g_L)
        la = m_last_access.get(k, -1)
        sz = int(getattr(v, "size", 1))

        if cand_key is None:
            cand_key, cand_H, cand_last, cand_size = k, H, la, sz
            continue

        if H < cand_H:
            cand_key, cand_H, cand_last, cand_size = k, H, la, sz
            continue

        if H == cand_H:
            # Prefer evicting older
            if la < cand_last:
                cand_key, cand_H, cand_last, cand_size = k, H, la, sz
                continue
            if la == cand_last:
                # Prefer evicting larger (to free space faster)
                if sz > cand_size:
                    cand_key, cand_H, cand_last, cand_size = k, H, la, sz
                    continue
                if sz == cand_size and k < cand_key:
                    cand_key, cand_H, cand_last, cand_size = k, H, la, sz

    return cand_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment resident frequency by HIT_INC.
    - Recompute priority to H = L + freq / norm_size using the current global L.
    - Update last access timestamp.
    """
    global g_L

    key = obj.key
    now = cache_snapshot.access_count

    freq = m_freq.get(key, 0.0) + HIT_INC
    m_freq[key] = freq
    m_H[key] = _current_priority(freq, obj, g_L)
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert (typically after a miss and admission):
    - Initialize resident frequency with INIT_FREQ (small, to curb one-hit pollution).
    - Set priority H = L + freq / norm_size so the item starts just above the current age.
    - Initialize last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count

    m_last_access[key] = now
    m_freq[key] = float(INIT_FREQ)
    m_H[key] = _current_priority(m_freq[key], obj, g_L)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Increase global age L to the priority of the evicted object (monotonic).
    - Clean all metadata of the evicted object.
    """
    global g_L

    ekey = evicted_obj.key
    evicted_H = m_H.get(ekey, g_L)
    # Age increases to the evicted priority; ensures older/stale items lose competitiveness.
    if evicted_H > g_L:
        g_L = evicted_H

    # Clean metadata
    m_H.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpryx6djii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7s5lm317.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdhooihit.pickle

Iteration 5: New subsample score 1.085043 is not better than old score 1.1511689999999999, skipping
Iteration 6: Selected program 1 score: 0.2117635416666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp03mawy6c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphper1sl9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2hwcjzgl.pickle

Iteration 6: Proposed new text for program: # Segmented GreedyDual-Size-Frequency (SGDSF)
# - Two segments (probation/protected) like SLRU:
#     * New items start in probation; a hit promotes to protected.
#     * Evictions prefer probation; protected is only used if probation is empty.
#   This protects items that show temporal reuse and filters out one-hit wonders.
# - Size-aware (GDSF): initial and per-hit boosts are normalized by size (1/size),
#   naturally evicting large one-hit items earlier while rewarding frequent/small ones.
# - Recency-aware: last_access timestamps provide LRU-like tie-breaking and
#   segment rebalancing demotes protected LRU when it grows too large.
# - Global aging (L): classic GreedyDual aging to adapt to shifting workloads.

# ----------------------
# Global metadata stores
# ----------------------
m_priority = dict()        # key -> float priority (H)
m_freq = dict()            # key -> int frequency (hit count)
m_last_access = dict()     # key -> int last access time (for LRU tie-break)
m_seg = dict()             # key -> 0 (probation) or 1 (protected)
m_age_L = 0.0              # global aging factor (monotonically non-decreasing)

# Segment byte counters (best-effort; assumed starting empty)
m_bytes_prob = 0           # bytes currently in probation
m_bytes_prot = 0           # bytes currently in protected

# Target share for protected segment (by bytes)
PROTECTED_RATIO = 0.80


# ----------------------
# Helpers
# ----------------------
def _priority_for(key, obj=None):
    """
    Safe fetch of an object's priority.
    If metadata is missing, default to the current aging baseline plus a small size-aware boost.
    """
    if key in m_priority:
        return m_priority[key]
    if obj is not None:
        sz = max(1, int(getattr(obj, "size", 1)))
        return m_age_L + 1.0 / float(sz)
    return m_age_L

def _seg_of(key):
    """Return segment id: 0 (probation) or 1 (protected). Defaults to probation."""
    return m_seg.get(key, 0)

def _rebalance_protected(cache_snapshot):
    """
    Keep the protected segment within its target byte budget by demoting LRU-protected items
    back to probation. This is invoked after promotions.
    """
    global m_bytes_prot, m_bytes_prob
    target_bytes = int(PROTECTED_RATIO * cache_snapshot.capacity)

    # Quick exit if already within budget
    if m_bytes_prot <= target_bytes:
        return

    # Demote LRU items from protected until within budget or no candidate exists.
    while m_bytes_prot > target_bytes:
        lru_key = None
        lru_time = None

        for k, v in cache_snapshot.cache.items():
            if _seg_of(k) != 1:
                continue
            la = m_last_access.get(k, -1)
            if lru_key is None or la < lru_time or (la == lru_time and k < lru_key):
                lru_key, lru_time = k, la

        if lru_key is None:
            # Nothing to demote
            break

        # Demote
        m_seg[lru_key] = 0
        sz = max(1, int(getattr(cache_snapshot.cache.get(lru_key, None), "size", 1)))
        m_bytes_prot -= sz
        m_bytes_prob += sz


def _pick_eviction_candidate(cache_snapshot, segment_id):
    """
    Choose eviction victim from a specified segment using GDSF priority as primary key:
    - Primary: lower priority H first
    - Secondary: older last access (LRU)
    - Tertiary: larger size (to free more space)
    - Final: lexicographic key (deterministic)
    Returns key or None if no candidate in that segment.
    """
    candid_key = None
    candid_pri = None
    candid_last = None
    candid_size = None

    for k, v in cache_snapshot.cache.items():
        if _seg_of(k) != segment_id:
            continue

        pri = _priority_for(k, v)
        la = m_last_access.get(k, -1)
        sz = max(1, int(getattr(v, "size", 1)))

        if candid_key is None:
            candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
            continue

        if pri < candid_pri:
            candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
            continue

        if pri == candid_pri:
            if la < candid_last:
                candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
                continue
            if la == candid_last:
                if sz > candid_size:
                    candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz
                    continue
                if sz == candid_size and k < candid_key:
                    candid_key, candid_pri, candid_last, candid_size = k, pri, la, sz

    return candid_key


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict victim using segmented GDSF:
    - Prefer evicting from probation (segment 0).
    - If probation is empty, evict from protected (segment 1).
    """
    if not cache_snapshot.cache:
        return None

    # Try probation first
    victim = _pick_eviction_candidate(cache_snapshot, segment_id=0)
    if victim is not None:
        return victim

    # Fallback to protected if probation empty
    victim = _pick_eviction_candidate(cache_snapshot, segment_id=1)
    return victim


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment frequency.
    - Increase priority by 1/size, but not below the current aging baseline.
    - Update last access time.
    - If the item is in probation, promote to protected and rebalance protected size.
    """
    global m_bytes_prob, m_bytes_prot

    key = obj.key
    sz = max(1, int(obj.size))

    # Initialize metadata if missing
    if key not in m_freq:
        m_freq[key] = 0
    if key not in m_priority:
        m_priority[key] = m_age_L + 1.0 / float(sz)
    if key not in m_last_access:
        m_last_access[key] = cache_snapshot.access_count - 1  # slightly in the past
    if key not in m_seg:
        # Assume probation if unknown
        m_seg[key] = 0
        # Best-effort adjust bytes if this is a truly new tracked key.
        # In normal operation keys are initialized on insert, so this is rarely used.
        m_bytes_prob += sz

    # Update stats
    m_freq[key] += 1
    # GDSF-like bump with baseline guard
    m_priority[key] = max(m_priority[key], m_age_L) + 1.0 / float(sz)
    m_last_access[key] = cache_snapshot.access_count

    # Promotion to protected on first (or subsequent) hit if in probation
    if m_seg[key] == 0:
        m_seg[key] = 1
        m_bytes_prob -= sz
        m_bytes_prot += sz
        # Keep protected within budget
        _rebalance_protected(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Place new object into probation.
    - Initialize frequency to 1 and priority to L + 1/size.
    - Record last access time.
    """
    global m_bytes_prob

    key = obj.key
    sz = max(1, int(obj.size))

    m_freq[key] = 1
    m_priority[key] = m_age_L + 1.0 / float(sz)
    m_last_access[key] = cache_snapshot.access_count
    m_seg[key] = 0  # probation
    m_bytes_prob += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Age baseline L up to the evicted object's priority (monotonic non-decreasing).
    - Remove evicted object's metadata and adjust segment byte counters.
    """
    global m_age_L, m_bytes_prob, m_bytes_prot

    ekey = evicted_obj.key
    esz = max(1, int(getattr(evicted_obj, "size", 1)))

    # Update aging baseline
    evicted_priority = m_priority.get(ekey, m_age_L)
    if evicted_priority > m_age_L:
        m_age_L = evicted_priority

    # Adjust segment bytes
    seg = m_seg.get(ekey, 0)
    if seg == 0:
        m_bytes_prob = max(0, m_bytes_prob - esz)
    else:
        m_bytes_prot = max(0, m_bytes_prot - esz)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5orbyz_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi94yppuu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp16b4vdw5.pickle

Iteration 6: New subsample score 0.528716 is better than old score 0.479468. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0t72ct9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa6916s8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1wur7fw8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb68i1_l9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvgll7ij_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph868rc4a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqni_qkni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplvaakyw3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi5wwrrp0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqis6zjgu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbkm9t_tl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf411sc68.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeuldd5c6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7ozd5kkn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvosg8izc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvrsrcjt5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdhysad6z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_bnl7gzc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprqnt6j5c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvcxm4kvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7kqgjkis.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb_h7me1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmporz52ga_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6u8c2kyn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn8813bep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpawybkp19.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn4pp1nx9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnpyaz958.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0emetphs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz8un_lyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqip81u28.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp690c818c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt2ezzla1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmproygn20n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppyjlmzxt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiadtn37m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfekljazk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5jesj66q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk4hw0hjz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpthdt9m32.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg5juhomh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb7mj8zod.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_ys5uz9x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp49jhrgoj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiepdrjrv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsfm64ake.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph8t4g72s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqwj5r4p1.pickle

Iteration 6: New program is on the linear pareto front
Iteration 6: Full valset score for new program: 0.23149604166666662
Iteration 6: Full train_val score for new program: 0.23149604166666662
Iteration 6: Individual valset scores for new program: [0.476754, 0.447058, 0.456336, 0.390326, 0.47767, 0.458953, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.344583, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.269797, 0.271169, 0.770606, 0.887215, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.754595, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 6: New valset pareto front scores: [0.476754, 0.449101, 0.460033, 0.417445, 0.47767, 0.459472, 0.271531, 0.498624, 0.538792, 0.531017, 0.083333, 0.361012, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.325915, 0.366935, 0.832175, 0.890467, 0.039832, 0.040909, 0.045558, 0.007003, 0.020672, 0.754595, 0.083333, 0.067961, 0.026022, 0.641937, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 6: Full valset pareto front score: 0.24026027083333326
Iteration 6: Updated valset pareto front programs: [{4}, {3}, {3}, {3}, {4}, {2}, {2, 4}, {0, 1, 4}, {3}, {0, 1, 2, 3, 4}, {3}, {3}, {2, 4}, {0, 1, 2, 3, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 3, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2}, {3}, {3}, {3}, {2, 4}, {3}, {2, 4}, {2, 4}, {2, 4}, {4}, {2, 4}, {2, 4}, {2, 4}, {3}, {1, 2, 3, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {3}, {2, 4}, {2, 4}, {2, 3, 4}, {2, 4}]
Iteration 6: Best valset aggregate score so far: 0.23149604166666662
Iteration 6: Best program as per aggregate score on train_val: 4
Iteration 6: Best program as per aggregate score on valset: 4
Iteration 6: Best score on valset: 0.23149604166666662
Iteration 6: Best score on train_val: 0.23149604166666662
Iteration 6: Linear pareto front program index: 4
Iteration 6: New program candidate index: 4
Iteration 7: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoz3kbidh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo066s33s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphfxcmr4s.pickle

Iteration 7: Proposed new text for program: # Windowed Segmented LRU with TinyLFU guidance (WSLRU-TLFU)
# - Three segments by bytes:
#   * Window (W): small recency buffer for new items (short-term bursts).
#   * Probation (P): main area for items that have been referenced at least once after W.
#   * Protected (S): items with at least two hits; shielded by LRU demotion back to P.
# - Eviction preference keeps W near its target size, then P, then S (LRU tails).
# - TinyLFU Count-Min sketch maintains global frequency estimates:
#   * On insert, high-estimate items bypass W into P (doorkeeper-like effect).
# - On hit:
#   * W -> P promotion (first confirmation).
#   * P -> S promotion (second confirmation).
#   * S stays in S and is moved to MRU via recency timestamp.
#   * If S exceeds its target bytes, demote the S-LRU back to P.
# - Size-aware tie-breaker on equal recency: evict larger item first.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
# Window (recency buffer) fraction of total capacity (bytes)
W_FRAC = 0.20           # 20% default window
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80           # 80% of main in protected
# TinyLFU admission threshold. est >= ADMIT_EST_THRESHOLD bypasses W into P.
ADMIT_EST_THRESHOLD = 2
EPS = 1e-12

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * W_FRAC)
    main_target = cap - w_target
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict LRU from a preferred segment to keep W near target and preserve multi-hit items.
    Priority:
      - If Window exceeds target, evict from Window (W-LRU).
      - Else evict from Probation (P-LRU).
      - If P empty, evict from Window; if both empty, evict from Protected (S-LRU).
    Tie-breakers inside a segment:
      1) older last access
      2) larger size
      3) lexicographic key
    """
    if not cache_snapshot.cache:
        return None

    w_target, _ = _targets(cache_snapshot)
    # Prefer evicting from W if it's above target; else from P.
    prefer_seg = SEG_WINDOW if m_bytes_W > w_target else SEG_PROBATION

    # Try preferred segment
    victim = _lru_key_in_segment(cache_snapshot, prefer_seg)
    if victim is not None:
        return victim

    # Fallbacks
    victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    if victim is not None:
        return victim

    victim = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if victim is not None:
        return victim

    # Shouldn't happen, but fallback: global LRU across all segments
    # (covers any metadata inconsistency)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch.
      - Update last access.
      - Promote along W -> P -> S.
      - If S exceeds its byte target, demote S-LRU back to P.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: just refresh recency; no structural change
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update sketch.
      - Initialize last access.
      - Admission: if TinyLFU estimate is high, bypass W into P; else insert into W.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    est = _sketch_estimate(key)
    sz = _size_of(obj)

    # Doorkeeper-style admission: heavy hitters bypass W into P.
    if est >= ADMIT_EST_THRESHOLD:
        m_seg[key] = SEG_PROBATION
        global m_bytes_P
        m_bytes_P += sz
    else:
        m_seg[key] = SEG_WINDOW
        global m_bytes_W
        m_bytes_W += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Sketch aging is handled on adds.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvnqefrqg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5saas6k1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6ysntobj.pickle

Iteration 7: New subsample score 0.168265 is better than old score 0.12457900000000001. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr7wnaz8_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjkyaomm_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_v4k5xvj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfzymz51_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp546nj_2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnl1qf41_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplg28xgmu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnivfmw0r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpysdzk2jx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg3_n1by9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfdda4sny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp40_csa_0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpatqh7_44.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqhutcgm6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp96k_effg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxbfh65pu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9tiomu33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjkgj_afn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2yswnf49.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgxcudicy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6nqudjr6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwscgcix6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw8mhwmdr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7f2601eq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjm44cqxv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8j42zrf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpokg_t4do.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9w3jvyt9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_7227yqk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4leak00r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6higf420.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7rjyjmoj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6qyfjhrw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv5p1h_y5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp9d4399i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjlybuwz3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo8fjf5ck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8py1dn9w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy87axjmk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpseace87j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8_e0z275.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8cxn51ze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3xhwv_2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2x5vto66.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4le3xmgo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxz5ai_2f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeh914lpa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpay3nc42p.pickle

Iteration 7: New program is on the linear pareto front
Iteration 7: Full valset score for new program: 0.23314141666666657
Iteration 7: Full train_val score for new program: 0.23314141666666657
Iteration 7: Individual valset scores for new program: [0.464647, 0.436026, 0.444216, 0.403041, 0.46125, 0.447192, 0.267943, 0.498624, 0.540222, 0.531017, 0.1, 0.33881, 0.042054, 0.0, 0.020246, 0.020006, 0.019389, 0.023053, 0.022219, 0.272227, 0.361849, 0.025674, 0.057382, 0.057382, 0.269793, 0.362903, 0.693148, 0.881054, 0.040132, 0.047727, 0.045558, 0.007015, 0.020684, 0.754425, 0.083333, 0.067961, 0.018239, 0.643483, 0.125461, 0.078736, 0.048259, 0.0723, 0.052632, 0.366667, 0.042007, 0.060309, 0.466258, 0.088235]
Iteration 7: New valset pareto front scores: [0.476754, 0.449101, 0.460033, 0.417445, 0.47767, 0.459472, 0.271531, 0.498624, 0.540222, 0.531017, 0.1, 0.361012, 0.042054, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.325915, 0.366935, 0.832175, 0.890467, 0.040132, 0.047727, 0.045558, 0.007015, 0.020684, 0.754595, 0.083333, 0.067961, 0.026022, 0.643483, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.366667, 0.042007, 0.074334, 0.466258, 0.088235]
Iteration 7: Full valset pareto front score: 0.24099949999999992
Iteration 7: Updated valset pareto front programs: [{4}, {3}, {3}, {3}, {4}, {2}, {2, 4}, {0, 1, 4, 5}, {5}, {0, 1, 2, 3, 4, 5}, {5}, {3}, {5}, {0, 1, 2, 3, 4, 5}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2, 3, 4, 5}, {2, 4}, {2, 4}, {2, 4}, {2, 4}, {2}, {3}, {3}, {3}, {5}, {5}, {2, 4, 5}, {5}, {5}, {4}, {2, 4, 5}, {2, 4, 5}, {2, 4}, {5}, {1, 2, 3, 4, 5}, {2, 4}, {2, 4}, {2, 4}, {2, 4, 5}, {3, 5}, {5}, {2, 4}, {2, 3, 4, 5}, {5}]
Iteration 7: Best valset aggregate score so far: 0.23314141666666657
Iteration 7: Best program as per aggregate score on train_val: 5
Iteration 7: Best program as per aggregate score on valset: 5
Iteration 7: Best score on valset: 0.23314141666666657
Iteration 7: Best score on train_val: 0.23314141666666657
Iteration 7: Linear pareto front program index: 5
Iteration 7: New program candidate index: 5
Iteration 8: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptje1kmai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmkjibguv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg302bpbc.pickle

Iteration 8: Proposed new text for program: # Adaptive Segmented GreedyDual + TinyLFU (AS-GD SLRU)
# - Two segments: probationary (new/recency) and protected (hot/frequent).
# - GreedyDual priority per item: H(k) = L + freq(k) / size(k)^alpha
#   * L is a global clock set to the priority of the last evicted item.
#   * freq(k) increases on hits in-cache; capped to bound priority.
#   * size-aware via exponent alpha in [0.5, 1.0].
# - TinyLFU Count-Min Sketch estimates initial frequency on insert (admission).
#   * Includes a size-aware doorkeeper to curb large one-hit pollution.
# - ARC-style ghosts (B1 for probationary, B2 for protected) adapt target
#   protected share (in bytes) to the workload by reacting to misses that hit ghosts.
# - Eviction prefers probationary items; if protected is over its adaptive budget,
#   evict from protected; ties resolved by LRU, then bigger size, then key.

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()       # key -> float, eviction priority H
m_freq = dict()           # key -> small int, in-cache frequency (hits since (re)admission)
m_last_access = dict()    # key -> int, last access (for LRU tie-break)
m_segment = dict()        # key -> 0 (probationary) or 1 (protected)
m_L = 0.0                 # global clock (GreedyDual)
m_inited = False          # global initialization guard

# Segment accounting (bytes)
m_bytes_prob = 0
m_bytes_prot = 0
m_target_prot_bytes = 0   # adaptive target for protected bytes

# ARC-style ghost caches (remember recently evicted keys by segment)
# dict key -> (time, size). Insertion order is preserved in Py3.7+, used to prune oldest.
m_ghost_b1 = dict()       # evicted from probationary
m_ghost_b2 = dict()       # evicted from protected

# TinyLFU sketch (Count-Min Sketch) for admission/initialization
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096       # power-of-two strongly preferred
m_sketch = None           # list of SKETCH_DEPTH arrays of length SKETCH_WIDTH
m_sketch_ops = 0          # how many increments since last decay
SKETCH_DECAY_EVERY = 20000  # periodically halve counts to keep sketch fresh

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 0.9             # size penalty exponent [0.5, 1.0]; <1 softens penalty on large objects
NEW_ADMIT_THRESHOLD = 1    # base doorkeeper for TinyLFU estimated freq
NEW_ITEM_BIAS = 0.0        # extra score for new items after thresholding
FREQ_CAP = 1023            # cap per-key in-cache frequency to bound priorities
PROT_EST_THRESH = 2        # if TinyLFU est - threshold >= this, insert into protected
GHOST_MAX = 8192           # max entries in each ghost list
ADAPT_STEP_FLOOR = 1024    # min step (bytes) to adjust target protected bytes
EPS = 1e-12                # numeric epsilon for ties

# ---------------
# Helper routines
# ---------------
def _ensure_inited(cache_snapshot):
    global m_inited, m_sketch, m_target_prot_bytes
    if m_inited:
        return
    # Initialize Count-Min Sketch
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    # Start with a balanced split
    m_target_prot_bytes = int(cache_snapshot.capacity * 0.5)
    m_inited = True

def _hash_idx(key, i):
    # Derive SKETCH_DEPTH distinct indices using 64-bit mixing
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _current_priority(key):
    return m_priority.get(key, m_L)

def _size_penalty_for_new(sz):
    # Size-aware doorkeeper bump for very large objects:
    # increase threshold roughly with log2(size), but gently.
    s = max(1, int(sz))
    # b ~= log2(size) - 12; negative for <= 4096B
    b = s.bit_length() - 1 - 12
    if b <= 0:
        return 0
    # grow slowly: +1 every 2 doublings beyond 4 KiB
    return b // 2

def _ghost_add(ghost, key, now, size):
    # Maintain bounded ghost list; remove oldest if exceeds cap
    ghost[key] = (now, size)
    if len(ghost) > GHOST_MAX:
        # remove oldest (smallest 'now'); O(n) scan but bounded by small GHOST_MAX
        oldest_k = None
        oldest_t = None
        for k, (t, _) in ghost.items():
            if oldest_t is None or t < oldest_t:
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost.pop(oldest_k, None)

def _segment_of(key):
    # default to probationary if unknown
    return m_segment.get(key, 0)

def _inc_bytes(seg, delta):
    global m_bytes_prob, m_bytes_prot
    if seg == 1:
        m_bytes_prot = max(0, m_bytes_prot + delta)
    else:
        m_bytes_prob = max(0, m_bytes_prob + delta)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim to evict:
      - Prefer evicting from the probationary segment.
      - If protected exceeds its adaptive byte target, evict from protected.
      - Within chosen segment, evict minimal H; ties: older last access, larger size, then key.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_inited(cache_snapshot)

    # Determine which segment to draw the victim from
    have_prob = False
    have_prot = False
    for k in cache_snapshot.cache.keys():
        if _segment_of(k) == 0:
            have_prob = True
        else:
            have_prot = True
        if have_prob and have_prot:
            break

    choose_seg = None
    # If protected is over budget, must evict from protected
    if have_prot and m_bytes_prot > m_target_prot_bytes:
        choose_seg = 1
    else:
        # Otherwise prefer probationary, if available
        if have_prob:
            choose_seg = 0
        elif have_prot:
            choose_seg = 1

    # Track best candidate within preferred and the other as fallback
    best_pref_key = None
    best_pref_pri = None
    best_pref_la = None
    best_pref_size = None

    best_alt_key = None
    best_alt_pri = None
    best_alt_la = None
    best_alt_size = None

    for k, v in cache_snapshot.cache.items():
        seg = _segment_of(k)
        pri = _current_priority(k)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        # Selection tracker
        def consider(cur_key, cur_pri, cur_la, cur_sz, best_key, best_pri, best_la, best_sz):
            if best_key is None:
                return k, pri, la, sz
            if pri < best_pri - EPS:
                return k, pri, la, sz
            if abs(pri - best_pri) <= EPS:
                if la < best_la:
                    return k, pri, la, sz
                if la == best_la:
                    if sz > best_sz:
                        return k, pri, la, sz
                    if sz == best_sz and k < best_key:
                        return k, pri, la, sz
            return best_key, best_pri, best_la, best_sz

        if choose_seg is None or seg == choose_seg:
            best_pref_key, best_pref_pri, best_pref_la, best_pref_size = consider(
                k, pri, la, sz, best_pref_key, best_pref_pri, best_pref_la, best_pref_size
            )
        else:
            best_alt_key, best_alt_pri, best_alt_la, best_alt_size = consider(
                k, pri, la, sz, best_alt_key, best_alt_pri, best_alt_la, best_alt_size
            )

    if best_pref_key is not None:
        return best_pref_key
    return best_alt_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Promote to protected on first hit if currently probationary.
      - Increase per-key frequency (capped).
      - Rejuvenate priority: H = L + freq / size^alpha
      - Update last access.
    """
    global m_L

    _ensure_inited(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    seg = _segment_of(key)
    if seg == 0:
        # Promote to protected on first hit
        m_segment[key] = 1
        _inc_bytes(0, -sz)
        _inc_bytes(1, +sz)
        # small bump to freq to reflect promotion benefit
        f = min(FREQ_CAP, m_freq.get(key, 0) + 1)
    else:
        f = m_freq.get(key, 0)
        if f < FREQ_CAP:
            f += 1

    m_freq[key] = f
    m_priority[key] = float(m_L) + float(f) / denom
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch (for this access).
      - Estimate initial frequency via sketch and apply a size-aware doorkeeper.
      - Segment placement:
          * If est - doorkeeper >= PROT_EST_THRESH -> protected
          * else probationary
      - Initialize priority: H = L + (NEW_ITEM_BIAS + init_f) / size^alpha
      - Update last access and per-key freq accordingly.
      - Adapt target protected bytes if key hits a ghost list (ARC-style).
    """
    _ensure_inited(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    # ARC-style adaptation using ghost hits
    # If this miss is for a key recently evicted from probationary/protected, adjust target.
    if key in m_ghost_b1:
        # Increase protected share: recency-heavy workload
        step = max(sz, ADAPT_STEP_FLOOR)
        m_target_prot_bytes = min(cache_snapshot.capacity, m_target_prot_bytes + step)
        m_ghost_b1.pop(key, None)
    elif key in m_ghost_b2:
        # Decrease protected share: frequency-heavy workload satisfied; admit more recency
        step = max(sz, ADAPT_STEP_FLOOR)
        m_target_prot_bytes = max(0, m_target_prot_bytes - step)
        m_ghost_b2.pop(key, None)

    est = _sketch_estimate(key)
    doorkeeper = NEW_ADMIT_THRESHOLD + _size_penalty_for_new(sz)
    init_f = max(0, int(est) - int(doorkeeper))
    init_f = min(init_f, FREQ_CAP)

    # Segment decision by estimated strength
    if init_f >= PROT_EST_THRESH:
        seg = 1  # protected
    else:
        seg = 0  # probationary

    m_segment[key] = seg
    _inc_bytes(seg, +sz)

    init_score = float(NEW_ITEM_BIAS) + float(init_f)
    m_freq[key] = init_f
    m_priority[key] = float(m_L) + (init_score / denom)
    m_last_access[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Advance global clock to the evicted item's priority (GreedyDual invariant).
      - Record ghost entry by its segment to guide adaptation.
      - Adjust per-segment byte accounting.
      - Clean all metadata of the evicted key.
    """
    global m_L

    _ensure_inited(cache_snapshot)

    ekey = evicted_obj.key
    esz = getattr(evicted_obj, "size", 1)
    enow = cache_snapshot.access_count
    seg = _segment_of(ekey)

    # Advance L to evicted priority to maintain GD invariant
    ev_pri = m_priority.get(ekey, m_L)
    if ev_pri > m_L:
        m_L = float(ev_pri)

    # Ghost the evicted key (for adaptation)
    if seg == 1:
        _ghost_add(m_ghost_b2, ekey, enow, esz)
    else:
        _ghost_add(m_ghost_b1, ekey, enow, esz)

    # Update segment accounting
    _inc_bytes(seg, -esz)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_segment.pop(ekey, None)
Subprocess stdout: Error in subprocess: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5j1rw3_.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 250, in get
    self.update_after_insert(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 268, in update_after_insert
    self.update_after_insert_func(self.snapshot, obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 294, in update_after_insert
    m_target_prot_bytes = max(0, m_target_prot_bytes - step)
                              ^
UnboundLocalError: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value

Subprocess stdout: Error in subprocess: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1uz85tnt.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 250, in get
    self.update_after_insert(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 268, in update_after_insert
    self.update_after_insert_func(self.snapshot, obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 294, in update_after_insert
    m_target_prot_bytes = max(0, m_target_prot_bytes - step)
                              ^
UnboundLocalError: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value

Subprocess stdout: Error in subprocess: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx1dd_do_.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 250, in get
    self.update_after_insert(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 268, in update_after_insert
    self.update_after_insert_func(self.snapshot, obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 289, in update_after_insert
    m_target_prot_bytes = min(cache_snapshot.capacity, m_target_prot_bytes + step)
                                                       ^^^^^^^^^^^^^^^^^^^
UnboundLocalError: cannot access local variable 'm_target_prot_bytes' where it is not associated with a value

Iteration 8: New subsample score 0.0 is not better than old score 0.909509, skipping
Iteration 9: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprym4vn5e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv7a86i_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4ems0udw.pickle

Iteration 9: Proposed new text for program: # Adaptive ARC-style Segmented Recency + Decayed, Size-aware Frequency (ARC-DF)
# Key ideas:
# - Two resident segments (like ARC):
#     * Window (seg=0): recency buffer for newly admitted items.
#     * Protected (seg=1): items that proved reuse (promoted on first hit).
# - Adaptive window share controlled by ghost history (B1/B2) as in ARC:
#     * B1: keys recently evicted from Window.
#     * B2: keys recently evicted from Protected.
#     * On insert, if key is in B1 -> increase window target (favor recency).
#       If key is in B2 -> decrease window target (favor frequency).
# - Protected eviction uses decayed, size-normalized frequency (like LRFU, per-byte gain).
# - Window eviction uses pure LRU (by last_access) to flush scans quickly.
# - Size-aware: all frequency gains are normalized by object size to protect small objects.
# - Decay half-life scales with cache size for stability.

# ----------------------------
# Metadata (module-global)
# ----------------------------
m_score = dict()         # key -> float, decayed frequency score
m_last_access = dict()   # key -> int, last access time (cache_snapshot.access_count)
m_seg = dict()           # key -> 0 (window), 1 (protected)

# Segment accounting (bytes)
m_bytes_window = 0
m_bytes_protected = 0

# ARC-style target for window segment (fraction of capacity bytes)
m_w_target = 0.25  # initial 25% capacity to recency window

# Ghost histories (key -> last_seen_time)
m_ghost_B1 = dict()  # recently evicted from window
m_ghost_B2 = dict()  # recently evicted from protected

# ----------------------------
# Tunables
# ----------------------------
# Frequency decay
HALFLIFE_MIN = 128          # base half-life in accesses (more responsive than before)
HALFLIFE_PER_ITEM = 4       # additional half-life per resident item

# Scoring
NEW_ITEM_BOOST = 0.25       # dampen admission score to reduce one-hit pollution

# Adaptation steps (ARC-style)
ETA_GHOST_UP = 0.08         # increase window target on B1 hit
ETA_GHOST_DOWN = 0.06       # decrease window target on B2 hit
W_MIN = 0.05                # lower bound on window target fraction
W_MAX = 0.85                # upper bound on window target fraction

# Ghost sizes
GHOST_MULTIPLIER = 4        # each ghost list capped at 4x current resident item count

# ----------------------------
# Helpers
# ----------------------------
def _half_life(cache_snapshot):
    n = max(1, len(cache_snapshot.cache))
    return max(HALFLIFE_MIN, HALFLIFE_PER_ITEM * n)

def _effective_score(key, now, half_life):
    base = m_score.get(key, 0.0)
    last = m_last_access.get(key, now)
    dt = now - last
    if dt <= 0:
        return base
    decay = pow(0.5, float(dt) / float(half_life))
    return base * decay

def _get_size(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _enforce_protected_budget(cache_snapshot):
    # Ensure bytes in protected <= capacity - target_window_bytes.
    global m_bytes_protected, m_bytes_window
    capacity = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    target_window_bytes = int(m_w_target * capacity)
    max_protected_bytes = max(0, capacity - target_window_bytes)

    if m_bytes_protected <= max_protected_bytes:
        return

    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    # Demote weakest protected items (lowest decayed score; tie-break by older last_access, then larger size)
    while m_bytes_protected > max_protected_bytes:
        weakest_key = None
        weakest_eff = None
        weakest_last = None
        weakest_size = None

        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k, 0) != 1:
                continue
            eff = _effective_score(k, now, hl)
            la = m_last_access.get(k, -1)
            sz = _get_size(v)

            if weakest_key is None:
                weakest_key, weakest_eff, weakest_last, weakest_size = k, eff, la, sz
                continue

            if eff < weakest_eff:
                weakest_key, weakest_eff, weakest_last, weakest_size = k, eff, la, sz
                continue

            if eff == weakest_eff:
                if la < weakest_last:
                    weakest_key, weakest_eff, weakest_last, weakest_size = k, eff, la, sz
                    continue
                if la == weakest_last:
                    if sz > weakest_size:
                        weakest_key, weakest_eff, weakest_last, weakest_size = k, eff, la, sz
                        continue
                    if sz == weakest_size and k < weakest_key:
                        weakest_key, weakest_eff, weakest_last, weakest_size = k, eff, la, sz

        if weakest_key is None:
            # No protected items found (inconsistency); fix counters and exit
            m_bytes_protected = 0
            break

        # Demote to window
        m_seg[weakest_key] = 0
        m_bytes_protected -= weakest_size
        m_bytes_window += weakest_size

def _trim_ghosts(limit_count):
    # Trim each ghost dict down to at most limit_count by removing oldest timestamps
    if limit_count <= 0:
        m_ghost_B1.clear()
        m_ghost_B2.clear()
        return

    if len(m_ghost_B1) > limit_count:
        # Remove oldest entries
        remove_k = sorted(m_ghost_B1.items(), key=lambda kv: kv[1])[: (len(m_ghost_B1) - limit_count)]
        for k, _ in remove_k:
            m_ghost_B1.pop(k, None)
    if len(m_ghost_B2) > limit_count:
        remove_k = sorted(m_ghost_B2.items(), key=lambda kv: kv[1])[: (len(m_ghost_B2) - limit_count)]
        for k, _ in remove_k:
            m_ghost_B2.pop(k, None)

def _window_lru_victim(cache_snapshot):
    # Choose LRU in window with tie-breakers: older last_access, lower score, larger size, lexicographic key
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    victim_key = None
    v_last = None
    v_eff = None
    v_size = None

    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k, 0) != 0:
            continue
        la = m_last_access.get(k, -1)
        eff = _effective_score(k, now, hl)
        sz = _get_size(v)

        if victim_key is None:
            victim_key, v_last, v_eff, v_size = k, la, eff, sz
            continue

        if la < v_last:
            victim_key, v_last, v_eff, v_size = k, la, eff, sz
            continue
        if la == v_last:
            if eff < v_eff:
                victim_key, v_last, v_eff, v_size = k, la, eff, sz
                continue
            if eff == v_eff:
                if sz > v_size:
                    victim_key, v_last, v_eff, v_size = k, la, eff, sz
                    continue
                if sz == v_size and k < victim_key:
                    victim_key, v_last, v_eff, v_size = k, la, eff, sz

    return victim_key

def _protected_weakest_victim(cache_snapshot):
    # Choose weakest in protected by effective score; ties: older last_access, larger size, lexicographic key
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)

    victim_key = None
    v_eff = None
    v_last = None
    v_size = None

    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k, 0) != 1:
            continue
        eff = _effective_score(k, now, hl)
        la = m_last_access.get(k, -1)
        sz = _get_size(v)

        if victim_key is None:
            victim_key, v_eff, v_last, v_size = k, eff, la, sz
            continue

        if eff < v_eff:
            victim_key, v_eff, v_last, v_size = k, eff, la, sz
            continue

        if eff == v_eff:
            if la < v_last:
                victim_key, v_eff, v_last, v_size = k, eff, la, sz
                continue
            if la == v_last:
                if sz > v_size:
                    victim_key, v_eff, v_last, v_size = k, eff, la, sz
                    continue
                if sz == v_size and k < victim_key:
                    victim_key, v_eff, v_last, v_size = k, eff, la, sz

    return victim_key

# ----------------------------
# Core API
# ----------------------------
def evict(cache_snapshot, obj):
    """
    Evict victim based on ARC-DF policy:
    - Prefer evicting from Window (LRU) if window bytes exceed the adaptive target.
    - Otherwise evict the weakest Protected item by decayed size-aware frequency.
    - Fall back to the other segment if the preferred one is empty.
    """
    if not cache_snapshot.cache:
        return None

    global m_bytes_window, m_bytes_protected

    capacity = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    target_window_bytes = int(m_w_target * capacity)

    # Primary choice: control window to target
    if m_bytes_window > target_window_bytes:
        victim = _window_lru_victim(cache_snapshot)
        if victim is not None:
            return victim

    # Otherwise evict from protected by weakest decayed score
    victim = _protected_weakest_victim(cache_snapshot)
    if victim is not None:
        return victim

    # If no protected items, evict LRU from window (last resort)
    return _window_lru_victim(cache_snapshot)

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Decay and then add size-normalized gain to frequency score.
    - Update last access time.
    - Promote Window -> Protected on first hit and adjust bytes counters.
    - Enforce protected budget if target decreased due to prior adaptations.
    """
    global m_bytes_window, m_bytes_protected

    key = obj.key
    now = cache_snapshot.access_count
    hl = _half_life(cache_snapshot)
    sz = _get_size(obj)

    # Update decayed score and last access
    base = _effective_score(key, now, hl)
    m_score[key] = base + 1.0 / float(sz)
    m_last_access[key] = now

    seg = m_seg.get(key, 0)
    if seg == 0:
        # Promote to protected on first reuse
        m_seg[key] = 1
        m_bytes_window = max(0, m_bytes_window - sz)
        m_bytes_protected += sz

        # Enforce protected budget in case target is tight
        _enforce_protected_budget(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Adjust window target using ghost histories (ARC adaptation).
    - Admit into Window with small size-normalized initial score.
    - Update last access.
    - Trim ghost lists to bounded size.
    """
    global m_w_target, m_bytes_window

    key = obj.key
    now = cache_snapshot.access_count
    sz = _get_size(obj)

    # ARC-style adaptation via ghosts: if key was seen in recent evictions
    if key in m_ghost_B1:
        # Miss for a recently evicted window item -> increase window share
        m_w_target = min(W_MAX, m_w_target + ETA_GHOST_UP)
        # Remove from ghost to prevent repeated strong nudges
        m_ghost_B1.pop(key, None)
    elif key in m_ghost_B2:
        # Miss for a recently evicted protected item -> decrease window share
        m_w_target = max(W_MIN, m_w_target - ETA_GHOST_DOWN)
        m_ghost_B2.pop(key, None)

    # Initialize metadata
    m_last_access[key] = now
    m_score[key] = NEW_ITEM_BOOST / float(sz)
    m_seg[key] = 0  # Window
    m_bytes_window += sz

    # Keep ghost lists bounded
    limit = GHOST_MULTIPLIER * max(1, len(cache_snapshot.cache))
    _trim_ghosts(limit)

    # Enforce protected budget in case target changed significantly
    _enforce_protected_budget(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Update segment byte counters.
    - Add evicted key to appropriate ghost list for future adaptation.
    - Clean metadata for the evicted key.
    - Enforce protected budget (target may have changed due to prior inserts).
    """
    global m_bytes_window, m_bytes_protected

    ekey = evicted_obj.key
    eseg = m_seg.get(ekey, 0)
    esize = _get_size(evicted_obj)

    # Update bytes accounting
    if eseg == 1:
        m_bytes_protected = max(0, m_bytes_protected - esize)
        # Ghost for protected evictions
        m_ghost_B2[ekey] = cache_snapshot.access_count
    else:
        m_bytes_window = max(0, m_bytes_window - esize)
        # Ghost for window evictions
        m_ghost_B1[ekey] = cache_snapshot.access_count

    # Clean metadata
    m_score.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)

    # Keep ghost lists bounded
    limit = GHOST_MULTIPLIER * max(1, len(cache_snapshot.cache))
    _trim_ghosts(limit)

    # Ensure protected segment respects its budget after eviction
    _enforce_protected_budget(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa1_u5_et.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuiwm0bzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj36omgb4.pickle

Iteration 9: New subsample score 0.994223 is not better than old score 1.0763289999999999, skipping
Iteration 10: Selected program 4 score: 0.23149604166666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnyvzzgqs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvo48n7dp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmputkxc7eb.pickle

Iteration 10: Proposed new text for program: # Adaptive SLRU with ARC-style ghost feedback (byte-aware)
# --------------------------------------------------------
# Design goals:
# - Two real segments, by bytes:
#     * Probation (segment 0): recency window for newly inserted or demoted items.
#     * Protected (segment 1): items that proved reuse (hit at least once in cache).
# - LRU within each segment for robust temporal locality capture.
# - Byte-aware: all targets and constraints are in bytes, so large objects are naturally
#   constrained and demotions/evictions free space effectively.
# - Adaptive split between segments using ARC-like ghost lists:
#     * B1 (ghost of probation) and B2 (ghost of protected), both tracked by LRU and bytes.
#     * When a miss inserts an item that appears in B1 (recent miss), increase the probation
#       target (favor recency). If it appears in B2 (frequent miss), decrease the probation
#       target (favor frequency).
#     * Ghost lists total bytes bounded by cache capacity (to avoid unbounded metadata).
# - Minimal metadata; no heavy frequency counters or global aging needed. This improves
#   stability across diverse workloads and mitigates scan pollution via the adaptive split.

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()     # key -> int last access time (LRU)
m_seg = dict()             # key -> 0 (probation) or 1 (protected)

# Segment byte counters
m_bytes_prob = 0           # bytes in probation
m_bytes_prot = 0           # bytes in protected

# Adaptive target for probation bytes (ARC's "p", in bytes)
m_target_prob_bytes = None  # lazily initialized to 20% of capacity

# Ghost caches (non-resident keys), ARC-style
# B1: recently evicted from probation; B2: recently evicted from protected
# Each maps key -> (size, last_access_time)
m_ghost_B1 = dict()
m_ghost_B2 = dict()
m_ghost_bytes_B1 = 0
m_ghost_bytes_B2 = 0


# ----------------------
# Helpers
# ----------------------
def _init_targets_if_needed(cache_snapshot):
    global m_target_prob_bytes
    if m_target_prob_bytes is None:
        # Start with a modest recency window: 20% of capacity by bytes
        cap = max(1, int(cache_snapshot.capacity))
        m_target_prob_bytes = int(0.20 * cap)


def _seg_of(key):
    """Return segment id: 0 (probation) or 1 (protected). Defaults to probation."""
    return m_seg.get(key, 0)


def _pick_lru_candidate(cache_snapshot, segment_id):
    """
    Choose LRU victim in a specified segment with deterministic tie-breaks.
    Secondary tie-break: prefer evicting larger object to free more space.
    Final tie-break: lexicographic key order.
    """
    lru_key = None
    lru_time = None
    lru_size = None

    for k, v in cache_snapshot.cache.items():
        if _seg_of(k) != segment_id:
            continue
        la = m_last_access.get(k, -1)
        sz = max(1, int(getattr(v, "size", 1)))

        if lru_key is None:
            lru_key, lru_time, lru_size = k, la, sz
            continue

        if la < lru_time:
            lru_key, lru_time, lru_size = k, la, sz
            continue

        if la == lru_time:
            # Evict larger item to free more space (helps convergence)
            if sz > lru_size or (sz == lru_size and k < lru_key):
                lru_key, lru_time, lru_size = k, la, sz

    return lru_key


def _rebalance_protected_by_target(cache_snapshot):
    """
    Keep protected bytes within its budget: capacity - m_target_prob_bytes.
    Demote LRU-protected items back to probation when protected exceeds budget.
    """
    global m_bytes_prot, m_bytes_prob
    _init_targets_if_needed(cache_snapshot)

    cap = max(1, int(cache_snapshot.capacity))
    prot_budget = max(0, cap - int(m_target_prob_bytes))

    if m_bytes_prot <= prot_budget:
        return

    # Demote LRU items from protected until within budget or none exist.
    while m_bytes_prot > prot_budget:
        # Find LRU in protected
        lru_key = None
        lru_time = None
        for k, v in cache_snapshot.cache.items():
            if _seg_of(k) != 1:
                continue
            la = m_last_access.get(k, -1)
            if lru_key is None or la < lru_time or (la == lru_time and k < lru_key):
                lru_key, lru_time = k, la

        if lru_key is None:
            break  # nothing to demote

        # Demote protected->probation
        sz = max(1, int(getattr(cache_snapshot.cache.get(lru_key, None), "size", 1)))
        m_seg[lru_key] = 0
        m_bytes_prot -= sz
        m_bytes_prob += sz


def _ghost_add(cache_snapshot, key, size, seg_id):
    """
    Add evicted key to appropriate ghost (B1 for probation, B2 for protected).
    Maintain total ghost bytes <= capacity. Evict from ghosts using ARC rule:
    if B1 bytes > target_prob_bytes -> evict from B1 else from B2.
    If chosen list is empty, evict oldest overall.
    """
    global m_ghost_bytes_B1, m_ghost_bytes_B2

    now = cache_snapshot.access_count
    sz = max(1, int(size))

    if seg_id == 0:
        m_ghost_B1[key] = (sz, now)
        m_ghost_bytes_B1 += sz
    else:
        m_ghost_B2[key] = (sz, now)
        m_ghost_bytes_B2 += sz

    # Enforce ghost capacity
    _init_targets_if_needed(cache_snapshot)
    cap = max(1, int(cache_snapshot.capacity))
    total_ghost = m_ghost_bytes_B1 + m_ghost_bytes_B2

    def evict_oldest_from(dct, is_b1):
        nonlocal m_ghost_bytes_B1, m_ghost_bytes_B2
        if not dct:
            return False
        # LRU in ghost dict
        oldest_key, oldest_la, oldest_sz = None, None, None
        for gk, (gsz, gla) in dct.items():
            if oldest_key is None or gla < oldest_la or (gla == oldest_la and gk < oldest_key):
                oldest_key, oldest_la, oldest_sz = gk, gla, gsz
        if oldest_key is not None:
            dct.pop(oldest_key, None)
            if is_b1:
                m_ghost_bytes_B1 = max(0, m_ghost_bytes_B1 - oldest_sz)
            else:
                m_ghost_bytes_B2 = max(0, m_ghost_bytes_B2 - oldest_sz)
            return True
        return False

    # Reduce until within capacity
    while total_ghost > cap:
        # Prefer ARC-style decision based on target
        chose = False
        if m_ghost_bytes_B1 > int(m_target_prob_bytes):
            chose = evict_oldest_from(m_ghost_B1, True)
        else:
            chose = evict_oldest_from(m_ghost_B2, False)

        # Fallback to evicting oldest overall if chosen list empty
        if not chose:
            # Find oldest among both
            oldest_set = None
            oldest_key, oldest_la, oldest_sz = None, None, None
            for is_b1, dct in ((True, m_ghost_B1), (False, m_ghost_B2)):
                for gk, (gsz, gla) in dct.items():
                    if oldest_key is None or gla < oldest_la or (gla == oldest_la and gk < oldest_key):
                        oldest_set = is_b1
                        oldest_key, oldest_la, oldest_sz = gk, gla, gsz
            if oldest_key is None:
                break
            if oldest_set:
                m_ghost_B1.pop(oldest_key, None)
                m_ghost_bytes_B1 = max(0, m_ghost_bytes_B1 - oldest_sz)
            else:
                m_ghost_B2.pop(oldest_key, None)
                m_ghost_bytes_B2 = max(0, m_ghost_bytes_B2 - oldest_sz)

        total_ghost = m_ghost_bytes_B1 + m_ghost_bytes_B2


def _adjust_target_on_ghost_hit(cache_snapshot, key, size):
    """
    ARC-like adaptive adjustment when a miss/insert hits a ghost:
    - If key in B1 (recent), increase probation target by object's size (favor recency).
    - If key in B2 (frequent), decrease probation target by object's size (favor frequency).
    Clamp to [0, capacity].
    Remove the key from the corresponding ghost list.
    """
    global m_target_prob_bytes, m_ghost_bytes_B1, m_ghost_bytes_B2

    _init_targets_if_needed(cache_snapshot)
    cap = max(1, int(cache_snapshot.capacity))
    sz = max(1, int(size))

    if key in m_ghost_B1:
        # Recent ghost hit -> more recency
        m_target_prob_bytes = min(cap, int(m_target_prob_bytes) + sz)
        gsz, _ = m_ghost_B1.pop(key, (0, 0))
        m_ghost_bytes_B1 = max(0, m_ghost_bytes_B1 - gsz)
    elif key in m_ghost_B2:
        # Frequent ghost hit -> more frequency
        m_target_prob_bytes = max(0, int(m_target_prob_bytes) - sz)
        gsz, _ = m_ghost_B2.pop(key, (0, 0))
        m_ghost_bytes_B2 = max(0, m_ghost_bytes_B2 - gsz)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict victim using adaptive SLRU:
    - Choose segment based on current probation bytes vs target (ARC-like):
        * If probation bytes exceed target, evict from probation.
        * Else evict from protected (unless it's empty).
      This balances recency vs frequency adaptively.
    - Within a segment, evict LRU; tie-break by larger size and then by key.
    """
    if not cache_snapshot.cache:
        return None

    _init_targets_if_needed(cache_snapshot)
    cap = max(1, int(cache_snapshot.capacity))

    # Decide segment for eviction
    evict_from_prob = False
    if m_bytes_prob == 0 and m_bytes_prot > 0:
        evict_from_prob = False
    elif m_bytes_prot == 0 and m_bytes_prob > 0:
        evict_from_prob = True
    else:
        # Prefer evicting from probation when it's above its target
        evict_from_prob = (m_bytes_prob > int(m_target_prob_bytes))

    segment_id = 0 if evict_from_prob else 1

    # Pick victim (LRU within chosen segment)
    victim = _pick_lru_candidate(cache_snapshot, segment_id=segment_id)

    # Fallback: if chosen segment empty (can happen at boundaries), try the other
    if victim is None:
        other_seg = 1 - segment_id
        victim = _pick_lru_candidate(cache_snapshot, segment_id=other_seg)

    return victim


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update last access time.
    - Promote probation -> protected on first (or subsequent) hit.
    - Rebalance protected to respect its budget (capacity - target_prob_bytes).
    """
    global m_bytes_prob, m_bytes_prot

    _init_targets_if_needed(cache_snapshot)

    key = obj.key
    sz = max(1, int(obj.size))

    # Initialize if missing (defensive)
    if key not in m_seg:
        m_seg[key] = 0
        m_bytes_prob += sz

    m_last_access[key] = cache_snapshot.access_count

    # Promote on hit if currently in probation
    if m_seg[key] == 0:
        m_seg[key] = 1
        m_bytes_prob = max(0, m_bytes_prob - sz)
        m_bytes_prot += sz
        # Keep protected within its budget
        _rebalance_protected_by_target(cache_snapshot)
    else:
        # Already protected: just update recency; no size change
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss handling has already decided to admit):
    - Adjust adaptive split if the key is in a ghost list (ARC-style).
    - Place new object into probation, record last access.
    """
    global m_bytes_prob

    _init_targets_if_needed(cache_snapshot)

    key = obj.key
    sz = max(1, int(obj.size))

    # Adapt window/frequency split based on ghost hit
    _adjust_target_on_ghost_hit(cache_snapshot, key, sz)

    # Initialize as probation (recency window)
    m_seg[key] = 0
    m_last_access[key] = cache_snapshot.access_count
    m_bytes_prob += sz

    # No immediate rebalancing here; eviction path will make room when needed.


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record evicted key into the corresponding ghost list (B1 if from probation, B2 if from protected).
    - Adjust segment byte counters and remove all metadata for the evicted key.
    - Enforce ghost capacity budget.
    """
    global m_bytes_prob, m_bytes_prot

    ekey = evicted_obj.key
    esz = max(1, int(getattr(evicted_obj, "size", 1)))
    seg = m_seg.get(ekey, 0)

    # Add to ghost
    _ghost_add(cache_snapshot, ekey, esz, seg)

    # Adjust segment bytes
    if seg == 0:
        m_bytes_prob = max(0, m_bytes_prob - esz)
    else:
        m_bytes_prot = max(0, m_bytes_prot - esz)

    # Clean metadata
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 150
    nonlocal m_ghost_bytes_B1, m_ghost_bytes_B2
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: no binding for nonlocal 'm_ghost_bytes_B1' found

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 150
    nonlocal m_ghost_bytes_B1, m_ghost_bytes_B2
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: no binding for nonlocal 'm_ghost_bytes_B1' found

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 150
    nonlocal m_ghost_bytes_B1, m_ghost_bytes_B2
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
SyntaxError: no binding for nonlocal 'm_ghost_bytes_B1' found

Iteration 10: New subsample score 0.0 is not better than old score 0.167041, skipping
Iteration 11: Selected program 4 score: 0.23149604166666662
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7_bhvg0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpae8eu9sh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsldx0xj6.pickle

Iteration 11: Proposed new text for program: # Learning-based Size-Aware Reuse Predictor (LRB-lite)
# Core idea:
# - Predict each object's next reuse time using an exponential moving average (EWMA) of its
#   inter-arrival times (reuse intervals).
# - Evict the object with the farthest predicted next reuse time per byte (largest "eviction score").
# - Items with no observed reuse yet are treated as low value (highest score) and evicted first,
#   avoiding pollution by one-hit wonders.
# - Maintain a bounded ghost history to seed predictors when an object returns, improving warm-starts.
#
# Metadata maintained:
# - m_last_access[key]: last access time (via cache_snapshot.access_count).
# - m_mu[key]: EWMA of inter-arrival time for the key (prediction of reuse interval).
# - m_freq[key]: hit count for key.
# - m_global_mu: global EWMA of observed inter-arrival times across hits (for sensible defaults).
# - Ghost history: past m_mu (and lightweight stats) for recently evicted keys to accelerate relearning.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()     # key -> int (last access time)
m_mu = dict()              # key -> float (EWMA inter-arrival time)
m_freq = dict()            # key -> int (hit count)

# Global EWMA of inter-arrival time across all keys (used as a default baseline)
m_global_mu = 64.0         # initialized to a moderate value; updated online

# Ghost history for warm starts on re-insert
g_mu = dict()              # key -> float (last known EWMA)
g_last = dict()            # key -> int (last time tracked in ghost)
g_freq = dict()            # key -> int (historical hits, optional)

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25           # learning rate for per-key EWMA (0 < BETA <= 1)
GLOBAL_BETA = 0.02         # learning rate for global EWMA
SIZE_EXP = 0.9             # size exponent for size-awareness in eviction score (0..1; 1 = strong)
FREQ_DAMP = 0.5            # frequency dampening; higher reduces score more for frequent items
DEFAULT_MU_MULT = 3.0      # scale for default mu on cold insert without ghost
GHOST_LIMIT_MIN = 1024     # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 4.0   # ghost capacity  factor * live cache item count (by count, not bytes)

# A very large number to stand in for "infinite" predicted delay
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    # Bound ghost size by count: max(min, factor * live_count)
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # Drop oldest ghost entries until within capacity
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict by oldest g_last (LRU-like by ghost last seen time)
    # To avoid n^2 behavior, do a linear scan only when needed.
    # Remove up to 10% excess per trim attempt for amortization.
    to_remove = len(g_mu) - limit
    # Collect (key, time) and sort by time ascending
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)

def _predicted_delta(key, now):
    """
    Return predicted remaining time until next access (>= 0).
    If no predictor exists yet, return a very large value (treated as 'far in the future').
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    # Predicted next access time
    next_t = la + mu
    # If overdue (next_t <= now), treat as zero gap (we expect it soon/now)
    return max(0.0, float(next_t - now))

def _eviction_score(key, obj, now):
    """
    Score used for eviction (higher => evict sooner):
    score = predicted_delta / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq)))
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    freq = m_freq.get(key, 1)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1)))
    return delta / denom

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse a tempered version of it.
    - Else, use a conservative default scaled by global mu (to prefer eviction if needed).
    """
    global m_mu, m_last_access, m_freq

    # Seed mu from ghost if present (shrink slightly to adapt)
    if key in g_mu:
        m_mu[key] = max(1.0, 0.8 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        # Cold start: prefer not to pin cold objects; large default based on global mu
        # Larger DEFAULT_MU_MULT => more aggressive against one-hit wonders
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now

def _update_predictor_on_hit(key, now):
    """
    Update per-key and global predictors on hit using EWMA of inter-arrival times.
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        # Per-key EWMA update
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global EWMA update
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # No prior access recorded; initialize conservatively
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_ghost_on_evict(evicted_key, now):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the largest predicted 'eviction score':
    - Predict remaining time until next access using EWMA of inter-arrival times.
    - Normalize by a size term (size^SIZE_EXP) and a mild frequency dampener.
    - Break ties by older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            # Prefer evicting less recently used (older last access)
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                # Prefer evicting larger object (frees more space)
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time and last access time.
    - Increment frequency.
    - Update global EWMA for default baselining.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline structures exist
    if key not in m_last_access or key not in m_mu:
        # Cold hit (rare): initialize from ghost or defaults, then proceed
        _seed_predictor_on_insert(key, now)
        # Adjust frequency because insert logic sets it to 1
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use a conservative default (scaled global mu).
    - Set last access time to now and frequency to 1 (or ghost-derived).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history (mu, lightweight stats) for the evicted key.
    - Trim ghost store to a bounded size.
    - Remove per-key metadata of the evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Record ghost
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpisdvyxuy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp37q0gfzv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7go_6apz.pickle

Iteration 11: New subsample score 0.48776699999999995 is better than old score 0.384501. Continue to full eval and add to candidate pool.
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbu2e1zxn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprn8f9zm5.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj9j5329h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq09jtpwb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyzyhe4vc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi404_ldo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2cycptb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjnjrs8au.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0q0hu3m0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx5ntqaln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjgulvad6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsmgoyiyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeuvdwfm0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9wekxkyi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppgxneken.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgdd0xl33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppxfkmxcz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7qhhc5yq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc54o201m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvha7vaze.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfs39pecb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2bmqqvvl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxpbxii2c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk_3tuv7h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyvtszv7p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk_1_8slq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp09j4jy9f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuk_rysf2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkcedb387.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdzewort_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjmtphlbm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptwj0m09z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwklgjf9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpamanjbue.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptci07lb3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp76n31a6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_l2gw2vc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4gwtcxnd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpctluh7_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd2xib5ek.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2f_v6e1p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbljz2ome.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4m3gvakj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppo79ac9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz3y2q0uy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0xyc3vj9.pickle

Iteration 11: Full valset score for new program: 0.22137172916666667
Iteration 11: Full train_val score for new program: 0.22137172916666667
Iteration 11: Individual valset scores for new program: [0.0, 0.417173, 0.418459, 0.0, 0.432262, 0.426208, 0.270335, 0.367086, 0.464069, 0.531017, 0.116667, 0.214032, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.24198, 0.423795, 0.0244, 0.059961, 0.059961, 0.26991, 0.105847, 0.658391, 0.852644, 0.177598, 0.047727, 0.072893, 0.040604, 0.07805, 0.65776, 0.098684, 0.157988, 0.182643, 0.590933, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.133987]
Iteration 11: New valset pareto front scores: [0.476754, 0.449101, 0.460033, 0.417445, 0.47767, 0.459472, 0.271531, 0.498624, 0.540222, 0.531017, 0.116667, 0.361012, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.272227, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.832175, 0.890467, 0.177598, 0.047727, 0.072893, 0.040604, 0.07805, 0.754595, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.133987]
Iteration 11: Full valset pareto front score: 0.26537872916666666
Iteration 11: Updated valset pareto front programs: [{4}, {3}, {3}, {3}, {4}, {2}, {2, 4}, {0, 1, 4, 5}, {5}, {0, 1, 2, 3, 4, 5, 6}, {6}, {3}, {6}, {0, 1, 2, 3, 4, 5, 6}, {6}, {6}, {6}, {6}, {6}, {2, 3, 4, 5}, {6}, {2, 4}, {6}, {6}, {2}, {3}, {3}, {3}, {6}, {5, 6}, {6}, {6}, {6}, {4}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6}, {6}, {6}, {6}, {6}, {3, 5, 6}, {6}, {6}, {2, 3, 4, 5, 6}, {6}]
Iteration 11: Best valset aggregate score so far: 0.23314141666666657
Iteration 11: Best program as per aggregate score on train_val: 5
Iteration 11: Best program as per aggregate score on valset: 5
Iteration 11: Best score on valset: 0.23314141666666657
Iteration 11: Best score on train_val: 0.23314141666666657
Iteration 11: Linear pareto front program index: 5
Iteration 11: New program candidate index: 6
Iteration 12: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7hhrgwl9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5v9q7hmv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpssckt_7j.pickle

Iteration 12: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with lightweight warm-start ghosts
# Core idea:
# - Each cached object k maintains a priority H_k = L + w(freq_k) / size_k^SIZE_EXP.
# - L is a global "age" that increases to the H of each eviction victim.
# - On a hit, increase the object's frequency and refresh its H using current L.
# - On insert, initialize H using L and a warm-started frequency from a small ghost history.
# - Evict the object with the smallest H (ties broken by older last access, then larger size).
# This policy is strongly size-aware and naturally ages entries without explicit time decay.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_freq = dict()          # key -> int (observed hit count in cache lifetime)
m_H = dict()             # key -> float (current GDSF priority)
g_L = 0.0                # global age

# Lightweight ghost history for warm-starts (by count, not bytes)
g_freq = dict()          # key -> int (historical frequency)
g_last = dict()          # key -> int (last time tracked in ghost)

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0           # strength of size-awareness (1.0 = divide by size)
FREQ_EXP = 0.5           # dampening exponent for frequency weight (sqrt if 0.5)
GHOST_LIMIT_MIN = 1024   # min ghost capacity by entry count
GHOST_LIMIT_FACTOR = 4.0 # ghost capacity ~= factor * live cache item count
GHOST_FREQ_DECAY = 0.5   # decay applied to ghost frequency when warming

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_freq) <= limit:
        return
    # Evict by oldest last seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_freq) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_freq.pop(k, None)
        g_last.pop(k, None)

def _weight(freq):
    # Frequency weight with dampening to avoid pinning
    # Ensures at least 1.0 for first-time entries.
    return max(1.0, float(freq) ** float(FREQ_EXP))

def _priority_from(freq, size, L):
    # H = L + w(freq)/size^SIZE_EXP
    denom = float(max(1.0, size) ** float(SIZE_EXP))
    return float(L) + (_weight(freq) / denom)

def _seed_from_ghost(key):
    # Warm-start frequency from ghost, decayed to avoid stale pinning
    if key in g_freq:
        gf = max(1, int(math.floor(GHOST_FREQ_DECAY * float(g_freq[key]))))
        return gf
    return 1

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest GDSF priority H:
      H_k = L + w(freq_k)/size_k^SIZE_EXP
    Tie-breakers:
      - Older last access first (LRU within equal priorities)
      - Larger size (frees more space)
      - Lexicographic key for determinism
    """
    global g_L
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    victim_key = None
    victim_H = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        sz = _size_of(v)
        Hk = m_H.get(k)
        if Hk is None:
            # Uninitialized entry: give a sensible default priority
            # Assume freq=1 for safe baseline
            Hk = _priority_from(1, sz, g_L)
            m_H[k] = Hk  # lazily initialize to prevent repeated recompute

        la = m_last_access.get(k, -1)

        if victim_key is None:
            victim_key, victim_H, victim_la, victim_sz = k, Hk, la, sz
            continue

        if Hk < victim_H:
            victim_key, victim_H, victim_la, victim_sz = k, Hk, la, sz
            continue

        if Hk == victim_H:
            # Prefer evicting less recently used
            if la < victim_la:
                victim_key, victim_H, victim_la, victim_sz = k, Hk, la, sz
                continue
            if la == victim_la:
                # Prefer evicting larger object
                if sz > victim_sz:
                    victim_key, victim_H, victim_la, victim_sz = k, Hk, la, sz
                    continue
                if sz == victim_sz and k < victim_key:
                    victim_key, victim_H, victim_la, victim_sz = k, Hk, la, sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment frequency.
    - Refresh priority H = L + w(freq)/size^SIZE_EXP using current L.
    - Update last access time.
    """
    global m_freq, m_H
    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    # Ensure freq exists
    f = m_freq.get(key, 0) + 1
    m_freq[key] = f

    # Update priority using current global age
    m_H[key] = _priority_from(f, sz, g_L)
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Warm-start frequency from ghost (decayed), else 1.
    - Initialize priority H = L + w(freq)/size^SIZE_EXP.
    - Set last access time.
    """
    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    f0 = _seed_from_ghost(key)
    m_freq[key] = f0
    m_H[key] = _priority_from(f0, sz, g_L)
    m_last_access[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Raise global age L to the evicted object's H (as in GreedyDual).
    - Record ghost frequency for warm-starts on future reinsert.
    - Trim ghost store.
    - Remove live metadata for the evicted key.
    """
    global g_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    eH = m_H.get(ekey, g_L)
    # GreedyDual aging step
    if eH > g_L:
        g_L = float(eH)

    # Record ghost info
    g_freq[ekey] = m_freq.get(ekey, 1)
    g_last[ekey] = now
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_H.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph6ft2c7j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_j9ygl6k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppe_a8e4d.pickle

Iteration 12: New subsample score 0.6177579999999999 is not better than old score 0.646104, skipping
Iteration 13: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb2bhrw1s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp0_my2rz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5bbqotec.pickle

Iteration 13: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with recency inflation
# - Size-aware: prioritizes objects by (1 / size), favoring small items.
# - Recency through inflation: a global inflation value L is set to the priority
#   of evicted items; refreshing an item sets its priority relative to current L,
#   naturally aging stale items.
# - Frequency-aware: hits add another (1 / size) to the object's priority, so
#   frequent items climb faster and survive longer.
# - Deterministic tie-breaking: older last-access first, then larger size, then key.

# Metadata
m_prio = dict()        # key -> float, GDSF priority H(k)
m_last_access = dict() # key -> int, last access time (access_count)

# Global inflation (recency) value
g_L = 0.0

# Tunable
NEW_ITEM_WEIGHT = 1.0  # admission weight per-size (1.0 is standard GDSF); lower to be stricter on new items


def _priority_now(key):
    """Return current stored priority for key; default to current inflation g_L."""
    return m_prio.get(key, g_L)


def evict(cache_snapshot, obj):
    """
    Evict the victim with the smallest priority H.
    Tie-breakers among equal H:
      1) older last access (LRU among equals)
      2) larger size (free more space and protect small objs)
      3) lexicographical key (determinism)
    """
    if not cache_snapshot.cache:
        return None

    cand_key = None
    cand_pr = None
    cand_last = None
    cand_size = None

    for k, v in cache_snapshot.cache.items():
        pr = _priority_now(k)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if cand_key is None:
            cand_key, cand_pr, cand_last, cand_size = k, pr, la, sz
            continue

        if pr < cand_pr:
            cand_key, cand_pr, cand_last, cand_size = k, pr, la, sz
            continue

        if pr == cand_pr:
            if la < cand_last:
                cand_key, cand_pr, cand_last, cand_size = k, pr, la, sz
                continue
            if la == cand_last:
                if sz > cand_size:
                    cand_key, cand_pr, cand_last, cand_size = k, pr, la, sz
                    continue
                if sz == cand_size and k < cand_key:
                    cand_key, cand_pr, cand_last, cand_size = k, pr, la, sz

    return cand_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Refresh priority to be at least current inflation g_L, then add (1 / size).
    - Update last access timestamp.
    """
    global m_prio, m_last_access

    key = obj.key
    now = cache_snapshot.access_count
    sz = max(1, int(getattr(obj, "size", 1)))

    base = _priority_now(key)
    if base < g_L:
        base = g_L

    # Frequency gain normalized by size
    m_prio[key] = base + (1.0 / float(sz))
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Initialize last access.
    - Set priority to current inflation g_L plus a per-size admission weight.
    """
    key = obj.key
    now = cache_snapshot.access_count
    sz = max(1, int(getattr(obj, "size", 1)))

    m_last_access[key] = now
    m_prio[key] = _priority_now(key)  # default to g_L if absent
    # Admission weight (size-aware)
    m_prio[key] = max(m_prio[key], g_L) + NEW_ITEM_WEIGHT * (1.0 / float(sz))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Raise inflation g_L to the evicted object's priority (recency inflation).
    - Clean metadata of evicted object.
    """
    global g_L, m_prio, m_last_access

    ekey = evicted_obj.key
    ev_pr = m_prio.get(ekey, g_L)

    # Recency inflation: future refreshes are relative to this threshold
    if ev_pr > g_L:
        g_L = ev_pr

    # Clean metadata of evicted
    m_prio.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp71dzbjy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcpwsueee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf71c600k.pickle

Iteration 13: New subsample score 0.63199 is not better than old score 0.6760630000000001, skipping
Iteration 14: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqqnjce_f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbkjpxllh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfw76q0w4.pickle

Iteration 14: Proposed new text for program: # GreedyDual-Size with First-Hit Promotion (GDSF+)
# - GreedyDual-Size priority (H) combines recency (via global inflation L) and size-awareness.
# - New items start with a small size-normalized reward added to the current inflation (L),
#   which naturally ages out scan noise as L rises.
# - Each hit adds a size-normalized reward to H; the first hit gets an extra promotion bonus
#   to strongly favor objects that demonstrate reuse at least once.
# - Victim is the item with the smallest H; ties break by older last access, then larger size.

# Metadata
m_H = dict()            # key -> float, current GreedyDual-Size priority
m_last_access = dict()  # key -> int, last access time (access_count)
m_hits = dict()         # key -> int, number of hits observed while resident (0 before first hit)

# Global inflation (recency aging)
m_L = 0.0               # float, monotonically non-decreasing inflation baseline

# Tunables
SIZE_EXPONENT = 1.0     # exponent for size normalization (1.0 = classic GDS/GDSF)
INSERT_REWARD = 0.2     # size-normalized reward on admission (smaller -> less pollution)
HIT_REWARD = 1.0        # size-normalized reward per hit
FIRST_HIT_BONUS = 1.0   # extra bonus on the first hit to reward reuse

def _norm(size):
    # Size-normalized weight: 1 / size^SIZE_EXPONENT
    sz = max(1, int(getattr(size, "size", size)))  # tolerate passing obj or raw size
    return 1.0 / float(sz ** SIZE_EXPONENT)

def evict(cache_snapshot, obj):
    """
    Evict the resident object with the smallest GreedyDual-Size priority H.
    Tie-breakers among equal H:
      1) older last access (favor more recent)
      2) larger size (free more space, protect compact objects)
      3) lexicographical key for determinism
    """
    if not cache_snapshot.cache:
        return None

    cand_key = None
    cand_H = None
    cand_last = None
    cand_size = None

    for k, v in cache_snapshot.cache.items():
        H = m_H.get(k, m_L)  # default to current inflation if missing
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)

        if cand_key is None:
            cand_key, cand_H, cand_last, cand_size = k, H, la, sz
            continue

        if H < cand_H:
            cand_key, cand_H, cand_last, cand_size = k, H, la, sz
            continue

        if H == cand_H:
            if la < cand_last:
                cand_key, cand_H, cand_last, cand_size = k, H, la, sz
                continue
            if la == cand_last:
                if sz > cand_size:
                    cand_key, cand_H, cand_last, cand_size = k, H, la, sz
                    continue
                if sz == cand_size and k < cand_key:
                    cand_key, cand_H, cand_last, cand_size = k, H, la, sz

    return cand_key

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Add size-normalized HIT_REWARD to the item's H.
    - On the first hit while resident, add an extra FIRST_HIT_BONUS.
    - Update last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count
    w = _norm(obj.size)

    # Initialize missing metadata defensively (should already exist for resident items)
    if key not in m_H:
        m_H[key] = m_L
    if key not in m_hits:
        m_hits[key] = 0

    # First-hit promotion bonus
    bonus = FIRST_HIT_BONUS if m_hits[key] == 0 else 0.0
    m_H[key] = m_H[key] + HIT_REWARD * w + bonus * w

    m_hits[key] += 1
    m_last_access[key] = now

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize H to current inflation L plus a small size-normalized INSERT_REWARD.
    - Initialize hit counter and last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count
    w = _norm(obj.size)

    m_H[key] = m_L + INSERT_REWARD * w
    m_hits[key] = 0
    m_last_access[key] = now

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Set global inflation L to the evicted item's H (classic GreedyDual update).
    - Clean all metadata for the evicted key.
    """
    global m_L
    ekey = evicted_obj.key
    eH = m_H.get(ekey, m_L)
    # GreedyDual inflation step
    if eH > m_L:
        m_L = eH

    # Clean metadata
    m_H.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphqtdsmtp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp51il4i7j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi1jckv39.pickle

Iteration 14: New subsample score 0.37516700000000003 is not better than old score 0.376457, skipping
Iteration 15: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp520l6tzr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp4jsul3r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbcwer232.pickle

Iteration 15: Proposed new text for program: # Windowed TinyLFU + Size-Aware SLRU (W-TinyLFU-SA)
# - Two logical segments by bytes:
#     * Window (probation, recency-biased LRU): ~20% of capacity by bytes.
#       New items are admitted here unconditionally (scan-resistant but allows warming).
#       Evictions from the window follow LRU with size-aware tie-breakers.
#     * Protected (frequency/size-aware): ~80% of capacity by bytes.
#       Items promoted here on first hit. Eviction priority:
#           H = Lp + freq / size^alpha   (GreedyDual-Size on protected)
#       Lp is the protected segment's clock advanced to the last evicted priority.
# - Frequency initialization via TinyLFU (Count-Min Sketch) with periodic decay.
# - Size-aware with exponent alpha in [0.6, 1.0] (default 0.8), balancing hits/byte vs not
#   overly penalizing moderately large objects.
# - Deterministic tie-breakers to ensure stable behavior.
#
# Expected improvements over prior GDSF-TLFU baseline:
#   * Warms up effectively (window recency) instead of evicting new items at baseline L.
#   * Better across diverse patterns; protected set favors small-hot items but still allows
#     larger items to stay if repeatedly accessed.
#   * Scan resistance via TinyLFU decay and GreedyDual clock aging in protected set.

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()       # key -> float, eviction priority H (protected segment only)
m_freq = dict()           # key -> small int, in-cache frequency (since (re)admission/promotion)
m_last_access = dict()    # key -> int, last access timestamp
m_seg = dict()            # key -> 0=window (probation), 1=protected
m_Lp = 0.0                # GreedyDual clock for protected segment

# Segment byte accounting (for choosing eviction source)
m_window_bytes = 0
m_protected_bytes = 0

# TinyLFU sketch (Count-Min Sketch)
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096       # power-of-two preferred
m_sketch = None           # list of SKETCH_DEPTH arrays of length SKETCH_WIDTH
m_sketch_ops = 0          # increments since last decay
SKETCH_DECAY_EVERY = 20000

m_inited = False          # sketch init guard

# --------------------
# Tunable parameters
# --------------------
WINDOW_RATIO = 0.20       # fraction of cache bytes targeted for window (probation)
SIZE_EXP = 0.80           # size penalty exponent (0.61.0). 0.8 balances better than 1.0
FREQ_CAP = 255            # cap per-key in-cache frequency
PROMOTION_BONUS = 1       # extra freq when promoting from window to protected
EPS = 1e-12               # numeric epsilon for ties

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _get_size(obj):
    return getattr(obj, "size", 1)

def _target_window_bytes(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    tw = int(cap * WINDOW_RATIO)
    return max(1, tw)

def _in_window(key):
    return m_seg.get(key, 0) == 0

def _in_protected(key):
    return m_seg.get(key, 0) == 1

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim:
      - Prefer evicting from window (probation) when it exceeds its byte budget.
      - Otherwise evict from protected using minimal priority H (GreedyDual-Size).
    Tie-breakers:
      * For window: LRU (oldest last_access), then larger size, then lexicographic key.
      * For protected: smallest H, then older last_access, then larger size, then lexicographic key.
    """
    if not cache_snapshot.cache:
        return None

    # Decide segment to evict from
    target_w = _target_window_bytes(cache_snapshot)
    have_window = False
    have_protected = False

    # We will scan once and track best candidates for each segment.
    # Window candidate (LRU):
    w_key = None
    w_la = None
    w_sz = None

    # Protected candidate (min priority):
    p_key = None
    p_pri = None
    p_la = None
    p_sz = None

    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _get_size(v)
        if _in_window(k):
            have_window = True
            if w_key is None:
                w_key, w_la, w_sz = k, la, sz
            else:
                if la < w_la:
                    w_key, w_la, w_sz = k, la, sz
                elif la == w_la:
                    if sz > w_sz:
                        w_key, w_la, w_sz = k, la, sz
                    elif sz == w_sz and k < w_key:
                        w_key, w_la, w_sz = k, la, sz
        else:
            have_protected = True
            pri = m_priority.get(k, m_Lp)
            if p_key is None:
                p_key, p_pri, p_la, p_sz = k, pri, la, sz
            else:
                if pri < p_pri - EPS:
                    p_key, p_pri, p_la, p_sz = k, pri, la, sz
                elif abs(pri - p_pri) <= EPS:
                    if la < p_la:
                        p_key, p_pri, p_la, p_sz = k, pri, la, sz
                    elif la == p_la:
                        if sz > p_sz:
                            p_key, p_pri, p_la, p_sz = k, pri, la, sz
                        elif sz == p_sz and k < p_key:
                            p_key, p_pri, p_la, p_sz = k, pri, la, sz

    # If window is over budget or protected is empty, evict from window; else from protected.
    evict_from_window = False
    if have_window:
        if (m_window_bytes > _target_window_bytes(cache_snapshot)) or not have_protected:
            evict_from_window = True

    if evict_from_window and w_key is not None:
        return w_key
    elif p_key is not None:
        return p_key
    # Fallbacks
    if w_key is not None:
        return w_key
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - If in window: promote to protected (once-hot). Initialize freq using sketch and bonus.
      - If in protected: increase per-key freq (capped) and re-anchor priority.
      - Update last access time.
    """
    global m_window_bytes, m_protected_bytes

    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = _get_size(obj)
    denom = _denom(sz)

    if _in_window(key):
        # Promote to protected on first hit
        est = _sketch_estimate(key)
        # Initialize/promote frequency with both prior (if any) and a small bonus
        f = m_freq.get(key, 0)
        # Give at least 1 + bonus; also leverage TinyLFU estimate mildly
        base = max(1, f)
        f_new = min(FREQ_CAP, base + PROMOTION_BONUS + min(3, est >> 1))
        m_freq[key] = f_new
        m_priority[key] = float(m_Lp) + float(f_new) / denom

        # Move bytes from window to protected
        m_seg[key] = 1
        m_window_bytes = max(0, m_window_bytes - sz)
        m_protected_bytes += sz
    else:
        # Stay in protected: increment frequency and rejuvenate priority
        f = m_freq.get(key, 0)
        if f < FREQ_CAP:
            f += 1
        m_freq[key] = f
        m_priority[key] = float(m_Lp) + float(f) / denom

    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch to record this access.
      - Admit into window (probation) unconditionally to capture recency.
      - Initialize minimal metadata; freq starts at 0 (window managed by LRU).
    """
    global m_window_bytes

    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = _get_size(obj)

    # Window admission
    m_seg[key] = 0  # window
    m_last_access[key] = now
    m_freq[key] = 0
    # Priority only used for protected; set baseline for completeness
    m_priority[key] = 0.0

    m_window_bytes += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - If the evicted item was in protected, advance protected clock Lp to its priority.
      - Adjust segment byte accounting.
      - Purge all metadata for evicted key.
    """
    global m_Lp, m_window_bytes, m_protected_bytes

    ekey = evicted_obj.key
    eseg = m_seg.get(ekey, 0)
    esize = _get_size(evicted_obj)

    if eseg == 1:
        ev_pri = m_priority.get(ekey, m_Lp)
        if ev_pri > m_Lp:
            m_Lp = float(ev_pri)
        m_protected_bytes = max(0, m_protected_bytes - esize)
    else:
        m_window_bytes = max(0, m_window_bytes - esize)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5wu34oa0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3ewzcy6s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg12cnc9k.pickle

Iteration 15: New subsample score 0.298844 is not better than old score 0.369108, skipping
Iteration 16: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpco73t0kx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphn7_5tvr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv4_8ejby.pickle

Iteration 16: Proposed new text for program: # Hybrid Size-Aware SLRU with Lateness-Aware Reuse Prediction (HS-SLRU)
# Core ideas:
# 1) Two segments (probationary/protected) a la SLRU:
#    - New objects enter probationary; only objects with at least two touches move to protected.
#    - Evict from probationary first to avoid pollution by one-hit wonders.
# 2) Lateness-aware reuse prediction:
#    - Predict next reuse with EWMA of inter-arrival times (mu).
#    - If an item is "overdue" (now > last_access + mu) and still hasn't been reused,
#      we increase its eviction score (penalize) proportionally to its lateness.
# 3) Size-aware eviction:
#    - Larger objects are more likely to be evicted all else equal (multiply score by size^SIZE_EXP).
# 4) Frequency-aware dampening:
#    - Mildly reduce eviction scores for frequently hit items; stronger dampening for protected segment.
# 5) Ghost history:
#    - Preserve mu and lightweight stats for evicted keys to warm-start on re-insert.
#
# Result:
# - Robust on diverse traces, improves miss rate vs. pure EWMA predictors that can "pin" overdue items
#   and vs. size-agnostic recency policies.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (hit count, clipped)
m_seg = dict()           # key -> int (0=probationary, 1=protected)

# Global EWMA of inter-arrival time across all keys (for reasonable initialization)
m_global_mu = 32.0

# Ghost history for warm starts on re-insert
g_mu = dict()            # key -> float (last known EWMA)
g_last = dict()          # key -> int (last time tracked in ghost)
g_freq = dict()          # key -> int (historical hits, optional)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Reuse prediction
EWMA_BETA = 0.30         # per-key EWMA learning rate
GLOBAL_BETA = 0.01       # global EWMA learning rate
DEFAULT_MU_MULT = 1.5    # default mu scaling on cold insert (without ghost)

# Score composition
LATE_PENALTY = 2.0       # how strongly to penalize overdue items (overdue -> bigger score)
W_PRED = 1.0             # weight on predicted time-to-next-use (or lateness)
W_REC = 0.25             # weight on time-since-last-access (recency component)

# Size & frequency awareness
SIZE_EXP = 0.85          # stronger preference to evict larger items (score *= size^exp)
FREQ_DAMP = 0.50         # frequency dampening
PROTECTED_FREQ_MULT = 1.6  # extra dampening factor applied to protected items
FREQ_CLIP = 64           # cap on per-key frequency to avoid unbounded growth

# SLRU segmenting
PROTECTED_RATIO = 0.70   # fraction of live items to target in protected segment (by count)

# Ghost history sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# A very large number to stand in for "infinite" predicted delay
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict oldest by g_last
    to_remove = len(g_mu) - limit
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)

def _protected_target_count(cache_snapshot):
    live_cnt = len(getattr(cache_snapshot, "cache", {}) or {})
    return int(PROTECTED_RATIO * live_cnt)

def _maybe_demote_protected(cache_snapshot):
    # Keep protected segment near target size by demoting oldest protected back to probationary
    target = _protected_target_count(cache_snapshot)
    if target <= 0:
        return
    # Count protected
    prot_keys = [k for k, seg in m_seg.items() if seg == 1 and k in cache_snapshot.cache]
    if len(prot_keys) <= target:
        return
    # Demote the oldest protected
    oldest_k = None
    oldest_la = _INF
    for k in prot_keys:
        la = m_last_access.get(k, -1)
        if la < oldest_la:
            oldest_la = la
            oldest_k = k
    if oldest_k is not None:
        m_seg[oldest_k] = 0  # demote

def _seed_predictor_on_insert(key, now):
    # Initialize per-key metadata on insert (probationary)
    if key in g_mu:
        m_mu[key] = max(1.0, 0.90 * float(g_mu[key]))
        # Use a tempered historical freq, but do not exceed clip
        m_freq[key] = max(1, min(FREQ_CLIP, int(1 + 0.5 * g_freq.get(key, 1))))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1
    m_last_access[key] = now
    m_seg[key] = 0  # probationary

def _update_predictor_on_hit(key, now):
    # Update per-key EWMA and global EWMA of inter-arrival times
    global m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Missing last access: initialize conservatively
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    # Update last access and frequency (bounded)
    m_last_access[key] = now
    m_freq[key] = min(FREQ_CLIP, m_freq.get(key, 0) + 1)

def _record_ghost_on_evict(evicted_key, now):
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)

def _effective_delta_for_score(key, now):
    # Lateness-aware remaining time to next use (>= 0, larger => more evictable)
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF  # completely cold -> easy eviction
    next_t = la + mu
    raw = float(next_t - now)
    if raw >= 0.0:
        return raw
    # Overdue: penalize proportional to how late it is
    return LATE_PENALTY * (-raw) + 1e-9

def _eviction_score(key, obj, now):
    # Higher score => better eviction candidate
    # base = W_PRED * effective_delta + W_REC * time_since_last_access
    ed = _effective_delta_for_score(key, now)
    la = m_last_access.get(key, now)
    ts = max(0.0, float(now - la))
    base = W_PRED * ed + W_REC * ts

    # Size weighting: larger items get higher score (evict sooner)
    size_term = float(_size_of(obj)) ** SIZE_EXP

    # Frequency damping; stronger for protected items
    freq = m_freq.get(key, 1)
    freq_term = 1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1))
    if m_seg.get(key, 0) == 1:
        freq_term *= PROTECTED_FREQ_MULT

    return (base * size_term) / max(1e-9, freq_term)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose an eviction victim:
    - Prefer evicting from probationary segment.
    - Use lateness-aware reuse prediction + size and frequency adjustments.
    - Tie-break by older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Build candidate set: probationary first, fallback to protected
    probationary = []
    protected = []
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            protected.append(k)
        else:
            probationary.append(k)

    candidates = probationary if probationary else protected
    if not candidates:
        return None

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k in candidates:
        v = cache[k]
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            # Prefer evicting less recently used (older last access)
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                # Prefer evicting larger object
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update EWMA predictor and last access.
    - Increment freq (bounded).
    - Promote to protected on second touch (SLRU).
    - Keep protected segment near target size by demoting oldest protected if needed.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Cold hit: seed if missing metadata (rare)
    if key not in m_last_access or key not in m_mu or key not in m_seg:
        _seed_predictor_on_insert(key, now)
        # Adjust frequency because insert logic sets it to 1; for a hit, we want to count the touch
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)

    # SLRU promotion: move from probationary to protected on second touch
    if m_seg.get(key, 0) == 0 and m_freq.get(key, 0) >= 2:
        m_seg[key] = 1  # protected
        _maybe_demote_protected(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize metadata (probationary), warm-start from ghost if present.
    - Set last access = now, freq initialized.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)
    # Optionally keep protected size in check even on insert
    _maybe_demote_protected(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record ghost history (mu, freq).
    - Trim ghost to bounded size.
    - Clean live metadata for the evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Record ghost
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2r7pwuyh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp95zrnq0e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp63mruour.pickle

Iteration 16: New subsample score 1.3335210000000002 is better than old score 1.097989. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuf0d9qq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpipy7pzoe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp559um7gb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuzco2cq8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm0dmkzan.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpucrj34ol.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy6fsunm1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpec2sta7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp34oa0szv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzqwjnpc_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpri5xcmxa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjjw_wwkn.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb6f1c1zw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzqwbib3p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzt4ax3kn.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsmmbr2tq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpndi9ea8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6p7jsjmd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgx2brgtx.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqc4c52fh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphzk0opa7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjzsith4z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg68ok_cp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46r7flxj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdsg06umh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvpew34e8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnmdb_mhp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp750c0e1q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg0oe307a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9ye6pj_q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpem_on_bn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxn6rpxw3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplf21mqvx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp21oqjh4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyxel3gdk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf54cla65.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbz1mfulf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7hn_f1ln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwt8lbv75.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7j4z4mcy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8wlgta9o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp301909xu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4r8rgfra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzzl9ctsm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuxgrtvot.pickle

Iteration 16: New program is on the linear pareto front
Iteration 16: Full valset score for new program: 0.24589052083333332
Iteration 16: Full train_val score for new program: 0.24589052083333332
Iteration 16: Individual valset scores for new program: [0.474494, 0.457682, 0.460699, 0.404552, 0.472429, 0.465986, 0.267943, 0.498624, 0.536646, 0.531017, 0.133333, 0.373446, 0.0, 0.0, 0.020388, 0.020569, 0.0, 0.023475, 0.0225, 0.272227, 0.362832, 0.0, 0.058672, 0.058672, 0.26979, 0.277218, 0.853029, 0.882252, 0.139862, 0.038636, 0.045558, 0.007003, 0.020672, 0.754084, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.121153, 0.109346, 0.140429, 0.046053, 0.366667, 0.146985, 0.136325, 0.453988, 0.140523]
Iteration 16: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.271531, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.272227, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.047727, 0.072893, 0.040604, 0.07805, 0.754595, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.140523]
Iteration 16: Full valset pareto front score: 0.2668839583333334
Iteration 16: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {2, 4}, {0, 1, 4, 5, 7}, {5}, {0, 1, 2, 3, 4, 5, 6, 7}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7}, {6}, {6}, {6}, {6}, {6}, {2, 3, 4, 5, 7}, {6}, {2, 4}, {6}, {6}, {2}, {3}, {7}, {3}, {6}, {5, 6}, {6}, {6}, {6}, {4}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7}, {6}, {6}, {6}, {6}, {3, 5, 6, 7}, {6}, {6}, {2, 3, 4, 5, 6}, {7}]
Iteration 16: Best valset aggregate score so far: 0.24589052083333332
Iteration 16: Best program as per aggregate score on train_val: 7
Iteration 16: Best program as per aggregate score on valset: 7
Iteration 16: Best score on valset: 0.24589052083333332
Iteration 16: Best score on train_val: 0.24589052083333332
Iteration 16: Linear pareto front program index: 7
Iteration 16: New program candidate index: 7
Iteration 17: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyr726iwb.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgkhj_9e6.pickle

Iteration 17: Proposed new text for program: # Adaptive Size-Aware Reuse Predictor (ASAP)
# Key improvements over previous version:
# 1) Correctly handles "overdue" items whose predicted next access time has already passed:
#    they are now considered cold (high eviction score), rather than being artificially protected.
# 2) Ghost history is trimmed in O(1) amortized time using a deque with versioned entries
#    (replaces previous O(n log n) sorting that could cause timeouts on large traces).
# 3) Slightly more adaptive learning on hits: accelerates EWMA updates when large gaps are observed.
# 4) Simplified and stronger size-awareness (per-byte normalization) and slightly milder default seeding.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()     # key -> int (last access time)
m_mu = dict()              # key -> float (EWMA inter-arrival time)
m_freq = dict()            # key -> int (hit count)

# Global EWMA of inter-arrival time across all keys (used as a default baseline)
m_global_mu = 64.0

# Ghost history for warm starts on re-insert (bounded by count, O(1) trim)
g_mu = dict()              # key -> float (last known EWMA)
g_freq = dict()            # key -> int (historical hits, optional)
g_stamp = dict()           # key -> int (version to avoid duplicate entries issues)
g_order = deque()          # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30           # base learning rate for per-key EWMA (0 < BETA <= 1)
GLOBAL_BETA = 0.01         # learning rate for global EWMA
SIZE_EXP = 1.0             # size exponent for size-awareness (1.0 = per-byte normalization)
FREQ_DAMP = 0.6            # frequency dampening; higher reduces score more for frequent items
DEFAULT_MU_MULT = 2.0      # scale for default mu on cold insert without ghost
GHOST_LIMIT_MIN = 1024     # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 2.0   # ghost capacity  factor * live cache item count (by count, not bytes)
OVERDUE_MULT = 2.0         # multiplier for lateness when an item is overdue
_INF = 1e30                # very large number for "infinite" predicted delay


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    # Bound ghost size by count: max(min, factor * live_count)
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    # Pop oldest entries until within capacity
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        # Remove mapping only if this is the latest stamp for k
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _predicted_delta(key, now):
    """
    Return a non-negative measure of "time-to-next-access".
    If overdue (predicted time already passed), treat lateness as a strong signal to evict soon:
    delta = (now - next_t) * OVERDUE_MULT.
    If no predictor exists yet, return very large value.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)
    # Overdue: the more overdue, the larger the delta => higher eviction score
    return float(now - next_t) * float(OVERDUE_MULT)

def _eviction_score(key, obj, now):
    """
    Score used for eviction (higher => evict sooner):
    score = predicted_delta / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq)))
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    freq = m_freq.get(key, 1)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1)))
    return delta / denom

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse a tempered version of it.
    - Else, use a moderate default scaled by global mu.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.8 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1
    m_last_access[key] = now

def _update_predictor_on_hit(key, now):
    """
    Update per-key and global predictors on hit using EWMA of inter-arrival times.
    More adaptive than fixed-beta EWMA: accelerate learning when a large gap is observed.
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)

        # Adaptive beta: if observed gap >> prev_mu, increase learning rate to adapt faster
        ratio = gap / max(1.0, prev_mu)
        if ratio <= 2.0:
            beta = EWMA_BETA
        else:
            beta = min(0.9, EWMA_BETA * ratio)

        m_mu[key] = (1.0 - beta) * prev_mu + beta * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # No prior access recorded; initialize conservatively
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_ghost_on_evict(evicted_key, now):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append; real trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    # Versioned append to avoid O(n) removals or duplicates problems
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the largest predicted 'eviction score':
    - Predict time until next access using EWMA of inter-arrival times with overdue penalty.
    - Normalize by size^SIZE_EXP and a mild frequency dampener.
    - Break ties by older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    # Full scan selection (kept for quality; ghost trimming was the real hotspot and is now O(1))
    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            # Prefer evicting less recently used (older last access)
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                # Prefer evicting larger object (frees more space)
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time and last access time.
    - Increment frequency.
    - Update global EWMA for default baselining.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline structures exist (rare cold-hit case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Adjust frequency because insert logic sets it to 1; this access will increment below
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use a moderate default (scaled global mu).
    - Set last access time to now and frequency to 1 (or ghost-derived).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history (mu, lightweight stats) for the evicted key.
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key metadata of the evicted key.
    """
    ekey = evicted_obj.key

    # Record ghost and trim
    _record_ghost_on_evict(ekey, _now(cache_snapshot))
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp26tgof4j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkt2mi6bd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsu21svss.pickle

Iteration 17: New subsample score 0.681551 is better than old score 0.661926. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppv1i0_qo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplh7wgads.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpck_pyntq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfpirrop9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkhaqfqq4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyioc7lys.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpixm9cjg8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1xckkpuo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt2hrbmhl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl75n7ubv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpicqgno7y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjm7qzugq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvm2mip39.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe0ixc444.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzwgl2e8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsak06tfs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphjeswz9_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2xqocs8o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfuu1hw_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0667ewc9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3zuzzi7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmjbnpavt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmrttsws6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2tusyo9u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo4ej4_hz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwpawngnk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnvtuo9b0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbsusxbzl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsk3e6x2x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvrjuu56z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoja45pwl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv_07uw2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt08z7f49.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzu7zs6fm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzxsw4hkw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqmk_w7mb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphvt6zadj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphwwnw8g_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfly_dgi9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa8mlhzh8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphizt4wik.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpywzy_oc7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptnbxoi9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4x9nb597.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiurgt0ur.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsykbp16p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9k2etq6_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsjm7icy6.pickle

Iteration 17: Full valset score for new program: 0.23209699999999991
Iteration 17: Full train_val score for new program: 0.23209699999999991
Iteration 17: Individual valset scores for new program: [0.44958, 0.433341, 0.440882, 0.393083, 0.444884, 0.437968, 0.252392, 0.498624, 0.537361, 0.531017, 0.108333, 0.340586, 0.023893, 0.0, 0.018689, 0.018597, 0.018177, 0.021226, 0.020672, 0.263978, 0.379548, 0.024988, 0.057382, 0.057382, 0.269779, 0.331653, 0.853029, 0.878145, 0.042228, 0.061364, 0.045558, 0.000718, 0.000599, 0.743363, 0.070175, 0.060018, 0.012046, 0.613601, 0.121771, 0.039645, 0.051924, 0.038931, 0.059211, 0.366667, 0.026378, 0.051613, 0.466258, 0.163399]
Iteration 17: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.271531, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.272227, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.040604, 0.07805, 0.754595, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 17: Full valset pareto front score: 0.2676446458333334
Iteration 17: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {2, 4}, {0, 1, 4, 5, 7, 8}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8}, {6}, {6}, {6}, {6}, {6}, {2, 3, 4, 5, 7}, {6}, {2, 4}, {6}, {6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {6}, {6}, {4}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8}, {8}]
Iteration 17: Best valset aggregate score so far: 0.24589052083333332
Iteration 17: Best program as per aggregate score on train_val: 7
Iteration 17: Best program as per aggregate score on valset: 7
Iteration 17: Best score on valset: 0.24589052083333332
Iteration 17: Best score on train_val: 0.24589052083333332
Iteration 17: Linear pareto front program index: 7
Iteration 17: New program candidate index: 8
Iteration 18: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphrg8epse.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvzf6ylf7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprjymwihh.pickle

Iteration 18: Proposed new text for program: # Segmented W-TinyLFU + Size-aware GreedyDual (S-W-TinyGDSF)
# - Three logical segments:
#    * WINDOW (W): recency buffer (~20% capacity). New items land here. Evict by LRU.
#    * PROBATION (P): main-cache probation (frequency-aware). Evict by GreedyDual-Size.
#    * PROTECTED (Q): main-cache protected (frequency-aware). Evict last; also GreedyDual-Size.
# - TinyLFU sketch initializes frequencies and guides promotion from W -> P.
# - GreedyDual clock (L) applied to P/Q items only (not W) to resist scans and age staleness.
# - Size-aware via exponent alpha: priority H = L + freq / size^alpha (for P/Q).
# - On hit:
#    * W: bump freq, possibly promote to P if popular (by hits or TinyLFU estimate).
#    * P: promote to Q.
#    * Q: bump freq.
# - Evict:
#    * Respect segment budgets: if a segment is over its target, evict from it.
#    * Otherwise, evict from P first (lowest H), then W (LRU), then Q (lowest H).
# - Deterministic tie-breakers.

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()     # key -> float, GreedyDual priority H (only meaningful for P/Q)
m_freq = dict()         # key -> small int, in-cache frequency (hits since (re)admission)
m_last_access = dict()  # key -> int, last access time (for LRU and ties)
m_seg = dict()          # key -> 'W'|'P'|'Q' (window/probation/protected)
m_L = 0.0               # GreedyDual global clock (for P/Q)
m_inited = False        # sketch initialization guard

# TinyLFU sketch (Count-Min Sketch) for admission/initialization
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list of SKETCH_DEPTH arrays of length SKETCH_WIDTH
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 1.0             # size penalty exponent in [0.5, 1.0]; 1.0 = GDSF
FREQ_CAP = 255             # cap per-key in-cache frequency
NEW_ADMIT_THRESHOLD = 1    # TinyLFU doorkeeper threshold for initializing freq
NEW_ITEM_BIAS = 0.0        # extra boost on insert (usually 0.0)

WINDOW_FRAC = 0.20         # fraction of capacity for recency window (bytes)
PROTECTED_FRAC = 0.80      # fraction of main (non-window) for protected segment

PROMOTE_MIN_HITS_W = 2     # promote W -> P after this many in-cache hits
PROMOTE_EST_THRESHOLD = 2  # alternatively promote W -> P if TinyLFU estimate >= this

EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _now(cache_snapshot):
    return cache_snapshot.access_count

def _get_size(obj):
    return getattr(obj, "size", 1)

def _current_priority_for_main(key, obj):
    # For P/Q items only. Fallback as baseline L if missing.
    if key in m_priority:
        return m_priority[key]
    f = m_freq.get(key, 1)
    return float(m_L) + float(f) / _denom(_get_size(obj))

def _segment_of(key):
    return m_seg.get(key, 'W')

def _promote_to_probation(key, obj):
    # Move W -> P; initialize priority based on current freq and size
    m_seg[key] = 'P'
    denom = _denom(_get_size(obj))
    f = m_freq.get(key, 1)
    m_priority[key] = float(m_L) + float(f) / denom

def _promote_to_protected(key, obj):
    # Move P -> Q or W -> Q; priority as in GreedyDual
    m_seg[key] = 'Q'
    denom = _denom(_get_size(obj))
    f = m_freq.get(key, 1)
    m_priority[key] = float(m_L) + float(f) / denom

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict guided by segment budgets and priorities:
      1) If Window over budget -> evict oldest (LRU) from Window.
      2) Else if Probation over budget -> evict lowest H from Probation.
      3) Else if Protected over budget -> evict lowest H from Protected.
      4) Else: prefer evicting from Probation (lowest H), fallback to Window (LRU), then Protected (lowest H).
    Ties:
      - Older last access first (LRU),
      - If equal, larger size,
      - If equal, lexicographic key.
    """
    if not cache_snapshot.cache:
        return None

    capacity = max(1, int(cache_snapshot.capacity))
    window_target = int(WINDOW_FRAC * capacity)
    main_target = capacity - window_target
    protected_target = int(PROTECTED_FRAC * main_target)
    probation_target = max(0, main_target - protected_target)

    # Track bytes per segment and best victims
    bytes_W = 0
    bytes_P = 0
    bytes_Q = 0

    # Candidates: (key, priority, last_access, size)
    cand_W = None     # oldest in W
    cand_P = None     # lowest H in P
    cand_Q = None     # lowest H in Q

    for k, v in cache_snapshot.cache.items():
        seg = _segment_of(k)
        la = m_last_access.get(k, -1)
        sz = _get_size(v)

        if seg == 'W':
            bytes_W += sz
            # choose LRU in window
            if cand_W is None:
                cand_W = (k, 0.0, la, sz)
            else:
                _, _, la_w, sz_w = cand_W
                if la < la_w or (la == la_w and (sz > sz_w or (sz == sz_w and k < cand_W[0]))):
                    cand_W = (k, 0.0, la, sz)

        elif seg == 'P':
            bytes_P += sz
            pri = _current_priority_for_main(k, v)
            if cand_P is None:
                cand_P = (k, pri, la, sz)
            else:
                _, pri_p, la_p, sz_p = cand_P
                if pri < pri_p - EPS or (abs(pri - pri_p) <= EPS and (la < la_p or (la == la_p and (sz > sz_p or (sz == sz_p and k < cand_P[0]))))):
                    cand_P = (k, pri, la, sz)

        else:  # 'Q'
            bytes_Q += sz
            pri = _current_priority_for_main(k, v)
            if cand_Q is None:
                cand_Q = (k, pri, la, sz)
            else:
                _, pri_q, la_q, sz_q = cand_Q
                if pri < pri_q - EPS or (abs(pri - pri_q) <= EPS and (la < la_q or (la == la_q and (sz > sz_q or (sz == sz_q and k < cand_Q[0]))))):
                    cand_Q = (k, pri, la, sz)

    # Enforce segment budgets
    if bytes_W > window_target and cand_W is not None:
        return cand_W[0]
    if bytes_P > probation_target and cand_P is not None:
        return cand_P[0]
    if bytes_Q > protected_target and cand_Q is not None:
        return cand_Q[0]

    # Otherwise: typical SLRU discipline: evict from probation first, then window, then protected.
    if cand_P is not None:
        return cand_P[0]
    if cand_W is not None:
        return cand_W[0]
    if cand_Q is not None:
        return cand_Q[0]
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Increase per-key in-cache frequency (cap).
      - Update last access.
      - Segment transitions:
          * W: if freq >= PROMOTE_MIN_HITS_W or TinyLFU est >= PROMOTE_EST_THRESHOLD -> W->P.
          * P: promote to Q.
          * Q: stay, just refresh GreedyDual priority.
      - For P/Q, recompute GreedyDual H = L + freq / size^alpha.
    """
    global m_L
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = _now(cache_snapshot)
    sz = _get_size(obj)
    denom = _denom(sz)

    f = m_freq.get(key, 1)
    if f < FREQ_CAP:
        f += 1
    m_freq[key] = f
    m_last_access[key] = now

    seg = _segment_of(key)
    if seg == 'W':
        est = _sketch_estimate(key)
        if f >= PROMOTE_MIN_HITS_W or est >= PROMOTE_EST_THRESHOLD:
            _promote_to_probation(key, obj)
        # else remain in W; no GreedyDual priority needed yet.
        return

    if seg == 'P':
        # Probation -> Protected on hit
        _promote_to_protected(key, obj)
        return

    # seg == 'Q': update GreedyDual priority
    m_priority[key] = float(m_L) + float(m_freq.get(key, f)) / denom


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch for this access.
      - Initialize in-cache frequency using TinyLFU (light doorkeeper) with base 1.
      - Place item in the WINDOW segment (recency buffer).
      - Record last access.
      - Do not set GreedyDual priority yet (only for P/Q).
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = _now(cache_snapshot)

    est = _sketch_estimate(key)
    # Base 1 + dampened TinyLFU contribution (post-threshold), capped
    init_f_boost = max(0, int(est) - int(NEW_ADMIT_THRESHOLD))
    init_f = 1 + min(FREQ_CAP - 1, init_f_boost // 4)  # conservative boost

    m_seg[key] = 'W'
    m_freq[key] = max(1, min(FREQ_CAP, init_f))
    m_last_access[key] = now
    # No m_priority needed for Window; it will be set upon promotion to P/Q.


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - If evicted key was in P/Q, advance GreedyDual clock to its priority.
      - Clean all metadata (segment, priority, freq, last-access).
      - TinyLFU aging handled in sketch add ops.
    """
    global m_L
    ekey = evicted_obj.key
    seg = _segment_of(ekey)

    # Only advance L when evicting an item from GreedyDual-managed segments (P/Q)
    if seg in ('P', 'Q'):
        ev_pri = m_priority.get(ekey, m_L)
        if ev_pri > m_L:
            m_L = float(ev_pri)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp3ru1kmr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1pp2nlhe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvfq3ff2_.pickle

Iteration 18: New subsample score 0.843808 is better than old score 0.835264. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx4b30pms.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkea0bnti.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprqoj7np_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw11boe8n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg1hgvv0_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6st36n9b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46f4y0q6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp29bbeyyo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjc9gv_j5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx27g4tyz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpact8m2in.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd7z6_pwn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp16baz7e1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfci49zti.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjqim6jij.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyo15d9wz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprok139ka.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg0l7rhyv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsrvl4d51.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2gg78hjv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpouvfpnqp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpng4ritbd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl2a1lnor.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsu567o2c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf60u6psc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptjlp5zbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjyeoynn1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzkq5duju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3e4x0_58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_x1c8dlc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8xh6zvp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo6nvou3e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6j63g73u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp77imd3ei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnuqxta00.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_o1z4zld.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw_uia164.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsw24v1_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptlyquwzq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj3w3izzt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuubplkb7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnqioeb3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiaw607fy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0kh2xdrd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsr77dfxg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpomr5c_y6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4t7xyi_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_5183i31.pickle

Iteration 18: Full valset score for new program: 0.232186875
Iteration 18: Full train_val score for new program: 0.232186875
Iteration 18: Individual valset scores for new program: [0.472718, 0.445657, 0.454397, 0.390237, 0.475638, 0.456705, 0.271531, 0.498624, 0.536289, 0.531017, 0.075, 0.342362, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.365782, 0.026164, 0.058672, 0.058672, 0.313431, 0.271169, 0.773585, 0.887558, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.754765, 0.083333, 0.067961, 0.026022, 0.634209, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 18: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.271531, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.272227, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.040604, 0.07805, 0.754765, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 18: Full valset pareto front score: 0.2676481875000001
Iteration 18: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {9, 2, 4}, {0, 1, 4, 5, 7, 8, 9}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}, {6}, {6}, {6}, {6}, {6}, {2, 3, 4, 5, 7, 9}, {6}, {9, 2, 4}, {6}, {6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {6}, {6}, {9}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7, 9}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8, 9}, {8}]
Iteration 18: Best valset aggregate score so far: 0.24589052083333332
Iteration 18: Best program as per aggregate score on train_val: 7
Iteration 18: Best program as per aggregate score on valset: 7
Iteration 18: Best score on valset: 0.24589052083333332
Iteration 18: Best score on train_val: 0.24589052083333332
Iteration 18: Linear pareto front program index: 7
Iteration 18: New program candidate index: 9
Iteration 19: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5rvthsms.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5g6_vx_r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbi7q6noz.pickle

Iteration 19: Proposed new text for program: # Hybrid GreedyDual-Size with Learned Reuse (GD-SLR)
# Core ideas:
# - Maintain a GreedyDual "age" (L) and per-item credit H = L + base_value.
# - base_value reflects predicted near-future utility per byte, combining:
#     * A learned reuse predictor from EWMA inter-arrival times (shorter => more valuable).
#     * A mild frequency boost (dampened).
#     * Size-awareness via size^SIZE_EXP (larger objects get less credit).
# - Evict the object with the smallest H (traditional GreedyDual behavior).
#   On eviction, raise L to the victim's H. This provides strong scan-resistance and a recency window.
# - Items with no observed reuse yet are given effectively zero base_value (only recency credit L),
#   so they are evicted preferentially unless they get a hit, avoiding pollution by one-hit wonders.
# - Maintain a bounded ghost history to warm-start predictors on re-insert.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()     # key -> int (last access time)
m_mu = dict()              # key -> float (EWMA inter-arrival time)
m_freq = dict()            # key -> int (hit count)
m_H = dict()               # key -> float (GreedyDual credit H = L + base_value)

# GreedyDual global age (L) and last chosen victim credit (to update L after eviction)
g_L = 0.0
g_last_victim_H = None

# Global EWMA of inter-arrival time across all keys (used as a default baseline)
m_global_mu = 64.0

# Ghost history for warm starts on re-insert
g_mu = dict()              # key -> float (last known EWMA)
g_last = dict()            # key -> int (last time tracked in ghost)
g_freq = dict()            # key -> int (historical hits, optional)

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30           # per-key EWMA learning rate
GLOBAL_BETA = 0.02         # global EWMA learning rate
SIZE_EXP = 0.8             # size exponent for size-awareness (0..1; higher favors small items)
FREQ_DAMP = 0.7            # frequency dampening; higher => stronger freq boost
DEFAULT_MU_MULT = 8.0      # default mu scale for cold insert without ghost (large => scan protection)
GHOST_LIMIT_MIN = 1024     # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 4.0   # ghost capacity  factor * live cache item count (by count)

# A very large number to stand in for "infinite" predicted delay
_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    to_remove = len(g_mu) - limit
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)

def _predicted_delta(key, now):
    """
    Return predicted remaining time until next access (>= 0).
    If no predictor exists yet, return a very large value (treated as 'far in the future').
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    return max(0.0, float(next_t - now))

def _base_value(key, obj, now):
    """
    Compute the base utility per byte for GreedyDual credit:
      base = (1 / (1 + predicted_delta)) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq-1)))
    This is larger for:
      - shorter predicted reuse delay,
      - smaller objects,
      - more frequently hit objects (mildly boosted).
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    v = 1.0 / (1.0 + float(delta))  # in [~0, 1]
    freq = m_freq.get(key, 1)
    freq_term = 1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1))
    size_term = (sz ** SIZE_EXP)
    denom = size_term * freq_term
    # Guard against any numerical anomalies
    if denom <= 0.0:
        denom = 1.0
    return v / denom

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse a tempered version of it.
    - Else, use a conservative default scaled by global mu (prevents pollution).
    - Set last access and initialize frequency.
    """
    global m_mu, m_last_access, m_freq

    if key in g_mu:
        m_mu[key] = max(1.0, 0.8 * float(g_mu[key]))  # slightly optimistic reuse shrink
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now

def _update_predictor_on_hit(key, now):
    """
    Update per-key and global predictors on hit using EWMA of inter-arrival times.
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Update global baseline
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Should be rare; defensively initialize
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_ghost_on_evict(evicted_key, now):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)

def _refresh_credit(key, obj, now):
    """
    Recompute and store GreedyDual credit H = g_L + base_value for key.
    """
    global m_H, g_L
    base = _base_value(key, obj, now)
    m_H[key] = float(g_L + base)

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    GreedyDual selection: evict the object with the smallest credit H.
    Tie-breakers: older last access (LRU), then larger size (free more space), then lexicographic key.
    """
    global g_last_victim_H

    cache = cache_snapshot.cache
    if not cache:
        g_last_victim_H = None
        return None

    # Choose min H; credits are updated on insert and hits.
    best_key = None
    best_H = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        H = m_H.get(k, g_L)  # default to g_L if unseen (base_value ~ 0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_H, best_la, best_sz = k, H, la, sz
            continue

        if H < best_H:
            best_key, best_H, best_la, best_sz = k, H, la, sz
            continue

        if H == best_H:
            # Prefer evicting less recently used (older last access)
            if la < best_la:
                best_key, best_H, best_la, best_sz = k, H, la, sz
                continue
            if la == best_la:
                # Prefer evicting larger object (frees more space)
                if sz > best_sz:
                    best_key, best_H, best_la, best_sz = k, H, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_H, best_la, best_sz = k, H, la, sz

    # Remember chosen credit so update_after_evict can bump g_L correctly.
    g_last_victim_H = best_H
    return best_key

def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival times and last access time.
    - Increment frequency.
    - Update global EWMA.
    - Refresh GreedyDual credit H = g_L + base_value.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure predictor exists (rare cold-hit)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # adjust freq because seed sets it to 1; a hit should count as the next one
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)
    _refresh_credit(key, obj, now)

def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor from ghost or conservative default.
    - Set last access to now and initial frequency.
    - Initialize GreedyDual credit H = g_L + base_value.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _seed_predictor_on_insert(key, now)
    _refresh_credit(key, obj, now)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Raise GreedyDual age g_L to the evicted object's credit (classic GreedyDual step).
    - Record ghost history (mu, lightweight stats) for the evicted key and trim ghost.
    - Remove per-key live metadata of the evicted key.
    """
    global g_L, g_last_victim_H

    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Bump GreedyDual age to victim's credit
    if g_last_victim_H is not None:
        g_L = float(g_last_victim_H)
    g_last_victim_H = None

    # Record ghost and trim
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_H.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzq11c0ni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpikg1b9nk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx6mpdz2b.pickle

Iteration 19: New subsample score 0.862366 is better than old score 0.766079. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1hw73zfd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpivpt7r_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplib19asi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkat7nhng.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsrxwwdrk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5h7ys03t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvmfqhquu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmk6vu89q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9oeh93u_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7b7m0dnn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprl07maoj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr0fjdarc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppv7o8kjj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6rf8y35f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgd1ipfnp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp67a38i6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq8nh01bj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmocl4ut7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmjby78nn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbz4nm942.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp76ei6jp1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprhir3r47.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0lw0_pn9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptt_feo0i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfgg1cw5p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsh1l5sa1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2cnt62ri.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfcgpdu06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn1q5iyb1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy8fijgf9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp56edw_75.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbc3owgxi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpglbul61_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt98gt5z8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp57b1f3ag.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplu2zm7sg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_273pds8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpam325wxv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn8j5qdnv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmproixbehe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgd7f0idm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplzuq50x2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpna4kv3md.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxb9rh2v3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi8p7n67v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8_0zh0i8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp74vcvsod.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpubq6e2_k.pickle

Iteration 19: Full valset score for new program: 0.21846106250000005
Iteration 19: Full train_val score for new program: 0.21846106250000005
Iteration 19: Individual valset scores for new program: [0.465293, 0.444548, 0.447852, 0.414422, 0.458202, 0.451401, 0.261962, 0.498624, 0.524848, 0.531017, 0.05, 0.310835, 0.023893, 0.0, 0.020105, 0.020006, 0.019119, 0.02221, 0.021938, 0.263061, 0.351032, 0.02538, 0.057382, 0.057382, 0.269775, 0.272177, 0.76862, 0.867363, 0.020365, 0.036364, 0.038724, 7.2e-05, 3.6e-05, 0.727025, 0.072368, 0.061783, 0.009162, 0.598145, 0.125461, 0.023011, 0.021381, 0.022864, 0.042763, 0.25, 0.021041, 0.022721, 0.441718, 0.03268]
Iteration 19: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.271531, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.272227, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.040604, 0.07805, 0.754765, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 19: Full valset pareto front score: 0.2676481875000001
Iteration 19: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {9, 2, 4}, {0, 1, 4, 5, 7, 8, 9, 10}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10}, {6}, {6}, {6}, {6}, {6}, {2, 3, 4, 5, 7, 9}, {6}, {9, 2, 4}, {6}, {6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {6}, {6}, {9}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8, 9}, {8}]
Iteration 19: Best valset aggregate score so far: 0.24589052083333332
Iteration 19: Best program as per aggregate score on train_val: 7
Iteration 19: Best program as per aggregate score on valset: 7
Iteration 19: Best score on valset: 0.24589052083333332
Iteration 19: Best score on train_val: 0.24589052083333332
Iteration 19: Linear pareto front program index: 7
Iteration 19: New program candidate index: 10
Iteration 20: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxuyllheo.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpikarkf53.pickle

Iteration 20: Proposed new text for program: # Hybrid Size-Aware Reuse Predictor with Uncertainty and Time-Decayed Frequency (SRPU-TDF)
# Goals:
# - Reduce miss rate by predicting each keys next reuse and incorporating uncertainty
# - Protect multi-hit objects via time-decayed frequency and a mild recency effect
# - Be size-aware without over-penalizing large items
# - Keep runtime low (avoid timeouts), especially in ghost-history maintenance
#
# Key ideas:
# 1) Predict remaining time until next use using EWMA of inter-arrival time (mu) and uncertainty
#    via EWMA of squared inter-arrival (m2) -> std = sqrt(max(0, m2 - mu^2))
#    predicted_remaining = max(0, (mu + k*std) - age)
#    where k is higher for low-frequency items (risk-aversion toward uncertain items).
# 2) Eviction score = predicted_remaining / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(effective_freq)))
#    where effective_freq is lazily time-decayed to prevent stale frequency from dominating.
# 3) One-hit wonder filtering: cold inserts get large default mu; reinsert warm-starts from ghost.
# 4) Ghost history is bounded and maintained in O(1) via an OrderedDict (LRU-like) to avoid timeouts.

import math
from collections import OrderedDict

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA of inter-arrival)
m_m2 = dict()            # key -> float (EWMA of squared inter-arrival), for std
m_freq = dict()          # key -> float (time-decayed frequency "weight")
m_fepoch = dict()        # key -> int (epoch at which m_freq is stored)

# Global inter-arrival EWMA for sensible defaults
m_global_mu = 64.0

# Ghost history (warm-start on reinsert), bounded by count using LRU order
g_mu = dict()            # key -> float (last known mu)
g_m2 = dict()            # key -> float (last known m2)
g_freq = dict()          # key -> float (last known decayed freq)
g_od = OrderedDict()     # key -> None (LRU order for ghost entries)

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.20         # per-key EWMA beta for mu and m2
GLOBAL_BETA = 0.01       # global EWMA beta
SIZE_EXP = 0.75          # size awareness; higher -> stronger penalty to large items
FREQ_DAMP = 0.8          # strength of frequency term in score denom
DEFAULT_MU_MULT = 4.0    # default mu = DEFAULT_MU_MULT * global_mu on cold insert

# Uncertainty handling (k parameter for sigma)
K_SIGMA_BASE = 0.5
K_SIGMA_BOOST = 2.0      # larger impact for low-frequency items

# Time-decayed frequency settings
DECAY_INTERVAL = 4096    # accesses per epoch
DECAY_FACTOR = 0.5       # per-epoch multiplicative decay

# Ghost capacity (by count)
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Recency grace window (small): just-hit items get a slight protection multiplier
RECENCY_GRACE = 1        # accesses
RECENCY_MULT = 0.5       # multiply score by this if within grace

# Numerical guard
_INF = 1e30
_MIN_STD = 0.0
_MIN_MU = 1.0
_MAX_GLOBAL_MU = 1e6


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _epoch(now):
    return int(now // DECAY_INTERVAL)

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # Maintain ghost with O(1) trims using an OrderedDict as LRU.
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_od) > limit:
        old_key, _ = g_od.popitem(last=False)
        g_mu.pop(old_key, None)
        g_m2.pop(old_key, None)
        g_freq.pop(old_key, None)

def _ghost_touch(key):
    # Move key to MRU position
    if key in g_od:
        try:
            g_od.move_to_end(key, last=True)
        except Exception:
            # Fallback: delete and re-add
            g_od.pop(key, None)
            g_od[key] = None
    else:
        g_od[key] = None

def _record_ghost_on_evict(evicted_key, now):
    # Store compact predictor state to accelerate relearning on returns
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_m2[evicted_key] = float(m_m2.get(evicted_key, mu * mu))
    # Store a small frequency hint; do not store epoch here (decay will be applied on use)
    g_freq[evicted_key] = float(m_freq.get(evicted_key, 1.0))
    _ghost_touch(evicted_key)

def _global_mu_update(gap):
    global m_global_mu
    m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * float(gap)
    m_global_mu = min(_MAX_GLOBAL_MU, max(_MIN_MU, m_global_mu))

def _seed_predictor_on_insert(key, now):
    # Initialize predictor for a new key, warmed by ghost if present
    ep = _epoch(now)

    if key in g_mu:
        base_mu = max(_MIN_MU, 0.8 * float(g_mu[key]))
        base_m2 = max(base_mu * base_mu, 0.8 * float(g_m2.get(key, base_mu * base_mu)))
        # Use a small freq hint from ghost (will be decayed lazily)
        hint_f = max(1.0, float(g_freq.get(key, 1.0)))
        m_mu[key] = base_mu
        m_m2[key] = base_m2
        m_freq[key] = hint_f
        m_fepoch[key] = ep  # anchor ghost hint at current epoch
    else:
        # Cold start: large mu to avoid polluting cache with one-hit wonders
        base_mu = max(_MIN_MU, DEFAULT_MU_MULT * float(m_global_mu))
        m_mu[key] = base_mu
        m_m2[key] = base_mu * base_mu
        m_freq[key] = 1.0
        m_fepoch[key] = ep

    m_last_access[key] = now

def _decayed_freq(key, now):
    # Lazily compute decayed frequency for scoring
    ep = _epoch(now)
    f = float(m_freq.get(key, 0.0))
    kep = int(m_fepoch.get(key, ep))
    de = ep - kep
    if de > 0 and f > 0.0:
        f = f * (DECAY_FACTOR ** de)
    return f

def _bump_freq_on_hit(key, now):
    # Bring stored freq to current epoch and add one
    ep = _epoch(now)
    f = float(m_freq.get(key, 0.0))
    kep = int(m_fepoch.get(key, ep))
    de = ep - kep
    if de > 0 and f > 0.0:
        f = f * (DECAY_FACTOR ** de)
    f += 1.0
    m_freq[key] = f
    m_fepoch[key] = ep

def _update_predictor_on_hit(key, now):
    # Update EWMA predictors based on observed gap
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = float(m_mu.get(key, gap))
        prev_m2 = float(m_m2.get(key, prev_mu * prev_mu))
        new_mu = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        new_m2 = (1.0 - EWMA_BETA) * prev_m2 + EWMA_BETA * (gap * gap)
        m_mu[key] = max(_MIN_MU, new_mu)
        m_m2[key] = max(m_mu[key] * m_mu[key], new_m2)  # ensure m2 >= mu^2
        _global_mu_update(gap)
    else:
        # Initialize conservatively if somehow missing
        base_mu = max(_MIN_MU, DEFAULT_MU_MULT * float(m_global_mu))
        m_mu[key] = base_mu
        m_m2[key] = base_mu * base_mu
    m_last_access[key] = now

def _predicted_remaining_with_uncertainty(key, now):
    # predicted remaining time until next access (>= 0), incorporating uncertainty
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF

    age = max(0.0, float(now - la))
    m2 = float(m_m2.get(key, mu * mu))
    var = max(0.0, m2 - mu * mu)
    std = math.sqrt(var) if var > 0.0 else _MIN_STD

    # Risk-aversion towards uncertain low-frequency items
    eff_f = _decayed_freq(key, now)
    k = K_SIGMA_BASE + K_SIGMA_BOOST / (1.0 + eff_f)

    return max(0.0, (float(mu) + k * std) - age)

def _eviction_score(key, obj, now):
    # Core eviction score: larger -> evict sooner
    sz = _size_of(obj)
    delta = _predicted_remaining_with_uncertainty(key, now)

    # Mild recency grace immediately after a hit
    la = m_last_access.get(key, now)
    if (now - la) <= RECENCY_GRACE:
        delta *= RECENCY_MULT

    eff_f = _decayed_freq(key, now)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0.0, eff_f)))
    return delta / denom if denom > 0 else _INF


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the largest eviction score:
    score = predicted_remaining_with_uncertainty / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(effective_freq)))
    Tie-breakers: older last access, larger size, lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    # Full scan for accuracy; ghost maintenance has O(1) trim to avoid timeouts
    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update EWMA(mu), EWMA(m2), last access.
    - Update global mu.
    - Lazily decayed frequency is normalized to current epoch and incremented.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Handle rare case of missing metadata (e.g., after resets)
    if key not in m_last_access or key not in m_mu or key not in m_m2:
        _seed_predictor_on_insert(key, now)
        # Remove the "+1" freq given by seed to correctly apply hit increment below
        m_freq[key] = max(0.0, m_freq.get(key, 1.0) - 1.0)

    _update_predictor_on_hit(key, now)
    _bump_freq_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Initialize predictor from ghost if available, otherwise from conservative default.
    - Set last access to now and frequency to 1 (decayed scheme).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record ghost history (mu, m2, decayed freq) and maintain LRU-bounded ghost store.
    - Clean live metadata for the evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Record ghost (O(1) maintenance)
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_m2.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_fepoch.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_ai7d661.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpki0vkg_7.pickle

Iteration 20: New subsample score 0.724648 is not better than old score 0.760638, skipping
Iteration 21: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_qrhza5z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_w4fu5_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplofe4zab.pickle

Iteration 21: Proposed new text for program: # Adaptive W-TinyLFU with Segmented LRU (AW-TLFU)
# - Three segments (by bytes):
#   * Window (W): short-term recency buffer for new items (always admit here).
#   * Probation (P): main area for items that left the window at least once.
#   * Protected (S): items with at least two hits; demoted back to P by LRU when S exceeds target.
# - Admission and eviction approximate Caffeine's W-TinyLFU:
#   * On a predicted W overflow at insertion, choose to evict P's LRU (admit) or W's LRU (reject)
#     based on TinyLFU estimate normalized by size (est/size).
# - Window size is adapted online (ARC-like) using small "ghost" histories:
#   * If a reinsert hits a W-ghost, increase W fraction.
#   * If a reinsert hits a Main-ghost (P/S), decrease W fraction.
# - TinyLFU Count-Min sketch provides global frequency estimates with periodic aging.
# - Size-aware: the frequency score is normalized by object size to prefer keeping small, frequent items.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Adaptive window sizing (ARC-inspired, but by fraction of bytes)
m_W_frac = 0.20         # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80           # 80% of main in protected
ADMIT_EPS = 1e-12       # epsilon for float comparisons
EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    # Compute target bytes for Window and Protected using adaptive window fraction
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)  # non-zero window
    main_target = max(0, cap - w_target)
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _score_for_key(cache_snapshot, key):
    """
    Size-aware frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate / size
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    return est / max(1.0, sz)

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return est / max(1.0, sz)

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision approximating W-TinyLFU admission:
      - If Window is above its target, evict from Window (W-LRU).
      - Else if the new insert would make Window exceed its target, run an admission
        comparison: evict P-LRU if new_score > score(P-LRU); otherwise evict W-LRU.
      - Else (no predicted W overflow), evict from Probation (P-LRU) if possible,
        else from Window; if both empty, evict from Protected as a last resort.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_sz = _size_of(obj)
    new_score = _score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_bytes_W + new_sz) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)

    if predict_overflow:
        if p_lru is None:
            # No P items to compare against -> evict from W if possible
            if w_lru is not None:
                return w_lru
            # Fall back further
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            if s_lru is not None:
                return s_lru
        else:
            # Admission comparison: new vs P-LRU
            p_score = _score_for_key(cache_snapshot, p_lru)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict from P; update_after_insert will move W-LRU to P
                return p_lru
            else:
                # Reject: evict from W
                if w_lru is not None:
                    return w_lru
                # If W empty, evict from P anyway
                return p_lru

    # No predicted W overflow: evict from P if possible, else W, else S.
    if p_lru is not None:
        return p_lru
    if w_lru is not None:
        return w_lru
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - Promote along W -> P -> S on each additional hit.
      - If S exceeds its byte target, demote S-LRU back to P.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        # First confirmation; graduate to P
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        # Second confirmation; promote to S
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency; structure unchanged
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Always insert into Window (recency admission).
      - Update sketch and recency.
      - Adapt window target using ghost histories:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - If Window exceeds its target, move its LRU to P (keeps W near target).
      - Keep S near its target by demoting its LRU to P when needed.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window
    sz = _size_of(obj)
    global m_bytes_W
    m_seg[key] = SEG_WINDOW
    m_bytes_W += sz

    # If Window is above target, move its LRU to Probation (does not change total size)
    w_target, _ = _targets(cache_snapshot)
    if m_bytes_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is not None:
            v = cache_snapshot.cache.get(candidate)
            if v is not None:
                csz = _size_of(v)
                _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppzn_70xr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplr4k82xl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwysqjwpb.pickle

Iteration 21: New subsample score 1.020122 is better than old score 0.947282. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvb9ry_3b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdbazngn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2ber93wx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx_xr6w0q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpssexxp6a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl5_l5s0o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpls4k2qe6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5kzwefw4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6x1c1f5_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpat509lmo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4mnlhvn9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_phtn2sp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi7hsi408.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpstk6xklb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzauhrlps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp88fay29e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphsqf3su4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpattzowjs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzozi1mm2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo__695fn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7rmg1w2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcnur8yk2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj5_ed6gv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7awlsto3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpar2ytve5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyoolfz6c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsj3s0nph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rn9tetc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphqz04khh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp54_x139e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkvl5wzhf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplgem2ppb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbk0nkko3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzc6h9jhk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpku5bpnr3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwf_3e34x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj88cdiyo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprus4obw6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyi8eg0xo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpja87_u85.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt9dw4tuf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx8fn2pew.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpha7zke7w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps4ku_fwz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt79ivvw_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpraxkwj7d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp582ty9v2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4c_1dq8u.pickle

Iteration 21: Full valset score for new program: 0.2409417083333333
Iteration 21: Full train_val score for new program: 0.2409417083333333
Iteration 21: Individual valset scores for new program: [0.467499, 0.445074, 0.450215, 0.413532, 0.459806, 0.454053, 0.278708, 0.498624, 0.532714, 0.531017, 0.108333, 0.325488, 0.040938, 0.0, 0.022087, 0.02226, 0.020601, 0.024178, 0.023766, 0.27681, 0.39528, 0.024792, 0.059961, 0.059961, 0.294792, 0.295363, 0.718967, 0.871128, 0.118898, 0.038636, 0.066059, 0.043992, 0.043968, 0.751191, 0.08114, 0.116505, 0.091088, 0.630603, 0.125461, 0.079568, 0.082468, 0.118956, 0.070724, 0.2, 0.106122, 0.113043, 0.466258, 0.104575]
Iteration 21: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.278708, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.27681, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.043992, 0.07805, 0.754765, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 21: Full valset pareto front score: 0.2679637708333334
Iteration 21: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}, {6}, {6}, {6}, {6}, {6}, {11}, {6}, {9, 2, 4}, {11, 6}, {11, 6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {11}, {6}, {9}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8, 9, 11}, {8}]
Iteration 21: Best valset aggregate score so far: 0.24589052083333332
Iteration 21: Best program as per aggregate score on train_val: 7
Iteration 21: Best program as per aggregate score on valset: 7
Iteration 21: Best score on valset: 0.24589052083333332
Iteration 21: Best score on train_val: 0.24589052083333332
Iteration 21: Linear pareto front program index: 7
Iteration 21: New program candidate index: 11
Iteration 22: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5ddhdnhp.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy6fkfvz1.pickle

Iteration 22: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with heap, ghost warm-start, and lazy invalidation
# Core idea:
# - Each cached object has a priority H = L + (freq^FREQ_EXP) / (size^SIZE_EXP)
#   where:
#     * L is the global "age" that increases to the priority of each eviction victim (GreedyDual aging).
#     * freq is the object's hit count (dampened by FREQ_EXP to prevent runaway).
#     * size penalizes large objects (SIZE_EXP controls strength).
# - Evict the object with the smallest H (lowest value density).
# - Maintain a min-heap of (H, version, key) for fast evictions; use lazy invalidation on updates.
# - Keep a bounded ghost history of past frequencies to seed new inserts (warm starts).
#
# Benefits:
# - Strong size-awareness (bytes-optimized) and frequency-aware.
# - Aging (L) automatically decays stale priorities without scanning all items.
# - O(log N) eviction selection via heap; avoids expensive full scans and timeouts on large traces.

import math
import heapq

# ----------------------
# Global metadata stores
# ----------------------
m_L = 0.0                 # Global age (GreedyDual L)
m_H = dict()              # key -> float (current priority H)
m_freq = dict()           # key -> int (hit count)
m_size = dict()           # key -> int (cached size in bytes)
m_last = dict()           # key -> int (last access time; tie-breakers/debug)
m_gen = dict()            # key -> int (version for lazy heap invalidation)
m_heap = []               # min-heap of (H, version, key)

# Track last eviction candidate to update L in update_after_evict
m_last_evicted_key = None
m_last_evicted_H = None

# Ghost history for warm starts on re-insert
g_freq = dict()           # key -> int (historical hits, damped)
g_time = dict()           # key -> int (last time inserted into ghost)
g_heap = []               # min-heap of (time, version, key) for ghost trimming
g_ver = dict()            # key -> int (ghost version for lazy invalidation)

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0            # size exponent (1 = strong size-awareness)
FREQ_EXP = 0.5            # frequency dampening exponent (0..1; 0.5 ~ sqrt)
GHOST_LIMIT_MIN = 4096    # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 4.0  # ghost capacity  factor * live cache item count
GHOST_DECAY = 0.85        # decay factor applied to store/seed historical freq
FREQ_SEED_CAP = 1000      # cap seeded freq from ghost to avoid runaway
MIN_SIZE = 1              # lower bound for size divisor

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(MIN_SIZE, int(getattr(obj, "size", MIN_SIZE)))

def _priority_from(freq, size):
    # Priority contribution beyond L
    return (float(freq) ** FREQ_EXP) / (float(size) ** SIZE_EXP)

def _heap_push(key, H):
    # Push into eviction heap with lazy invalidation
    ver = m_gen.get(key, 0) + 1
    m_gen[key] = ver
    heapq.heappush(m_heap, (float(H), ver, key))

def _ghost_push(key, t):
    ver = g_ver.get(key, 0) + 1
    g_ver[key] = ver
    g_time[key] = t
    heapq.heappush(g_heap, (int(t), ver, key))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    # Trim by oldest timestamp using heap with lazy invalidation
    while len(g_freq) > limit and g_heap:
        t, ver, k = heapq.heappop(g_heap)
        if g_ver.get(k, None) != ver:
            continue  # stale
        # Remove this ghost entry
        g_ver.pop(k, None)
        g_time.pop(k, None)
        g_freq.pop(k, None)

def _ensure_eviction_priority(cache_snapshot, key, obj_if_known=None):
    # Ensure m_H and heap contain a current entry for the key.
    # This is used as a fallback when the heap is empty or missing entries.
    if key in m_H:
        return m_H[key]
    size = _size_of(obj_if_known) if obj_if_known is not None else _size_of(cache_snapshot.cache.get(key))
    m_size[key] = size
    # Seed frequency conservatively if unknown; default to 1
    f = max(1, int(m_freq.get(key, 1)))
    H = m_L + _priority_from(f, size)
    m_H[key] = H
    _heap_push(key, H)
    return H

def _pick_victim_from_heap(cache_snapshot):
    # Pop until a valid top is found
    cache = cache_snapshot.cache
    while m_heap:
        H, ver, k = heapq.heappop(m_heap)
        if m_gen.get(k, None) != ver:
            continue  # stale
        if k not in cache:
            # No longer resident; drop metadata (lazy cleanup)
            m_H.pop(k, None)
            m_freq.pop(k, None)
            m_size.pop(k, None)
            m_last.pop(k, None)
            m_gen.pop(k, None)
            continue
        # Valid victim
        return k, H
    return None, None

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest GreedyDual-Size-Frequency priority H:
      H = L + (freq^FREQ_EXP) / (size^SIZE_EXP)
    Using a min-heap for O(log N) selection with lazy invalidation.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Try heap first
    key, H = _pick_victim_from_heap(cache_snapshot)
    if key is None:
        # Heap is empty or fully stale; rebuild minimal set on-the-fly
        # Compute minimal H among current cache entries
        min_key = None
        min_H = None
        for k, v in cache.items():
            Hk = _ensure_eviction_priority(cache_snapshot, k, v)
            if min_H is None or Hk < min_H or (Hk == min_H and m_last.get(k, 0) < m_last.get(min_key, 0)):
                min_key, min_H = k, Hk
        key, H = min_key, min_H

    # Record for update_after_evict to advance L
    global m_last_evicted_key, m_last_evicted_H
    m_last_evicted_key = key
    m_last_evicted_H = float(H) if H is not None else None
    return key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Increment frequency.
    - Recompute priority H = L + (freq^FREQ_EXP)/(size^SIZE_EXP).
    - Push updated entry into heap (lazy invalidation for old one).
    """
    key = obj.key
    size = _size_of(obj)
    now = _now(cache_snapshot)

    f = m_freq.get(key, 0) + 1
    m_freq[key] = f
    m_size[key] = size
    m_last[key] = now

    H = m_L + _priority_from(f, size)
    m_H[key] = H
    _heap_push(key, H)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Warm-start frequency from ghost (decayed), else 1.
    - Initialize priority H = L + (freq^FREQ_EXP)/(size^SIZE_EXP).
    - Push into heap.
    """
    key = obj.key
    size = _size_of(obj)
    now = _now(cache_snapshot)

    # Seed frequency from ghost with decay and cap
    gf = g_freq.get(key, 0)
    if gf > 0:
        seed_f = max(1, min(FREQ_SEED_CAP, int(math.ceil(GHOST_DECAY * gf))))
    else:
        seed_f = 1

    m_freq[key] = seed_f
    m_size[key] = size
    m_last[key] = now

    H = m_L + _priority_from(seed_f, size)
    m_H[key] = H
    _heap_push(key, H)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Advance global age L to the victim's H (GreedyDual aging).
    - Record ghost history with decayed frequency for warm starts.
    - Trim ghost to a bounded size.
    - Remove live metadata for the evicted key (heap uses lazy invalidation).
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Advance age L to the victim's priority
    global m_L, m_last_evicted_key, m_last_evicted_H
    if m_last_evicted_key == ekey and m_last_evicted_H is not None:
        m_L = max(m_L, float(m_last_evicted_H))
    else:
        # Fallback: use current H if available
        eh = m_H.get(ekey, None)
        if eh is not None:
            m_L = max(m_L, float(eh))

    # Record ghost: decayed frequency
    f = m_freq.get(ekey, 1)
    gf = max(1, int(math.ceil(GHOST_DECAY * f)))
    g_freq[ekey] = min(FREQ_SEED_CAP, gf)
    _ghost_push(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata for the evicted key
    m_H.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_size.pop(ekey, None)
    m_last.pop(ekey, None)
    m_gen.pop(ekey, None)

    # Clear last-evicted markers
    m_last_evicted_key = None
    m_last_evicted_H = None
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzuy__gi9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa2ujls3w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp7dtunuf.pickle

Iteration 22: New subsample score 0.47383099999999995 is better than old score 0.043962. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsp52k3br.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpraoirad7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdpmybjk9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpstq_7nm2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptb977vwm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv47kg0a_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpslju2cwv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2vcajo94.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps7bzf1pm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_tss_cp5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7a1nsrse.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7r8lkrmq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyq2ydeu1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptg9jjb29.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpukm_effh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcdwyu191.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1jt5lfy5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfs7qeipm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm6rgbgfy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpup30ly06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp430swg18.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnr9af_oi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa_lq3dpr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwb_7gmrg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd1e7ya42.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5au6bo56.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcc7dl8qx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2x9zbwq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmw7bfywd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8lp_o64s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbzkcyeso.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpenp1_863.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgg_ag93c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm0tm7caz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0t7jypce.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4ctm8w0z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr8yy_4wh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphsgcg7s8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeosxpg4c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2klaemsu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzovjb_rv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprgltijt5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz012ef1b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5qx7h0lc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpicwo6xul.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpavs3lv2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfv8kcqno.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9eathj5o.pickle

Iteration 22: Full valset score for new program: 0.21842264583333335
Iteration 22: Full train_val score for new program: 0.21842264583333335
Iteration 22: Individual valset scores for new program: [0.456307, 0.424235, 0.43664, 0.386859, 0.448896, 0.444771, 0.261962, 0.498624, 0.534144, 0.531017, 0.05, 0.323268, 0.023893, 0.0, 0.019114, 0.018879, 0.018312, 0.021788, 0.021235, 0.266728, 0.354966, 0.025086, 0.057382, 0.057382, 0.269777, 0.27621, 0.832175, 0.878487, 0.020066, 0.036364, 0.038724, 0.000132, 0.000108, 0.745575, 0.070175, 0.060018, 0.009544, 0.620814, 0.121771, 0.023011, 0.02077, 0.026572, 0.041118, 0.2, 0.020965, 0.02216, 0.435583, 0.03268]
Iteration 22: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.278708, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.27681, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.043992, 0.07805, 0.754765, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 22: Full valset pareto front score: 0.2679637708333334
Iteration 22: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {6}, {6}, {6}, {6}, {6}, {11}, {6}, {9, 2, 4}, {11, 6}, {11, 6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {11}, {6}, {9}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8, 9, 11}, {8}]
Iteration 22: Best valset aggregate score so far: 0.24589052083333332
Iteration 22: Best program as per aggregate score on train_val: 7
Iteration 22: Best program as per aggregate score on valset: 7
Iteration 22: Best score on valset: 0.24589052083333332
Iteration 22: Best score on train_val: 0.24589052083333332
Iteration 22: Linear pareto front program index: 7
Iteration 22: New program candidate index: 12
Iteration 23: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc0d3gkp7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprt0tcdqn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy6tyvrfv.pickle

Iteration 23: Proposed new text for program: # Improved Adaptive W-TinyLFU with Segmented LRU (AW-TinyLFU++)
# Key upgrades over the previous version:
# - Correct SLRU semantics for Window on hit (no premature W->P promotion).
# - Stronger, size-aware, cross-segment victim selection: when not predicting W overflow,
#   evict the lower "worthiness" (TinyLFU_estimate/size) between W-LRU and P-LRU.
# - Keep segments tightly within byte targets:
#   * Demote S-LRU proactively when S exceeds its target.
#   * After insertion, move multiple W-LRUs to P until W bytes are within target.
# - Lightweight online hill-climbing to adapt window size using hit distribution
#   (increase W if more hits occur in W; decrease otherwise). Still keeps ghost-based
#   feedback for reinsertion hints.
# - Minor tie-break improvements and safeguards.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Adaptive window sizing (ARC + hill-climbing)
m_W_frac = 0.20         # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Hill-climbing using hit distribution
m_hit_balance = 0       # positive -> more hits in W; negative -> more hits in Main
HILL_STEP = 64

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80           # 80% of main in protected
ADMIT_EPS = 1e-12       # epsilon for float comparisons
EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)
    main_target = max(0, cap - w_target)
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _score_for_key(cache_snapshot, key):
    """
    Size-aware frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate / size
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    return est / max(1.0, sz)

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return est / max(1.0, sz)

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

def _maybe_hill_climb(seg_hit):
    """
    Adjust window fraction based on where hits occur.
    seg_hit: SEG_WINDOW or SEG_PROBATION/SEG_PROTECTED
    """
    global m_hit_balance, m_W_frac
    if seg_hit == SEG_WINDOW:
        m_hit_balance += 1
    else:
        m_hit_balance -= 1

    if m_hit_balance >= HILL_STEP:
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
        # dampen, keep some momentum
        m_hit_balance = HILL_STEP // 2
    elif m_hit_balance <= -HILL_STEP:
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)
        m_hit_balance = -HILL_STEP // 2

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision (improved):
      - Keep segments balanced by proactively demoting from S if over target.
      - If Window is currently oversized, evict W-LRU to restore balance.
      - Predict W overflow after insertion:
          * If overflow: run W-TinyLFU admission vs P-LRU (size-aware). Evict P-LRU if new wins;
            otherwise evict W-LRU (i.e., reject admission to main).
          * If no overflow: choose the lower-worthiness between W-LRU and P-LRU to evict
            (size-aware score), instead of always evicting from P. This reduces evicting
            of higher-value P items when W contains lower-value entries.
      - If W and P are empty, evict S-LRU as last resort.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_sz = _size_of(obj)
    new_score = _score_for_new(obj)

    # Keep S within target so P has viable victims
    _demote_S_if_over_target(cache_snapshot, exclude_key=None)

    w_target, _ = _targets(cache_snapshot)

    # If W currently oversized, evict from W to restore balance
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict Window overflow by admitting new object into W
    predict_overflow = (m_bytes_W + new_sz) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)

    if predict_overflow:
        if p_lru is None:
            # Can't compare; evict from W if possible, else from S
            if w_lru is not None:
                return w_lru
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            if s_lru is not None:
                return s_lru
        else:
            # Admission comparison (size-aware)
            p_score = _score_for_key(cache_snapshot, p_lru)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict P-LRU (keep W as recency buffer)
                return p_lru
            else:
                # Reject admission to main: evict W-LRU (or P if W empty)
                if w_lru is not None:
                    return w_lru
                return p_lru

    # No predicted W overflow:
    # choose the worse of W-LRU vs P-LRU by score to evict
    if w_lru is not None and p_lru is not None:
        sw = _score_for_key(cache_snapshot, w_lru)
        sp = _score_for_key(cache_snapshot, p_lru)
        if sw + ADMIT_EPS < sp:
            return w_lru
        if sp + ADMIT_EPS < sw:
            return p_lru
        # Tie-breaker: older first, then larger size, then key
        la_w = m_last_access.get(w_lru, -1)
        la_p = m_last_access.get(p_lru, -1)
        if la_w < la_p:
            return w_lru
        if la_p < la_w:
            return p_lru
        # same age: evict larger to free more space
        sz_w = _size_of(cache_snapshot.cache.get(w_lru))
        sz_p = _size_of(cache_snapshot.cache.get(p_lru))
        if sz_w > sz_p:
            return w_lru
        if sz_p > sz_w:
            return p_lru
        return min(w_lru, p_lru)

    if p_lru is not None:
        return p_lru
    if w_lru is not None:
        return w_lru

    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - SLRU behavior:
          * W hit: stay in W (refresh recency only).
          * P hit: promote to S.
          * S hit: refresh recency.
      - Keep S near its target by demoting S-LRU when needed.
      - Hill-climb window fraction based on hit location.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        # Refresh only; do NOT promote W->P on hit (correct SLRU semantics)
        _maybe_hill_climb(SEG_WINDOW)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _maybe_hill_climb(SEG_PROBATION)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency
        _maybe_hill_climb(SEG_PROTECTED)
        # keep S within target if necessary (exclude the just-hit key)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Always insert into Window (recency admission).
      - Update sketch and recency.
      - Adapt window target using ghost histories and hill-climbing:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - If Window exceeds its target, move its LRU(s) to P until W <= target.
      - Keep S near its target by demoting its LRU to P when needed.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window
    sz = _size_of(obj)
    global m_bytes_W
    m_seg[key] = SEG_WINDOW
    m_bytes_W += sz

    # If Window above target, move its LRU(s) to Probation until W is within target.
    w_target, _ = _targets(cache_snapshot)
    # Exclude the just inserted key from being the candidate to move immediately.
    while m_bytes_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is None:
            break
        v = cache_snapshot.cache.get(candidate)
        if v is None:
            break
        csz = _size_of(v)
        _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphz28ut3e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt9ofm8bb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmparmzgw89.pickle

Iteration 23: New subsample score 1.027714 is not better than old score 1.031407, skipping
Iteration 24: Selected program 9 score: 0.232186875
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpokvskkt7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprkvmdg6k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphqrhtc0g.pickle

Iteration 24: Proposed new text for program: # Adaptive W-TinyLFU + SLRU + Size-aware GreedyDual with admission gating
# Key ideas:
#   - Three segments:
#       * WINDOW (W): recency buffer. New items land here. Evict by LRU.
#       * PROBATION (P): main-cache probation. Evict by GreedyDual-Size (GDSF).
#       * PROTECTED (Q): main-cache protected. Evict last; also GreedyDual-Size.
#   - TinyLFU sketch gives global popularity estimates. We combine in-cache hits
#     with a small (dampened) TinyLFU boost to compute effective frequency.
#   - GreedyDual global clock (L) is used for P/Q to age stale entries and resist scans.
#   - Admission gating at eviction time: before evicting from P, compare incoming
#     object's TinyLFU score (size-normalized) against the candidate P victims score.
#     If the incoming object looks worse, we evict from the WINDOW instead. This protects
#     valuable main-cache entries from being displaced by transient traffic.
#   - On hit:
#       * W: bump freq and promote to P when sufficiently hot.
#       * P: promote to Q.
#       * Q: bump priority only.
#   - Size-aware via exponent alpha: priority H = L + eff_freq / size^alpha (for P/Q).

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()     # key -> float, GreedyDual priority H (only meaningful for P/Q)
m_freq = dict()         # key -> small int, in-cache frequency (hits since (re)admission)
m_last_access = dict()  # key -> int, last access time (for LRU and ties)
m_seg = dict()          # key -> 'W'|'P'|'Q' (window/probation/protected)
m_L = 0.0               # GreedyDual global clock (for P/Q)
m_inited = False        # sketch initialization guard

# TinyLFU sketch (Count-Min Sketch)
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096     # power of two for fast masking
m_sketch = None         # list of SKETCH_DEPTH arrays of length SKETCH_WIDTH
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 1.0             # size penalty exponent in [0.5, 1.0]; 1.0 = GDSF
FREQ_CAP = 255             # cap per-key in-cache frequency
NEW_ADMIT_THRESHOLD = 2    # TinyLFU doorkeeper threshold for initializing freq
NEW_ITEM_BIAS = 0.0        # extra boost on insert (usually 0.0)

WINDOW_FRAC = 0.25         # fraction of capacity (bytes) for the recency window
PROTECTED_FRAC = 0.80      # fraction of main (non-window) for protected segment

PROMOTE_MIN_HITS_W = 2     # promote W -> P after this many in-cache hits
PROMOTE_EST_THRESHOLD = 3  # alternatively promote W -> P if TinyLFU estimate >= this

EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _now(cache_snapshot):
    return cache_snapshot.access_count

def _get_size(obj):
    return getattr(obj, "size", 1)

def _segment_of(key):
    return m_seg.get(key, 'W')

def _effective_freq(key):
    # Combine in-cache hits with a small TinyLFU boost (dampened)
    f = m_freq.get(key, 1)
    est = _sketch_estimate(key)
    boost = est >> 3  # 1/8 of sketch estimate
    return max(1, min(FREQ_CAP, f + boost))

def _current_priority_for_main(key, obj):
    # For P/Q items only. If not set, compute from global clock and effective freq.
    if key in m_priority:
        return m_priority[key]
    f_eff = _effective_freq(key)
    return float(m_L) + float(f_eff) / _denom(_get_size(obj))

def _set_priority_for_main(key, obj):
    f_eff = _effective_freq(key)
    m_priority[key] = float(m_L) + float(f_eff) / _denom(_get_size(obj))

def _promote_to_probation(key, obj):
    m_seg[key] = 'P'
    _set_priority_for_main(key, obj)

def _promote_to_protected(key, obj):
    m_seg[key] = 'Q'
    _set_priority_for_main(key, obj)

def _admission_score_for_obj(key, size):
    # size-normalized TinyLFU score for admission comparisons
    est = _sketch_estimate(key)
    return (float(est) + float(NEW_ITEM_BIAS)) / _denom(size)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Adaptive eviction:
      - Maintain segment budgets in bytes.
      - Prefer evicting from Probation (P) by GreedyDual-Size (lowest H).
      - Admission gating: before evicting from P, compare incoming object's
        TinyLFU score (size-normalized) with P victim's score. If incoming is
        worse, evict from Window (W) instead (LRU). This protects valuable main entries.
      - If a segment is over budget, evict from it regardless (respect limits).
      - Protected (Q) is evicted last.
    """
    if not cache_snapshot.cache:
        return None

    # Compute byte budgets
    capacity = max(1, int(cache_snapshot.capacity))
    window_target = max(0, int(WINDOW_FRAC * capacity))
    main_target = capacity - window_target
    protected_target = max(0, int(PROTECTED_FRAC * main_target))
    probation_target = max(0, main_target - protected_target)

    # Track segment sizes and best victims
    bytes_W = 0
    bytes_P = 0
    bytes_Q = 0

    # Candidates: (key, priority, last_access, size, extra_tie)
    cand_W = None     # oldest in W (LRU)
    cand_P = None     # lowest H in P
    cand_Q = None     # lowest H in Q

    for k, v in cache_snapshot.cache.items():
        seg = _segment_of(k)
        la = m_last_access.get(k, -1)
        sz = _get_size(v)

        if seg == 'W':
            bytes_W += sz
            if cand_W is None:
                cand_W = (k, 0.0, la, sz, 0.0)
            else:
                _, _, la_w, sz_w, _ = cand_W
                # LRU, then larger size to free more space, then key tie-breaker
                if la < la_w or (la == la_w and (sz > sz_w or (sz == sz_w and k < cand_W[0]))):
                    cand_W = (k, 0.0, la, sz, 0.0)

        elif seg == 'P':
            bytes_P += sz
            pri = _current_priority_for_main(k, v)
            # Use TinyLFU score for tie-breaking among equal priorities
            tl_score = _admission_score_for_obj(k, sz)
            if cand_P is None:
                cand_P = (k, pri, la, sz, tl_score)
            else:
                _, pri_p, la_p, sz_p, s_p = cand_P
                if pri < pri_p - EPS:
                    cand_P = (k, pri, la, sz, tl_score)
                elif abs(pri - pri_p) <= EPS:
                    # If equal H: pick lower TinyLFU score, then older, then larger size
                    if tl_score < s_p - EPS or (abs(tl_score - s_p) <= EPS and (la < la_p or (la == la_p and (sz > sz_p or (sz == sz_p and k < cand_P[0]))))):
                        cand_P = (k, pri, la, sz, tl_score)

        else:  # 'Q'
            bytes_Q += sz
            pri = _current_priority_for_main(k, v)
            tl_score = _admission_score_for_obj(k, sz)
            if cand_Q is None:
                cand_Q = (k, pri, la, sz, tl_score)
            else:
                _, pri_q, la_q, sz_q, s_q = cand_Q
                if pri < pri_q - EPS:
                    cand_Q = (k, pri, la, sz, tl_score)
                elif abs(pri - pri_q) <= EPS:
                    if tl_score < s_q - EPS or (abs(tl_score - s_q) <= EPS and (la < la_q or (la == la_q and (sz > sz_q or (sz == sz_q and k < cand_Q[0]))))):
                        cand_Q = (k, pri, la, sz, tl_score)

    # Budget enforcement first
    if bytes_W > window_target and cand_W is not None:
        return cand_W[0]
    if bytes_P > probation_target and cand_P is not None:
        return cand_P[0]
    if bytes_Q > protected_target and cand_Q is not None:
        return cand_Q[0]

    # Admission gating: prefer evicting from probation, but if the incoming
    # object looks worse than the best P victim (by TinyLFU/size score),
    # evict from Window instead (if possible).
    incoming_score = _admission_score_for_obj(obj.key, _get_size(obj))

    if cand_P is not None:
        if cand_W is not None:
            vk, _, _, vsz, vscore = cand_P
            # Compare normalized popularity; bias toward keeping P victim unless incoming clearly better
            if incoming_score + EPS < vscore:
                return cand_W[0]
        return cand_P[0]

    if cand_W is not None:
        return cand_W[0]

    if cand_Q is not None:
        return cand_Q[0]

    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Increase per-key in-cache frequency (cap).
      - Update last access time.
      - Segment transitions:
          * W: promote to P if freq >= PROMOTE_MIN_HITS_W or TinyLFU est >= PROMOTE_EST_THRESHOLD.
          * P: promote to Q.
          * Q: stay; refresh GreedyDual priority.
      - For P/Q, recompute GreedyDual H = L + eff_freq / size^alpha.
    """
    global m_L
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = _now(cache_snapshot)
    sz = _get_size(obj)

    # bump in-cache frequency (capped)
    f = m_freq.get(key, 1)
    if f < FREQ_CAP:
        f += 1
    m_freq[key] = f
    m_last_access[key] = now

    seg = _segment_of(key)
    if seg == 'W':
        est = _sketch_estimate(key)
        if f >= PROMOTE_MIN_HITS_W or est >= PROMOTE_EST_THRESHOLD:
            _promote_to_probation(key, obj)
        return

    if seg == 'P':
        _promote_to_protected(key, obj)
        return

    # seg == 'Q': refresh GreedyDual priority using effective frequency
    _set_priority_for_main(key, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch for this access.
      - Initialize in-cache frequency using a light doorkeeper (base 1 + small boost).
      - Place item in the WINDOW segment (recency buffer).
      - Record last access time.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = _now(cache_snapshot)

    est = _sketch_estimate(key)
    # conservative TinyLFU-based initial boost
    init_boost = max(0, est - int(NEW_ADMIT_THRESHOLD))
    init_f = 1 + min(FREQ_CAP - 1, init_boost // 4)

    m_seg[key] = 'W'
    m_freq[key] = max(1, min(FREQ_CAP, init_f))
    m_last_access[key] = now
    # m_priority not used for W; will be set when promoted to P/Q.


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - If evicted key was in P/Q, advance GreedyDual clock to its priority (aging).
      - Clean all metadata (segment, priority, freq, last-access).
    """
    global m_L
    ekey = evicted_obj.key
    seg = _segment_of(ekey)

    if seg in ('P', 'Q'):
        ev_pri = m_priority.get(ekey, m_L)
        if ev_pri > m_L:
            m_L = float(ev_pri)

    # Clean metadata for evicted key
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcol3ru8d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmunbjgf0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr4v1h2ht.pickle

Iteration 24: New subsample score 0.344637 is not better than old score 0.344637, skipping
Iteration 25: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpogyh0vau.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6cuk_r27.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppv2eyb1t.pickle

Iteration 25: Proposed new text for program: # Adaptive SLRU + GreedyDual-Size-Frequency with Ghost History (ARC-style steering)
# - Two segments (SLRU): probation (recency) and protected (frequency/value).
# - Protected eviction uses GreedyDual-Size-Frequency (GDSF) priority H with global inflation L.
#   This injects recency into frequency via L, and is size-aware (per-byte benefit).
# - Probation eviction is pure LRU (very responsive to scans and phase changes).
# - Ghost history (recently evicted keys) steers the target share between segments:
#     * Re-insert of key seen in protected-ghost -> increase protected share (more frequency).
#     * Re-insert of key seen in probation-ghost -> decrease protected share (more recency).
# - Adaptive protected budget is enforced by bytes, not by count.
# - Hits in probation promote to protected; protected hits increase H.
#
# Metadata
m_H = dict()            # key -> float, GDSF priority
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> 0 probation, 1 protected

# Segment accounting
m_num_protected = 0     # count of protected items (for sanity)
m_bytes_protected = 0   # bytes in protected segment

# Adaptive target (fraction of bytes to protect)
m_p_target = 0.5

# GDSF inflation (recency) term
m_L = 0.0

# Ghost histories (LRU via timestamp)
# Keys recently evicted from probation or protected. Values: last seen access_count.
g_probation = dict()    # key -> last_eviction_time
g_protected = dict()    # key -> last_eviction_time

# Tunables
P_MIN = 0.05
P_MAX = 0.95

# Share adaptation steps
ETA_UP_PROMOTE = 0.03         # increase protected share on promotion
ETA_DOWN_PHIT = 0.005         # decrease protected share on protected hits
ETA_EVICT_PROBATION = 0.02    # decrease protected share when evicting probation
ETA_EVICT_PROTECTED = 0.02    # increase protected share when evicting protected
ETA_GHOST_PROBATION = 0.03    # on re-insert of probation-ghost
ETA_GHOST_PROTECTED = 0.03    # on re-insert of protected-ghost

# Priority boosts (size-aware)
NEW_ITEM_BOOST = 0.05         # very small; avoid pollution by one-timers
GHOST_PROTECTED_BOOST = 1.0   # strong hint if item was frequent before
GHOST_PROBATION_BOOST = 0.2   # mild hint if item was only recent before

# Ghost limits (by entry count, adaptive with live cache size)
def _ghost_limit(cache_snapshot):
    # Keep ghosts at up to 4x current resident items (per list capped at 2x)
    n = max(1, len(cache_snapshot.cache))
    return 2 * n  # per list

def _prune_ghosts(cache_snapshot):
    # Remove oldest entries beyond limit for each ghost list
    limit = _ghost_limit(cache_snapshot)

    if len(g_probation) > limit:
        # remove oldest entries
        over = len(g_probation) - limit
        # find 'over' smallest timestamps
        for _ in range(over):
            oldest_key = None
            oldest_t = None
            for k, t in g_probation.items():
                if oldest_key is None or t < oldest_t or (t == oldest_t and k < oldest_key):
                    oldest_key, oldest_t = k, t
            if oldest_key is None:
                break
            g_probation.pop(oldest_key, None)

    if len(g_protected) > limit:
        over = len(g_protected) - limit
        for _ in range(over):
            oldest_key = None
            oldest_t = None
            for k, t in g_protected.items():
                if oldest_key is None or t < oldest_t or (t == oldest_t and k < oldest_key):
                    oldest_key, oldest_t = k, t
            if oldest_key is None:
                break
            g_protected.pop(oldest_key, None)


def _recount_protected(cache_snapshot):
    """
    Sanity: recompute protected counts/bytes if they drift.
    """
    global m_num_protected, m_bytes_protected
    count = 0
    bytes_sum = 0
    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k, 0) == 1:
            count += 1
            bytes_sum += int(getattr(v, "size", 1))
    m_num_protected = count
    m_bytes_protected = bytes_sum


def _enforce_protected_budget(cache_snapshot):
    """
    Ensure protected bytes do not exceed the adaptive target. Demote weakest protected
    items by GDSF priority H (ties by older last access, then larger size, then key).
    """
    global m_bytes_protected, m_num_protected

    total_bytes = max(0, int(getattr(cache_snapshot, "size", 0)))
    if total_bytes <= 0:
        m_bytes_protected = 0
        m_num_protected = 0
        return

    target_bytes = int(m_p_target * total_bytes)

    # Sanity repair if necessary
    if m_bytes_protected < 0 or m_bytes_protected > total_bytes or m_num_protected < 0 or m_num_protected > len(cache_snapshot.cache):
        _recount_protected(cache_snapshot)

    while m_bytes_protected > target_bytes and m_num_protected > 0:
        # Find weakest protected by smallest H
        weakest_key = None
        weakest_H = None
        weakest_last = None
        weakest_size = None

        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k, 0) != 1:
                continue
            H = m_H.get(k, m_L)
            la = m_last_access.get(k, -1)
            sz = int(getattr(v, "size", 1))

            if weakest_key is None:
                weakest_key, weakest_H, weakest_last, weakest_size = k, H, la, sz
                continue

            if H < weakest_H:
                weakest_key, weakest_H, weakest_last, weakest_size = k, H, la, sz
                continue

            if H == weakest_H:
                if la < weakest_last:
                    weakest_key, weakest_H, weakest_last, weakest_size = k, H, la, sz
                    continue
                if la == weakest_last:
                    if sz > weakest_size:
                        weakest_key, weakest_H, weakest_last, weakest_size = k, H, la, sz
                        continue
                    if sz == weakest_size and k < weakest_key:
                        weakest_key, weakest_H, weakest_last, weakest_size = k, H, la, sz

        if weakest_key is None:
            # nothing to demote
            break

        # Demote weakest protected to probation
        m_seg[weakest_key] = 0
        m_num_protected -= 1
        m_bytes_protected -= weakest_size
        # Keep its H as-is (will be used if promoted later)


def evict(cache_snapshot, obj):
    """
    Choose eviction victim:
      - Prefer evicting from probation (recency), using LRU among probation keys.
      - If no probation keys remain, evict the protected key with the smallest GDSF priority H.
    Tie-breakers:
      1) older last access (LRU)
      2) larger size (free more space, favor small objs)
      3) lexicographical key (deterministic)
    """
    if not cache_snapshot.cache:
        return None

    # First try probation (recency window)
    victim_key = None
    victim_last = None
    victim_size = None

    found_probation = False
    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k, 0) != 0:
            continue
        found_probation = True
        la = m_last_access.get(k, -1)
        sz = int(getattr(v, "size", 1))

        if victim_key is None:
            victim_key, victim_last, victim_size = k, la, sz
            continue

        if la < victim_last:
            victim_key, victim_last, victim_size = k, la, sz
            continue
        if la == victim_last:
            if sz > victim_size:
                victim_key, victim_last, victim_size = k, la, sz
                continue
            if sz == victim_size and k < victim_key:
                victim_key, victim_last, victim_size = k, la, sz

    if found_probation:
        return victim_key

    # All are protected: evict by smallest H (GDSF)
    victim_key = None
    victim_H = None
    victim_last = None
    victim_size = None

    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k, 0) != 1:
            continue
        H = m_H.get(k, m_L)
        la = m_last_access.get(k, -1)
        sz = int(getattr(v, "size", 1))

        if victim_key is None:
            victim_key, victim_H, victim_last, victim_size = k, H, la, sz
            continue

        if H < victim_H:
            victim_key, victim_H, victim_last, victim_size = k, H, la, sz
            continue

        if H == victim_H:
            if la < victim_last:
                victim_key, victim_H, victim_last, victim_size = k, H, la, sz
                continue
            if la == victim_last:
                if sz > victim_size:
                    victim_key, victim_H, victim_last, victim_size = k, H, la, sz
                    continue
                if sz == victim_size and k < victim_key:
                    victim_key, victim_H, victim_last, victim_size = k, H, la, sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update last access.
      - If in probation: promote to protected, initialize/raise H to at least L, add size-aware gain.
      - If in protected: add size-aware gain to H.
      - Adapt protected share based on promotions/hits.
      - Enforce protected budget by bytes.
    """
    global m_num_protected, m_bytes_protected, m_p_target

    key = obj.key
    now = cache_snapshot.access_count
    sz = max(1, int(getattr(obj, "size", 1)))

    m_last_access[key] = now

    seg = m_seg.get(key, 0)
    if seg == 0:
        # Promote to protected on hit
        m_seg[key] = 1
        m_num_protected += 1
        m_bytes_protected += sz

        baseH = m_H.get(key, m_L)
        # Ensure at least L, then add per-byte value increment
        m_H[key] = max(baseH, m_L) + (1.0 / float(sz))

        # Promotions indicate longer-term value -> slightly increase protected share
        m_p_target = min(P_MAX, m_p_target + ETA_UP_PROMOTE)

        _enforce_protected_budget(cache_snapshot)
    else:
        # Protected hit: reward by size-aware gain
        m_H[key] = m_H.get(key, m_L) + (1.0 / float(sz))

        # Many protected hits could starve recency adaptation -> give probation a tiny bump
        m_p_target = max(P_MIN, m_p_target - ETA_DOWN_PHIT)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss and any necessary evictions):
      - Initialize last access.
      - If key is present in ghost history:
          * If in protected-ghost: admit directly to protected with boosted H and increase protected share.
          * If in probation-ghost: admit to probation with modest boost and decrease protected share.
      - Else: admit to probation with very small H (anti-pollution).
      - Enforce protected budget if we admitted into protected.
      - Prune ghost history to its size limit.
    """
    global m_num_protected, m_bytes_protected, m_p_target

    key = obj.key
    now = cache_snapshot.access_count
    sz = max(1, int(getattr(obj, "size", 1)))

    m_last_access[key] = now

    in_g_prot = key in g_protected
    in_g_prob = key in g_probation

    if in_g_prot:
        # Strong signal: was frequent earlier -> place into protected immediately
        m_seg[key] = 1
        m_num_protected += 1
        m_bytes_protected += sz

        m_H[key] = max(m_L, m_H.get(key, m_L)) + (GHOST_PROTECTED_BOOST / float(sz))
        # Increase protected share target
        m_p_target = min(P_MAX, m_p_target + ETA_GHOST_PROTECTED)

        # Clean ghost entry
        g_protected.pop(key, None)

        _enforce_protected_budget(cache_snapshot)
    else:
        # Default: probation
        m_seg[key] = 0
        # Gentle admission; extra nudge if it was evicted from probation recently
        boost = GHOST_PROBATION_BOOST if in_g_prob else NEW_ITEM_BOOST
        m_H[key] = max(m_L, m_H.get(key, m_L)) + (boost / float(sz))

        if in_g_prob:
            # Favor recency if we're seeing many items bouncing in probation
            m_p_target = max(P_MIN, m_p_target - ETA_GHOST_PROBATION)
            g_probation.pop(key, None)

    _prune_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of a victim:
      - Update GDSF inflation L if victim was protected (L = max(L, H_victim)).
      - Adjust adaptive protected share based on which segment was evicted.
      - Update protected counters/bytes for protected evictions.
      - Add evicted key to the appropriate ghost list with its eviction time.
      - Clean metadata for evicted key.
      - Enforce protected budget in case target shrank.
    """
    global m_num_protected, m_bytes_protected, m_p_target, m_L

    now = cache_snapshot.access_count
    ekey = evicted_obj.key
    eseg = m_seg.get(ekey, 0)
    esize = max(1, int(getattr(evicted_obj, "size", 1)))

    # If protected victim: raise L to its H (GDSF recency aging)
    if eseg == 1:
        m_L = max(m_L, m_H.get(ekey, m_L))

    # Adjust protected share target
    if eseg == 0:
        m_p_target = max(P_MIN, m_p_target - ETA_EVICT_PROBATION)
        g_probation[ekey] = now
    else:
        m_p_target = min(P_MAX, m_p_target + ETA_EVICT_PROTECTED)
        g_protected[ekey] = now

    # Update protected accounting
    if eseg == 1:
        m_num_protected = max(0, m_num_protected - 1)
        m_bytes_protected = max(0, m_bytes_protected - esize)

    # Clean metadata
    m_H.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)

    _prune_ghosts(cache_snapshot)
    _enforce_protected_budget(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp89lj42m0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfk9g601x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeax713wd.pickle

Iteration 25: New subsample score 1.385545 is not better than old score 1.4105619999999999, skipping
Iteration 26: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbu6l_2qb.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv2yk8e70.pickle

Iteration 26: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with ghost-assisted warm starts and a lazy min-heap
# Goals:
# - Strong, size-aware, time-efficient victim selection using a min-heap (avoids O(n) scans).
# - Robust to skewed and mixed workloads via GDS aging (global L) and frequency-based priority.
# - Avoid cache pollution by penalizing large cold objects and using a ghost to warm-start reinserted keys.
#
# Priority model (to keep items):
#   H(key) = L + weight / size^SIZE_EXP
# where:
#   - L is a global "aging" value that increases to the priority of each evicted item
#     (implements implicit, monotonic aging for all items).
#   - weight = 1 + FREQ_MULT * log1p(freq), tracking utility with diminishing returns.
# Eviction chooses the item with MINIMUM H (least valuable per byte after aging).
#
# Metadata maintained:
# - m_freq[key]: observed hits (frequency) for key (capped).
# - m_last_access[key]: last access time (used only for tie-breaking and ghost).
# - m_H[key]: last computed priority H for the key (for L update on eviction).
# - m_seq[key]: last heap sequence number for the key (used for lazy heap delete).
# - m_size[key]: cached size of the key's object at last touch (for stable priority).
# - Global:
#   * m_L: global aging scalar (monotonically increases on evictions).
#   * m_seq_ctr: increasing counter to tag heap entries.
#   * heap: min-heap of (H, seq, key) entries (lazy deletion via seq check).
# - Ghost history (by count, bounded):
#   * g_freq[key]: last known freq for key.
#   * g_last[key]: last time the key was tracked in ghost (for trimming order).
#
# Notes:
# - This implementation does not depend on 'now' inside the eviction score, enabling a stable priority queue.
# - Ties are broken by: lowest H first; then older last access; then larger size; then lexicographic key.
# - Frequency is damped via log1p and capped to avoid runaway and ensure good aging dynamics.
#
# Complexity:
# - evict: expected O(log n) per eviction with lazy cleanup of stale heap entries.
# - update_after_hit/insert: O(log n) to push a fresh heap entry for the touched key.

import math
import heapq

# ----------------------
# Global metadata stores
# ----------------------
m_freq = dict()          # key -> int
m_last_access = dict()   # key -> int
m_H = dict()             # key -> float (last computed priority)
m_seq = dict()           # key -> int (last heap sequence)
m_size = dict()          # key -> int (cached size for stable priority)

# Min-heap storing (H, seq, key)
_heap = []
m_seq_ctr = 0
m_L = 0.0                # global aging parameter

# Ghost by count (bounded)
g_freq = dict()          # key -> int
g_last = dict()          # key -> int

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0           # size exponent; 1.0 = strong size awareness, <1 relaxes penalty
FREQ_MULT = 2.0          # multiplier on log1p(freq) to reward repeated hits
MAX_FREQ = 1_000_000     # hard cap on frequency to avoid numerical explosion
GHOST_LIMIT_MIN = 1024   # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 4.0 # ghost capacity  factor * live cache item count
GHOST_FREQ_INHERIT = 0.7 # on reinsert: start with ~70% of ghosted frequency (tempered warm start)

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_freq) <= limit:
        return
    # Drop oldest by g_last (LRU over ghost)
    # Remove up to exact overflow amount
    overflow = len(g_freq) - limit
    # Gather (key, time) and sort by oldest
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    for i in range(min(overflow, len(items))):
        k, _ = items[i]
        g_freq.pop(k, None)
        g_last.pop(k, None)

def _weight_from_freq(freq):
    # Base utility weight uses diminishing returns with log1p
    return 1.0 + FREQ_MULT * math.log1p(max(0, int(freq)))

def _priority_from_state(size, freq):
    denom = float(size) ** float(SIZE_EXP)
    w = _weight_from_freq(freq)
    return m_L + (w / max(1.0, denom))

def _heap_push(key, H):
    global m_seq_ctr
    m_seq_ctr += 1
    seq = m_seq_ctr
    m_seq[key] = seq
    m_H[key] = float(H)
    heapq.heappush(_heap, (float(H), int(seq), key))

def _ensure_heap_entry(key, obj):
    # Compute priority and push an entry to the heap
    sz = _size_of(obj)
    m_size[key] = sz
    freq = m_freq.get(key, 0)
    H = _priority_from_state(sz, freq)
    _heap_push(key, H)

def _valid_heap_entry(entry, cache_snapshot):
    # Validate that a heap entry is current and the key is still live
    H, seq, key = entry
    if key not in cache_snapshot.cache:
        return False
    if m_seq.get(key, None) != seq:
        return False
    return True

def _pop_valid_min(cache_snapshot):
    # Pop until we find a valid (non-stale) entry or heap is empty
    while _heap:
        H, seq, key = heapq.heappop(_heap)
        # Skip stale or non-live items
        if not _valid_heap_entry((H, seq, key), cache_snapshot):
            continue
        return (H, seq, key)
    return None

def _record_ghost(ekey, now):
    # Save lightweight frequency and last seen time
    if ekey in m_freq:
        g_freq[ekey] = int(m_freq[ekey])
    else:
        # if missing, assume single observation
        g_freq[ekey] = 1
    g_last[ekey] = int(now)

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the minimum priority H (GreedyDual-Size-Frequency).
    - Uses a min-heap for O(log n) victim selection and lazy deletion for stale entries.
    - Ties inherently broken by heap ordering; further tie-breaks handled in lazy manner by updating H.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Ensure all live keys have at least one heap entry; touch only missing.
    # This is a safety net for initial state; afterwards, inserts/hits maintain the heap.
    # To avoid O(n) behavior, only do this if heap is empty.
    if not _heap:
        for k, v in cache.items():
            if k not in m_seq:
                # Initialize minimal metadata for existing items (cold start)
                if k not in m_freq:
                    m_freq[k] = 0
                if k not in m_last_access:
                    m_last_access[k] = _now(cache_snapshot)
                _ensure_heap_entry(k, v)

    # Pop until we find a valid victim
    entry = _pop_valid_min(cache_snapshot)
    if entry is None:
        # Fallback: if heap couldn't supply, use a simple LRU+size tie-break
        # (very unlikely except at startup anomalies)
        now = _now(cache_snapshot)
        best_key = None
        best_la = None
        best_sz = None
        for k, v in cache.items():
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if best_key is None or la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))):
                best_key, best_la, best_sz = k, la, sz
        return best_key

    H, seq, victim_key = entry
    # Return victim key; L will be updated in update_after_evict using m_H[victim_key]
    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Increment frequency (capped).
    - Update last access time.
    - Recompute priority H and push to heap (lazy invalidation handles stale entries).
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (cold-start hit)
    if key not in m_freq:
        # Check ghost; seed a tempered frequency
        base_freq = int(g_freq.get(key, 0))
        seeded = int(max(0, round(GHOST_FREQ_INHERIT * base_freq)))
        m_freq[key] = seeded
        m_last_access[key] = now
        m_size[key] = _size_of(obj)

    # Update frequency (capped)
    m_freq[key] = min(MAX_FREQ, m_freq.get(key, 0) + 1)
    m_last_access[key] = now
    m_size[key] = _size_of(obj)

    # Recompute priority and push
    H = _priority_from_state(m_size[key], m_freq[key])
    _heap_push(key, H)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize frequency from ghost (tempered) or 0 for cold insert.
    - Set last access time to now.
    - Compute priority H and push a heap entry.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Seed frequency from ghost, tempered
    base_freq = int(g_freq.get(key, 0))
    freq0 = int(max(0, round(GHOST_FREQ_INHERIT * base_freq)))
    m_freq[key] = freq0
    m_last_access[key] = now
    m_size[key] = _size_of(obj)

    # Push heap entry
    H = _priority_from_state(m_size[key], m_freq[key])
    _heap_push(key, H)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Advance global age L to the evicted item's priority, implementing GreedyDual aging.
    - Record ghost frequency for the evicted key and trim ghost to a bounded size.
    - Remove per-key live metadata for the evicted key (heap uses lazy invalidation).
    """
    global m_L

    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Advance age L to priority of evicted item (if known)
    evicted_H = m_H.get(ekey, None)
    if evicted_H is not None:
        # L should be monotonic non-decreasing
        if evicted_H > m_L:
            m_L = float(evicted_H)

    # Record ghost info
    _record_ghost(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata (heap entries remain but are invalid due to m_seq deletion)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_H.pop(ekey, None)
    m_size.pop(ekey, None)
    m_seq.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiie2a9c3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnqij_hnk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcl5c0v8b.pickle

Iteration 26: New subsample score 0.36172899999999997 is better than old score 0.222514. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp8otvb4g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgc9d96tk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzidg8ex4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcdhdiku0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6b9u73os.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1ueoz2za.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzamslx66.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu3slf9j7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6_e0kbgb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb_o_y33_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjp4jxlf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq2ucvsg1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0kse_txt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp18gjlvpq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzpnt6tzh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsm_txz5u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx6c7tj_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj2xb3zpb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_2qsgzz6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzzdq_0l1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuyzzf0e1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo50tawrh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqq0ptf_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps20tma_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpepi954dc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgnz3k1rr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppu6917nj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmowmcuuk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv4qc97sg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphc6mfg3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9dms7ci2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxj9w6ob6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbkaa0yuu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_94kw8c3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsdiyr03v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpldqlnx_i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz_urtyg0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl0b1a311.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvfpvsjr1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpknkv9fx3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsr9238u2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4uldjqz3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiv7w4zm9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptpukn5p2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiuszkfbt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbcwmga1b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_42ra9ca.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpijlmh913.pickle

Iteration 26: Full valset score for new program: 0.2219503333333334
Iteration 26: Full train_val score for new program: 0.2219503333333334
Iteration 26: Individual valset scores for new program: [0.466853, 0.447875, 0.454457, 0.403041, 0.46708, 0.458953, 0.266746, 0.498624, 0.538434, 0.531017, 0.066667, 0.341474, 0.023893, 0.0, 0.01968, 0.019724, 0.018715, 0.02235, 0.021657, 0.268561, 0.336283, 0.025576, 0.057382, 0.057382, 0.269778, 0.271169, 0.815293, 0.884991, 0.020066, 0.036364, 0.038724, 0.00012, 7.2e-05, 0.750681, 0.070175, 0.0609, 0.009798, 0.631633, 0.121771, 0.023843, 0.021381, 0.026108, 0.042763, 0.233333, 0.020965, 0.023001, 0.435583, 0.03268]
Iteration 26: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.278708, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021139, 0.02474, 0.024047, 0.27681, 0.423795, 0.026164, 0.059961, 0.059961, 0.325915, 0.366935, 0.853029, 0.890467, 0.177598, 0.061364, 0.072893, 0.043992, 0.07805, 0.754765, 0.098684, 0.157988, 0.182643, 0.643483, 0.125461, 0.153867, 0.120342, 0.175962, 0.095395, 0.366667, 0.158725, 0.169705, 0.466258, 0.163399]
Iteration 26: Full valset pareto front score: 0.2679637708333334
Iteration 26: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13}, {6}, {6}, {6}, {6}, {6}, {11}, {6}, {9, 2, 4}, {11, 6}, {11, 6}, {2}, {3}, {8, 7}, {3}, {6}, {8}, {6}, {11}, {6}, {9}, {6}, {6}, {6}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11}, {6}, {6}, {6}, {6}, {3, 5, 6, 7, 8}, {6}, {6}, {2, 3, 4, 5, 6, 8, 9, 11}, {8}]
Iteration 26: Best valset aggregate score so far: 0.24589052083333332
Iteration 26: Best program as per aggregate score on train_val: 7
Iteration 26: Best program as per aggregate score on valset: 7
Iteration 26: Best score on valset: 0.24589052083333332
Iteration 26: Best score on train_val: 0.24589052083333332
Iteration 26: Linear pareto front program index: 7
Iteration 26: New program candidate index: 13
Iteration 27: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp04ixx79j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgbzdv5uk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqmpmygzt.pickle

Iteration 27: Proposed new text for program: # Size-aware Decayed Reuse with Segmented Protection (SDR-SP)
# Core ideas:
# - Maintain a decayed-reuse score per key (LRFU-style) that blends recency and frequency via an
#   exponential decay with an adaptive half-life based on observed inter-arrival times.
# - Make eviction size-aware by normalizing retention value by size^GAMMA (penalize large items).
# - Protect items that have been hit at least once (protected segment), preferring eviction from
#   probationary items first (new/never-hit).
# - Boost retention for items predicted to be reused soon using a lightweight EWMA-based reuse-time
#   predictor (per-key + global baseline).
# - Keep a bounded ghost history to warm-start decayed scores and reuse-time predictors on reinserts.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Recency/frequency decay score (LRFU-like)
m_score = dict()       # key -> float (decayed frequency score at last update time)
m_score_ts = dict()    # key -> int (time when m_score was last updated)

# Segments: 0 = probation (never hit), 1 = protected (hit at least once)
m_seg = dict()         # key -> int

# Predictive reuse timing (EWMA of inter-arrival)
m_last_access = dict() # key -> int
m_mu = dict()          # key -> float (EWMA of inter-arrival)
m_freq = dict()        # key -> int (hit count)

# Global EWMA of inter-arrival times to adapt half-life
m_global_mu = 64.0

# Ghost history for warm starts
g_mu = dict()          # key -> float
g_last = dict()        # key -> int
g_freq = dict()        # key -> int
g_score = dict()       # key -> float (decayed score snapshot at eviction)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Decay half-life (in access-count time units); adapted from m_global_mu
HL_BASE = 256.0
HL_MIN = 16.0
HL_MAX = 16384.0
HL_ADAPT_K = 0.6       # larger => retain history longer as global gaps grow

# Size-awareness
ALPHA_SIZE_CREDIT = 0.6  # credit per hit is 1 / size^alpha (favor small items slightly)
GAMMA_SIZE_EVICT = 0.8   # retention normalization divisor by size^gamma (evict large items earlier)

# Segment/urgency boosts
PROTECT_BOOST = 1.6      # multiply retention for protected items
URGENCY_WEIGHT = 1.8     # weight for urgency boost if predicted to arrive soon

# EWMA for reuse predictor
EWMA_BETA = 0.25
GLOBAL_BETA = 0.02

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 3.0

# Numerics
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict oldest ghost entries by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_mu) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)
        g_score.pop(k, None)

def _half_life():
    # Adapt the half-life to the global mean gap: longer gaps => longer memory
    hl = HL_BASE + HL_ADAPT_K * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    hl = _half_life()
    # Exponential decay: every 'hl' accesses halves the accumulated score
    return 0.5 ** (float(delta) / float(hl))

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _predicted_delta(key, now):
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    return max(0.0, float(next_t - now))

def _urgency_boost(key, now):
    """
    Boost retention if predicted next access is soon relative to expected interval.
    """
    pd = _predicted_delta(key, now)
    if pd >= _INF / 2:
        return 1.0
    mu = m_mu.get(key, None)
    norm = mu if (mu is not None and mu > 0.0) else _half_life()
    frac = 1.0 / (1.0 + (pd / max(1.0, float(norm))))
    return 1.0 + URGENCY_WEIGHT * frac

def _current_decayed_score(key, now):
    """
    Lazily compute the decayed score at 'now' from the stored score and timestamp.
    """
    base = float(m_score.get(key, 0.0))
    ts = m_score_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _record_ghost_on_evict(evicted_key, now):
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 0)
    g_score[evicted_key] = float(_current_decayed_score(evicted_key, now))

def _seed_predictor_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert, with warm-start from ghost if available.
    """
    # Reuse-time predictor
    if key in g_mu:
        m_mu[key] = max(1.0, 0.85 * float(g_mu[key]))
        m_freq[key] = max(0, int(g_freq.get(key, 0)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0

    m_last_access[key] = now

    # Decayed score: warm-start if ghost exists
    if key in g_score:
        m_score[key] = 0.80 * float(g_score[key])
    else:
        m_score[key] = 0.0
    m_score_ts[key] = now

    # Segment: start in probation; if seen frequent in past, optionally begin protected
    if g_freq.get(key, 0) >= 2:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

def _update_predictors_on_hit(key, now, obj):
    """
    Update reuse-time predictor (per-key + global) and decayed score.
    """
    # Reuse-time predictor EWMA
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Decayed score update (lazy decay, then add credit)
    cur = _current_decayed_score(key, now)
    credit = _size_credit(obj)
    m_score[key] = cur + credit
    m_score_ts[key] = now

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest retention value:
      retention = decayed_score(now) / size^GAMMA
                  * urgency_boost(key)
                  * (PROTECT_BOOST if protected else 1)
    Lower retention => worse to keep => evict.
    Tie-breakers (in order):
      - Prefer evicting probationary items
      - Older last access (LRU)
      - Larger size
      - Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    victim_key = None
    victim_val = None
    victim_ret = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        s_now = _current_decayed_score(k, now)
        sz = _size_of(v)
        # Base retention (value to keep)
        retention = (s_now + _EPS) / (float(sz) ** GAMMA_SIZE_EVICT)
        # Boost retention if predicted to be reused soon
        retention *= _urgency_boost(k, now)
        # Protect items that have seen hits
        if m_seg.get(k, 0) == 1:
            retention *= PROTECT_BOOST

        la = m_last_access.get(k, -1)
        seg = m_seg.get(k, 0)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_ret = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_ret:
            better = True
        elif retention == victim_ret:
            # Prefer evicting probationary items
            if seg < victim_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_ret = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update per-key/global EWMA reuse predictor.
      - Update decayed score with size-aware credit.
      - Promote to protected segment.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if somehow missing (cold hit)
    if key not in m_score_ts or key not in m_mu or key not in m_last_access:
        _seed_predictor_on_insert(key, now, obj)

    _update_predictors_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Initialize per-key reuse predictor from ghost or conservative default.
      - Initialize decayed score from ghost or zero.
      - Start in probation; if ghost shows prior frequency >=2, start protected.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, score, lightweight stats).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_score.pop(ekey, None)
    m_score_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7jji6jtu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy86or4x_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvoyh4xz8.pickle

Iteration 27: New subsample score 0.8225800000000001 is better than old score 0.819818. Continue to full eval and add to candidate pool.
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1v4tij80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp55w086nu.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rs1gsqb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz4pwu3g7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppciggt6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgqyo234t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe79m59xc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwo84hh3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv4bfloyd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgig6euro.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvzd_91u_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa0zy6cyy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpej2d6575.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptf9_wbiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnyn6mnk1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4sb5ts7y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi3rpympc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe3l3a85m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb1mttfwc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpws8nhtve.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvawfxj_s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaz46lz7m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptzk54a2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9ki5x9_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp54v0u4ve.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg951exn3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdc_luvfc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqumyggxs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgakku3qi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5via4xs3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcdt99m58.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjr9t4v3q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpta9dsmj3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy9v0vzhi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxzjdlyk6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdjkxlf60.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_r5ibd10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfmbpw9s8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuattemyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6sdmhypy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk0877_8m.pickle

Iteration 27: Full valset score for new program: 0.17844416666666663
Iteration 27: Full train_val score for new program: 0.17844416666666663
Iteration 27: Individual valset scores for new program: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.236842, 0.367086, 0.0, 0.531017, 0.133333, 0.259325, 0.084406, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.229148, 0.428712, 0.024596, 0.060606, 0.060606, 0.288637, 0.139113, 0.744786, 0.87438, 0.178197, 0.040909, 0.072893, 0.087816, 0.087816, 0.658952, 0.098684, 0.159753, 0.218379, 0.59763, 0.125461, 0.157749, 0.114233, 0.178125, 0.077303, 0.15, 0.188077, 0.17195, 0.466258, 0.156863]
Iteration 27: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.278708, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026164, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.061364, 0.072893, 0.087816, 0.087816, 0.754765, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.120342, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 27: Full valset pareto front score: 0.27079031250000013
Iteration 27: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {14, 7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, {6, 14}, {6, 14}, {14}, {6, 14}, {6, 14}, {11}, {14}, {9, 2, 4}, {14}, {14}, {2}, {3}, {8, 7}, {3}, {14}, {8}, {6, 14}, {14}, {14}, {9}, {6, 14}, {14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14}, {14}, {6}, {14}, {6}, {3, 5, 6, 7, 8}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14}, {8}]
Iteration 27: Best valset aggregate score so far: 0.24589052083333332
Iteration 27: Best program as per aggregate score on train_val: 7
Iteration 27: Best program as per aggregate score on valset: 7
Iteration 27: Best score on valset: 0.24589052083333332
Iteration 27: Best score on train_val: 0.24589052083333332
Iteration 27: Linear pareto front program index: 7
Iteration 27: New program candidate index: 14
Iteration 28: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpudfzjna2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaydn88j1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphtbq62y5.pickle

Iteration 28: Proposed new text for program: # Adaptive Replacement Cache (ARC) with count-based capacity
# Key ideas:
# - Two resident segments:
#   T1: recent (probationary) keys
#   T2: frequent (protected) keys
# - Two ghost lists of recently evicted keys:
#   B1: keys evicted from T1
#   B2: keys evicted from T2
# - Target p controls the balance between recency (T1) and frequency (T2).
#   It adapts online based on hits in B1/B2 per the ARC algorithm.
#
# Behavior summary:
# - On insert:
#     * If key is in B1: increase p and place in T2 (promote to frequent).
#     * If key is in B2: decrease p and place in T2.
#     * Otherwise: place in T1.
# - On hit:
#     * If in T1: move to T2 (becomes frequent).
#     * If in T2: refresh recency in T2.
# - Eviction:
#     * Prefer evicting from T1 when |T1| > p (recency pressure),
#       or if the incoming key is in B2 and |T1| == p (protects frequency).
#     * Otherwise evict from T2.
#   The evicted key is recorded into the corresponding ghost list (B1 or B2).
#
# Notes:
# - Capacity is treated as number of objects (count-based), as per task statement.
# - We use simple timestamps (cache_snapshot.access_count) to maintain LRU order.
# - Ghost lists are kept to at most 'capacity' keys each.

# -------------
# Global state
# -------------
# Resident segments (key -> last access time for LRU)
_t1 = dict()  # recent/probation
_t2 = dict()  # frequent/protected

# Ghost lists (evicted history) (key -> last seen time for LRU)
_b1 = dict()  # ghosts from T1
_b2 = dict()  # ghosts from T2

# Live placement: key -> 1 (T1) or 2 (T2)
_m_place = dict()

# Last access time for live keys (key -> int)
_m_last_access = dict()

# Target size for T1 (float, adapted)
_p_target = 0.0


# -------------
# Helpers
# -------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _capacity_items(cache_snapshot):
    cap = int(getattr(cache_snapshot, "capacity", 0))
    if cap <= 0:
        # Fallback: at least current live count; avoid zero capacity
        cap = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return cap

def _lru_key(d):
    if not d:
        return None
    # Stable tie-breaker: older time first, then lexicographic key
    return min(d.items(), key=lambda kv: (kv[1], kv[0]))[0]

def _move_to_t1_mru(key, ts):
    # Insert or refresh at MRU of T1 (represented by updating timestamp)
    _t1[key] = ts
    _m_place[key] = 1

def _move_to_t2_mru(key, ts):
    _t2[key] = ts
    _m_place[key] = 2

def _remove_from_resident(key):
    _t1.pop(key, None)
    _t2.pop(key, None)
    _m_place.pop(key, None)
    _m_last_access.pop(key, None)

def _trim_ghosts(capacity):
    # Keep each ghost list bounded by capacity
    while len(_b1) > capacity:
        victim = _lru_key(_b1)
        if victim is None:
            break
        _b1.pop(victim, None)
    while len(_b2) > capacity:
        victim = _lru_key(_b2)
        if victim is None:
            break
        _b2.pop(victim, None)

def _adapt_p_on_b1_hit(capacity):
    # p = min(c, p + max(1, |B2|/|B1|))
    global _p_target
    if len(_b1) > 0:
        delta = max(1.0, float(len(_b2)) / float(len(_b1)))
    else:
        delta = 1.0
    _p_target = min(float(capacity), float(_p_target) + delta)

def _adapt_p_on_b2_hit():
    # p = max(0, p - max(1, |B1|/|B2|))
    global _p_target
    if len(_b2) > 0:
        delta = max(1.0, float(len(_b1)) / float(len(_b2)))
    else:
        delta = 1.0
    _p_target = max(0.0, float(_p_target) - delta)

def _replace_pick_victim(new_key, capacity, now_ts):
    """
    ARC replacement decision. Returns the key to evict (must be resident).
    Uses current _p_target and presence of new_key in B2.
    """
    t1_size = len(_t1)
    t2_size = len(_t2)
    p_int = int(_p_target)  # compare with integer boundary

    # Prefer evicting from T1 if either:
    # - |T1| > p, or
    # - (incoming key was in B2 and |T1| == p)
    if t1_size >= 1 and (t1_size > p_int or (new_key in _b2 and t1_size == p_int)):
        victim = _lru_key(_t1)
        if victim is not None:
            return victim
    # Otherwise evict from T2
    victim = _lru_key(_t2)
    if victim is not None:
        return victim

    # Fallbacks (should be rare): choose oldest among all live keys
    # Combine both segments
    if _t1 or _t2:
        all_map = {}
        all_map.update(_t1)
        all_map.update(_t2)
        fallback = _lru_key(all_map)
        if fallback is not None:
            return fallback

    # Last-resort scan of cache_snapshot will be handled by caller if needed
    return None


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    ARC eviction: choose a victim based on current p and ghost history wrt the incoming key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    capacity = _capacity_items(cache_snapshot)

    # Try ARC replacement victim
    victim = _replace_pick_victim(obj.key, capacity, now)
    if victim is not None and victim in cache:
        return victim

    # Fallback: evict global LRU by observed last access among live cache
    # Use our tracked timestamps if available; otherwise fall back to snapshot order.
    # Build a map of key -> ts
    ts_map = {}
    for k in cache.keys():
        ts = _m_last_access.get(k, None)
        if ts is None:
            # If missing, approximate with 0 to make it a likely victim
            ts = 0
        ts_map[k] = ts
    fallback = _lru_key(ts_map)
    return fallback


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update last access time.
      - If in T1: move to T2 (promote to frequent).
      - If in T2: refresh recency in T2.
      - If untracked (metadata lost): add to T2 as MRU.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _m_last_access[key] = now

    place = _m_place.get(key, 0)
    if place == 1:
        # Move from T1 -> T2
        _t1.pop(key, None)
        _move_to_t2_mru(key, now)
        # Ensure no ghost duplication
        _b1.pop(key, None)
        _b2.pop(key, None)
    elif place == 2:
        # Refresh T2 recency
        _move_to_t2_mru(key, now)
        _b1.pop(key, None)
        _b2.pop(key, None)
    else:
        # Not tracked (unexpected). Put into T2 to reflect it is useful.
        _t1.pop(key, None)
        _move_to_t2_mru(key, now)
        _b1.pop(key, None)
        _b2.pop(key, None)

    # Keep ghosts bounded
    _trim_ghosts(_capacity_items(cache_snapshot))


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - If key is in B1: increase p and insert into T2 MRU.
      - If key is in B2: decrease p and insert into T2 MRU.
      - Else: insert into T1 MRU.
      - Record last access time.
      - Trim ghosts to capacity bounds.
    """
    key = obj.key
    now = _now(cache_snapshot)
    capacity = _capacity_items(cache_snapshot)
    _m_last_access[key] = now

    # If the key was in ghosts, adapt p and place into T2 (frequent)
    if key in _b1:
        _adapt_p_on_b1_hit(capacity)
        _b1.pop(key, None)
        _t1.pop(key, None)
        _move_to_t2_mru(key, now)
        _b2.pop(key, None)
    elif key in _b2:
        _adapt_p_on_b2_hit()
        _b2.pop(key, None)
        _t1.pop(key, None)
        _move_to_t2_mru(key, now)
        _b1.pop(key, None)
    else:
        # Fresh key: insert into T1 (recency)
        _t2.pop(key, None)
        _move_to_t1_mru(key, now)
        _b1.pop(key, None)
        _b2.pop(key, None)

    _trim_ghosts(capacity)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Move the evicted key into corresponding ghost list (B1 if from T1, B2 if from T2).
      - Remove all live metadata for the evicted key.
      - Trim ghosts.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)
    capacity = _capacity_items(cache_snapshot)

    # Identify which segment it was in
    place = _m_place.get(ekey, 0)
    # Remove from resident sets
    _t1.pop(ekey, None)
    _t2.pop(ekey, None)
    _m_place.pop(ekey, None)
    _m_last_access.pop(ekey, None)

    # Record in the appropriate ghost list
    if place == 1:
        _b1[ekey] = now
        _b2.pop(ekey, None)
    elif place == 2:
        _b2[ekey] = now
        _b1.pop(ekey, None)
    else:
        # Unknown origin: conservatively place into B1
        _b1[ekey] = now
        _b2.pop(ekey, None)

    _trim_ghosts(capacity)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprw9u3n4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp88zw2hcr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvvf1bjl8.pickle

Iteration 28: New subsample score 0.16123700000000002 is not better than old score 0.256433, skipping
Iteration 29: Selected program 8 score: 0.23209699999999991
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz9yy3_ko.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpup3mtbgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppw1l_ywd.pickle

Iteration 29: Proposed new text for program: # SARC-H: Size-Aware ARC with Hyperbolic eviction scoring
# - Two resident segments:
#     T1 (probation/recency) and T2 (protected/frequency)
# - Two ghost histories:
#     B1 (recently evicted from T1) and B2 (recently evicted from T2)
# - Adaptive target p (in bytes) for T1 vs T2, adjusted on ghost hits (ARC-style)
# - Within the chosen segment, evict the item with the largest hyperbolic score:
#     score = age / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq-1)))
#   which favors removing old and large objects while keeping frequent small ones
# - O(1) amortized ghost trimming using versioned deques

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_freq = dict()          # key -> int (hit count)
m_seg = dict()           # key -> 1 (T1 probation) or 2 (T2 protected)

# Segment sizes (bytes)
t1_bytes = 0
t2_bytes = 0

# Adaptive target size (bytes) for T1 (probation)
p_target_bytes = 0

# Track last-seen capacity to clamp p and set defaults
_last_capacity = 0

# Ghost histories (size-aware, O(1) trim with versioned deque)
b1 = dict()              # key -> size (bytes) for ghosts from T1
b2 = dict()              # key -> size (bytes) for ghosts from T2
b1_bytes = 0
b2_bytes = 0
b1_order = deque()       # deque of (key, stamp) oldest first
b2_order = deque()
b1_stamp = dict()        # key -> version stamp for B1
b2_stamp = dict()        # key -> version stamp for B2

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.1           # hyperbolic exponent; >1 evicts large items more aggressively
FREQ_DAMP = 0.5          # frequency dampening for eviction score
P_INIT_FRAC = 0.5        # initial fraction of capacity assigned to T1 target
GHOST_FACTOR = 1.0       # total ghost capacity  factor * cache capacity (in bytes)
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ensure_capacity(cache_snapshot):
    """Initialize target p and clamp if capacity changed."""
    global _last_capacity, p_target_bytes
    cap = int(getattr(cache_snapshot, "capacity", 0))
    if cap <= 0:
        cap = 1
    if _last_capacity != cap:
        # Initialize or clamp p to P_INIT_FRAC of new capacity
        if _last_capacity == 0:
            p_target_bytes = int(P_INIT_FRAC * cap)
        else:
            p_target_bytes = max(0, min(cap, int(p_target_bytes)))
        _last_capacity = cap

def _ghost_limit_bytes(cache_snapshot):
    cap = int(getattr(cache_snapshot, "capacity", 1))
    return max(cap, int(GHOST_FACTOR * cap))

def _trim_ghost(cache_snapshot):
    """Trim B1+B2 total bytes to within ghost capacity, O(1) amortized."""
    global b1_bytes, b2_bytes
    limit = _ghost_limit_bytes(cache_snapshot)
    # Pop from the larger ghost first to maintain balance
    while (b1_bytes + b2_bytes) > limit:
        if b1_bytes >= b2_bytes:
            # trim B1 from oldest
            while b1_order:
                k, s = b1_order.popleft()
                if b1_stamp.get(k, None) == s and k in b1:
                    sz = b1.pop(k, 0)
                    b1_bytes -= sz
                    b1_stamp.pop(k, None)
                    break
        else:
            # trim B2 from oldest
            while b2_order:
                k, s = b2_order.popleft()
                if b2_stamp.get(k, None) == s and k in b2:
                    sz = b2.pop(k, 0)
                    b2_bytes -= sz
                    b2_stamp.pop(k, None)
                    break

def _hyperbolic_score(key, obj, now):
    """
    Eviction score within a segment: higher => evict sooner
    score = age / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq-1)))
    """
    last = m_last_access.get(key, now)
    age = max(1, now - last)
    sz = _size_of(obj)
    freq = m_freq.get(key, 1)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1)))
    return float(age) / max(_EPS, denom)

def _pick_victim_from_segment(cache_snapshot, seg_id, now):
    """
    Pick the eviction victim from segment seg_id by maximum hyperbolic score.
    Returns (key or None, score).
    """
    cache = cache_snapshot.cache
    best_k = None
    best_s = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        if m_seg.get(k, 1) != seg_id:
            continue
        s = _hyperbolic_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_k is None:
            best_k, best_s, best_la, best_sz = k, s, la, sz
            continue

        if s > best_s:
            best_k, best_s, best_la, best_sz = k, s, la, sz
            continue

        if s == best_s:
            # Tie-breakers: older last access, then larger size, then lexicographic key
            if la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))):
                best_k, best_s, best_la, best_sz = k, s, la, sz

    return best_k, best_s

def _adapt_p_on_ghost_reuse(cache_snapshot, obj_key, obj_size, from_b1):
    """
    ARC-style adaptation of p:
    - If a miss hits in B1 (recent), increase p (favor recency).
    - If a miss hits in B2 (frequent), decrease p (favor frequency).
    Size-aware increment scaled by relative ghost sizes.
    """
    global p_target_bytes
    cap = _last_capacity or int(getattr(cache_snapshot, "capacity", 1))
    cap = max(1, cap)

    if from_b1:
        # Increase p; bigger increase when B2 is larger than B1
        scale = b2_bytes / max(1.0, float(b1_bytes))
        delta = int(max(1.0, obj_size * max(1.0, scale)))
        p_target_bytes = min(cap, p_target_bytes + delta)
    else:
        # Decrease p; bigger decrease when B1 is larger than B2
        scale = b1_bytes / max(1.0, float(b2_bytes))
        delta = int(max(1.0, obj_size * max(1.0, scale)))
        p_target_bytes = max(0, p_target_bytes - delta)

def _add_to_ghost(cache_snapshot, key, size, to_b1):
    """
    Record an evicted key into B1 (if from T1) or B2 (if from T2).
    """
    global b1_bytes, b2_bytes
    if to_b1:
        b1[key] = size
        b1_bytes += size
        s = b1_stamp.get(key, 0) + 1
        b1_stamp[key] = s
        b1_order.append((key, s))
    else:
        b2[key] = size
        b2_bytes += size
        s = b2_stamp.get(key, 0) + 1
        b2_stamp[key] = s
        b2_order.append((key, s))
    _trim_ghost(cache_snapshot)

def _remove_from_ghost_if_present(key):
    """
    Remove key from ghosts if present. Returns:
    (found_in_b1: bool, found_in_b2: bool, removed_size)
    """
    global b1_bytes, b2_bytes
    in_b1 = key in b1
    in_b2 = key in b2
    removed_size = 0
    if in_b1:
        sz = b1.pop(key, 0)
        b1_bytes -= sz
        removed_size += sz
        # bump stamp to invalidate existing deque entries
        b1_stamp[key] = b1_stamp.get(key, 0) + 1
    if in_b2:
        sz = b2.pop(key, 0)
        b2_bytes -= sz
        removed_size += sz
        b2_stamp[key] = b2_stamp.get(key, 0) + 1
    return in_b1, in_b2, removed_size

def _segment_bytes(seg_id):
    return t1_bytes if seg_id == 1 else t2_bytes

def _segment_has_items(cache_snapshot, seg_id):
    cache = cache_snapshot.cache
    for k in cache.keys():
        if m_seg.get(k, 1) == seg_id:
            return True
    return False

def _recount_segments_if_needed(cache_snapshot):
    """
    Optional safety: If segment byte counters drift, recompute from cache.
    We keep it cheap; only do it when an assertion would fail.
    """
    global t1_bytes, t2_bytes
    total_calc_t1 = 0
    total_calc_t2 = 0
    for k, v in cache_snapshot.cache.items():
        seg = m_seg.get(k, 1)
        if seg == 1:
            total_calc_t1 += _size_of(v)
        else:
            total_calc_t2 += _size_of(v)
    t1_bytes, t2_bytes = total_calc_t1, total_calc_t2


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose eviction victim:
    - Prefer evicting from T1 (probation) if T1 bytes exceed p_target_bytes.
    - Otherwise evict from T2.
    - If the chosen segment is empty, fall back to the other segment.
    - Within a segment, evict the item with the largest hyperbolic score.
    """
    _ensure_capacity(cache_snapshot)
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Decide which segment to evict from (ARC-style)
    choose_seg = None
    # Handle potential drift by clamping with recount when needed
    if (t1_bytes + t2_bytes) > (cache_snapshot.size + 8):  # small tolerance
        _recount_segments_if_needed(cache_snapshot)

    if t1_bytes > p_target_bytes and _segment_has_items(cache_snapshot, 1):
        choose_seg = 1
    elif _segment_has_items(cache_snapshot, 2):
        choose_seg = 2
    elif _segment_has_items(cache_snapshot, 1):
        choose_seg = 1
    else:
        # Degenerate: if our metadata lost track, pick global best
        choose_seg = 1

    victim, _ = _pick_victim_from_segment(cache_snapshot, choose_seg, now)
    if victim is None and choose_seg == 1:
        victim, _ = _pick_victim_from_segment(cache_snapshot, 2, now)
    if victim is None and choose_seg == 2:
        victim, _ = _pick_victim_from_segment(cache_snapshot, 1, now)

    return victim


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update last access and frequency.
    - If in T1 (probation), promote to T2 (protected).
    """
    global t1_bytes, t2_bytes
    _ensure_capacity(cache_snapshot)

    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    # Update recency/frequency
    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    seg = m_seg.get(key, 1)
    if seg == 1:
        # Promote to T2
        m_seg[key] = 2
        t1_bytes = max(0, t1_bytes - sz)
        t2_bytes += sz
    # If already in T2, nothing else to do (recency already updated)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
    - Adjust p if key is found in B1/B2 (ghost hits).
    - Remove from ghost if present.
    - Insert into T1 (probation), set last access and frequency=1.
    """
    global t1_bytes, t2_bytes, p_target_bytes
    _ensure_capacity(cache_snapshot)

    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    # Adapt target p if the key is in ghost
    in_b1, in_b2, _ = _remove_from_ghost_if_present(key)
    if in_b1:
        _adapt_p_on_ghost_reuse(cache_snapshot, key, sz, from_b1=True)
    elif in_b2:
        _adapt_p_on_ghost_reuse(cache_snapshot, key, sz, from_b1=False)

    # Insert into T1 (probation)
    m_seg[key] = 1
    t1_bytes += sz
    m_last_access[key] = now
    # Start frequency at 1 (this access)
    m_freq[key] = 1

    # Clamp p to capacity
    p_target_bytes = max(0, min(_last_capacity or cache_snapshot.capacity, p_target_bytes))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Add the evicted key to the appropriate ghost (B1 if from T1, else B2).
    - Remove live metadata and update segment byte counters.
    - Trim ghosts to a bounded size.
    """
    global t1_bytes, t2_bytes
    _ensure_capacity(cache_snapshot)

    ekey = evicted_obj.key
    esize = _size_of(evicted_obj)
    seg = m_seg.get(ekey, 1)

    # Record into ghost history
    _add_to_ghost(cache_snapshot, ekey, esize, to_b1=(seg == 1))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_seg.pop(ekey, None)

    # Update segment byte counters
    if seg == 1:
        t1_bytes = max(0, t1_bytes - esize)
    else:
        t2_bytes = max(0, t2_bytes - esize)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr4ex90ie.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp2tpxtj_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfzhcho0q.pickle

Iteration 29: New subsample score 0.583432 is better than old score 0.575536. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpznm_ho5k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8esxiytv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxiu7ssxs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp47_nt_re.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptn3qntvb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmporxn63wp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq1ev2_72.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7aogpdjk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa937mike.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpys5x0unj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiozvs7_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbykmpkg1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2xker_uf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3bfcc9c3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeg352eqk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_uuw9tvr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpurj5nw24.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqeqvy0zw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6gfa959r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe46yeysh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfnu4dugs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcycrgw5n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppok0brux.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzyu6scc3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbr1k97yh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7231v996.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3jjes5zc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcj_yf6r1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx08v3vg5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9wz0lqq6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz6orwn2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa1v2719q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9gxxgh4a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpznkfabof.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo4imawx2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcm69_z3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgu0jkr5k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3q9ewa9g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmhgyyx0n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprstoxk26.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptncuwihd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaut0n_3c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6fdadbb5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjybeto_o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2w6rmo9d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp44tjoz0c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2fxg4sdl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphweehm0e.pickle

Iteration 29: Full valset score for new program: 0.22532183333333333
Iteration 29: Full train_val score for new program: 0.22532183333333333
Iteration 29: Individual valset scores for new program: [0.472772, 0.448517, 0.460215, 0.401885, 0.475477, 0.464833, 0.269139, 0.498624, 0.537004, 0.531017, 0.075, 0.310391, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.393314, 0.026164, 0.058672, 0.058672, 0.269831, 0.214718, 0.723932, 0.885162, 0.039533, 0.038636, 0.045558, 0.004585, 0.006915, 0.753404, 0.083333, 0.067961, 0.020529, 0.633694, 0.125461, 0.043804, 0.058644, 0.065657, 0.052632, 0.15, 0.040939, 0.053857, 0.453988, 0.081699]
Iteration 29: New valset pareto front scores: [0.476754, 0.457682, 0.460699, 0.417445, 0.47767, 0.465986, 0.278708, 0.498624, 0.540222, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026164, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.061364, 0.072893, 0.087816, 0.087816, 0.754765, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.120342, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 29: Full valset pareto front score: 0.27079031250000013
Iteration 29: Updated valset pareto front programs: [{4}, {7}, {7}, {3}, {4}, {7}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15}, {5}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {14, 7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}, {6, 14}, {6, 14}, {14}, {6, 14}, {6, 14}, {11}, {14}, {9, 2, 4, 15}, {14}, {14}, {2}, {3}, {8, 7}, {3}, {14}, {8}, {6, 14}, {14}, {14}, {9}, {6, 14}, {14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15}, {14}, {6}, {14}, {6}, {3, 5, 6, 7, 8}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14}, {8}]
Iteration 29: Best valset aggregate score so far: 0.24589052083333332
Iteration 29: Best program as per aggregate score on train_val: 7
Iteration 29: Best program as per aggregate score on valset: 7
Iteration 29: Best score on valset: 0.24589052083333332
Iteration 29: Best score on train_val: 0.24589052083333332
Iteration 29: Linear pareto front program index: 7
Iteration 29: New program candidate index: 15
Iteration 30: Selected program 7 score: 0.24589052083333332
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8nk1b804.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3o1s5tuf.pickle

Iteration 30: Proposed new text for program: # Adaptive Size-Aware ARC (AS-ARC)
# - Two resident segments:
#     T1 (recent, objects seen once) and T2 (frequent, objects seen >=2 times)
#   Both are maintained as LRU lists (OrderedDict), MRU at the end.
# - Two ghost segments:
#     B1 and B2 store only keys (and last known sizes) of items recently evicted from T1/T2.
#   They provide adaptive feedback to shift the target size of T1 (p_bytes).
# - Adaptation:
#     On insert of a key found in B1 -> increase p_bytes (favor recency).
#     On insert of a key found in B2 -> decrease p_bytes (favor frequency).
#   p_bytes is in bytes and clamped to [0, capacity].
# - Eviction:
#     Evict from T1 if T1_bytes > p_bytes; otherwise from T2.
#   Each eviction returns the LRU key of the chosen segment.
# - Size-aware:
#     All accounting and adaptation are done in bytes (not counts), making the policy more robust
#     when object sizes vary.
# - Complexity:
#     O(1) average-time operations, avoiding full scans to prevent timeouts on large traces.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
T1 = OrderedDict()  # key -> None (LRU -> MRU)
T2 = OrderedDict()  # key -> None (LRU -> MRU)

# Ghost segments (keys only)
B1 = OrderedDict()  # key -> None
B2 = OrderedDict()  # key -> None

# Segment assignment for live keys: 0 = T1, 1 = T2
m_seg = dict()      # key -> int

# Last known size for keys (live or ghost)
m_size = dict()     # key -> int

# Frequency counter (bounded, lightweight)
m_freq = dict()     # key -> int
FREQ_CLIP = 64

# Byte accounting for segments
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# Target bytes for T1 (adaptation knob)
p_bytes = None

# Ghost sizing
# Keep ghosts bounded by at most 2x capacity bytes (combined B1+B2).
GHOST_BYTES_FACTOR = 2.0


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _ensure_p(snapshot):
    global p_bytes
    cap = _cap(snapshot)
    if p_bytes is None:
        p_bytes = cap // 2
    else:
        if cap > 0:
            if p_bytes < 0:
                p_bytes = 0
            elif p_bytes > cap:
                p_bytes = cap
        else:
            p_bytes = 0

def _trim_ghosts(snapshot):
    # Ensure B1+B2 total bytes <= factor * capacity
    global b1_bytes, b2_bytes
    cap = _cap(snapshot)
    limit = int(GHOST_BYTES_FACTOR * max(1, cap))
    # Remove from the oldest side, biased to the larger ghost first.
    # Alternate between B1/B2 by size to keep both under control.
    while (b1_bytes + b2_bytes) > limit and (B1 or B2):
        # Prefer shrinking the larger ghost
        target = B1 if b1_bytes >= b2_bytes else B2
        if target:
            k, _ = target.popitem(last=False)
            sz = m_size.get(k, 1)
            if target is B1:
                b1_bytes -= sz
            else:
                b2_bytes -= sz
            # Size record can remain for potential future info

def _move_to_mru(od, key):
    # Reinsert to move to MRU if present
    if key in od:
        od.move_to_end(key, last=True)

def _pop_lru_from(od):
    # Pop and return LRU key, or None if empty
    if not od:
        return None
    k, _ = od.popitem(last=False)
    return k

def _lru_key_of(od):
    # Peek LRU key without removing; None if empty
    if not od:
        return None
    for k in od:
        return k
    return None

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front of the provided OrderedDict if they are not in cache.
    Adjust byte counters accordingly using m_size.
    """
    global t1_bytes, t2_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = _lru_key_of(od)
        if k is None:
            break
        if k not in cache:
            # stale; drop it
            _ = od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                t1_bytes = max(0, t1_bytes - sz)
                m_seg.pop(k, None)
            else:
                t2_bytes = max(0, t2_bytes - sz)
                m_seg.pop(k, None)
            changed = True

def _ensure_tracking_on_hit(obj):
    # Ensure we track size and bounded frequency
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    m_freq[k] = min(FREQ_CLIP, m_freq.get(k, 0) + 1)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using size-aware ARC rules:
    - Evict from T1 if T1_bytes > p_bytes, otherwise evict from T2.
    - If a chosen segment is empty due to asynchrony, fallback to the other.
    - Returns the LRU key of the chosen segment.
    """
    global t1_bytes, t2_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_p(cache_snapshot)

    # Keep segment heads free of any stale keys (defensive)
    _cleanup_stale_front(cache_snapshot, T1, seg_id=0)
    _cleanup_stale_front(cache_snapshot, T2, seg_id=1)

    # Decide segment for eviction
    choose_T1 = False
    if T1 and T2:
        choose_T1 = (t1_bytes > p_bytes)
    elif T1:
        choose_T1 = True
    elif T2:
        choose_T1 = False
    else:
        # Our structures are empty but cache is not (out of sync). Fall back to any key in cache.
        for k in cache:
            return k
        return None

    # Return LRU key from chosen segment; if empty, fallback to the other
    if choose_T1:
        k = _lru_key_of(T1)
        if k is not None:
            return k
        # fallback
        k = _lru_key_of(T2)
        return k
    else:
        k = _lru_key_of(T2)
        if k is not None:
            return k
        # fallback
        k = _lru_key_of(T1)
        return k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - If in T1: promote to T2 (move to MRU of T2).
    - If in T2: refresh recency (move to MRU).
    - If untracked (rare): insert directly into T2 as it's at least a second touch.
    """
    global t1_bytes, t2_bytes
    k = obj.key
    sz = _size_of(obj)
    _ensure_p(cache_snapshot)
    _ensure_tracking_on_hit(obj)

    if k in T1:
        # Promote T1 -> T2
        T1.pop(k, None)
        t1_bytes = max(0, t1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in T2:
        # Refresh recency
        _move_to_mru(T2, k)
        m_seg[k] = 1
    else:
        # Defensive: not tracked but a hit => treat as frequent
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1

    # Ensure size record is up-to-date
    m_size[k] = sz


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - If key is in B1: increase p_bytes by key size and place in T2.
    - If key is in B2: decrease p_bytes by key size and place in T2.
    - Else: place in T1.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, p_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    # Do not increment frequency on insert (it will happen on hits)
    m_freq[k] = m_freq.get(k, 0)

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    if k in B1:
        # Favor recency: grow T1 target
        p_bytes = min(cap, p_bytes + sz)
        # Move from B1 -> T2
        B1.pop(k, None)
        b1_bytes = max(0, b1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in B2:
        # Favor frequency: shrink T1 target
        p_bytes = max(0, p_bytes - sz)
        # Move from B2 -> T2
        B2.pop(k, None)
        b2_bytes = max(0, b2_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    else:
        # New key: goes to T1
        T1[k] = None
        t1_bytes += sz
        m_seg[k] = 0

    _trim_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting a victim:
    - Move the evicted key to the corresponding ghost list (B1 if from T1, B2 if from T2).
    - Update byte accounting and clean live metadata.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes
    _ensure_p(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)
    m_size[ekey] = esz  # record last known size for ghost

    # Determine which segment it came from
    seg = m_seg.get(ekey, None)

    if seg == 0:
        # From T1 -> B1
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        # Record into B1 (MRU)
        B1[ekey] = None
        b1_bytes += esz
    elif seg == 1:
        # From T2 -> B2
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)
        # Record into B2 (MRU)
        B2[ekey] = None
        b2_bytes += esz
    else:
        # Unknown segment (defensive): put into B1
        B1[ekey] = None
        b1_bytes += esz
        # Try to remove from T1/T2 if present
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)

    # Clean live metadata
    m_seg.pop(ekey, None)
    # Keep m_size[ekey] for ghost use; keep m_freq as lightweight history if later reused

    _trim_ghosts(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfi6uy6zc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwikpupaw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplfru234r.pickle

Iteration 30: New subsample score 0.36297 is better than old score 0.340188. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa0nt24od.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi6x5u9ic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb4qubqpa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppsoeg7vs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8mrrbaf7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphr713fyk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqog39w04.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpexcfy9yn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0az_oj0d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgube_pyy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprdk0e4qi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv8dof0m8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps_9sz9m8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr8d4aff7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2wch24ty.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp57zfkh9y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptcm64x0i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo6vfsuv9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxyy1nd9t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp33h9rysf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw_xvmpxq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx27dx4uw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2r4ydbhc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphbcosy2p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmuxburki.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2792xg1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjiaivl53.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1xnloy3m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprg4f4ccj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmparsx14dg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6o9p7idd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfdu9tqy4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0kzbzdiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnbxlkud8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfh_f9gn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdn7pzxjx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp37clz1be.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4zvdzrte.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6as3uk79.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdo0q_zco.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbs0mf8ny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcnns4uwb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd1uezwwe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplsegikq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq6jyny34.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppq0jiy7s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvwcn6rli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplcfjssuz.pickle

Iteration 30: Full valset score for new program: 0.23630508333333336
Iteration 30: Full train_val score for new program: 0.23630508333333336
Iteration 30: Individual valset scores for new program: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.269139, 0.498624, 0.541294, 0.531017, 0.083333, 0.315719, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.356932, 0.026556, 0.058672, 0.058672, 0.269818, 0.346774, 0.843098, 0.886189, 0.095538, 0.038636, 0.045558, 0.029531, 0.029959, 0.7226, 0.083333, 0.067961, 0.052978, 0.631633, 0.125461, 0.064597, 0.035431, 0.045419, 0.052632, 0.3, 0.037966, 0.047405, 0.453988, 0.081699]
Iteration 30: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.278708, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.061364, 0.072893, 0.087816, 0.087816, 0.754765, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.120342, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 30: Full valset pareto front score: 0.27159466666666676
Iteration 30: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {14, 7}, {7}, {6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}, {6, 14}, {6, 14}, {14}, {6, 14}, {6, 14}, {11}, {14}, {16}, {14}, {14}, {2}, {3}, {8, 7}, {3}, {14}, {8}, {6, 14}, {14}, {14}, {9}, {6, 14}, {14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16}, {14}, {6}, {14}, {6}, {3, 5, 6, 7, 8}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14}, {8}]
Iteration 30: Best valset aggregate score so far: 0.24589052083333332
Iteration 30: Best program as per aggregate score on train_val: 7
Iteration 30: Best program as per aggregate score on valset: 7
Iteration 30: Best score on valset: 0.24589052083333332
Iteration 30: Best score on train_val: 0.24589052083333332
Iteration 30: Linear pareto front program index: 7
Iteration 30: New program candidate index: 16
Iteration 31: Selected program 8 score: 0.23209699999999991
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_fry8fzq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphzi3tdjp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9wyr3d6o.pickle

Iteration 31: Proposed new text for program: # ReASAP-2Q: Predictive, size-aware eviction with 2Q-style protection
# Key improvements over previous ASAP version:
# 1) Corrects "overdue" handling: if an item is past its predicted next access time,
#    we treat it as imminent (delta = 0), not cold. This prevents evicting hot items.
# 2) Adds a simple 2Q-like segmentation: items enter a probation segment and are promoted
#    to protected on first hit. Eviction penalizes probation items, improving scan resistance.
# 3) Keeps size-aware normalization and mild frequency dampening.
# 4) Maintains compact ghost history for quick warm-starts.
# 5) Keeps O(1) amortized ghost trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (hit count)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25           # per-key EWMA learning rate
GLOBAL_BETA = 0.005        # global EWMA learning rate (slow)
SIZE_EXP = 1.0             # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.7            # frequency dampening in denominator
DEFAULT_MU_MULT = 4.0      # default mu multiplier for cold inserts (scaled by global mu)
GHOST_LIMIT_MIN = 1024     # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0   # ghost capacity  factor * live cache item count
STAGE_FACTOR_PROTECTED = 0.35  # multiplicative discount to eviction score for protected items
_INF = 1e30                # very large number for "infinite" predicted delay


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _predicted_delta(key, now):
    """
    Predict non-negative time-to-next-access.
    - If no predictor exists: return a very large value (cold).
    - If overdue (now > la + mu), treat as imminent: delta = 0 (do NOT penalize).
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)
    # Overdue -> delta = 0 (likely to arrive soon)
    return 0.0

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    score = (predicted_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq-1))) * stage_factor
    with a discount for protected items.
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    freq = m_freq.get(key, 1)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1)))
    base = delta / denom

    # 2Q-style protection: protected items get a multiplicative discount
    stage = m_stage.get(key, 0)
    if stage >= 1:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse a tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - New items start in probation (stage=0).
    """
    if key in g_mu:
        # Temper reused mu slightly to avoid overconfidence
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment frequency.
    - Promote from probation to protected on first hit (2Q).
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold hit without prior metadata (very rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Promotion to protected on first hit (or keep protected)
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the highest eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time.
    - Update last access time and frequency.
    - Promote to protected on first hit.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Set last access time and start in probation (stage=0).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key.
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key metadata of the evicted key.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpahniaef8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk9yemh2b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph5006j7v.pickle

Iteration 31: New subsample score 0.411166 is better than old score 0.253709. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnutiavib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfhb5u8fu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcsgtjoy2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxk5z4fkr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp34vyfvgn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpch3fms2y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmta06r7j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp81l6ffy5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_3k8lduc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqtv8mc33.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkn4_ra_1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpprgwy9j8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1sl2aib7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj63bbqrm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2si3x8cy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxqsvw_lr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxlvuzo_h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp78w6gjsv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwgyn808q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt53zmik6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpye7bvybo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqtvxttui.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3g5kgdkx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp101vtpe0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_7qxvi8z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmbu021cp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnr2pabtl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdsicpa3r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv682y5yt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcjfmf6__.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt__b8lw9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc26yeio0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn5jq9aiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2mskt_k0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdxr6ijjq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx4y52k5p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprspgjoea.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpddfbbfpb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqehfphn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx81gwt8y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj0s_1xcu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbt1x2kdt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyo6vtbci.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpazu92g_o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjvb7c8my.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpblqyp16x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0jyy9tac.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpalbcxqpj.pickle

Iteration 31: Full valset score for new program: 0.24124691666666667
Iteration 31: Full train_val score for new program: 0.24124691666666667
Iteration 31: Individual valset scores for new program: [0.396847, 0.385711, 0.385734, 0.360452, 0.395892, 0.389023, 0.239234, 0.367086, 0.494101, 0.531017, 0.108333, 0.267762, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.229148, 0.428712, 0.024596, 0.060606, 0.060606, 0.282265, 0.120968, 0.790467, 0.87147, 0.178197, 0.040909, 0.072893, 0.085997, 0.085935, 0.649932, 0.098684, 0.159753, 0.213204, 0.599691, 0.125461, 0.156917, 0.122175, 0.175962, 0.077303, 0.366667, 0.185942, 0.170547, 0.466258, 0.156863]
Iteration 31: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.278708, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.061364, 0.072893, 0.087816, 0.087816, 0.754765, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 31: Full valset pareto front score: 0.27163285416666677
Iteration 31: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {8}, {17, 6, 14}, {14}, {14}, {9}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17}, {8}]
Iteration 31: Best valset aggregate score so far: 0.24589052083333332
Iteration 31: Best program as per aggregate score on train_val: 7
Iteration 31: Best program as per aggregate score on valset: 7
Iteration 31: Best score on valset: 0.24589052083333332
Iteration 31: Best score on train_val: 0.24589052083333332
Iteration 31: Linear pareto front program index: 7
Iteration 31: New program candidate index: 17
Iteration 32: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppwvkk2tr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_2zsbdx_.pickle

Killing subprocess...
Iteration 32: Proposed new text for program: # GreedyDual-Size with Segmented Protection and ARC-style Adaptation (GDS-SP-ARC)
# Goals:
# - High hit rate with simple, fast metadata maintenance (avoid timeouts).
# - Size-aware priority using GreedyDual-Size (GDS): priority h = L + credit(size).
# - Two segments: probation (T1) and protected (T2) with adaptive target split (ARC-style).
# - Ghost histories for T1 and T2 (B1/B2) to adapt segment target without heavy operations.
# - O(1) amortized ghost trimming and lightweight scans on eviction.

# ----------------------
# Global metadata
# ----------------------
# GDS priorities
m_h = dict()           # key -> float (priority)
GDS_L = 0.0            # global aging value

# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()         # key -> int

# Lightweight stats
m_last_access = dict() # key -> int (for LRU-ish tie-breaks)
m_hits = dict()        # key -> int (hit count, optional)

# Ghost histories for ARC-style adaptation
# B1: recently evicted from probation (T1); B2: recently evicted from protected (T2)
b1_set = dict()        # key -> last time seen
b2_set = dict()        # key -> last time seen
b1_q = []              # append-only queue of keys in B1 order of insertion
b2_q = []              # append-only queue of keys in B2 order of insertion
b1_head = 0            # pop pointer for B1 queue
b2_head = 0            # pop pointer for B2 queue

# Target protected fraction (ARC-style). Adapted on ghost hits.
p_ratio = 0.50         # in [0, 1]

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-aware credit: smaller items get higher credit.
ALPHA_SIZE = 0.7

# Credit per event
CREDIT_INSERT = 0.25       # initial credit on insert (bypasses low-value items until they get a hit)
CREDIT_HIT = 1.0           # credit added on hit
CREDIT_GHOST_B1 = 0.7      # extra multiplier if key was seen in B1 (recently evicted from probation)
CREDIT_GHOST_B2 = 1.4      # extra multiplier if key was seen in B2 (recently evicted from protected)

# ARC-style adaptation step on ghost hit
ADAPT_STEP = 0.06          # adjust p_ratio by this step
P_MIN = 0.05
P_MAX = 0.95

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 3.0   # total ghost entries ~ 3x live item count (amortized O(1) trimming)

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _credit_for_size(sz, base_credit):
    return float(base_credit) / (float(sz) ** ALPHA_SIZE)

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _ghost_budgets(cache_snapshot):
    # Split ghost budget between B1 and B2 based on p_ratio (favor B2 when protected segment target is larger).
    total = _ghost_capacity_limit(cache_snapshot)
    b2_budget = int(round(p_ratio * total))
    if b2_budget < 1:
        b2_budget = 1
    if b2_budget > total - 1:
        b2_budget = total - 1
    b1_budget = total - b2_budget
    return b1_budget, b2_budget

def _trim_ghost(cache_snapshot):
    # Trim B1 and B2 to their budgets using amortized O(1) pop from the append-only queues.
    global b1_head, b2_head
    b1_budget, b2_budget = _ghost_budgets(cache_snapshot)

    # B1 trimming
    while len(b1_set) > b1_budget:
        # advance head to next valid key
        while b1_head < len(b1_q) and b1_q[b1_head] not in b1_set:
            b1_head += 1
        if b1_head >= len(b1_q):
            break
        k = b1_q[b1_head]
        b1_head += 1
        b1_set.pop(k, None)

    # B2 trimming
    while len(b2_set) > b2_budget:
        while b2_head < len(b2_q) and b2_q[b2_head] not in b2_set:
            b2_head += 1
        if b2_head >= len(b2_q):
            break
        k = b2_q[b2_head]
        b2_head += 1
        b2_set.pop(k, None)

def _record_ghost_on_evict(evicted_key, now, was_protected):
    # Add the evicted key to B1 or B2 depending on its segment at eviction.
    if was_protected:
        b2_set[evicted_key] = now
        b2_q.append(evicted_key)
    else:
        b1_set[evicted_key] = now
        b1_q.append(evicted_key)

def _maybe_adapt_on_insert(key):
    global p_ratio
    # ARC-style adaptation:
    # - ghost hit in B1 (recent) => increase recency (decrease protected target) => p_ratio -= step
    # - ghost hit in B2 (frequent) => increase protected target => p_ratio += step
    if key in b1_set and key not in b2_set:
        p_ratio = max(P_MIN, p_ratio - ADAPT_STEP)
    elif key in b2_set:
        p_ratio = min(P_MAX, p_ratio + ADAPT_STEP)

def _init_if_missing_on_live(key, now, obj):
    # Ensure minimal metadata exists for live items (safety for cold starts).
    if key not in m_h:
        m_h[key] = float(GDS_L)
    if key not in m_seg:
        m_seg[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_hits:
        m_hits[key] = 0


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict from the segment that exceeds its target:
      - target protected count = round(p_ratio * total_items)
      - if protected_count > target => evict from protected; else from probation (if any)
    Within a segment, choose the item with the lowest GDS priority h.
    Tie-breakers:
      - older last access (LRU-ish)
      - larger size (frees more space)
      - lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Compute segment counts and track best candidates per segment
    total_items = 0
    prot_count = 0

    best0_key = None  # best victim in probation
    best0_val = None
    best0_h = None
    best0_la = None
    best0_sz = None

    best1_key = None  # best victim in protected
    best1_val = None
    best1_h = None
    best1_la = None
    best1_sz = None

    now = _now(cache_snapshot)

    for k, v in cache.items():
        total_items += 1
        seg = m_seg.get(k, 0)
        if seg == 1:
            prot_count += 1

        h = m_h.get(k, GDS_L)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        # Select per-segment minimal h
        if seg == 0:
            if best0_key is None:
                best0_key, best0_val, best0_h, best0_la, best0_sz = k, v, h, la, sz
            else:
                better = False
                if h < best0_h - _EPS:
                    better = True
                elif abs(h - best0_h) <= _EPS:
                    if la < best0_la:
                        better = True
                    elif la == best0_la:
                        if sz > best0_sz:
                            better = True
                        elif sz == best0_sz and k < best0_key:
                            better = True
                if better:
                    best0_key, best0_val, best0_h, best0_la, best0_sz = k, v, h, la, sz
        else:
            if best1_key is None:
                best1_key, best1_val, best1_h, best1_la, best1_sz = k, v, h, la, sz
            else:
                better = False
                if h < best1_h - _EPS:
                    better = True
                elif abs(h - best1_h) <= _EPS:
                    if la < best1_la:
                        better = True
                    elif la == best1_la:
                        if sz > best1_sz:
                            better = True
                        elif sz == best1_sz and k < best1_key:
                            better = True
                if better:
                    best1_key, best1_val, best1_h, best1_la, best1_sz = k, v, h, la, sz

    target_prot = int(round(p_ratio * max(1, total_items)))
    # Decide which segment to evict from
    if prot_count > target_prot and best1_key is not None:
        return best1_key
    elif best0_key is not None:
        return best0_key
    elif best1_key is not None:
        return best1_key
    else:
        return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update last access and hit count.
      - Promote to protected segment if currently probation.
      - Increase priority via size-aware credit: h = max(h, L) + credit(size).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _init_if_missing_on_live(key, now, obj)

    # Promote on first hit from probation to protected
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1

    # Update stats
    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Update GDS priority
    sz = _size_of(obj)
    incr = _credit_for_size(sz, CREDIT_HIT)
    cur = m_h.get(key, GDS_L)
    if cur < GDS_L:
        cur = GDS_L
    m_h[key] = cur + incr


def update_after_insert(cache_snapshot, obj):
    """
    On insert (post-admission by the framework):
      - Adjust ARC target p_ratio if key is in ghosts (B1/B2).
      - Initialize last access and hits.
      - Start in protected if coming from ghosts (ARC behavior), else probation.
      - Initialize GDS priority with global age plus size-aware credit; boost if from B2/B1.
      - Remove the key from ghosts to prevent duplicate ghost entries.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Adapt target split based on which ghost the key hits
    _maybe_adapt_on_insert(key)

    from_b2 = key in b2_set
    from_b1 = key in b1_set

    # Remove from ghosts (now the key is live again)
    if from_b1:
        b1_set.pop(key, None)
    if from_b2:
        b2_set.pop(key, None)

    # Initialize stats
    m_last_access[key] = now
    m_hits[key] = 0

    # Segment on insert: start protected if returning (ARC); else probation
    if from_b2 or from_b1:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

    # Initialize priority
    sz = _size_of(obj)
    base_credit = CREDIT_INSERT
    if from_b2:
        base_credit *= CREDIT_GHOST_B2
    elif from_b1:
        base_credit *= CREDIT_GHOST_B1

    m_h[key] = max(float(GDS_L), m_h.get(key, float(GDS_L))) + _credit_for_size(sz, base_credit)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Update global age L to the evicted item's priority (GDS aging).
      - Record the evicted key in the appropriate ghost (B1 for probation, B2 for protected).
      - Trim ghosts to budget (amortized O(1)).
      - Cleanup live metadata of the evicted key.
    """
    global GDS_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Capture which segment it was in
    was_protected = (m_seg.get(ekey, 0) == 1)

    # Update GDS global age to victim's priority
    ev_h = m_h.get(ekey, float(GDS_L))
    GDS_L = float(ev_h)

    # Record in appropriate ghost history
    _record_ghost_on_evict(ekey, now, was_protected)
    _trim_ghost(cache_snapshot)

    # Cleanup live metadata
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_h.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp74k_8eqc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp19kbqw78.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptym0ooh4.pickle

Iteration 32: New subsample score 0.9040679999999999 is better than old score 0.585575. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1g8awb6v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp00drt728.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr27lfn57.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw1ih_foj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo379drhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9isrmlal.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjtqp8x1b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpikvfouvx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnebk1m8w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpooeeccgv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0wyu6f5b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsi1gi5_0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg8msl2th.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl1l0l005.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxh26kmci.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpay5a5hu1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptuuoeq9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9mj562bs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpabkyvyt0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqvu3kttm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_8dwabz7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx16eormr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0_hp4j2d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1t2udhkc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw9_rub6e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqu7xh9as.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfbyfsnzn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpat2s575m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwfrhpuv8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvm_hdc_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn1u7_i1s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphl4ckjgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3mou75dt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4h6gx3jc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr6ordaay.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2bz_r1ps.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplsd_1z6l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprasjsv3_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxwzh0kq0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3fpmq6wo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqjakfmbw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprzjzhapz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5fd4bu9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnri2dl1k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyd_ox41h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1dv4eshs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8b187fuy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiewk0kv7.pickle

Iteration 32: Full valset score for new program: 0.22947741666666666
Iteration 32: Full train_val score for new program: 0.22947741666666666
Iteration 32: Individual valset scores for new program: [0.446728, 0.451144, 0.457063, 0.345603, 0.431941, 0.457512, 0.251196, 0.498624, 0.537361, 0.531017, 0.108333, 0.368117, 0.040045, 0.0, 0.020954, 0.021133, 0.020062, 0.023615, 0.022782, 0.269478, 0.384464, 0.026164, 0.058672, 0.058672, 0.276017, 0.327621, 0.816286, 0.888413, 0.021863, 0.077273, 0.05467, 0.000251, 0.000216, 0.749319, 0.078947, 0.110327, 0.018705, 0.629057, 0.125461, 0.023011, 0.022602, 0.024409, 0.055921, 0.316667, 0.021423, 0.023562, 0.460123, 0.062092]
Iteration 32: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.278708, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.754765, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 32: Full valset pareto front score: 0.27196429166666675
Iteration 32: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {11}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {18}, {17, 6, 14}, {14}, {14}, {9}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17}, {8}]
Iteration 32: Best valset aggregate score so far: 0.24589052083333332
Iteration 32: Best program as per aggregate score on train_val: 7
Iteration 32: Best program as per aggregate score on valset: 7
Iteration 32: Best score on valset: 0.24589052083333332
Iteration 32: Best score on train_val: 0.24589052083333332
Iteration 32: Linear pareto front program index: 7
Iteration 32: New program candidate index: 18
Iteration 33: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp64_6kzw2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjlrrk25v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxvf66y0n.pickle

Iteration 33: Proposed new text for program: # Improved: Size-aware, Doorkeeper-TinyLFU + Segmented (W/P/S) with
#           smarter admission, score-based victim selection in P,
#           large-object aware routing, and adaptive S fraction.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Doorkeeper (filters first occurrences to reduce TinyLFU noise)
DOOR_MAX = 32768
m_door = dict()          # key -> last_seen_time (LRU-ish by timestamp)
m_door_ops = 0
DOOR_DECAY_EVERY = 50000 # periodically clear the doorkeeper to avoid unbounded bias

# Adaptive window sizing (ARC-inspired, but by fraction of bytes)
m_W_frac = 0.20          # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Adaptive protected fraction within main
m_S_frac = 0.80          # start with 80% of main in protected
S_FRAC_MIN = 0.50
S_FRAC_MAX = 0.95
S_FRAC_STEP = 0.02
m_s_promotions = 0
m_s_demotions = 0
m_last_s_adjust_access = 0
S_ADAPT_EVERY = 30000

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Score = estimate / size^ALPHA
ALPHA = 0.90
ADMIT_EPS = 1e-12
EPS = 1e-12

# Large-object routing thresholds (insert directly to P if very large)
LARGE_FRAC_OF_CAP = 0.50  # if obj.size > 50% of capacity -> insert into P (not W)

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    # 64-bit mix for stability
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        # periodic aging (halve all counters)
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    # Compute targets for Window and Protected (by bytes)
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)
    main_target = max(0, cap - w_target)
    s_target = int(main_target * m_S_frac)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _insert_into_segment(key, seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    m_seg[key] = seg
    if seg == SEG_WINDOW:
        m_bytes_W += size
    elif seg == SEG_PROBATION:
        m_bytes_P += size
    elif seg == SEG_PROTECTED:
        m_bytes_S += size

def _maybe_adjust_S_frac(cache_snapshot):
    global m_s_promotions, m_s_demotions, m_last_s_adjust_access, m_S_frac
    now = cache_snapshot.access_count
    if now - m_last_s_adjust_access < S_ADAPT_EVERY:
        return
    m_last_s_adjust_access = now
    # If many promotions stick to S (few demotions), reflect that by increasing S fraction.
    promos = m_s_promotions
    demos = m_s_demotions
    # reset counters
    m_s_promotions = 0
    m_s_demotions = 0
    if promos > demos * 1.10:
        m_S_frac = min(S_FRAC_MAX, m_S_frac + S_FRAC_STEP)
    elif demos > promos * 1.10:
        m_S_frac = max(S_FRAC_MIN, m_S_frac - S_FRAC_STEP)

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S, m_s_demotions
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)
    m_s_demotions += 1

def _door_should_count(cache_snapshot, key):
    """
    Doorkeeper: returns True if this key should be counted into the TinyLFU sketch now.
    On first sight in a short window, we record it but do not count. On repeat, we count.
    Periodically reset to avoid stale positives.
    """
    global m_door_ops, m_door
    now = cache_snapshot.access_count
    m_door_ops += 1
    if key in m_door:
        # repeat within door horizon -> count
        m_door[key] = now
        # Aging/prune
        if len(m_door) > DOOR_MAX:
            # evict oldest by timestamp
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return True
    else:
        # first occurrence -> record only
        m_door[key] = now
        if len(m_door) > DOOR_MAX:
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return False

def _score_for_key(cache_snapshot, key):
    """
    Size-aware frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate / size^ALPHA
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    return est / max(1.0, pow(sz, ALPHA))

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return est / max(1.0, pow(sz, ALPHA))

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

def _p_victim_by_score(cache_snapshot, exclude_key=None):
    """
    Choose a victim in Probation with the lowest score (TinyLFU/size^ALPHA).
    Tie-breakers:
      1) lower score
      2) older last_access
      3) larger size (frees more)
      4) lexicographic key
    """
    best_k = None
    best_score = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != SEG_PROBATION:
            continue
        sc = _score_for_key(cache_snapshot, k)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_score, best_la, best_sz = k, sc, la, sz
            continue
        if sc < best_score - EPS:
            best_k, best_score, best_la, best_sz = k, sc, la, sz
            continue
        if abs(sc - best_score) <= EPS:
            if la < best_la:
                best_k, best_score, best_la, best_sz = k, sc, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_k, best_score, best_la, best_sz = k, sc, la, sz
                    continue
                if sz == best_sz and k < best_k:
                    best_k, best_score, best_la, best_sz = k, sc, la, sz
    return best_k

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision (improved W-TinyLFU):
      - Keep Window close to its target (evict W-LRU if oversized).
      - On predicted Window overflow at insertion:
         * Compare new item's score vs. the worst (by score) victim in Probation.
         * If new wins, evict that P victim (admit); else evict W-LRU (reject).
      - If no predicted overflow, evict from P (by worst score); if empty, fall back to W-LRU; then S-LRU.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_sz = _size_of(obj)
    new_score = _score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_bytes_W + new_sz) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)

    if predict_overflow:
        p_victim = _p_victim_by_score(cache_snapshot)
        if p_victim is None:
            # No P items; evict from W if possible, else from S
            if w_lru is not None:
                return w_lru
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            return s_lru
        else:
            # Admission comparison: size-aware score
            p_score = _score_for_key(cache_snapshot, p_victim)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict P victim; W-LRU (if any) will be moved to P in update_after_insert
                return p_victim
            else:
                # Reject: evict from W to keep W within bound
                if w_lru is not None:
                    return w_lru
                # If W empty, evict from P anyway
                return p_victim

    # No predicted W overflow: evict from P by worst score if possible; else W; else S.
    p_victim = _p_victim_by_score(cache_snapshot)
    if p_victim is not None:
        return p_victim
    if w_lru is not None:
        return w_lru
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update doorkeeper/sketch and recency.
      - W: keep in W (recency buffer behavior).
      - P: promote to S.
      - S: refresh recency.
      - Enforce S target by demoting S-LRU to P if needed.
      - Periodically adapt S fraction based on promotions/demotions balance.
    """
    _ensure_sketch(cache_snapshot)

    # On hit, always count into TinyLFU
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        # window is pure recency: keep in W, just refresh timestamp
        pass
    elif seg == SEG_PROBATION:
        # Promote to protected
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        # Track promotions for S-fraction adaptation
        global m_s_promotions
        m_s_promotions += 1
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency; structure unchanged
        pass

    _maybe_adjust_S_frac(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Doorkeeper: only count into TinyLFU if it is not the first sighting in short horizon.
      - Adaptive window sizing via ghosts:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - Large-object routing: very large objects go directly to Probation to avoid blowing up W.
      - Otherwise insert into Window (recency admission).
      - If Window exceeds its target, move its LRU to P (keeps W near target).
      - Keep Protected within target by demoting S-LRU to P when needed.
    """
    _ensure_sketch(cache_snapshot)

    # Doorkeeper filtering (reduce sketch noise from one-timers)
    if _door_should_count(cache_snapshot, obj.key):
        _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Choose insertion segment (large-object aware)
    sz = _size_of(obj)
    cap = max(1, int(cache_snapshot.capacity))
    w_target, _ = _targets(cache_snapshot)
    insert_to_P = (sz > int(cap * LARGE_FRAC_OF_CAP)) or (sz > w_target and w_target > 0)

    if insert_to_P:
        _insert_into_segment(key, SEG_PROBATION, sz)
    else:
        _insert_into_segment(key, SEG_WINDOW, sz)

    # Keep W within its target by moving its LRU to P if needed
    if m_bytes_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is not None:
            v = cache_snapshot.cache.get(candidate)
            if v is not None:
                csz = _size_of(v)
                _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)

    _maybe_adjust_S_frac(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
    # Clean doorkeeper entry to avoid stale growth
    m_door.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1j31qvac.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnwpqzwmc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplmf4408o.pickle

Iteration 33: New subsample score 1.04355 is better than old score 1.02664. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv8hz4ms9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpffp9ulue.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsxyh9x86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvf5ilsll.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplp5pcj1r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6bhlt48a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpozwnte56.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp69mjpp10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplk4vxcvf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoqnq2s6n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptj8fcjv2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwpqnqoov.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgksxgp5s.pickle

Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplnrncz3q.pickle

Killing subprocess...
Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq80qxb9j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpox1xceph.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7jxqfc2v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0t_l6kzs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw0i_786f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuxya2c46.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvnbvyn4y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1bmobnya.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpecxq4s_2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbgwc86b4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5p2my4f1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwf14czso.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpov0tpb1c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaupyq7o9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnkrl_gbe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbi3x_gfg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp13hf5l74.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphpnjncru.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptq7yrp3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzoaquklx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo48t1hqz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv0l6u9lo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk_5h5d9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiodexq2r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm_z_ajmg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl9nmyf1t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpctjy8o05.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe40_578i.pickle

Iteration 33: Full valset score for new program: 0.24042418749999997
Iteration 33: Full train_val score for new program: 0.24042418749999997
Iteration 33: Individual valset scores for new program: [0.465131, 0.43912, 0.448155, 0.405619, 0.459699, 0.451862, 0.288278, 0.498624, 0.521988, 0.531017, 0.108333, 0.311279, 0.064533, 0.0, 0.0, 0.02226, 0.0, 0.0, 0.0, 0.272227, 0.40118, 0.0, 0.058672, 0.059961, 0.307149, 0.298387, 0.747766, 0.868903, 0.119197, 0.036364, 0.066059, 0.054921, 0.054921, 0.761572, 0.087719, 0.126214, 0.118341, 0.603812, 0.118081, 0.099251, 0.092242, 0.11911, 0.077303, 0.15, 0.125638, 0.115568, 0.466258, 0.117647]
Iteration 33: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.761572, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 33: Full valset pareto front score: 0.27230547916666675
Iteration 33: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {18}, {17, 6, 14}, {14}, {14}, {19}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19}, {8}]
Iteration 33: Best valset aggregate score so far: 0.24589052083333332
Iteration 33: Best program as per aggregate score on train_val: 7
Iteration 33: Best program as per aggregate score on valset: 7
Iteration 33: Best score on valset: 0.24589052083333332
Iteration 33: Best score on train_val: 0.24589052083333332
Iteration 33: Linear pareto front program index: 7
Iteration 33: New program candidate index: 19
Iteration 34: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4y3tv_mi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps88wiz0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1zv2zkqn.pickle

Iteration 34: Proposed new text for program: # GreedyDual-Size-Frequency with Segmented Protection (GDSF-SP)
# Goals:
# - Strong size-awareness via GreedyDual-Size (GDS) with frequency reinforcement (GDSF).
# - Aging via global L parameter: priorities naturally decay as L rises with evictions.
# - Segmented protection: prefer evicting probationary (never-hit) items before protected ones.
# - Lightweight metadata with robust tie-breakers (LRU, size).
# - Ghost reuse: warm-start on reinsert based on prior hit count to reduce rewarm misses.

import math

# ----------------------
# Global metadata
# ----------------------
# GDSF priority (H), per key
m_H = dict()          # key -> float
# Segment: 0 = probation (new/never-hit), 1 = protected (hit at least once or promoted)
m_seg = dict()        # key -> int
# Last access time (for LRU tiebreakers)
m_last = dict()       # key -> int
# Hit count (for promotion decisions and ghost warm-start)
m_hits = dict()       # key -> int

# Global aging parameter for GreedyDual (rises to evicted item's H)
g_L = 0.0

# Ghost metadata to warm-start on reinsert
g_hits = dict()       # key -> int (evicted cumulative hits)
g_last = dict()       # key -> int (last seen time at eviction)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size exponent (penalize larger items)
ALPHA_SIZE = 0.85

# Segment promotion policy
# Target protected fraction (soft cap; frequent items can exceed it)
PROTECTED_FRACTION = 0.30
# Minimum hits to force promotion even if target protected fraction is reached
PROMOTE_HITS = 2

# Ghost decay (to avoid stale ghosts dominating)
GHOST_HALF_LIFE = 4096.0  # in access-count units
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0  # ghost size limit relative to live item count

_EPS = 1e-12


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _credit_for(obj):
    # Size-aware credit for admission/hits
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE)

def _protected_target(cache_snapshot):
    cache = getattr(cache_snapshot, "cache", {}) or {}
    n = max(1, len(cache))
    return max(1, int(PROTECTED_FRACTION * n))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_hits) <= limit:
        return
    # Evict oldest ghost entries by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_hits) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_hits.pop(k, None)
        g_last.pop(k, None)

def _ghost_hits_decayed(key, now):
    """
    Return a decayed ghost hit count for warm-starting.
    """
    h = g_hits.get(key, 0)
    if h <= 0:
        return 0
    last = g_last.get(key, now)
    delta = max(0.0, float(now - last))
    if GHOST_HALF_LIFE <= 0:
        return h
    decay = 0.5 ** (delta / GHOST_HALF_LIFE)
    val = float(h) * decay
    # Round down slightly to be conservative
    return int(math.floor(val + 1e-6))

def _ensure_seeded_on_access(cache_snapshot, obj):
    """
    Ensure metadata exists for key (handles corner cases).
    """
    key = obj.key
    now = _now(cache_snapshot)
    global g_L

    if key in m_H:
        # Already present
        if key not in m_last:
            m_last[key] = now
        if key not in m_seg:
            m_seg[key] = 0
        if key not in m_hits:
            m_hits[key] = 0
        return

    # Seed on cold access (rare; normally handled by update_after_insert)
    base_credit = _credit_for(obj)
    gh = _ghost_hits_decayed(key, now)
    # If we've seen it before (ghost), warm-start both priority and segment
    bonus = math.sqrt(max(0, gh)) * base_credit
    m_H[key] = float(g_L) + base_credit + bonus
    m_seg[key] = 1 if gh >= PROMOTE_HITS else 0
    m_hits[key] = 0
    m_last[key] = now


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest GreedyDual-Size-Frequency priority H.
    - Prefer evicting from probationary segment first (seg=0).
    - If no probationary items exist, evict global min H.
    Tie-breakers (in order):
      - Lower H first
      - Probationary over protected (only when considering global set)
      - Older last access (LRU)
      - Larger size
      - Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Find candidate in probation
    prob_victim = None   # tuple of (H, last_access, size, key)
    prot_victim = None

    for k, v in cache.items():
        H = float(m_H.get(k, 0.0))
        la = int(m_last.get(k, now))
        sz = _size_of(v)
        seg = int(m_seg.get(k, 0))

        cand = (H, la, sz, k)

        if seg == 0:
            if prob_victim is None:
                prob_victim = cand
            else:
                # Compare for eviction
                if cand[0] < prob_victim[0] - _EPS:
                    prob_victim = cand
                elif abs(cand[0] - prob_victim[0]) <= _EPS:
                    # LRU
                    if cand[1] < prob_victim[1]:
                        prob_victim = cand
                    elif cand[1] == prob_victim[1]:
                        # Free more quickly
                        if cand[2] > prob_victim[2]:
                            prob_victim = cand
                        elif cand[2] == prob_victim[2] and cand[3] < prob_victim[3]:
                            prob_victim = cand
        else:
            if prot_victim is None:
                prot_victim = cand
            else:
                if cand[0] < prot_victim[0] - _EPS:
                    prot_victim = cand
                elif abs(cand[0] - prot_victim[0]) <= _EPS:
                    if cand[1] < prot_victim[1]:
                        prot_victim = cand
                    elif cand[1] == prot_victim[1]:
                        if cand[2] > prot_victim[2]:
                            prot_victim = cand
                        elif cand[2] == prot_victim[2] and cand[3] < prot_victim[3]:
                            prot_victim = cand

    victim_tuple = prob_victim if prob_victim is not None else prot_victim
    if victim_tuple is None:
        return None
    return victim_tuple[3]


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increase GDSF priority by size-aware credit.
      - Update last access time and hit count.
      - Promote to protected if eligible (frequency- or capacity-driven).
    """
    key = obj.key
    now = _now(cache_snapshot)

    _ensure_seeded_on_access(cache_snapshot, obj)

    # Priority reinforcement (GDSF): add credit per hit
    credit = _credit_for(obj)
    m_H[key] = float(m_H.get(key, 0.0)) + credit

    # Update recency and frequency
    m_last[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Segmented protection: promote from probation on first hit (or if meets hit threshold),
    # but throttle by target protected fraction unless sufficiently frequent.
    if m_seg.get(key, 0) == 0:
        protected_count = sum(1 for s in m_seg.values() if s == 1)
        target = _protected_target(cache_snapshot)
        if protected_count < target or m_hits.get(key, 0) >= PROMOTE_HITS:
            m_seg[key] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Initialize GDSF priority as H = L + credit (+ ghost-based bonus).
      - Start in probation; if ghost shows prior frequency >= PROMOTE_HITS, start protected.
      - Initialize last access time and reset hit count.
    """
    key = obj.key
    now = _now(cache_snapshot)
    global g_L

    base_credit = _credit_for(obj)

    # Warm-start from ghost hits (decayed)
    gh = _ghost_hits_decayed(key, now)
    bonus = math.sqrt(max(0, gh)) * base_credit

    m_H[key] = float(g_L) + base_credit + bonus
    m_seg[key] = 1 if gh >= PROMOTE_HITS else 0
    m_last[key] = now
    m_hits[key] = 0


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Advance global aging parameter L to the evicted item's priority (classic GDS).
      - Record ghost metadata (decayed hit count) to warm-start future inserts.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)
    global g_L

    # Advance L to evicted item's H (if known)
    evicted_H = float(m_H.get(ekey, g_L))
    if evicted_H > g_L:
        g_L = evicted_H

    # Record ghost stats for warm-start
    gh = m_hits.get(ekey, 0)
    if gh > 0:
        g_hits[ekey] = gh
        g_last[ekey] = now
        _trim_ghost(cache_snapshot)
    else:
        # Even with zero hits, remember last seen time (weak signal)
        g_hits.pop(ekey, None)
        g_last[ekey] = now
        _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_H.pop(ekey, None)
    m_seg.pop(ekey, None)
    m_last.pop(ekey, None)
    m_hits.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3fldjvgv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwhz4101v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppto43jk5.pickle

Iteration 34: New subsample score 0.613513 is not better than old score 0.789068, skipping
Iteration 35: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm0ikye0k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp889adne_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3cdt5fbk.pickle

Iteration 35: Proposed new text for program: # Windowed TinyLFU-inspired, Size-Aware SLRU
# - Three resident segments (all OrderedDict with LRU->MRU order):
#     W  (Window/Eden; brand-new items)
#     P  (Main Probation; admitted but unproven)
#     Q  (Main Protected; frequently used)
# - Admission/Eviction:
#     When space is needed, choose a victim mostly between W's LRU and P's LRU by
#     comparing frequency-per-byte of the incoming object vs P's LRU (TinyLFU-style).
#     Fall back to the best among available LRU candidates if required.
# - Hits:
#     On hit in W or P -> promote to Q (protected). On hit in Q -> refresh MRU.
#     Q is bounded by q_target and excess demotes its LRU back to P (no eviction).
# - Size-aware:
#     All budgets and comparisons use bytes. Frequency comparisons use frequency-per-byte
#     (favoring small but frequent items).
# - Frequency estimator:
#     Lightweight per-key counter with lazy exponential decay using epochs to avoid stale history.
#     Counts are updated on both hits and misses (TinyLFU principle).
# - Adaptive window size:
#     w_target_bytes is adjusted by feedback:
#        - If we evict from P in favor of a new access, increase window target.
#        - If we evict from W instead, decrease window target.
#     Clamped between [min_w_frac, max_w_frac] of capacity.
# - Complexity:
#     O(1) average per operation. No full scans.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Segments
W = OrderedDict()   # Window/Eden: key -> None
P = OrderedDict()   # Main Probation: key -> None
Q = OrderedDict()   # Main Protected: key -> None

# Segment assignment for live keys: 0 = W, 1 = P, 2 = Q
m_seg = dict()      # key -> int

# Size map for keys (live)
m_size = dict()     # key -> int

# Lightweight frequency with lazy aging
m_freq = dict()     # key -> int
m_fepoch = dict()   # key -> last epoch applied

# Byte accounting for segments
w_bytes = 0
p_bytes = 0
q_bytes = 0

# Targets
w_target_bytes = None  # adaptive window target
# q_target_bytes is derived from capacity and w_target_bytes

# ----------------------
# Tunables
# ----------------------
W_FRACTION_DEFAULT = 0.125
PROTECTED_FRACTION_DEFAULT = 0.80
MIN_W_FRAC = 0.03
MAX_W_FRAC = 0.50

# Frequency settings
FREQ_MAX = 255
AGING_WINDOW = 8192        # accesses per epoch
DECAY_SHIFT_PER_EPOCH = 1  # divide by 2 per epoch

# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _epoch(snapshot):
    return max(0, _now(snapshot) // max(1, AGING_WINDOW))

def _ensure_targets(snapshot):
    global w_target_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        w_target_bytes = 0
        return
    if w_target_bytes is None:
        w_target_bytes = int(W_FRACTION_DEFAULT * cap)
    # Clamp within [MIN_W_FRAC, MAX_W_FRAC] of capacity
    min_w = int(MIN_W_FRAC * cap)
    max_w = int(MAX_W_FRAC * cap)
    if w_target_bytes < min_w:
        w_target_bytes = min_w
    elif w_target_bytes > max_w:
        w_target_bytes = max_w

def _q_target(snapshot):
    cap = _cap(snapshot)
    if cap <= 0:
        return 0
    main_bytes = max(0, cap - int(w_target_bytes or 0))
    return int(PROTECTED_FRACTION_DEFAULT * main_bytes)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _pop_lru_from(od):
    if not od:
        return None
    k, _ = od.popitem(last=False)
    return k

def _lru_key_of(od):
    if not od:
        return None
    for k in od:
        return k
    return None

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front (LRU) if they are not in cache.
    Adjust byte counters accordingly using m_size and m_seg.
    """
    global w_bytes, p_bytes, q_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = _lru_key_of(od)
        if k is None:
            break
        if k not in cache:
            od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                w_bytes = max(0, w_bytes - sz)
            elif seg_id == 1:
                p_bytes = max(0, p_bytes - sz)
            else:
                q_bytes = max(0, q_bytes - sz)
            if m_seg.get(k) == seg_id:
                m_seg.pop(k, None)
            # keep m_size/m_freq as history
            changed = True

def _cleanup_segments(snapshot):
    _cleanup_stale_front(snapshot, W, seg_id=0)
    _cleanup_stale_front(snapshot, P, seg_id=1)
    _cleanup_stale_front(snapshot, Q, seg_id=2)

def _age_key_freq(snapshot, k):
    """Lazy age a key's frequency to current epoch."""
    cur_epoch = _epoch(snapshot)
    last = m_fepoch.get(k, cur_epoch)
    if cur_epoch > last:
        delta = min(8, cur_epoch - last)  # cap decay steps
        if k in m_freq:
            m_freq[k] = max(0, m_freq.get(k, 0) >> (DECAY_SHIFT_PER_EPOCH * delta))
        m_fepoch[k] = cur_epoch
    else:
        m_fepoch[k] = cur_epoch

def _freq_get(snapshot, k):
    _age_key_freq(snapshot, k)
    return m_freq.get(k, 0)

def _freq_bump(snapshot, k, inc=1):
    _age_key_freq(snapshot, k)
    m_freq[k] = min(FREQ_MAX, m_freq.get(k, 0) + max(0, inc))

def _metric_per_byte(snapshot, k, sz=None, plus_one=False):
    """Return TinyLFU-like admission metric: freq_per_byte."""
    if sz is None:
        sz = m_size.get(k, 1)
    f = _freq_get(snapshot, k)
    if plus_one:
        f = min(FREQ_MAX, f + 1)
    return float(f) / float(max(1, sz))

def _maybe_demote_protected(snapshot):
    """If Q exceeds its target, demote its LRU to P (no eviction)."""
    global q_bytes, p_bytes
    q_target = _q_target(snapshot)
    while q_bytes > q_target and Q:
        k = _pop_lru_from(Q)
        if k is None:
            break
        sz = m_size.get(k, 1)
        q_bytes = max(0, q_bytes - sz)
        # Move to P MRU
        P[k] = None
        p_bytes += sz
        m_seg[k] = 1  # now in P

def _record_size(obj):
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    return k, sz

def _adapt_window(snapshot, delta_bytes):
    """Adjust w_target_bytes adaptively and clamp."""
    global w_target_bytes
    _ensure_targets(snapshot)
    w_target_bytes = int(w_target_bytes + delta_bytes)
    _ensure_targets(snapshot)

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using a Windowed-TinyLFU-inspired rule:
    - Prefer comparing W.LRU vs P.LRU using frequency-per-byte of the incoming object.
      If incoming is "hotter" per byte than P.LRU, evict P.LRU (admit new burst).
      Otherwise evict W.LRU (prefer keeping established items).
    - Fall back to the best available LRU among W, P, Q by minimal (freq+1)/size, with bias
      to avoid evicting from Q unless necessary.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_targets(cache_snapshot)
    _cleanup_segments(cache_snapshot)

    # LRU candidates by segment
    kW = _lru_key_of(W)
    kP = _lru_key_of(P)
    kQ = _lru_key_of(Q)

    # Incoming object's metric (count this access conceptually with +1)
    new_key = getattr(obj, "key", None)
    new_sz = _size_of(obj)
    new_metric = float(_freq_get(cache_snapshot, new_key) + 1) / float(max(1, new_sz))

    victim = None

    # Primary decision: compare W.LRU vs P.LRU when both exist
    if kW is not None and kP is not None:
        p_metric = _metric_per_byte(cache_snapshot, kP)
        # If new is "hotter" per byte than P.LRU, evict P.LRU; else evict W.LRU
        if new_metric > p_metric:
            victim = kP
            # Feedback: increase window (favor recency bursts)
            _adapt_window(cache_snapshot, delta_bytes=+min(new_sz, max(1, _cap(cache_snapshot)//32)))
        else:
            victim = kW
            # Feedback: decrease window (favor established main)
            _adapt_window(cache_snapshot, delta_bytes=-min(new_sz, max(1, _cap(cache_snapshot)//32)))

    # If only one of W or P exists, choose from it
    elif kW is not None:
        victim = kW
        _adapt_window(cache_snapshot, delta_bytes=-min(new_sz, max(1, _cap(cache_snapshot)//32)))
    elif kP is not None:
        victim = kP
        _adapt_window(cache_snapshot, delta_bytes=+min(new_sz, max(1, _cap(cache_snapshot)//32)))
    else:
        # Last resort: pick from Q or any cache key if we lost tracking
        if kQ is not None:
            victim = kQ
        else:
            # Structures empty but cache not; choose arbitrary LRU by scanning dict order
            for k in cache:
                victim = k
                break

    # As a safety net for very large objects, if Q exists and provides a worse metric than chosen victim,
    # select the globally weakest among available LRU heads, but still try to avoid Q unless chosen score
    # is significantly worse.
    if victim is not None:
        # Compare simple (freq+1)/size among available candidates; pick the minimal
        candidates = []
        for k in (kW, kP, kQ):
            if k is None:
                continue
            szk = m_size.get(k, 1)
            fk = _freq_get(cache_snapshot, k)
            metric = float(fk + 1) / float(max(1, szk))
            candidates.append((metric, k))
        if candidates:
            best_metric, best_key = min(candidates, key=lambda x: x[0])
            # Only override if our chosen victim is not already the weakest
            # and the best is significantly weaker (20% less)
            chosen_sz = m_size.get(victim, 1)
            chosen_metric = float(_freq_get(cache_snapshot, victim) + 1) / float(max(1, chosen_sz))
            if best_key != victim and best_metric < 0.8 * chosen_metric:
                victim = best_key

    return victim


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Bump frequency.
    - If in W or P: promote to Q (protected).
    - If in Q: refresh recency.
    - Keep Q bounded by q_target via demotion to P (no eviction).
    """
    global w_bytes, p_bytes, q_bytes
    _ensure_targets(cache_snapshot)

    k, sz = _record_size(obj)
    _freq_bump(cache_snapshot, k, inc=1)

    if k in W:
        # W -> Q
        W.pop(k, None)
        w_bytes = max(0, w_bytes - sz)
        Q[k] = None
        q_bytes += sz
        m_seg[k] = 2
    elif k in P:
        # P -> Q
        P.pop(k, None)
        p_bytes = max(0, p_bytes - sz)
        Q[k] = None
        q_bytes += sz
        m_seg[k] = 2
    elif k in Q:
        # Refresh recency in Q
        _move_to_mru(Q, k)
        m_seg[k] = 2
    else:
        # Untracked but a hit: treat as protected (frequent)
        Q[k] = None
        q_bytes += sz
        m_seg[k] = 2

    # Enforce protected size target by demotion
    _maybe_demote_protected(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On miss (after insertion by the cache):
    - Record size and bump frequency.
    - Insert into W (Window) at MRU.
    - No eviction here; future evictions will rebalance segments.
    """
    global w_bytes
    _ensure_targets(cache_snapshot)

    k, sz = _record_size(obj)
    _freq_bump(cache_snapshot, k, inc=1)  # TinyLFU counts misses too

    # Place into Window
    W[k] = None
    w_bytes += sz
    m_seg[k] = 0

    # Optional: if W is wildly larger than target, softly move oldest from Q to P to preserve Q bound
    _maybe_demote_protected(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After the cache evicts a victim:
    - Remove it from our segment structures and adjust byte counters.
    - Keep frequency and size history to aid future decisions.
    """
    global w_bytes, p_bytes, q_bytes
    _ensure_targets(cache_snapshot)

    k = evicted_obj.key
    sz = _size_of(evicted_obj)
    # Record last known size for future use
    m_size[k] = sz

    seg = m_seg.pop(k, None)
    if seg == 0:
        if k in W:
            W.pop(k, None)
            w_bytes = max(0, w_bytes - sz)
    elif seg == 1:
        if k in P:
            P.pop(k, None)
            p_bytes = max(0, p_bytes - sz)
    elif seg == 2:
        if k in Q:
            Q.pop(k, None)
            q_bytes = max(0, q_bytes - sz)
    else:
        # Unknown; try to purge from all
        if k in W:
            W.pop(k, None)
            w_bytes = max(0, w_bytes - sz)
        if k in P:
            P.pop(k, None)
            p_bytes = max(0, p_bytes - sz)
        if k in Q:
            Q.pop(k, None)
            q_bytes = max(0, q_bytes - sz)

    # No ghosts kept; frequency/history retained for admission scoring
    # Maintain protected demotion if necessary
    _maybe_demote_protected(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxz26rccj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp83q9s3e6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxbbbp347.pickle

Iteration 35: New subsample score 0.289759 is not better than old score 0.28988, skipping
Iteration 36: Selected program 6 score: 0.22137172916666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwwdcs_br.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfigwoscq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_8czcylt.pickle

Iteration 36: Proposed new text for program: # GreedyDual-Size with Sublinear Frequency (GDS-Flex)
# ----------------------------------------------------
# Core idea:
# - Maintain a GreedyDual priority H for each cached key:
#       H(key) = L + weight(key) / size(key)^SIZE_EXP
#   where L is a global "age" raised to the H of the last evicted item.
# - On each hit or insert, refresh H(key) based on current L and a sublinear
#   frequency-derived weight. This blends recency (via L) and popularity (via
#   a diminishing-returns frequency function) with strong size awareness.
# - Eviction always removes the item with the smallest H (ties: older last access,
#   then larger size, then lexicographic key).
#
# Why this works:
# - Size-aware: H scales inversely with size^SIZE_EXP, preferring to keep small,
#   high-value objects.
# - Recency: L increases on each eviction, gradually aging out items whose H isn't
#   refreshed by hits. Even once-popular items will eventually be evicted if not
#   accessed for long enough.
# - Popularity: A sublinear log-based weight from cumulative hits rewards objects
#   that receive repeated hits across time without letting a brief early burst pin them.
# - Cold-item control: New cold items get a smaller initial weight (COLD_BONUS)
#   to avoid pollution from one-hit wonders.
# - Ghost history: Stores lightweight hit counts for recently evicted keys to warm-start
#   their weight when they reappear (with decay over time), improving miss rates on re-entry.
#
# Metadata:
# - m_H[key]: current GreedyDual priority for key.
# - m_hits[key]: cumulative hit counter (float) for key (sublinear weight uses log1p).
# - m_last[key]: last access time for key (for tie-breaking and ghost decay).
# - L_age: global GreedyDual "age" (float).
# - Ghost:
#     g_hits[key]: last known hit counter for that key when evicted.
#     g_last[key]: time the key was recorded in the ghost.
#
# Complexity:
# - Eviction scanning is O(n) over live items (common for this challenge framework).
# - Metadata updates are O(1).
#
# Tunables:
# - SIZE_EXP: strength of size awareness (1.0 = strong byte-awareness).
# - FREQ_LOG_WEIGHT: weight applied to log1p(hit_count).
# - COLD_BONUS: initial weight for cold items (smaller makes them easier to evict).
# - GHOST_LIMIT_MIN / GHOST_LIMIT_FACTOR: bounds for ghost history size (by count).
# - GHOST_HALF_LIFE: decays ghosted hit counters over access_count to avoid stale popularity.


import math

# ----------------------
# Global metadata stores
# ----------------------
m_H = dict()         # key -> float (GreedyDual priority)
m_hits = dict()      # key -> float (cumulative hits; used for sublinear popularity)
m_last = dict()      # key -> int (last access time; tie-breaking)

# Global GreedyDual "age"
L_age = 0.0

# Ghost history for warm starts on re-insert
g_hits = dict()      # key -> float (last known hits)
g_last = dict()      # key -> int (time recorded in ghost)


# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0            # size exponent for size-awareness (>= 0); 1.0 = strong by bytes
FREQ_LOG_WEIGHT = 1.25    # multiplier for log1p(hits) to reward repeated hits (sublinear)
COLD_BONUS = 0.5          # weight for cold insertions (before any hit)
GHOST_HALF_LIFE = 20000.0 # in access_count units; larger => slower decay of ghosted popularity

GHOST_LIMIT_MIN = 1024    # minimum ghost capacity by count
GHOST_LIMIT_FACTOR = 4.0  # ghost capacity  factor * live cache item count (by count, not bytes)

_EPS = 1e-12


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_hits) <= limit:
        return
    # Drop oldest entries first (LRU in ghost)
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_hits) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_hits.pop(k, None)
        g_last.pop(k, None)

def _decayed_ghost_hits(key, now):
    """
    Return ghosted hits decayed by time since last record (exponential with half-life).
    """
    gh = g_hits.get(key, None)
    gl = g_last.get(key, None)
    if gh is None or gl is None:
        return 0.0
    dt = max(0.0, float(now - gl))
    if dt <= 0.0 or GHOST_HALF_LIFE <= 0.0:
        return float(gh)
    decay = 0.5 ** (dt / float(GHOST_HALF_LIFE))
    return float(gh) * decay

def _weight_from_hits(hits):
    """
    Sublinear popularity contribution. Cold items (hits <= 0) get COLD_BONUS.
    """
    if hits <= 0.0:
        return float(COLD_BONUS)
    return 1.0 + float(FREQ_LOG_WEIGHT) * math.log1p(max(0.0, float(hits)))

def _priority_from_hits(size_bytes, hits, L):
    denom = max(_EPS, float(size_bytes) ** float(SIZE_EXP))
    return float(L) + (_weight_from_hits(hits) / denom)

def _ensure_priority_for_key(key, obj, now):
    """
    Ensure m_H exists for a live key (defensive; normally set on insert/hit).
    """
    if key in m_H:
        return
    # Seed hits from ghost (decayed), else cold
    seed_hits = _decayed_ghost_hits(key, now) if key in g_hits else 0.0
    m_hits[key] = float(seed_hits)
    m_last[key] = now
    m_H[key] = _priority_from_hits(_size_of(obj), m_hits[key], L_age)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the key with the smallest GreedyDual priority H:
    - H(key) = L_age + weight(key) / size(key)^SIZE_EXP
    - Ties: older last access first, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Ensure all have priorities (defensive; should already be set)
    for k, v in cache.items():
        _ensure_priority_for_key(k, v, now)

    best_key = None
    best_H = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        Hk = m_H.get(k, float("inf"))
        la = m_last.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_H, best_la, best_sz = k, Hk, la, sz
            continue

        # Choose smallest H
        if Hk < best_H:
            best_key, best_H, best_la, best_sz = k, Hk, la, sz
            continue

        if Hk == best_H:
            # Prefer evicting less recently used (older last access)
            if la < best_la:
                best_key, best_H, best_la, best_sz = k, Hk, la, sz
                continue
            if la == best_la:
                # Prefer evicting larger object (frees more space)
                if sz > best_sz:
                    best_key, best_H, best_la, best_sz = k, Hk, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_H, best_la, best_sz = k, Hk, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment hits.
    - Refresh priority: H = L_age + weight(hits)/size^SIZE_EXP.
    - Update last access time.
    """
    global L_age

    key = obj.key
    now = _now(cache_snapshot)
    size_b = _size_of(obj)

    # Initialize if missing (defensive)
    if key not in m_hits:
        seed = _decayed_ghost_hits(key, now) if key in g_hits else 0.0
        m_hits[key] = float(seed)
        m_last[key] = now
        m_H[key] = _priority_from_hits(size_b, m_hits[key], L_age)

    # Update hits and refresh priority
    m_hits[key] = float(m_hits.get(key, 0.0)) + 1.0
    m_last[key] = now
    m_H[key] = _priority_from_hits(size_b, m_hits[key], L_age)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Warm-start hits from ghost (with decay), else cold (0).
    - Initialize priority: H = L_age + weight(hits)/size^SIZE_EXP.
    - Set last access time to now.
    """
    key = obj.key
    now = _now(cache_snapshot)
    size_b = _size_of(obj)

    seed_hits = _decayed_ghost_hits(key, now) if key in g_hits else 0.0
    m_hits[key] = float(seed_hits)
    m_last[key] = now
    m_H[key] = _priority_from_hits(size_b, m_hits[key], L_age)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    On eviction:
    - Raise global age L_age to the evicted key's H (GreedyDual aging).
    - Store ghost history (hits, time) for the evicted key.
    - Trim ghost to bounded size.
    - Remove live metadata for the evicted key.
    """
    global L_age

    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Update global age to evicted priority (if present)
    evicted_H = m_H.get(ekey, None)
    if evicted_H is not None:
        L_age = max(float(L_age), float(evicted_H))

    # Record ghost
    g_hits[ekey] = float(m_hits.get(ekey, 0.0))
    g_last[ekey] = now
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_H.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_last.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm2l1by9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsw0404s1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rr08zt5.pickle

Iteration 36: New subsample score 0.429078 is better than old score 0.395746. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8x558ow4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx9bgqzmo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprglfk26r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvysfrfwj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyf14sebq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg2_2g9bg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpden8ehfs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi_i03uwk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph3wehpvj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo8n5_wqw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp37wf8zee.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9ma8oz01.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0i8b2u34.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoctshx2l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1ankvglu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy_uwxz9s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjuiv4_dg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk6d8ss8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp4a7p_vl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnn2pyeg6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk56ttp2x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprhk1lp6_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwbf0ndds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4bieu6ic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpanvs6nfe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiint5djt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbx10sh5f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rq6a764.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf2hzuaft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfrizgeo8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsfgwa95g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpktcnfaxs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqnqp_d76.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg48qx3_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsug_b0wi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl1ze6sbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcp3o474g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr8ffbdnv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjagipipv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptj8cv4qz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu2wuhlww.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9lrhmt_a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2bmf8zqz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7x2bis23.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4r1i1fud.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1eacauw9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpukyhcjw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo6bxvzyb.pickle

Iteration 36: Full valset score for new program: 0.2269130833333334
Iteration 36: Full train_val score for new program: 0.2269130833333334
Iteration 36: Individual valset scores for new program: [0.473526, 0.452428, 0.460033, 0.397173, 0.474622, 0.463334, 0.269139, 0.498624, 0.538077, 0.531017, 0.075, 0.343694, 0.023893, 0.0, 0.020105, 0.020006, 0.019119, 0.022631, 0.021938, 0.269478, 0.342183, 0.026164, 0.057382, 0.057382, 0.269798, 0.274194, 0.818272, 0.88636, 0.038934, 0.036364, 0.038724, 0.00012, 8.4e-05, 0.752212, 0.070175, 0.063548, 0.025895, 0.633179, 0.125461, 0.049903, 0.038485, 0.060714, 0.042763, 0.233333, 0.041549, 0.05021, 0.435583, 0.04902]
Iteration 36: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.761572, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 36: Full valset pareto front score: 0.27230547916666675
Iteration 36: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {18}, {17, 6, 14}, {14}, {14}, {19}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19}, {8}]
Iteration 36: Best valset aggregate score so far: 0.24589052083333332
Iteration 36: Best program as per aggregate score on train_val: 7
Iteration 36: Best program as per aggregate score on valset: 7
Iteration 36: Best score on valset: 0.24589052083333332
Iteration 36: Best score on train_val: 0.24589052083333332
Iteration 36: Linear pareto front program index: 7
Iteration 36: New program candidate index: 20
Iteration 37: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpon696exk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu3k7gw7u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpphq02x07.pickle

Iteration 37: Proposed new text for program: # ARC-inspired Segmented Recency/Frequency with Ghost History (sARC)
# Robust across mixed workloads with adaptive balancing between recency (T1) and frequency (T2).
# - Two resident segments:
#     * T1 (recent): LRU region for newly admitted items and one-hit wonders.
#     * T2 (frequent): LFU-biased region for items with repeated hits.
# - Two non-resident ghost sets (B1/B2) remember recently evicted keys and guide adaptation:
#     * If a miss hits B1 (recent ghost), favor recency (shrink T2 target).
#     * If a miss hits B2 (frequent ghost), favor frequency (grow T2 target).
# - Eviction:
#     * Prefer evicting from the segment that exceeds its adaptive target.
#     * Within T1: pure LRU.
#     * Within T2: lowest frequency first, break ties by recency (older first).
# - Lightweight frequency with periodic global decay to avoid unbounded growth.

# ----------------------
# Metadata (globals)
# ----------------------
m_last_access = dict()   # key -> int timestamp (access_count)
m_seg = dict()           # key -> 0 (T1: recent), 1 (T2: frequent)
m_freq = dict()          # key -> float frequency score

# Ghost histories (key -> timestamp)
m_ghost1 = dict()        # B1: ghost for T1 evictions
m_ghost2 = dict()        # B2: ghost for T2 evictions

# Segment accounting and adaptation
m_num_t2 = 0             # number of items currently in T2
m_t2_target = 0.5        # adaptive target fraction for T2 share (0..1)

# ----------------------
# Tunables
# ----------------------
ETA_ARC = 0.05           # step to adjust T2 target on ghost hits
T2_TARGET_MIN = 0.05     # lower bound on T2 fraction
T2_TARGET_MAX = 0.95     # upper bound on T2 fraction

# Frequency decay to prevent unbounded growth and track shifts
DECAY_EVERY_ACCESSES = 50000
DECAY_FACTOR = 0.5

# ----------------------
# Helpers
# ----------------------
def _maybe_decay_freq(cache_snapshot):
    """
    Periodically decay frequencies to give more weight to recent behavior.
    """
    if cache_snapshot.access_count % DECAY_EVERY_ACCESSES != 0:
        return
    for k in list(m_freq.keys()):
        m_freq[k] *= DECAY_FACTOR
        if m_freq[k] < 1e-6:
            m_freq.pop(k, None)

def _trim_ghosts(cache_snapshot):
    """
    Keep ghost histories bounded (like ARC: B1+B2 <= N).
    We approximate by bounding each ghost set to at most N.
    Evict oldest entries (LRU by timestamp).
    """
    N = max(1, len(cache_snapshot.cache))
    def trim_one(ghost_dict, cap):
        while len(ghost_dict) > cap:
            # Remove the oldest (min timestamp)
            oldest_key, oldest_ts = None, None
            for k, ts in ghost_dict.items():
                if oldest_ts is None or ts < oldest_ts or (ts == oldest_ts and k < oldest_key):
                    oldest_key, oldest_ts = k, ts
            ghost_dict.pop(oldest_key, None)

    trim_one(m_ghost1, N)
    trim_one(m_ghost2, N)

def _choose_victim_from_segment(cache_snapshot, segment):
    """
    Choose eviction victim from a given segment.
    - segment 0 (T1): pure LRU (oldest last_access)
    - segment 1 (T2): LFU-biased, then LRU tie-break, then larger size, then key
    """
    cand_key = None
    # Use tuples for comparison based on segment rules
    if segment == 0:
        # T1: minimize (last_access, -size, key) -> oldest first, larger size next, deterministic key
        best_tuple = None
        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k, 0) != 0:
                continue
            la = m_last_access.get(k, -1)
            sz = getattr(v, "size", 1)
            t = (la, -sz, k)
            if best_tuple is None or t < best_tuple:
                best_tuple = t
                cand_key = k
    else:
        # T2: minimize (freq, last_access, -size, key)
        best_tuple = None
        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k, 0) != 1:
                continue
            f = m_freq.get(k, 0.0)
            la = m_last_access.get(k, -1)
            sz = getattr(v, "size", 1)
            t = (f, la, -sz, k)
            if best_tuple is None or t < best_tuple:
                best_tuple = t
                cand_key = k
    return cand_key

# ----------------------
# Core API
# ----------------------
def evict(cache_snapshot, obj):
    """
    ARC-style victim selection:
    - Compute target T2 size = round(m_t2_target * N).
    - If T2 currently larger than target, evict from T2; else evict from T1.
    - Fallback to the other segment if chosen segment is empty.
    """
    if not cache_snapshot.cache:
        return None

    N = len(cache_snapshot.cache)
    p_count = int(m_t2_target * N + 0.5)

    # Decide which segment to evict from
    if m_num_t2 > p_count:
        victim = _choose_victim_from_segment(cache_snapshot, 1)
        if victim is not None:
            return victim
        # Fallback
        return _choose_victim_from_segment(cache_snapshot, 0)
    else:
        victim = _choose_victim_from_segment(cache_snapshot, 0)
        if victim is not None:
            return victim
        # Fallback
        return _choose_victim_from_segment(cache_snapshot, 1)

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update recency.
    - Increase frequency (unit gain).
    - Promote from T1 to T2 on first hit.
    - Periodically decay frequencies.
    """
    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Frequency bump
    m_freq[key] = m_freq.get(key, 0.0) + 1.0

    # Promote to T2 if currently in T1
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1
        global m_num_t2
        m_num_t2 += 1

    _maybe_decay_freq(cache_snapshot)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - If key was in B1: favor recency -> shrink T2 target; insert into T2 (as promotion).
    - If key was in B2: favor frequency -> grow T2 target; insert into T2.
    - Else: new key -> insert into T1.
    - Initialize recency and a small starting frequency.
    - Remove the key from ghost sets (if present) and trim ghosts.
    """
    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    global m_t2_target, m_num_t2

    if key in m_ghost1:
        # Ghost hit in recent set -> favor recency (smaller T2)
        m_t2_target = max(T2_TARGET_MIN, m_t2_target - ETA_ARC)
        m_ghost1.pop(key, None)
        # Place into T2 directly
        m_seg[key] = 1
        m_num_t2 += 1
        m_freq[key] = max(2.0, m_freq.get(key, 0.0) + 2.0)
    elif key in m_ghost2:
        # Ghost hit in frequent set -> favor frequency (larger T2)
        m_t2_target = min(T2_TARGET_MAX, m_t2_target + ETA_ARC)
        m_ghost2.pop(key, None)
        # Place into T2 directly
        m_seg[key] = 1
        m_num_t2 += 1
        m_freq[key] = max(3.0, m_freq.get(key, 0.0) + 3.0)
    else:
        # Brand new -> T1
        m_seg[key] = 0
        # Start small, to prevent one-hit pollution
        m_freq[key] = m_freq.get(key, 0.0) + 1.0

    _trim_ghosts(cache_snapshot)
    _maybe_decay_freq(cache_snapshot)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Add evicted key to appropriate ghost set (B1 for T1, B2 for T2) with timestamp.
    - Update T2 accounting if needed.
    - Clean resident metadata.
    - Trim ghost sets.
    """
    ekey = evicted_obj.key
    now = cache_snapshot.access_count

    seg = m_seg.get(ekey, 0)
    if seg == 1:
        # Was in T2
        global m_num_t2
        m_num_t2 = max(0, m_num_t2 - 1)
        m_ghost2[ekey] = now
    else:
        # Was in T1
        m_ghost1[ekey] = now

    # Clean resident metadata
    m_seg.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)

    _trim_ghosts(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr_26jzun.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe9xwriyh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwbikthw8.pickle

Iteration 37: New subsample score 0.635679 is better than old score 0.6304909999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwxnw8n1t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi4gpggdg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb5von4w5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6e06uei8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmyyhge7j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1wj2ml66.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5bttaxo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr_6v5837.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7_mcvl2i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn4iqst1m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5cvpk1be.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp82atxd_b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuj81xijh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpti78mp61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppw5yph45.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpplm92owy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwdwt6e36.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcdochpy0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp49233_ry.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxbxd62w4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvbe6j1hp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzgtr4urw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfbmhj2ao.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3kfdlxfx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp033ogg4y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6r1f4_t7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdull2b_w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbzzyelxk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzwzf9s2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_zkqxsrh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6vdzsxm0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7wu4eucd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1zr5in4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzlcaxz8g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8w2a09d4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq6zshgz8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphmaviukk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp08hcayeq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvoe9uwor.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyv6kxcng.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6j4fyuhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjk23edck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8yp_ktdu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppd0pvpyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvd1b7a7r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3tzl1bra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuujhfqpn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo39ym5ho.pickle

Iteration 37: Full valset score for new program: 0.24360862499999994
Iteration 37: Full train_val score for new program: 0.24360862499999994
Iteration 37: Individual valset scores for new program: [0.479768, 0.45272, 0.463426, 0.4162, 0.47628, 0.46466, 0.269139, 0.474833, 0.538077, 0.531017, 0.075, 0.357016, 0.040045, 0.0, 0.021237, 0.020992, 0.019927, 0.023475, 0.022641, 0.272227, 0.377581, 0.026164, 0.058672, 0.058672, 0.282601, 0.325605, 0.743793, 0.888413, 0.069182, 0.038636, 0.045558, 0.026575, 0.028976, 0.73179, 0.083333, 0.067961, 0.076879, 0.631118, 0.125461, 0.142501, 0.120342, 0.162058, 0.052632, 0.266667, 0.161851, 0.139691, 0.460123, 0.081699]
Iteration 37: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.761572, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 37: Full valset pareto front score: 0.27230547916666675
Iteration 37: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {18}, {17, 6, 14}, {14}, {14}, {19}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19}, {8}]
Iteration 37: Best valset aggregate score so far: 0.24589052083333332
Iteration 37: Best program as per aggregate score on train_val: 7
Iteration 37: Best program as per aggregate score on valset: 7
Iteration 37: Best score on valset: 0.24589052083333332
Iteration 37: Best score on train_val: 0.24589052083333332
Iteration 37: Linear pareto front program index: 7
Iteration 37: New program candidate index: 21
Iteration 38: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgxaupax5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoy7ord6h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgs44merh.pickle

Iteration 38: Proposed new text for program: # Adaptive Size-Aware Segmented LRU with Decayed-Frequency (AS-SLRU-DF)
# Design goals:
# - Robust across workloads using a simple two-segment SLRU (probation/protected) with adaptive split.
# - Size-aware eviction: prefer evicting larger items when value-to-keep is similar.
# - Penalize old references and stale frequency using exponential decay (LRFU-lite without prediction).
# - Promotion on first hit; controlled demotion from protected if it grows beyond adaptive target.
# - Lightweight ghost history to warm-start frequency after reinsertions.
#
# Notes:
# - Eviction prefers probationary items; protected items are only considered if probation is empty.
# - Retention scoring blends recency and decayed frequency, normalized by size^gamma.
# - Adaptive protected share p_ratio is nudged based on where hits occur (probation vs protected).

import math

# ----------------------
# Global metadata stores
# ----------------------
# Per-key state
m_last_access = dict()  # key -> int (last access time)
m_freq = dict()         # key -> float (decayed frequency counter)
m_freq_ts = dict()      # key -> int (last time m_freq was updated)
m_seg = dict()          # key -> int (0=probation, 1=protected)

# Ghost history for warm starts
g_freq = dict()         # key -> float (approx recent frequency)
g_last = dict()         # key -> int   (last seen time in ghost)

# Global adaptives
m_global_mu = 64.0      # EWMA of inter-arrival gaps (access-count units)
p_ratio = 0.5           # target protected share in [P_MIN, P_MAX]

# ----------------------
# Tunable hyperparameters
# ----------------------
# Decay half-life bounds and adaptation
HL_MIN = 16.0
HL_MAX = 8192.0
HL_SCALE = 4.0          # half-life ~ HL_SCALE * m_global_mu

# Retention scoring
REC_WEIGHT = 1.0
FREQ_WEIGHT = 1.35
GAMMA_SIZE_EVICT = 0.7      # size normalization; larger => evict big items earlier
PROTECT_MULT = 1.35         # retention multiplier for protected items

# Adaptive protected share
P_MIN = 0.10
P_MAX = 0.90
P_STEP_UP = 0.025           # increase when probation hits (need more protection)
P_STEP_DOWN = 0.012         # decrease when protected hits (need more probation)

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _live_count(cache_snapshot):
    cache = getattr(cache_snapshot, "cache", {}) or {}
    return len(cache)

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, _live_count(cache_snapshot))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_freq) <= limit:
        return
    # evict oldest ghosts first
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_freq) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_freq.pop(k, None)
        g_last.pop(k, None)

def _half_life():
    hl = HL_SCALE * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    return 0.5 ** (float(delta) / float(_half_life()))

def _decayed_freq_now(key, now):
    f = float(m_freq.get(key, 0.0))
    ts = m_freq_ts.get(key, None)
    if ts is None or f <= 0.0:
        return 0.0
    df = _decay_factor(now - ts)
    return f * df

def _target_protected_count(cache_snapshot):
    live_cnt = _live_count(cache_snapshot)
    if live_cnt <= 1:
        return 0
    tgt = int(round(p_ratio * live_cnt))
    if tgt < int(math.ceil(P_MIN * live_cnt)):
        tgt = int(math.ceil(P_MIN * live_cnt))
    if tgt > int(math.floor(P_MAX * live_cnt)):
        tgt = int(math.floor(P_MAX * live_cnt))
    # ensure not exceeding live_cnt-1 to keep some probation space
    tgt = min(tgt, max(0, live_cnt - 1))
    return max(0, tgt)

def _protected_keys(cache_snapshot):
    cache = cache_snapshot.cache
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            yield k

def _probation_keys(cache_snapshot):
    cache = cache_snapshot.cache
    for k in cache.keys():
        if m_seg.get(k, 0) == 0:
            yield k

def _seg_counts(cache_snapshot):
    prot = 0
    prob = 0
    for k in cache_snapshot.cache.keys():
        if m_seg.get(k, 0) == 1:
            prot += 1
        else:
            prob += 1
    return prot, prob

def _demote_overflow_protected(cache_snapshot, now=None):
    # If protected segment exceeds target, demote LRU from protected to probation
    prot_count, _ = _seg_counts(cache_snapshot)
    tgt = _target_protected_count(cache_snapshot)
    if prot_count <= tgt:
        return
    if now is None:
        now = _now(cache_snapshot)
    # find LRU in protected by last access (older is worse)
    lru_k = None
    lru_la = None
    for k in _protected_keys(cache_snapshot):
        la = m_last_access.get(k, -1)
        if lru_k is None or la < lru_la or (la == lru_la and k < lru_k):
            lru_k = k
            lru_la = la
    if lru_k is not None:
        m_seg[lru_k] = 0  # demote to probation

def _retention_value(k, obj, now):
    # Recency component: exponential decay with age
    la = m_last_access.get(k, None)
    age = float(now - la) if la is not None else 1e9
    rec = math.exp(-age / max(1.0, _half_life()))

    # Decayed frequency component
    f_now = _decayed_freq_now(k, now)
    freq_term = math.sqrt(max(0.0, f_now))

    base = REC_WEIGHT * rec + FREQ_WEIGHT * freq_term
    if m_seg.get(k, 0) == 1:
        base *= PROTECT_MULT

    sz = _size_of(obj)
    retention = base / (float(sz) ** GAMMA_SIZE_EVICT)
    return retention, la, sz

def _record_ghost_on_evict(evicted_key, now):
    # store decayed freq snapshot and last time
    f_now = _decayed_freq_now(evicted_key, now)
    if f_now > 0.0:
        g_freq[evicted_key] = float(f_now)
        g_last[evicted_key] = now

def _warm_start_on_insert(key, now):
    if key in g_freq:
        # warm frequency slightly decayed
        warmed = float(g_freq[key]) * 0.7
        m_freq[key] = max(0.0, warmed)
        m_freq_ts[key] = now
    else:
        m_freq[key] = 0.0
        m_freq_ts[key] = now


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose eviction victim.
    Strategy:
    - Prefer evicting from probation segment; only consider protected if probation is empty.
    - Within chosen set, evict the item with minimal retention value:
        retention = (REC_WEIGHT*exp(-age/HL) + FREQ_WEIGHT*sqrt(decayed_freq_now))
                    * (PROTECT_MULT if protected else 1)
                    / size^GAMMA_SIZE_EVICT
    Tie-breakers on equal retention:
      - prefer probationary
      - older last access (LRU)
      - larger size
      - lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Determine candidate pool
    probation = list(_probation_keys(cache_snapshot))
    candidates = probation if len(probation) > 0 else list(_protected_keys(cache_snapshot))
    if not candidates:
        candidates = list(cache.keys())

    victim_key = None
    victim_ret = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k in candidates:
        v = cache[k]
        ret, la, sz = _retention_value(k, v, now)
        seg = m_seg.get(k, 0)

        if victim_key is None:
            victim_key = k
            victim_ret = ret
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if ret < victim_ret - 1e-15:
            better = True
        elif abs(ret - victim_ret) <= 1e-15:
            # prefer evicting probationary
            if seg < victim_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                la0 = -1 if la is None else la
                la1 = -1 if victim_la is None else victim_la
                if la0 < la1:
                    better = True
                elif la0 == la1:
                    # free more space
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key = k
            victim_ret = ret
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update global and per-key inter-arrival EWMAs.
      - Update decayed frequency counter (lazy decay, then +1).
      - Update last access time.
      - Promote from probation to protected.
      - Adjust protected target share (p_ratio) based on where the hit occurred.
      - Demote oldest protected if protected size exceeds target.
    """
    key = obj.key
    now = _now(cache_snapshot)

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        # update global EWMA of gaps
        globals()['m_global_mu'] = (0.98 * float(m_global_mu)) + (0.02 * gap)

    # Ensure initialized
    if key not in m_freq_ts:
        _warm_start_on_insert(key, now)

    # Decayed frequency update
    f_now = _decayed_freq_now(key, now)
    m_freq[key] = f_now + 1.0
    m_freq_ts[key] = now

    # Update last access
    m_last_access[key] = now

    # Segment handling and adaptive target
    seg = m_seg.get(key, 0)
    if seg == 0:
        # probation hit -> promote and increase protected share
        m_seg[key] = 1
        globals()['p_ratio'] = min(P_MAX, p_ratio + P_STEP_UP)
    else:
        # protected hit -> slightly reduce protected share to allow exploration
        globals()['p_ratio'] = max(P_MIN, p_ratio - P_STEP_DOWN)

    # Keep protected within target by demoting LRU protected if needed
    _demote_overflow_protected(cache_snapshot, now=now)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Initialize last access time.
      - Warm-start decayed frequency from ghost if available; otherwise zero.
      - Start in probation.
      - Ensure protected segment respects current target (in case of prior promotions).
    """
    key = obj.key
    now = _now(cache_snapshot)

    m_last_access[key] = now
    _warm_start_on_insert(key, now)
    m_seg[key] = 0  # probation

    # Keep protected within target
    _demote_overflow_protected(cache_snapshot, now=now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record ghost metadata (decayed freq snapshot and last seen time).
      - Trim ghost to bounded size.
      - Remove all live metadata for evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_freq_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4rjer8fo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpefj5o014.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8xbkva2s.pickle

Iteration 38: New subsample score 0.532231 is better than old score 0.37692899999999996. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppmorzxa7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfdaxdix8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq53idxhf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfchzu5r6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw8ivp7ev.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbqjr79_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoykprf9s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2tm6ait5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuz0kixes.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprcx3y5bs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphc4h39r7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaw3v_2ev.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd49y7m0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyjzk2oke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsgut3q73.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeoigx2d4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmoxns30p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp12v7uqil.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe76t07lr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppp8c6j84.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcbansbp7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9snm3m02.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbc0ak62l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpop8dvjsi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu29f61y8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe50b0dfi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnguevd51.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp89wvu0s1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp_cxceh2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5_k72wc0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdnoydi1u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpujk1w4kb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbm5ynwo2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuvge4dzj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2ak05yft.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb_wpyzt8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8wiokg6i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj7g_dvz7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe2ktyp02.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgsxtfk9q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptq1npw22.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2cbtp7n1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkk4ouy1d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4dc33z8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4piyxhzs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpturj5a40.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuf6w2wyc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_er3v_fz.pickle

Iteration 38: Full valset score for new program: 0.23162991666666666
Iteration 38: Full train_val score for new program: 0.23162991666666666
Iteration 38: Individual valset scores for new program: [0.475517, 0.456164, 0.460396, 0.400996, 0.475317, 0.46541, 0.269139, 0.498624, 0.537719, 0.531017, 0.075, 0.345915, 0.03997, 0.0, 0.020671, 0.020569, 0.019793, 0.023475, 0.0225, 0.272227, 0.364798, 0.026164, 0.058672, 0.058672, 0.269804, 0.278226, 0.754717, 0.883108, 0.039832, 0.038636, 0.045558, 0.007003, 0.020672, 0.75, 0.083333, 0.067961, 0.026022, 0.632148, 0.125461, 0.090934, 0.063531, 0.072455, 0.052632, 0.233333, 0.041854, 0.074334, 0.466258, 0.081699]
Iteration 38: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.761572, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 38: Full valset pareto front score: 0.27230547916666675
Iteration 38: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}, {17, 6, 14}, {17, 6, 14}, {17, 14}, {17, 6, 14}, {17, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14}, {18}, {17, 6, 14}, {14}, {14}, {19}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14}, {14}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22}, {8}]
Iteration 38: Best valset aggregate score so far: 0.24589052083333332
Iteration 38: Best program as per aggregate score on train_val: 7
Iteration 38: Best program as per aggregate score on valset: 7
Iteration 38: Best score on valset: 0.24589052083333332
Iteration 38: Best score on train_val: 0.24589052083333332
Iteration 38: Linear pareto front program index: 7
Iteration 38: New program candidate index: 22
Iteration 39: Selected program 19 score: 0.24042418749999997
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptu8_7flw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp79xe6nho.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdvdptwdq.pickle

Iteration 39: Proposed new text for program: # Improved: Size-aware W-TinyLFU + Segmented (W/P/S) with
#           score-based P eviction, large-object-aware routing,
#           ghost-driven window adaptation, hit-distribution window tuning,
#           early S bypass for hot/tiny items, and W-hit conditional promotion.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Doorkeeper (filters first occurrences to reduce TinyLFU noise)
DOOR_MAX = 32768
m_door = dict()          # key -> last_seen_time (LRU-ish by timestamp)
m_door_ops = 0
DOOR_DECAY_EVERY = 50000 # periodically clear the doorkeeper to avoid unbounded bias

# Adaptive window sizing (ARC-inspired, but by fraction of bytes)
m_W_frac = 0.20          # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Adaptive protected fraction within main
m_S_frac = 0.80          # start with 80% of main in protected
S_FRAC_MIN = 0.50
S_FRAC_MAX = 0.95
S_FRAC_STEP = 0.02
m_s_promotions = 0
m_s_demotions = 0
m_last_s_adjust_access = 0
S_ADAPT_EVERY = 30000

# Hit-distribution based W adaptation (complements ghosts)
m_w_hits = 0
m_p_hits = 0
m_s_hits = 0
m_last_w_adjust_access = 0
W_ADAPT_HITS_EVERY = 40000
W_HIT_INCREASE_THRESH = 0.60
W_HIT_DECREASE_THRESH = 0.20

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Score = estimate / size^ALPHA
ALPHA = 0.90
ADMIT_EPS = 1e-12
EPS = 1e-12

# Large-object routing thresholds (insert directly to P if very large)
LARGE_FRAC_OF_CAP = 0.50  # if obj.size > 50% of capacity -> insert into P (not W)

# Bypass / promotion thresholds using TinyLFU estimate (after doorkeeper)
P_BYPASS_EST = 2          # if est >= 2 on miss, insert to P (skip W)
S_BYPASS_EST = 5          # if est >= 5 on miss and size small, insert directly into S
W_TO_P_EST = 2            # if W-hit and est >= 2, promote to P
W_TO_S_EST = 5            # if W-hit and est very high & small, promote to S
BYPASS_SIZE_FRAC = 0.10   # direct-to-S only if size <= 10% of capacity
TINY_FRAC_OF_CAP = 0.01   # size threshold to consider an item "tiny" for promotions

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    # 64-bit mix for stability
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        # periodic aging (halve all counters)
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    try:
        sz = getattr(obj_or_size, "size", None)
        if sz is None:
            sz = int(obj_or_size)
    except Exception:
        sz = 1
    return max(1, int(sz))

def _targets(cache_snapshot):
    # Compute targets for Window and Protected (by bytes)
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)
    main_target = max(0, cap - w_target)
    s_target = int(main_target * m_S_frac)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _insert_into_segment(key, seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    m_seg[key] = seg
    if seg == SEG_WINDOW:
        m_bytes_W += size
    elif seg == SEG_PROBATION:
        m_bytes_P += size
    elif seg == SEG_PROTECTED:
        m_bytes_S += size

def _maybe_adjust_S_frac(cache_snapshot):
    global m_s_promotions, m_s_demotions, m_last_s_adjust_access, m_S_frac
    now = cache_snapshot.access_count
    if now - m_last_s_adjust_access < S_ADAPT_EVERY:
        return
    m_last_s_adjust_access = now
    promos = m_s_promotions
    demos = m_s_demotions
    m_s_promotions = 0
    m_s_demotions = 0
    if promos > demos * 1.10:
        m_S_frac = min(S_FRAC_MAX, m_S_frac + S_FRAC_STEP)
    elif demos > promos * 1.10:
        m_S_frac = max(S_FRAC_MIN, m_S_frac - S_FRAC_STEP)

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S, m_s_demotions
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)
    m_s_demotions += 1

def _door_should_count(cache_snapshot, key):
    """
    Doorkeeper: returns True if this key should be counted into the TinyLFU sketch now.
    On first sight in a short window, we record it but do not count. On repeat, we count.
    Periodically reset to avoid stale positives.
    """
    global m_door_ops, m_door
    now = cache_snapshot.access_count
    m_door_ops += 1
    if key in m_door:
        # repeat within door horizon -> count
        m_door[key] = now
        # Aging/prune
        if len(m_door) > DOOR_MAX:
            # evict oldest by timestamp
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return True
    else:
        # first occurrence -> record only
        m_door[key] = now
        if len(m_door) > DOOR_MAX:
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return False

def _score_for_key(cache_snapshot, key):
    """
    Size-aware frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate / size^ALPHA
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    return est / max(1.0, pow(sz, ALPHA))

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return est / max(1.0, pow(sz, ALPHA))

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

def _p_victim_by_score(cache_snapshot, exclude_key=None):
    """
    Choose a victim in Probation with the lowest score (TinyLFU/size^ALPHA).
    Tie-breakers:
      1) lower score
      2) older last_access
      3) larger size (frees more)
      4) lexicographic key
    """
    best_k = None
    best_score = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != SEG_PROBATION:
            continue
        sc = _score_for_key(cache_snapshot, k)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_score, best_la, best_sz = k, sc, la, sz
            continue
        if sc < best_score - EPS:
            best_k, best_score, best_la, best_sz = k, sc, la, sz
            continue
        if abs(sc - best_score) <= EPS:
            if la < best_la:
                best_k, best_score, best_la, best_sz = k, sc, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_k, best_score, best_la, best_sz = k, sc, la, sz
                    continue
                if sz == best_sz and k < best_k:
                    best_k, best_score, best_la, best_sz = k, sc, la, sz
    return best_k

def _maybe_adjust_W_frac_by_hits(cache_snapshot):
    """
    Adapt W fraction based on where hits occur.
    If many hits occur in W, recency matters -> increase W.
    If few hits happen in W (most in P/S), decrease W to favor main.
    """
    global m_w_hits, m_p_hits, m_s_hits, m_last_w_adjust_access, m_W_frac
    now = cache_snapshot.access_count
    if now - m_last_w_adjust_access < W_ADAPT_HITS_EVERY:
        return
    m_last_w_adjust_access = now
    total = m_w_hits + m_p_hits + m_s_hits
    if total == 0:
        return
    w_ratio = m_w_hits / float(total)
    if w_ratio > W_HIT_INCREASE_THRESH:
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif w_ratio < W_HIT_DECREASE_THRESH:
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)
    # reset counters
    m_w_hits = 0
    m_p_hits = 0
    m_s_hits = 0

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision (enhanced W-TinyLFU):
      - Keep Window close to its target (evict W-LRU if oversized).
      - On predicted Window overflow at insertion:
         * Compare new item's score vs. the worst (by score) victim in Probation.
         * If new wins, evict that P victim (admit); else evict W-LRU (reject).
         * If W empty, fall back to P victim.
      - If no predicted W overflow: evict from P (by worst score); if empty, fall back to W-LRU; then S-LRU.
      - Fallback: global LRU if metadata inconsistent.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_sz = _size_of(obj)
    new_score = _score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_bytes_W + new_sz) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)

    if predict_overflow:
        p_victim = _p_victim_by_score(cache_snapshot)
        if p_victim is None:
            # No P items; evict from W if possible, else from S
            if w_lru is not None:
                return w_lru
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            return s_lru
        else:
            # Admission comparison: size-aware score
            p_score = _score_for_key(cache_snapshot, p_victim)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict P victim; W-LRU (if any) may be moved to P in update_after_insert
                return p_victim
            else:
                # Reject: evict from W to keep W within bound (if none, fallback to P)
                if w_lru is not None:
                    return w_lru
                return p_victim

    # No predicted W overflow: evict from P by worst score if possible; else W; else S.
    p_victim = _p_victim_by_score(cache_snapshot)
    if p_victim is not None:
        return p_victim
    if w_lru is not None:
        return w_lru
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Count into TinyLFU and refresh recency.
      - W: conditional promotion to P/S if sufficiently hot (TinyLFU est) and/or tiny.
      - P: promote to S.
      - S: refresh recency only.
      - Keep S within target; periodically adapt fractions (S fraction and W fraction).
    """
    _ensure_sketch(cache_snapshot)

    # On hit, always count into TinyLFU
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    # Hit-distribution counters
    global m_w_hits, m_p_hits, m_s_hits
    if seg == SEG_WINDOW:
        m_w_hits += 1
    elif seg == SEG_PROBATION:
        m_p_hits += 1
    else:
        m_s_hits += 1

    cap = max(1, int(cache_snapshot.capacity))
    tiny_thresh = max(1, int(cap * TINY_FRAC_OF_CAP))
    est = _sketch_estimate(key)

    if seg == SEG_WINDOW:
        # Conditional promotion from W based on estimated hotness and size
        if est >= W_TO_S_EST and sz <= max(1, int(cap * BYPASS_SIZE_FRAC)):
            _promote(key, SEG_WINDOW, SEG_PROTECTED, sz)
            # bound S
            _demote_S_if_over_target(cache_snapshot, exclude_key=key)
        elif est >= W_TO_P_EST or sz <= tiny_thresh:
            _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
        # else: keep in W (recency buffer)
    elif seg == SEG_PROBATION:
        # Promote to protected
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        # Track promotions for S-fraction adaptation
        global m_s_promotions
        m_s_promotions += 1
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency; structure unchanged
        pass

    _maybe_adjust_S_frac(cache_snapshot)
    _maybe_adjust_W_frac_by_hits(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Doorkeeper: only count into TinyLFU if it is not the first sighting in short horizon.
      - Adaptive window sizing via ghosts:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - Hot/tiny bypass: if estimated hot (TinyLFU) and small, insert directly into S.
      - Otherwise:
          * Large-object routing: very large objects go directly to Probation to avoid blowing up W.
          * If moderately hot (est >= P_BYPASS_EST), bypass W to P.
          * Else insert into Window (recency admission).
      - Keep W near its target by moving its LRU to P if needed.
      - Keep S within target by demoting S-LRU to P when needed.
      - Periodically adapt W and S fractions.
    """
    _ensure_sketch(cache_snapshot)

    # Doorkeeper filtering (reduce sketch noise from one-timers)
    if _door_should_count(cache_snapshot, obj.key):
        _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Choose insertion segment (hot/tiny bypass and large-object aware)
    sz = _size_of(obj)
    cap = max(1, int(cache_snapshot.capacity))
    w_target, _ = _targets(cache_snapshot)

    est = _sketch_estimate(key)
    tiny_thresh = max(1, int(cap * TINY_FRAC_OF_CAP))
    bypass_s_allowed = (sz <= max(1, int(cap * BYPASS_SIZE_FRAC)))

    inserted_seg = None
    if est >= S_BYPASS_EST and bypass_s_allowed:
        # Extremely hot and small: protect aggressively
        _insert_into_segment(key, SEG_PROTECTED, sz)
        inserted_seg = SEG_PROTECTED
    else:
        insert_to_P = (sz > int(cap * LARGE_FRAC_OF_CAP)) or (sz > w_target and w_target > 0) or (est >= P_BYPASS_EST)
        if insert_to_P:
            _insert_into_segment(key, SEG_PROBATION, sz)
            inserted_seg = SEG_PROBATION
        else:
            _insert_into_segment(key, SEG_WINDOW, sz)
            inserted_seg = SEG_WINDOW

    # Keep W within its target by moving its LRU to P if needed
    if m_bytes_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is not None:
            v = cache_snapshot.cache.get(candidate)
            if v is not None:
                csz = _size_of(v)
                _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)

    _maybe_adjust_S_frac(cache_snapshot)
    _maybe_adjust_W_frac_by_hits(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
      - Clean up doorkeeper entry.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
    # Clean doorkeeper entry to avoid stale growth
    m_door.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplxd24o4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo_mld5_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcafenjqa.pickle

Iteration 39: New subsample score 1.648123 is not better than old score 1.672861, skipping
Iteration 40: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprf2bnzfu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgh0vyokv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6urrqri9.pickle

Iteration 40: Proposed new text for program: # HybDRP-ARC: Size-aware Hybrid Deadtime + Recency with ARC-style adaptation
# Summary of key ideas:
# - Combines predicted time-to-next-use (EWMA deadtime) with age (recency) and size awareness.
# - Two segments (probation/protected) with dynamic byte-budget balanced via ARC-style ghost feedback.
# - Soft overdue handling: mildly protects near-due items, but lets very overdue items be evicted.
# - Fast-track reinsert from ghost history (especially previously-protected or multi-hit items).
# - Compact ghost store with stage to drive adaptation and warm-start predictors.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (hit count)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Stage bytes accounting
_bytes_stage0 = 0
_bytes_stage1 = 0

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# ARC-like protected target in bytes
_arc_target_bytes = 0     # lazily initialized to a fraction of capacity

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stage = dict()         # key -> int (historical stage at eviction)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
# Predictors
EWMA_BETA = 0.25           # per-key EWMA learning rate
GLOBAL_BETA = 0.01         # global EWMA learning rate (slow)
DEFAULT_MU_MULT = 2.5      # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_GRACE_FRAC = 0.5   # fraction of mu considered "grace window" when overdue

# Scoring weights
SIZE_EXP_PROBATION = 1.10  # stronger size penalty in probation (scan resistance)
SIZE_EXP_PROTECTED = 0.85  # milder size penalty in protected (reward proven items)
FREQ_DAMP = 0.7            # frequency dampening (log scale)
AGE_WEIGHT_PROB = 0.6      # recency weight in probation
AGE_WEIGHT_PROT = 0.15     # recency weight in protected

# ARC-like adaptation
PROTECTED_INIT_FRAC = 0.6  # initial protected byte budget as fraction of capacity
ADAPT_STEP_MIN = 1024      # minimum adapt step in bytes
GHOST_LIMIT_MIN = 2048     # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.5   # ghost capacity  factor * live cache item count

# Misc
_INF = 1e30                # very large number for "infinite" predicted delay


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stage.pop(k, None)
            g_stamp.pop(k, None)

def _ensure_targets(cache_snapshot):
    global _arc_target_bytes
    cap = int(getattr(cache_snapshot, "capacity", 0) or 0)
    if cap <= 0:
        return
    if _arc_target_bytes <= 0:
        _arc_target_bytes = int(PROTECTED_INIT_FRAC * cap)

def _predicted_delta_soft(key, now):
    """
    Predict a non-negative time-to-next-access with soft-overdue handling.
    - If no predictor exists: return a very large value (cold).
    - If age <= mu: delta = mu - age (time remaining).
    - If overdue: grace window of OVERDUE_GRACE_FRAC*mu has delta=0, beyond that grows linearly.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    mu = max(1.0, float(mu))
    age = max(0.0, float(now - la))
    if age <= mu:
        return float(mu - age)
    over = age - mu
    grace = OVERDUE_GRACE_FRAC * mu
    if over <= grace:
        return 0.0
    return float(over - grace)

def _freq_boost(freq):
    return (1.0 + FREQ_DAMP * math.log1p(max(0, freq - 1)))

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    score = (delta_soft + age_weight * age) / (size^SIZE_EXP * freq_boost)
    with stage-specific weights and size exponents.
    """
    sz = _size_of(obj)
    la = m_last_access.get(key, None)
    age = 0.0 if la is None else max(0.0, float(now - la))
    delta = _predicted_delta_soft(key, now)
    freq = m_freq.get(key, 1)
    stage = m_stage.get(key, 0)

    if stage >= 1:
        num = delta + AGE_WEIGHT_PROT * age
        exp = SIZE_EXP_PROTECTED
    else:
        num = delta + AGE_WEIGHT_PROB * age
        exp = SIZE_EXP_PROBATION

    denom = (sz ** exp) * _freq_boost(freq)
    return float(num) / float(denom if denom > 0 else 1.0)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history (mu, freq, stage) for evicted key to accelerate relearning and ARC adaptation.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    g_stage[evicted_key] = m_stage.get(evicted_key, 0)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _adapt_arc_target(cache_snapshot, toward_protected, step_bytes):
    """
    Adjust protected target bytes up/down, bounded in [0, capacity].
    """
    global _arc_target_bytes
    _ensure_targets(cache_snapshot)
    cap = int(getattr(cache_snapshot, "capacity", 0) or 0)
    if cap <= 0:
        return
    step = max(ADAPT_STEP_MIN, int(step_bytes))
    if toward_protected:
        _arc_target_bytes = min(cap, _arc_target_bytes + step)
    else:
        _arc_target_bytes = max(0, _arc_target_bytes - step)

def _stage_bytes_of(stage):
    return _bytes_stage1 if stage == 1 else _bytes_stage0

def _on_stage_change(key, new_stage, obj_size):
    """
    Move bytes between segments when changing stage (0<->1).
    """
    global _bytes_stage0, _bytes_stage1
    old = m_stage.get(key, 0)
    if old == new_stage:
        return
    sz = int(obj_size)
    if old == 0 and new_stage == 1:
        _bytes_stage0 = max(0, _bytes_stage0 - sz)
        _bytes_stage1 += sz
    elif old == 1 and new_stage == 0:
        _bytes_stage1 = max(0, _bytes_stage1 - sz)
        _bytes_stage0 += sz
    m_stage[key] = new_stage

def _seed_predictor_on_insert(cache_snapshot, obj, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from ghost if available; otherwise seed from global mu.
    - Choose stage: default probation; fast-track to protected if strong ghost signal (prior protected or freq>=2).
    - Update stage byte counters.
    """
    global _bytes_stage0, _bytes_stage1, m_global_mu

    key = obj.key
    sz = _size_of(obj)

    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        gh_freq = max(1, int(g_freq.get(key, 1)))
        m_freq[key] = gh_freq
        prior_stage = int(g_stage.get(key, 0))
        # Fast-track to protected if previously protected or had multiple hits
        new_stage = 1 if (prior_stage == 1 or gh_freq >= 2) else 0
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1
        new_stage = 0

    m_last_access[key] = now
    m_stage[key] = new_stage
    if new_stage == 1:
        _bytes_stage1 += sz
    else:
        _bytes_stage0 += sz

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment frequency.
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * float(prev_mu) + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold hit without prior metadata (very rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict one resident object guided by ARC-style segment budgets:
    - Prefer evicting from the segment exceeding its byte budget.
    - Within the chosen segment, use the hybrid eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    _ensure_targets(cache_snapshot)
    now = _now(cache_snapshot)
    cap = int(getattr(cache_snapshot, "capacity", 0) or 0)

    # Decide preferred segment to evict from
    # If protected bytes exceed target -> evict from protected; else from probation
    prefer_stage = 1 if (_bytes_stage1 > min(_arc_target_bytes, cap)) else 0

    def pick_from_segment(target_stage):
        best_key = None
        best_score = -1.0
        best_la = None
        best_sz = None

        for k, v in cache.items():
            if m_stage.get(k, 0) != target_stage:
                continue
            s = _eviction_score(k, v, now)
            la = m_last_access.get(k, -1)
            sz = _size_of(v)

            if best_key is None:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue

            if s > best_score:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue

            if s == best_score:
                if la < best_la:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if la == best_la:
                    if sz > best_sz:
                        best_key, best_score, best_la, best_sz = k, s, la, sz
                        continue
                    if sz == best_sz and k < best_key:
                        best_key, best_score, best_la, best_sz = k, s, la, sz
        return best_key

    # Try preferred segment first, then fallback to the other if empty
    victim = pick_from_segment(prefer_stage)
    if victim is None:
        victim = pick_from_segment(1 - prefer_stage)

    # As a last fallback (shouldn't happen), pick any with max score
    if victim is None:
        best_key = None
        best_score = -1.0
        best_la = None
        best_sz = None
        for k, v in cache.items():
            s = _eviction_score(k, v, now)
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if best_key is None or s > best_score or \
               (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
                best_key, best_score, best_la, best_sz = k, s, la, sz
        victim = best_key

    return victim


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time, global mu, last access time, and frequency.
    - Promote from probation to protected on first hit; update byte counters on promotion.
    """
    _ensure_targets(cache_snapshot)
    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(cache_snapshot, obj, now)
        # Don't add an extra count for this call; _update_predictor_on_hit will increment

    # Promotion on first hit
    if m_stage.get(key, 0) == 0:
        _on_stage_change(key, 1, sz)

    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Stage selection:
      * Default probation; fast-track to protected if strong ghost signal (prior protected or freq>=2).
    - ARC adaptation:
      * If ghost shows prior probation (B1) => favor recency (shrink protected target).
      * If ghost shows prior protected (B2) => favor frequency (grow protected target).
    """
    _ensure_targets(cache_snapshot)
    key = obj.key
    now = _now(cache_snapshot)

    # ARC adaptation driven by ghost signal
    if key in g_stage:
        prior_stage = int(g_stage.get(key, 0))
        toward_protected = (prior_stage == 1)  # B2 hit => grow protected; B1 hit => shrink protected
        _adapt_arc_target(cache_snapshot, toward_protected=toward_protected, step_bytes=_size_of(obj))

    _seed_predictor_on_insert(cache_snapshot, obj, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key (mu, freq, stage).
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key metadata of the evicted key.
    - Update byte counters for the evicted segment.
    """
    global _bytes_stage0, _bytes_stage1

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)
    estage = m_stage.get(ekey, 0)

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)

    # Update bytes accounting
    if estage == 1:
        _bytes_stage1 = max(0, _bytes_stage1 - esz)
    else:
        _bytes_stage0 = max(0, _bytes_stage0 - esz)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz0ci9vac.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbv9fey3m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc_4gl0mo.pickle

Iteration 40: New subsample score 0.508921 is not better than old score 0.559448, skipping
Iteration 41: Selected program 8 score: 0.23209699999999991
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt9zlemat.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphie3470z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppqw7ykfg.pickle

Iteration 41: Proposed new text for program: # GreedyDual-Size with Frequency and Light Reuse (GDSF-LR)
# A robust, size-aware eviction policy with built-in aging, frequency reinforcement, and
# mild reuse prediction. It generally performs well across diverse workloads including scans,
# bursty temporal locality, and mixed-size objects.
#
# Key ideas:
# - GreedyDual-Size: each item maintains a "priority" H = L + cost/size^exp. The aging
#   variable L is set to the priority of the last evicted item, providing global aging.
# - Frequency reinforcement: cost includes a log-dampened hit count so frequently-accessed
#   items get higher priority when refreshed.
# - Light reuse prediction: we maintain an EWMA of inter-arrival times and add a small
#   1/mu boost to favor items with short observed gaps.
# - Size awareness: divide by size^exp (exp1.0 defaults to per-byte normalization) so large
#   items require proportionally higher value to stay.
# - Optional ghost history: on reinsert, warm-start frequency and mu from a compact ghost
#   history store, trimmed in O(1) amortized time via a versioned deque.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()     # key -> int (last access time)
m_freq = dict()            # key -> int (hit count)
m_mu = dict()              # key -> float (EWMA inter-arrival time)
m_prio = dict()            # key -> float (GreedyDual priority H = L + cost/size^exp)

# Global EWMA of inter-arrival time across all keys (used as a default baseline)
m_global_mu = 64.0

# GreedyDual aging factor (non-decreasing)
m_L = 0.0

# Ghost history for warm starts on re-insert (bounded by count, O(1) trim)
g_freq = dict()            # key -> int (historical hits, lightly decayed on use)
g_mu = dict()              # key -> float (last known EWMA gap)
g_stamp = dict()           # key -> int (version counter per key)
g_order = deque()          # deque of (key, stamp) in LRU order of ghost insertions

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0             # size exponent for size-awareness (1.0 = per-byte normalization)
FREQ_WEIGHT = 1.0          # weight of log-dampened frequency in cost
MU_WEIGHT = 0.5            # weight of reuse predictor term (1/mu) in cost
EWMA_BETA = 0.30           # per-key EWMA learning rate for inter-arrival times
GLOBAL_BETA = 0.01         # global EWMA learning rate
GHOST_LIMIT_MIN = 1024     # minimal number of ghost entries to keep
GHOST_LIMIT_FACTOR = 1.0   # ghost capacity  factor * live cache item count
GHOST_FREQ_RETAIN = 0.7    # when warming from ghost, retain this fraction of ghost freq
MIN_COST = 1.0             # base cost (ensures floor on value regardless of stats)
EPS = 1e-12                # numerical epsilon

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _cost_components(key):
    """
    Compute additive 'cost' components for GreedyDual priority:
      cost = MIN_COST + FREQ_WEIGHT * log1p(freq-1) + MU_WEIGHT * (1/mu)
    """
    f = m_freq.get(key, 1)
    mu = m_mu.get(key, m_global_mu)
    freq_term = FREQ_WEIGHT * math.log1p(max(0, f - 1))
    mu_term = MU_WEIGHT * (1.0 / max(1.0, float(mu)))
    return MIN_COST + freq_term + mu_term

def _priority_from_cost(cost, size):
    return float(m_L) + float(cost) / (float(size) ** float(SIZE_EXP))

def _ensure_priority_for_key(key, obj):
    """
    Ensure m_prio has an entry for key; if missing, derive from current stats.
    """
    if key in m_prio:
        return
    sz = _size_of(obj)
    cost = _cost_components(key)
    m_prio[key] = _priority_from_cost(cost, sz)

def _update_mu_on_hit(key, now):
    """
    Update per-key and global EWMA of inter-arrival times on hit.
    """
    global m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        # Mild adaptivity: if big gap, increase learning a bit for faster adjustment
        ratio = gap / max(1.0, prev_mu)
        beta = EWMA_BETA if ratio <= 2.0 else min(0.9, EWMA_BETA * ratio)
        m_mu[key] = (1.0 - beta) * prev_mu + beta * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, float(m_global_mu))

def _seed_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert, optionally warming from ghost.
    """
    if key in g_mu:
        # Warm start from ghost
        gf = g_freq.get(key, 1)
        m_freq[key] = max(1, int(math.ceil(GHOST_FREQ_RETAIN * gf)))
        m_mu[key] = max(1.0, float(g_mu[key]))
    else:
        m_freq[key] = 1
        m_mu[key] = max(1.0, float(m_global_mu))

    m_last_access[key] = now
    # Set initial GreedyDual priority
    sz = _size_of(obj)
    cost = _cost_components(key)
    m_prio[key] = _priority_from_cost(cost, sz)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact ghost history for the evicted key.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose the victim with the smallest GreedyDual priority H = L + cost/size^exp.
    Tie-breakers: older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Ensure all candidates have a priority (in case of rare edge cases)
    for k, v in cache.items():
        _ensure_priority_for_key(k, v)

    victim_key = None
    victim_prio = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        pr = m_prio.get(k, 0.0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if victim_key is None:
            victim_key, victim_prio, victim_la, victim_sz = k, pr, la, sz
            continue

        # Evict the smallest priority
        if pr < victim_prio - EPS:
            victim_key, victim_prio, victim_la, victim_sz = k, pr, la, sz
            continue
        if abs(pr - victim_prio) <= EPS:
            # Older last access
            if la < victim_la:
                victim_key, victim_prio, victim_la, victim_sz = k, pr, la, sz
                continue
            if la == victim_la:
                # Larger size
                if sz > victim_sz:
                    victim_key, victim_prio, victim_la, victim_sz = k, pr, la, sz
                    continue
                if sz == victim_sz and k < victim_key:
                    victim_key, victim_prio, victim_la, victim_sz = k, pr, la, sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update per-key EWMA of inter-arrival times and global EWMA.
    - Increment frequency.
    - Refresh GreedyDual priority: H = L + cost/size^exp.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline metadata exists (rare if a hit occurs immediately after cold insert)
    if key not in m_last_access or key not in m_mu or key not in m_freq:
        _seed_on_insert(key, now, obj)
        # Adjust frequency because seed set freq=1; this access counts below

    _update_mu_on_hit(key, now)
    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Refresh priority based on updated stats
    sz = _size_of(obj)
    cost = _cost_components(key)
    m_prio[key] = _priority_from_cost(cost, sz)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Warm-start from ghost when available; otherwise use global baselines.
    - Initialize GreedyDual priority using cost and size.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Update GreedyDual aging factor L to the evicted key's priority (monotonic).
    - Record ghost history for potential warm-start.
    - Trim ghost store.
    - Remove per-key metadata for the evicted key.
    """
    global m_L

    ekey = evicted_obj.key
    ev_prio = m_prio.get(ekey, 0.0)
    if ev_prio > m_L:
        m_L = ev_prio

    # Record ghost and trim
    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_prio.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp84d2i5f7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpal2_wa8p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps96dzkkn.pickle

Iteration 41: New subsample score 1.195762 is not better than old score 1.253249, skipping
Iteration 42: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9gk7a8y_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdpxngd1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rbh595i.pickle

Iteration 42: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with size-aware scoring
# - Each resident item keeps a score H (integer "priority").
# - Eviction picks the item with the smallest H (min-heap).
# - On insert: H = L + inc(size) * (1 + alpha * freq_hint), where L is the "age" (last evicted H).
# - On hit: H += inc(size).
# - L (age) is set to the H of the last-evicted item (standard GreedyDual).
# - inc(size) = ceil(SCALE / size), favoring small items; all arithmetic is integer to avoid float drift.
# - freq_hint uses a small capped counter retained across evictions to mildly bias admission.
# - All operations are O(log n) due to the heap; lazy deletion is used to handle updated scores.

from heapq import heappush, heappop

# ----------------------
# Global state
# ----------------------
# Priority heap of (H, seq, key), min-heap
_heap = []
_seq = 0  # tie-breaker sequence increasing with every push

# Score per live key
m_H = dict()       # key -> int

# Last known size for keys (live or history)
m_size = dict()    # key -> int

# Lightweight frequency hint (capped), retained across evictions
m_freq = dict()    # key -> int
FREQ_CLIP = 64

# GreedyDual "age"
L_age = 0  # int

# Integer scaling for H increments. Larger gives finer discrimination across sizes.
SCALE = 100000

# Admission boost alpha as rational alpha_num/alpha_den to stay in integers.
# Effective factor = 1 + alpha * freq_hint, with freq_hint in [0, FREQ_CLIP]
ALPHA_NUM = 1
ALPHA_DEN = 16  # alpha = 1/16 = 0.0625; mild bias


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        s = int(getattr(obj, "size", 1))
        return s if s > 0 else 1
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _inc_for_size(sz):
    # Integer increment proportional to 1/size; ensure at least 1.
    # Use ceiling-like behavior to avoid zeros for large sizes.
    return max(1, (SCALE + sz - 1) // sz)

def _push_heap(key, H):
    global _seq
    _seq += 1
    heappush(_heap, (H, _seq, key))

def _ensure_freq_entry(k):
    # Do not increment here; just ensure key exists in the dict
    if k not in m_freq:
        m_freq[k] = 0

def _freq_bump(k):
    m_freq[k] = min(FREQ_CLIP, m_freq.get(k, 0) + 1)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose the victim with the smallest H (GreedyDual). Lazy-delete stale heap entries.
    If heap is unexpectedly empty, fall back to evict an arbitrary key from the cache.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    # Pop until we find a live and up-to-date entry
    while _heap:
        H, _, k = heappop(_heap)
        if k not in cache:
            # Not in cache anymore (stale); skip
            continue
        curr = m_H.get(k, None)
        if curr is None or curr != H:
            # Score changed (stale tuple); skip
            continue
        # Found the current minimum
        return k

    # Fallback: in rare cases where the heap is empty or fully stale, evict any key.
    # This should be uncommon because we push on every insert/hit.
    for k in cache:
        return k
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increase the item's priority H by inc(size) (favoring small, frequent items).
    - Bump a capped frequency hint for future reinsertions (admission boost).
    """
    k = obj.key
    sz = _size_of(obj)
    inc = _inc_for_size(sz)

    # Track size and frequency hint
    m_size[k] = sz
    _freq_bump(k)

    # If the key is not tracked (defensive), start from current age.
    oldH = m_H.get(k, L_age)
    newH = oldH + inc
    m_H[k] = newH
    _push_heap(k, newH)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Initialize H = L_age + inc(size) * (1 + alpha * freq_hint).
      freq_hint is a capped counter retained across evictions.
    - Push into the heap for eviction ordering.
    """
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    _ensure_freq_entry(k)

    base = _inc_for_size(sz)
    freq_hint = m_freq.get(k, 0)
    # admission multiplier = 1 + (ALPHA_NUM/ALPHA_DEN) * freq_hint
    # compute base + floor(base * freq_hint * ALPHA_NUM / ALPHA_DEN)
    boost = (base * freq_hint * ALPHA_NUM) // ALPHA_DEN
    H = L_age + base + boost

    m_H[k] = H
    _push_heap(k, H)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting:
    - Update L_age to the evicted key's H (GreedyDual rule).
    - Remove its live score; keep its freq hint for potential future admission boost.
    """
    global L_age
    ekey = evicted_obj.key
    # Use the last known score if available to advance age
    ev_H = m_H.pop(ekey, None)
    if ev_H is not None:
        if ev_H > L_age:
            L_age = ev_H
    # Keep m_freq[ekey] as a history-based hint (do not clear).
    # m_size[ekey] can also be kept as last-known size; no need to remove.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv4p_3m3s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7a2bjpiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc8bydhlc.pickle

Iteration 42: New subsample score 0.106937 is not better than old score 0.168688, skipping
Iteration 43: Selected program 7 score: 0.24589052083333332
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy8rn2pbs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqnxxzxz_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpragjtkx7.pickle

Iteration 43: Proposed new text for program: # Adaptive Decayed-Frequency SLRU with ARC-style Targeting and Lateness Prediction (ADF-ARC)
# Goals:
# - Reduce miss rate by combining:
#   1) SLRU segments (probationary/protected) to guard against cache pollution.
#   2) Adaptive segment sizing using ARC-style ghost feedback (B1/B2) to balance recency vs. frequency.
#   3) Lateness-aware reuse prediction (EWMA of inter-arrival times).
#   4) Time-decayed frequency (EWMA + exponential time decay) to avoid pinning stale heavy-hitters.
#   5) Mild size bias, but conservative in case capacity is object-count-based.
#
# Notes:
# - Evict primarily from probationary if it exceeds adaptive target (ARC p); else from protected.
# - Ghost lists record which segment the victim came from to adaptively adjust p on reinsert.
# - Frequency is decayed as a function of time; very recent inserts receive a short "young shield".
#
# The policy uses only read-only inputs from cache_snapshot and obj. All metadata managed here.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_seg = dict()            # key -> int (0=probationary/T1, 1=protected/T2)

# Decayed frequency (TinyLFU-like via EWMA + time decay)
m_ff = dict()             # key -> float (decayed frequency value)
m_ff_t = dict()           # key -> int (time of last frequency update)

# Global EWMA of inter-arrival time (for cold-start)
m_global_mu = 32.0

# Ghost history for predictor warm start
g_mu = dict()             # key -> float (last known EWMA)
g_last = dict()           # key -> int (last time tracked in ghost)
g_ff = dict()             # key -> float (historical decayed frequency)

# ARC-style ghost lists (track eviction origin to adapt T1 vs T2 target)
B1 = dict()               # keys evicted from probationary (T1): key -> last time
B2 = dict()               # keys evicted from protected (T2): key -> last time

# ----------------------
# Tunable hyperparameters
# ----------------------
# Reuse prediction (EWMA)
EWMA_BETA = 0.30
GLOBAL_BETA = 0.01
DEFAULT_MU_MULT = 1.25    # slightly tighter than before for better cold-start conservatism

# Scoring weights
LATE_PENALTY = 1.8        # penalize overdue keys (make them more evictable)
W_PRED = 1.0
W_REC = 0.15              # small recency component

# Size bias (keep conservative; many environments have object-count capacity)
SIZE_EXP = 0.25

# Decayed frequency (EWMA + time decay)
FREQ_ALPHA = 0.18         # EWMA update on hit
FREQ_HALF_LIFE = 1024.0   # accesses; adjust for traces with different scales
FREQ_DAMP = 0.80          # dampening strength in denominator
PROTECTED_FREQ_MULT = 1.8 # stronger dampening for protected items

# Young shield for newly accessed items (avoid evicting very fresh entries)
YOUNG_WINDOW = 6.0        # accesses
YOUNG_SHIELD = 0.80       # divide score by up to (1+YOUNG_SHIELD) if very young

# ARC-style adaptive targeting
ARC_P_INIT_FRAC = 0.30    # initial fraction of items in T1 (probationary)
ARC_P_ADJ_MIN = 1         # minimum adjustment step
ARC_P_SMOOTH = 0.50       # smoothing when applying adjustments

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0  # total of all ghost maps relative to live count

# Internal constants
_INF = 1e30

# Adaptive target for T1 size (in item counts, not bytes); adjusted using ghosts
_arc_p = 0.0  # target count for probationary (T1); protected target = live_count - _arc_p


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _live_count(cache_snapshot):
    return len(getattr(cache_snapshot, "cache", {}) or {})

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, _live_count(cache_snapshot))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghosts(cache_snapshot):
    # Ensure total ghosts (g_mu + B1 + B2) stay within limit by evicting oldest entries
    limit = _ghost_capacity_limit(cache_snapshot)
    total = len(g_mu) + len(B1) + len(B2)
    if total <= limit:
        return

    # Build a combined list of (key, time, source) and evict the oldest first
    combined = []
    for k, t in g_last.items():
        combined.append((k, t, 'G'))  # predictor ghost
    for k, t in B1.items():
        combined.append((k, t, 'B1'))
    for k, t in B2.items():
        combined.append((k, t, 'B2'))
    combined.sort(key=lambda x: (x[1], x[0]))

    to_remove = total - limit
    i = 0
    while to_remove > 0 and i < len(combined):
        k, _, src = combined[i]
        if src == 'G':
            g_mu.pop(k, None)
            g_last.pop(k, None)
            g_ff.pop(k, None)
        elif src == 'B1':
            B1.pop(k, None)
        else:
            B2.pop(k, None)
        to_remove -= 1
        i += 1

def _arc_targets(cache_snapshot):
    # Returns (t1_target_count, t2_target_count) with bounds
    live = _live_count(cache_snapshot)
    if live <= 0:
        return 0, 0
    p = max(0.0, min(float(live), float(_arc_p)))
    t1_target = int(round(p))
    t1_target = max(0, min(live, t1_target))
    t2_target = max(0, live - t1_target)
    return t1_target, t2_target

def _protected_target_count(cache_snapshot):
    # Derived from ARC targets: protected (T2) target count
    _, t2_target = _arc_targets(cache_snapshot)
    return t2_target

def _maybe_demote_protected(cache_snapshot):
    # Keep T2 (protected) near target: demote oldest T2 items if oversized
    target = _protected_target_count(cache_snapshot)
    if target <= 0:
        return
    cache = cache_snapshot.cache
    prot_keys = [k for k, seg in m_seg.items() if seg == 1 and k in cache]
    if len(prot_keys) <= target:
        return
    # Demote oldest protected back to probationary
    oldest_k = None
    oldest_la = _INF
    for k in prot_keys:
        la = m_last_access.get(k, -1)
        if la < oldest_la:
            oldest_la = la
            oldest_k = k
    if oldest_k is not None:
        m_seg[oldest_k] = 0

def _seed_predictor_on_insert(key, now):
    # Initialize predictor and decayed frequency for a cold insert
    if key in g_mu:
        m_mu[key] = max(1.0, 0.90 * float(g_mu[key]))
        f0 = float(g_ff.get(key, 0.0))
        m_ff[key] = f0
        m_ff_t[key] = now
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_ff[key] = 0.0
        m_ff_t[key] = now
    m_last_access[key] = now
    m_seg[key] = 0  # start in probationary (T1)

def _update_predictor_on_hit(key, now):
    # Update per-key EWMA inter-arrival and global EWMA
    global m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now

def _decayed_freq(key, now):
    # Compute lazily decayed frequency value
    f0 = float(m_ff.get(key, 0.0))
    t0 = int(m_ff_t.get(key, now))
    dt = max(0, now - t0)
    if dt > 0 and f0 > 0.0:
        # Exponential decay f *= 2^-(dt / half_life)
        decay = math.pow(0.5, float(dt) / max(1.0, FREQ_HALF_LIFE))
        f0 *= decay
    return f0

def _update_freq_on_hit(key, now):
    # EWMA update of decayed frequency
    f = _decayed_freq(key, now)
    f_new = (1.0 - FREQ_ALPHA) * f + FREQ_ALPHA * 1.0
    m_ff[key] = f_new
    m_ff_t[key] = now

def _record_ghost_on_evict(evicted_key, now):
    # Keep predictor ghosts for warm start on future insert
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_ff[evicted_key] = float(_decayed_freq(evicted_key, now))

def _effective_delta_for_score(key, now):
    # Lateness-aware remaining time to next use (>=0; larger => more evictable)
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF  # unknown -> easy eviction
    next_t = la + mu
    raw = float(next_t - now)
    if raw >= 0.0:
        return raw
    # Overdue: penalize by how late it is
    return LATE_PENALTY * (-raw) + 1e-9

def _young_shield_multiplier(ts):
    # ts: time since last access
    if ts <= 0:
        return 1.0 + YOUNG_SHIELD
    if ts >= YOUNG_WINDOW:
        return 1.0
    frac = 1.0 - (float(ts) / YOUNG_WINDOW)
    return 1.0 + YOUNG_SHIELD * max(0.0, frac)

def _eviction_score(key, obj, now):
    # Higher score => better eviction candidate
    ed = _effective_delta_for_score(key, now)
    la = m_last_access.get(key, now)
    ts = max(0.0, float(now - la))

    # Base score: predicted time-to-next-use plus small recency factor
    base = W_PRED * ed + W_REC * ts

    # Young shield: make very recent items less evictable
    base /= _young_shield_multiplier(ts)

    # Size bias (mild)
    size_term = float(_size_of(obj)) ** SIZE_EXP

    # Decayed frequency dampening
    df = _decayed_freq(key, now)
    denom = 1.0 + FREQ_DAMP * math.log1p(max(0.0, df))
    if m_seg.get(key, 0) == 1:
        denom *= PROTECTED_FREQ_MULT

    return (base * size_term) / max(1e-9, denom)

def _arc_adjust_on_reinsert(key, now):
    # Adjust ARC target p based on which ghost list the key hits
    global _arc_p
    in_b1 = key in B1
    in_b2 = key in B2
    if not in_b1 and not in_b2:
        return

    b1n = max(1, len(B1))
    b2n = max(1, len(B2))
    # ARC: if key in B1 => favor recency (increase T1 target p)
    #      if key in B2 => favor frequency (decrease T1 target p)
    if in_b1:
        delta = max(ARC_P_ADJ_MIN, int(b2n / b1n))
        _arc_p += ARC_P_SMOOTH * float(delta)
        B1.pop(key, None)
    elif in_b2:
        delta = max(ARC_P_ADJ_MIN, int(b1n / b2n))
        _arc_p -= ARC_P_SMOOTH * float(delta)
        B2.pop(key, None)
    # Bound p to reasonable range based on current live size (will be clamped when used)
    live = max(0, _live_count(getattr(_arc_adjust_on_reinsert, "snap", type("S", (), {"cache": {}})) ))
    if live > 0:
        _arc_p = max(0.0, min(float(live), _arc_p))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose an eviction victim:
    - Prefer evicting from probationary (T1) if |T1| > target p (ARC); else from protected (T2).
    - Score by lateness-aware reuse prediction, with decayed frequency dampening and mild size bias.
    - Tie-break by older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Split into segments
    t1 = []
    t2 = []
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            t2.append(k)
        else:
            t1.append(k)

    # ARC-style candidate segment selection
    t1_target, _ = _arc_targets(cache_snapshot)
    candidates = None
    if len(t1) > t1_target:
        candidates = t1
    elif t2:
        candidates = t2
    else:
        candidates = t1

    if not candidates:
        return None

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k in candidates:
        v = cache[k]
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update lateness predictor (EWMA of gaps) and last access.
    - Update decayed frequency (EWMA + time decay).
    - Promote to protected (T2) on first hit in T1.
    - Keep protected segment near its adaptive target by demoting oldest T2 if needed.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (should be rare for hits)
    if key not in m_last_access or key not in m_mu or key not in m_seg:
        _seed_predictor_on_insert(key, now)

    _update_predictor_on_hit(key, now)
    _update_freq_on_hit(key, now)

    # SLRU promotion: move from T1 to T2 on hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1  # promote to protected (T2)
        _maybe_demote_protected(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    After insert (following a miss):
    - Adjust ARC target p if key is in B1/B2 (ghosts), then remove it from that ghost list.
    - Initialize metadata (probationary T1), warm start from predictor ghost if present.
    - Keep protected size near target via possible demotions.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # ARC adaptation based on ghost where this key was found
    # Attach snapshot to function to allow live bound in helper (no effect if unavailable)
    _arc_adjust_on_reinsert.snap = cache_snapshot
    _arc_adjust_on_reinsert(key, now)

    # Initialize per-key metadata
    _seed_predictor_on_insert(key, now)

    # Keep T2 within target
    _maybe_demote_protected(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record predictor ghost (mu, decayed frequency).
    - Add victim to ARC ghost B1 (if from T1) or B2 (if from T2).
    - Trim ghosts to bounded size.
    - Clean live metadata for evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Determine segment before removal
    seg = m_seg.get(ekey, 0)

    # Predictor ghost
    _record_ghost_on_evict(ekey, now)

    # ARC ghosts: remember where it came from
    if seg == 0:
        B1[ekey] = now
        # Remove from B2 if present (avoid duplicates)
        B2.pop(ekey, None)
    else:
        B2[ekey] = now
        B1.pop(ekey, None)

    # Trim all ghosts
    _trim_ghosts(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_seg.pop(ekey, None)
    m_ff.pop(ekey, None)
    m_ff_t.pop(ekey, None)


# -------------- Initialization of ARC p (optional) --------------
# This hook is not required by the framework, but we can set an initial p the first time.
def _init_arc_p_if_needed(cache_snapshot):
    global _arc_p
    if _arc_p == 0.0 and _live_count(cache_snapshot) > 0:
        _arc_p = ARC_P_INIT_FRAC * float(_live_count(cache_snapshot))

# Try to initialize on module import (no-op if snapshot not available)
try:
    _init_arc_p_if_needed(type("S", (), {"cache": {}}))
except Exception:
    pass
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx2n8ybi4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy82yyw5u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpni6cxuah.pickle

Iteration 43: New subsample score 0.874717 is not better than old score 0.882293, skipping
Iteration 44: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy4f9itys.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbdbqhh1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptx6of_7g.pickle

Iteration 44: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with lazy heap updates and lightweight history
# - Each cached key has a priority: priority = L + cost/size
#   where:
#     L is a global "inflation" value raised to the priority of each evicted victim.
#     cost is a lightweight popularity proxy (bounded frequency counter).
#     size is the object's size in bytes (default 1).
# - Eviction: always evict the key with the lowest current priority.
#   Implemented with a min-heap and lazy deletion for O(log n) average operations.
# - On hit: increase cost and raise its priority (push a new heap entry).
# - On insert (after miss): seed cost from history (or 1) and push a heap entry.
# - Ghost/history: bounded per-key frequency m_freq persists after eviction, acting
#   as tiny history; no explicit ghost lists needed since L ages the whole cache.
# - Size-aware: large objects are penalized (cost/size), improving byte efficiency.
# - Scan-resistant: L increases on each eviction, aging out older entries without
#   requiring global scans or costly decays.

from heapq import heappush, heappop
from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Priority queue (min-heap) of (priority, ticket, key)
_pq = []
_entry = {}          # key -> (priority, ticket) for validity check
_ticket = 0          # monotonic tie-breaker
_L = 0.0             # global inflation (age)

# Lightweight per-key metadata
m_size = {}          # key -> last known size (>0)
m_freq = {}          # key -> bounded frequency
FREQ_CLIP = 64

# Remember the last victim's priority so update_after_evict can bump L
_last_evict_key = None
_last_evict_pri = 0.0


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _touch_and_get(obj):
    """
    Update and return (size, new_freq) for key on access (hit or miss).
    """
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    m_freq[k] = min(FREQ_CLIP, m_freq.get(k, 0) + 1)
    return sz, m_freq[k]

def _priority(cost, size):
    # GDSF: priority = L + cost / size
    if size <= 0:
        size = 1
    return _L + (float(cost) / float(size))

def _push_entry(key, pri):
    global _ticket
    _ticket += 1
    _entry[key] = (pri, _ticket)
    heappush(_pq, (pri, _ticket, key))

def _valid_top(cache):
    """
    Pop heap until we find a valid, resident key.
    Returns (key, pri) or (None, None) if none found.
    """
    cache_map = getattr(cache, "cache", {}) or {}
    while _pq:
        pri, t, k = heappop(_pq)
        # Skip if key is not in cache anymore
        if k not in cache_map:
            continue
        # Skip if this entry is stale (replaced by a newer push)
        cur = _entry.get(k)
        if cur is None:
            continue
        if cur == (pri, t):
            return k, pri
        # else stale heap entry; continue
    return None, None

def _seed_missing_entries(cache_snapshot, budget=64):
    """
    If our heap is empty or under-represented (e.g., warm start), seed up to `budget`
    entries from current cache keys to avoid falling back to random eviction.
    """
    cache_map = getattr(cache_snapshot, "cache", {}) or {}
    if not cache_map:
        return
    count = 0
    for k, obj in cache_map.items():
        if k in _entry:
            continue
        sz = _size_of(obj)
        # Use lightweight history if available, otherwise minimal cost=1
        cost = max(1, m_freq.get(k, 0))
        pri = _priority(cost, sz)
        _push_entry(k, pri)
        count += 1
        if count >= budget:
            break


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using GDSF priority (min priority evicted).
    - Uses a min-heap with lazy deletion; seeds if heap is empty.
    - Stores the chosen victim's priority so update_after_evict can bump L.
    """
    global _last_evict_key, _last_evict_pri

    cache_map = getattr(cache_snapshot, "cache", {}) or {}
    if not cache_map:
        _last_evict_key = None
        _last_evict_pri = 0.0
        return None

    # Ensure we have some representation of current cache if PQ is empty (warm/initial start)
    if not _pq:
        _seed_missing_entries(cache_snapshot, budget=64)

    # Find the valid minimum-priority resident key
    k, pri = _valid_top(cache_snapshot)
    if k is None:
        # If still none (e.g., all entries stale), seed and try once more
        _seed_missing_entries(cache_snapshot, budget=64)
        k, pri = _valid_top(cache_snapshot)

    if k is None:
        # Fallback: evict any key from cache (rare)
        for any_k in cache_map:
            _last_evict_key = any_k
            _last_evict_pri = _L  # conservative
            return any_k
        return None

    _last_evict_key = k
    _last_evict_pri = pri
    return k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increase key's frequency (bounded) and raise its priority.
    - Push a new heap entry; old entries are lazily discarded.
    """
    k = obj.key
    sz, freq = _touch_and_get(obj)
    pri = _priority(freq, sz)
    _push_entry(k, pri)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Count the access toward frequency (TinyLFU-style).
    - Seed priority using current L and cost/size, then push to heap.
    """
    k = obj.key
    sz, freq = _touch_and_get(obj)
    # Ensure at least minimal cost on first admission
    cost = max(1, freq)
    pri = _priority(cost, sz)
    _push_entry(k, pri)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After a victim is evicted:
    - Advance global L to the victim's priority (aging).
    - Remove its live entry mapping.
    - Keep lightweight m_freq to remember popularity history.
    """
    global _L
    ekey = getattr(evicted_obj, "key", None)

    # Age the system: raise L to the evicted priority (classic GD aging)
    if ekey is not None and ekey == _last_evict_key:
        # L should be non-decreasing
        if _last_evict_pri > _L:
            _L = _last_evict_pri

    # Remove live entry for evicted key if present
    if ekey is not None:
        _entry.pop(ekey, None)
    # Do not clear m_size/m_freq; they serve as lightweight history.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgukzjjkb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp92_9j5ls.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp87wzmg4d.pickle

Iteration 44: New subsample score 1.332636 is not better than old score 1.3443559999999999, skipping
Iteration 45: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphyi2ua8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgviuu3z5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcvhlzoqp.pickle

Iteration 45: Proposed new text for program: # Adaptive GreedyDual-Size-Frequency with Segmented Protection (AGDSF-SP)
# Core ideas:
# - Use a GreedyDual-style priority with global aging (H) to blend recency and frequency
#   without explicit time-decay. Priorities are updated only on accesses.
# - Make eviction size-aware by normalizing by size^GAMMA.
# - Maintain two segments: probation (new/never-hit) and protected (hit at least once).
#   Protected items get an extra multiplicative priority boost.
# - Keep a small ghost history to warm-start frequency on reinsertions (cheap TinyLFU-like memory).
# - Evict the item with the smallest stored priority (L). After eviction, raise H to the victim's L.

import math

# ----------------------
# Global metadata stores
# ----------------------
# GreedyDual priority and stats
m_L = dict()           # key -> float (stored priority L at last access)
m_freq = dict()        # key -> int   (hit count since insertion; capped)
m_seg = dict()         # key -> int   (0 = probation, 1 = protected)
m_last_access = dict() # key -> int   (last access time for tie-breaking only)

# Global aging value (non-decreasing)
g_H = 0.0

# Ghost history for warm starts
g_freq = dict()        # key -> int (capped historical hit count)
g_last = dict()        # key -> int (last seen time in ghost)


# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-awareness exponent: higher => penalize large items more
GAMMA_SIZE = 0.75

# Frequency gain: how much a hit boosts priority weight relative to a cold insert
FREQ_GAIN = 2.5

# Cap on per-key frequency to avoid unbounded bias
FREQ_CAP = 15

# Protected segment multiplicative boost (makes hit-at-least-once items harder to evict)
PROTECT_MULT = 1.35

# Ghost sizing and warm-start
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0
GHOST_DECAY = 0.6        # decay past frequency upon re-entry
PROMOTE_ON_REENTRY_FREQ = 2  # start protected if historical freq >= this

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_freq) <= limit:
        return
    # Evict oldest ghosts by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_freq) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_freq.pop(k, None)
        g_last.pop(k, None)

def _freq_capped(key):
    return min(FREQ_CAP, int(m_freq.get(key, 0)))

def _weight_for(key, obj):
    """
    Compute size-normalized weight given current frequency.
    Base = 1 for new items; each hit adds FREQ_GAIN.
    """
    sz = float(_size_of(obj))
    f = _freq_capped(key)
    base = 1.0 + FREQ_GAIN * float(f)
    return base / (sz ** GAMMA_SIZE)

def _boost_for_segment(key):
    return PROTECT_MULT if m_seg.get(key, 0) == 1 else 1.0

def _ensure_priority(key, obj):
    """
    Ensure m_L has a value for the key; if missing, synthesize from current globals.
    This does not mutate frequency or segments; used defensively in evict().
    """
    if key in m_L:
        return m_L[key]
    # Fallback: treat as cold probationary
    w = _weight_for(key, obj)
    b = 1.0  # probation default (no protection)
    return float(g_H) + b * w

def _assign_priority_on_access(key, obj):
    """
    After an access (hit or insert), recompute and store the GreedyDual priority:
      L_i = H + boost(segment) * weight(frequency, size)
    """
    global g_H
    w = _weight_for(key, obj)
    b = _boost_for_segment(key)
    m_L[key] = float(g_H) + b * w


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest stored priority L.
    Tie-breakers (in order) to improve stability:
      - Prefer evicting probationary items (segment 0)
      - Older last access (LRU)
      - Larger size (free space faster)
      - Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    victim_key = None
    victim_val = None
    victim_L = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        Lk = _ensure_priority(k, v)
        seg = m_seg.get(k, 0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_L = Lk
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if Lk < victim_L - _EPS:
            better = True
        elif abs(Lk - victim_L) <= _EPS:
            if seg < victim_seg:
                better = True
            elif seg == victim_seg:
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_L = Lk
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increment (capped) frequency.
      - Promote to protected segment if not already.
      - Recompute and store GreedyDual priority using current global age H.
      - Update last access time.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize missing metadata defensively (rare)
    if key not in m_freq:
        m_freq[key] = 0
    if key not in m_seg:
        m_seg[key] = 0

    # Update stats
    m_freq[key] = min(FREQ_CAP, m_freq.get(key, 0) + 1)
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1  # promote to protected on first hit

    m_last_access[key] = now

    # Recompute priority
    _assign_priority_on_access(key, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Warm-start frequency from ghost with decay; otherwise start at 0.
      - Start in probation; if historical frequency high, start protected.
      - Initialize GreedyDual priority from current H.
      - Set last access time.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Warm-start frequency from ghost
    if key in g_freq:
        hist = int(g_freq.get(key, 0))
        m_freq[key] = min(FREQ_CAP, int(round(GHOST_DECAY * float(hist))))
        # Segment: start protected if strong prior signal
        m_seg[key] = 1 if m_freq[key] >= PROMOTE_ON_REENTRY_FREQ else 0
    else:
        m_freq[key] = 0
        m_seg[key] = 0

    m_last_access[key] = now

    # Initialize priority
    _assign_priority_on_access(key, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Raise global age H to the victim's priority (GreedyDual aging).
      - Record lightweight ghost stats (capped frequency, last seen).
      - Trim ghost to a bounded size.
      - Remove all live metadata of the evicted key.
    """
    global g_H
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Raise global age to victim's priority
    victim_L = m_L.get(ekey, None)
    if victim_L is not None:
        if victim_L > g_H:
            g_H = float(victim_L)

    # Record ghost stats
    g_freq[ekey] = _freq_capped(ekey)
    g_last[ekey] = now
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_L.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_seg.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgkte1djp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt3rqpy4w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp670df7d5.pickle

Iteration 45: New subsample score 0.15277 is not better than old score 0.216734, skipping
Iteration 46: Selected program 8 score: 0.23209699999999991
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmyp9b7aa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5alw30m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyyvps75i.pickle

Iteration 46: Proposed new text for program: # Hybrid GDSF + Decayed Frequency + Reuse Proximity (GDF-RP)
# Goal: size-aware, frequency-aware, and reuse-time-aware eviction with low overhead.
# Key ideas:
# 1) GreedyDual-Size-Frequency (GDSF) priority with global aging L to resist scans and stale items.
# 2) Lightweight global decayed frequency per key (epoch-based right-shift) across hits and inserts.
# 3) Reuse proximity bonus from an EWMA inter-arrival predictor (per-key mu) to protect near-future hits.
# 4) Ghost history to warm-start mu on reinsert; O(1) amortized trimming.
#
# Eviction: choose key with minimal "effective priority" = stored priority * (1 + TIME_BONUS * proximity)
#           where proximity = 1 / (1 + predicted_delta / global_mu). Soon-to-be-reused items (high proximity)
#           get boosted priority and are less likely to be evicted.
#
# Metadata updates:
# - On hit/insert: bump decayed frequency and refresh priority to L + base, base ~ sqrt(freq)/size^alpha.
# - On hit: update EWMA gap predictor and global_mu.
# - On insert: seed mu from ghost or global baseline.
# - On evict: set L to the evicted item's stored priority (standard GDS aging), record ghost.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
# Per-key recency and reuse predictor
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)

# Global EWMA of inter-arrival time (characteristic time scale)
m_global_mu = 64.0

# GDSF priority and aging
m_priority = dict()      # key -> float (stored priority H = L + base)
gds_L = 0.0              # global aging value

# Global decayed frequency (epoch-based right shift)
f_count = dict()         # key -> int (logical count since its epoch)
f_epoch = dict()         # key -> int (epoch when the count was last materialized)
g_epoch = 0              # current global epoch
_last_decay_access = 0   # last access_count at which epoch advanced

# Ghost history (for warm starts on re-insert)
g_mu = dict()            # key -> float (last known EWMA)
g_stamp = dict()         # key -> int (versioning to avoid stale deque entries)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
# Frequency decay
DECAY_INTERVAL = 16384   # advance epoch every this many accesses (divide counts by 2 lazily)
# Size-awareness: priority base divided by size^SIZE_EXP
SIZE_EXP = 1.0
# Frequency saturation: use sqrt to limit runaway bias
USE_SQRT_FREQ = True

# EWMA predictor for reuse time
EWMA_BETA = 0.25         # learning rate for per-key EWMA of inter-arrival times
GLOBAL_BETA = 0.01       # learning rate for global EWMA

# Reuse proximity bonus at eviction time (0 => pure GDSF)
TIME_BONUS = 0.75        # extra priority multiplier for near-future hits (0..1 typical)

# Ghost capacity (by count of keys)
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0

# Default seeding when no history
DEFAULT_MU_MULT = 2.0

_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    # Advance global epoch in O(1) based on access_count to decay frequencies lazily
    global g_epoch, _last_decay_access
    diff = now - _last_decay_access
    if diff >= DECAY_INTERVAL:
        steps = int(diff // DECAY_INTERVAL)
        if steps > 0:
            g_epoch += steps
            _last_decay_access += steps * DECAY_INTERVAL

def _freq_get(key):
    # Return decayed frequency without materializing it into the store
    c = f_count.get(key, 0)
    e = f_epoch.get(key, g_epoch)
    d = g_epoch - e
    if d <= 0:
        return c
    # right shift by d ~ divide by 2^d
    return int(c >> d)

def _freq_bump(key, inc=1):
    # Lazily apply decay to this key, then add inc, and stamp current epoch
    c = f_count.get(key, 0)
    e = f_epoch.get(key, g_epoch)
    d = g_epoch - e
    if d > 0:
        c = int(c >> d)
    c += int(inc)
    f_count[key] = c
    f_epoch[key] = g_epoch
    return c

def _predicted_delta(key, now):
    """
    Non-negative predicted time until next access.
    If no predictor, return large value.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)
    # If overdue, lateness implies lower proximity (further in future effectively)
    return float(now - next_t)

def _proximity(key, now):
    """
    0..1 measure: higher means next access is sooner relative to global timescale.
    proximity = 1 / (1 + delta / tau), tau ~ global_mu
    """
    tau = max(1.0, float(m_global_mu))
    d = _predicted_delta(key, now)
    if d >= _INF * 0.5:
        return 0.0
    return 1.0 / (1.0 + (d / tau))

def _base_priority_from_freq_size(freq, size):
    # Base priority is frequency-saturated per-byte value
    if USE_SQRT_FREQ:
        f = math.sqrt(max(0.0, float(freq)))
    else:
        f = float(freq)
    denom = float(size) ** float(SIZE_EXP)
    return (1.0 + f) / max(1.0, denom)

def _refresh_priority(key, size):
    """
    Compute and store the GDSF priority for a key at the current time:
    H = L + base(freq, size)
    """
    global m_priority
    freq = _freq_get(key)
    base = _base_priority_from_freq_size(freq, size)
    m_priority[key] = float(gds_L + base)
    return m_priority[key]

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key predictor and last access timestamp on insert.
    Use ghost mu if available; otherwise a moderate default.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.8 * float(g_mu[key]))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now

def _update_predictor_on_hit(key, now):
    """
    Update per-key EWMA of inter-arrival times and global baseline.
    """
    global m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now

def _record_ghost_on_evict(evicted_key):
    """
    Record compact history for warm start: only EWMA mu.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the key with the minimal effective priority:
      effective = stored_priority * (1 + TIME_BONUS * proximity),
    where proximity (0..1) protects near-future hits.
    Tiebreakers: older last-access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    best_key = None
    best_eff = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        sz = _size_of(v)

        # Ensure a stored priority exists (should be set on hit/insert; compute if missing)
        H = m_priority.get(k, None)
        if H is None:
            H = _refresh_priority(k, sz)

        prox = _proximity(k, now)  # 0..1
        eff = H * (1.0 + TIME_BONUS * prox)

        la = m_last_access.get(k, -1)

        if best_key is None:
            best_key, best_eff, best_la, best_sz = k, eff, la, sz
            continue

        if eff < best_eff:
            best_key, best_eff, best_la, best_sz = k, eff, la, sz
            continue

        if eff == best_eff:
            if la < best_la:
                best_key, best_eff, best_la, best_sz = k, eff, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_eff, best_la, best_sz = k, eff, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_eff, best_la, best_sz = k, eff, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch for decayed frequencies.
    - Bump decayed frequency for the key.
    - Update EWMA gap predictor and global_mu; refresh last access.
    - Refresh GDSF priority H = L + base(freq, size).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _freq_bump(key, inc=1)
    _update_predictor_on_hit(key, now)
    _refresh_priority(key, _size_of(obj))


def update_after_insert(cache_snapshot, obj):
    """
    After insert (miss->insert):
    - Advance epoch and bump decayed frequency for this access.
    - Seed EWMA mu from ghost or global baseline; set last access.
    - Refresh GDSF priority H = L + base(freq, size).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _freq_bump(key, inc=1)
    _seed_predictor_on_insert(key, now)
    _refresh_priority(key, _size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Update global aging L to the evicted entry's stored priority (GDS rule).
    - Record ghost mu for warm re-insert; trim ghost store.
    - Remove per-key live metadata (priority, recency, predictor).
    Note: global decayed frequency is stream-wide and retained across evictions.
    """
    global gds_L
    ekey = evicted_obj.key

    # Update aging value L based on stored priority (compute if somehow missing)
    H = m_priority.get(ekey, None)
    if H is None:
        # Fallback: compute on the fly
        sz = _size_of(evicted_obj)
        freq = _freq_get(ekey)
        base = _base_priority_from_freq_size(freq, sz)
        H = float(gds_L + base)
    gds_L = max(float(gds_L), float(H))

    # Record ghost and trim
    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata (keep global frequency)
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_priority.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4k9nfa9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf379do0y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm9e21h52.pickle

Iteration 46: New subsample score 0.936497 is not better than old score 0.953191, skipping
Iteration 47: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqw5akmse.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp68l2qbnr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqbhc2xiy.pickle

Iteration 47: Proposed new text for program: # Size-aware GreedyDual + TinyLFU (GDSF-TLFU)
# ------------------------------------------------------------
# A simple, strong baseline that combines:
#   - TinyLFU (Count-Min Sketch) for global frequency estimation with decay.
#   - GreedyDual-Size-Frequency (GDSF) for eviction priority:
#       H(key) = L + freq_estimate(key) / size(key)
#     where L is the priority of the last evicted object (global "age" floor).
#   - Size-aware: small objects are favored for the same frequency.
#   - Recency: L increases on every eviction so that priorities re-anchor to
#     the present when an item is hit or inserted (H is recomputed as L + score).
#
# Notes:
#   - This design avoids multi-segment complexity and works well across diverse
#     workloads, especially when outright "admission rejection" is not possible.
#   - On hit/insert, we refresh an item's H using the current L and TinyLFU estimate.
#   - On eviction, we remove the item with the minimal H; L is set to that H.
#   - TinyLFU decays periodically to bound stale frequencies.
#
# ------------------------------------------------------------
# Globals / Metadata
# ------------------------------------------------------------
m_last_access = dict()  # key -> last access time (access_count)
m_H = dict()            # key -> current GDSF priority H = L + est/size
m_L = 0.0               # global "age" floor, set to H of last evicted item

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Tie-breaking epsilon
EPS = 1e-12

# ------------------------------------------------------------
# Helpers
# ------------------------------------------------------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    try:
        sz = getattr(obj_or_size, "size", obj_or_size)
    except Exception:
        sz = obj_or_size
    return max(1, int(sz))

def _priority_for_key(cache_snapshot, key):
    """
    Return the current eviction priority H for a cached key.
    If we lack an explicit stored H (metadata gap), compute a conservative default:
      H = L + TinyLFU_estimate(key)/size(key)
    """
    H = m_H.get(key)
    if H is not None:
        return H
    v = cache_snapshot.cache.get(key)
    if v is None:
        return float("inf")
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    return m_L + (est / max(1.0, sz))

def _priority_for_new(obj):
    """
    Priority for a new object to be inserted:
      H_new = L + TinyLFU_estimate(obj)/size(obj)
    This is used for metadata initialization (not for eviction comparison as
    we must always evict an existing item when the cache is full).
    """
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return m_L + (est / max(1.0, sz))

# ------------------------------------------------------------
# Core policy
# ------------------------------------------------------------
def evict(cache_snapshot, obj):
    """
    Choose the victim to evict:
      - Pick the resident with the minimal priority H.
      - Tie-breakers: older last_access (LRU), larger size (frees more), then key.
      - Set of candidates is all currently cached objects.
    Rationale:
      - GDSF ensures items with low (freq/size) and stale H are evicted first.
      - Items recently touched/inserted get H re-anchored to current L and survive.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)

    best_k = None
    best_H = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        H = _priority_for_key(cache_snapshot, k)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_k is None:
            best_k, best_H, best_la, best_sz = k, H, la, sz
            continue

        # Primary: smallest H first
        if H < best_H - EPS:
            best_k, best_H, best_la, best_sz = k, H, la, sz
            continue
        if abs(H - best_H) <= EPS:
            # Secondary: older (smaller last_access)
            if la < best_la:
                best_k, best_H, best_la, best_sz = k, H, la, sz
                continue
            if la == best_la:
                # Tertiary: larger size to free more space
                if sz > best_sz:
                    best_k, best_H, best_la, best_sz = k, H, la, sz
                    continue
                if sz == best_sz and k < best_k:
                    best_k, best_H, best_la, best_sz = k, H, la, sz

    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increment TinyLFU counters (frequency).
      - Update recency timestamp.
      - Recompute H = L + est/size to re-anchor priority to "now".
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    est = _sketch_estimate(key)
    sz = float(_size_of(obj))
    m_H[key] = m_L + (est / max(1.0, sz))


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss becomes resident):
      - Increment TinyLFU counters.
      - Record recency.
      - Initialize H = L + est/size.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    m_H[key] = _priority_for_new(obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Set L to the evicted item's H (GreedyDual aging step).
      - Remove all its metadata.
    """
    global m_L

    k = evicted_obj.key
    # Use stored H if available; if missing, compute conservative fallback
    victim_H = m_H.pop(k, None)
    if victim_H is None:
        victim_H = _priority_for_key(cache_snapshot, k)

    # Age floor rises to victim's priority
    m_L = max(m_L, float(victim_H))

    m_last_access.pop(k, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqtin09uj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyqcc_jj7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpplp2kare.pickle

Iteration 47: New subsample score 0.068813 is not better than old score 0.093451, skipping
Iteration 48: Selected program 2 score: 0.22998966666666665
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp01ujkkpp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplc1pu19h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpokx93ib3.pickle

Iteration 48: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF)
# - Size-aware: normalizes value by object size (per byte fairness).
# - Recency via global inflation L: recently touched items get higher priority automatically.
# - Frequency: each hit raises an object's value, making it resilient to eviction.
#
# Priority H(k) = L + value(k) / size(k)
# - On insert: value(k) = NEW_ITEM_VALUE (small to avoid single-touch pollution)
# - On hit:    value(k) += HIT_VALUE
# - Evict:     pick the item with the smallest H; then set L to that H (global aging)
#
# This is a simple, robust policy widely used for web/proxy caches, performs well
# with variable-size items and mixed recency/frequency patterns without complex tuning.

# Metadata
m_value = dict()        # key -> float, accumulated value (frequency, cost) of key
m_last_access = dict()  # key -> int, last access time (access_count)

# Global inflation (recency)
m_L = 0.0               # global "age" or inflation; increases to the H of evicted item

# Tunables
NEW_ITEM_VALUE = 0.25   # small initial value to reduce one-hit pollution
HIT_VALUE = 1.0         # increment on each hit


def _size(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1


def _priority(key, obj):
    """
    Current priority H = L + value/size, computed lazily from current L.
    """
    val = m_value.get(key, NEW_ITEM_VALUE)
    sz = _size(obj)
    return m_L + (val / float(sz))


def evict(cache_snapshot, obj):
    """
    Evict the resident item with the smallest priority H.
    Tie-breakers:
      1) older last access (LRU among equals)
      2) larger size (frees more space, protects many small objects)
      3) lexicographical key (determinism)
    """
    if not cache_snapshot.cache:
        return None

    now = cache_snapshot.access_count

    victim_key = None
    victim_H = None
    victim_last = None
    victim_size = None

    for k, v in cache_snapshot.cache.items():
        H = _priority(k, v)
        la = m_last_access.get(k, -1)
        sz = _size(v)

        if victim_key is None:
            victim_key, victim_H, victim_last, victim_size = k, H, la, sz
            continue

        if H < victim_H:
            victim_key, victim_H, victim_last, victim_size = k, H, la, sz
            continue

        if H == victim_H:
            if la < victim_last:
                victim_key, victim_H, victim_last, victim_size = k, H, la, sz
                continue
            if la == victim_last:
                if sz > victim_size:
                    victim_key, victim_H, victim_last, victim_size = k, H, la, sz
                    continue
                if sz == victim_size and k < victim_key:
                    victim_key, victim_H, victim_last, victim_size = k, H, la, sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increase the object's value (frequency).
    - Update last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count

    m_value[key] = m_value.get(key, NEW_ITEM_VALUE) + HIT_VALUE
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Initialize with small value to avoid cache pollution by one-timers.
    - Initialize last access time.
    """
    key = obj.key
    now = cache_snapshot.access_count

    m_value[key] = NEW_ITEM_VALUE
    m_last_access[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Set global inflation L to the evicted object's priority (aging others).
    - Remove all metadata for the evicted object.
    """
    global m_L

    ekey = evicted_obj.key
    # Compute evicted priority with current L before removing metadata
    evicted_H = _priority(ekey, evicted_obj)
    m_L = evicted_H

    # Clean metadata
    m_value.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpini6hnkb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3tabs1rj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjazgz1lf.pickle

Iteration 48: New subsample score 0.704743 is not better than old score 0.704743, skipping
Iteration 49: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn_9nzh6f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi4xd8xuk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpud3arnr4.pickle

Iteration 49: Proposed new text for program: # Adaptive W-TinyLFU + Segmented SLRU + Size-aware GreedyDual aging (AW-TLFU-SLRU-GD)
# Goals:
# - TinyLFU for global frequency estimates with decay
# - Segmented SLRU (Window W -> Probation P -> Protected S) for recency confirmations
# - Size-aware priorities using GreedyDual-style aging (priority = L + freq/size)
# - Adaptive window sizing via ARC-like ghost feedback (increase W if we miss on W-evicted keys, shrink otherwise)
# - Eviction chooses across segment tails using the smallest priority (size-aware, frequency-aware)

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096  # must be power-of-two
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# GreedyDual global "inflation" to age priorities (non-decreasing)
m_L = 0.0
m_priority = dict()  # key -> float (priority = m_L + freq/size)

# Ghost caches for ARC-like adaptive window sizing
m_ghost_W = dict()   # keys recently evicted from Window
m_ghost_M = dict()   # keys recently evicted from Main (Probation/Protected)
GHOST_MAX = 8192     # capacity in keys for each ghost
GHOST_STEP = 0.02    # window fraction adjustment step per ghost hit
W_MIN = 0.05
W_MAX = 0.50

# --------------------
# Tunable parameters
# --------------------
# Start with a moderate window; adjusted dynamically by ghost hits
m_w_frac = 0.20
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80
# TinyLFU admission guidance baseline threshold (used as a floor)
ADMIT_EST_THRESHOLD = 1

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# Small epsilon for float arithmetic
EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_w_frac)
    main_target = cap - w_target
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _priority_for(key, obj_or_size):
    """
    Compute or fetch the GreedyDual-style priority: L + freq/size
    Uses current global m_L and TinyLFU estimate.
    """
    pri = m_priority.get(key)
    if pri is not None:
        return pri
    est = _sketch_estimate(key)
    sz = _size_of(obj_or_size)
    return m_L + (est + 1.0) / max(1.0, float(sz))

def _set_priority(key, obj_or_size):
    """
    Recompute and store current priority for a key.
    """
    pri = _priority_for(key, obj_or_size)
    m_priority[key] = pri
    return pri

def _adjust_window(delta):
    global m_w_frac
    m_w_frac = max(W_MIN, min(W_MAX, m_w_frac + delta))

def _ghost_add(ghost_map, key, time_now):
    ghost_map[key] = time_now
    if len(ghost_map) > GHOST_MAX:
        # Remove oldest in this ghost map
        oldest_k = None
        oldest_t = None
        for k, t in ghost_map.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_map.pop(oldest_k, None)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict based on size-aware GreedyDual priority among segment tails, with a slight bias
    to trim Window if above target:
      - Gather LRU from W, P, and S (if present).
      - Compute priority = L + TinyLFU_est / size.
      - Pick the minimal effective priority as victim.
      - Bias: if W is above its target, subtract a small bias from W items to prefer their eviction.
      - Tie-breakers:
         1) older last access
         2) larger size
         3) lexicographic key
    """
    if not cache_snapshot.cache:
        return None

    w_target, _ = _targets(cache_snapshot)
    bias_W = 0.25 if m_bytes_W > w_target else 0.0

    candidates = []
    kW = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    if kW is not None:
        candidates.append(kW)
    kP = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)
    if kP is not None:
        candidates.append(kP)
    kS = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if kS is not None:
        candidates.append(kS)

    if not candidates:
        # Fallback: global LRU
        best_k = None
        best_la = None
        best_sz = None
        for k, v in cache_snapshot.cache.items():
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if best_k is None or la < best_la or (
                la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
            ):
                best_k, best_la, best_sz = k, la, sz
        return best_k

    choose_k = None
    choose_eff = None
    choose_la = None
    choose_sz = None

    for k in candidates:
        v = cache_snapshot.cache[k]
        pri = _priority_for(k, v)
        seg = m_seg.get(k, SEG_PROBATION)
        eff = pri - (bias_W if seg == SEG_WINDOW else 0.0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if choose_k is None:
            choose_k, choose_eff, choose_la, choose_sz = k, eff, la, sz
            continue
        if eff < choose_eff - EPS:
            choose_k, choose_eff, choose_la, choose_sz = k, eff, la, sz
            continue
        if abs(eff - choose_eff) <= EPS:
            # tie-breakers
            if la < choose_la or (la == choose_la and (sz > choose_sz or (sz == choose_sz and k < choose_k))):
                choose_k, choose_eff, choose_la, choose_sz = k, eff, la, sz

    return choose_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Refresh recency.
      - Promote along W -> P -> S.
      - Update GreedyDual priority (L + freq/size).
      - If S exceeds its byte target, demote S-LRU back to P.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: just recency update, handled by m_last_access
        pass

    # Update size-aware priority for GreedyDual
    _set_priority(key, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch and recency.
      - Adaptive window sizing via ghost hits:
         * If key appears in ghost_W, increase window (recency beneficial).
         * If key appears in ghost_M, decrease window (frequency/long-term beneficial).
      - Admission: choose segment using TinyLFU estimate vs Probation's victim estimate, with size guard.
      - Initialize GreedyDual priority.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # ARC-like adaptation via ghost hits
    if key in m_ghost_W:
        _adjust_window(+GHOST_STEP)
        m_ghost_W.pop(key, None)
    elif key in m_ghost_M:
        _adjust_window(-GHOST_STEP)
        m_ghost_M.pop(key, None)

    est = _sketch_estimate(key)
    sz = _size_of(obj)

    # Decide initial segment (admission placement)
    # - Size guard: if item larger than window target, bypass W into P.
    # - Compare TinyLFU estimate vs Probation's LRU estimate to decide bypass (W-TinyLFU-like).
    w_target, _ = _targets(cache_snapshot)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)
    p_est = _sketch_estimate(p_lru) if p_lru is not None else 0

    place_in_P = False
    if sz > max(1, w_target):
        place_in_P = True
    elif est >= max(ADMIT_EST_THRESHOLD, p_est):
        place_in_P = True

    global m_bytes_W, m_bytes_P
    if place_in_P:
        m_seg[key] = SEG_PROBATION
        m_bytes_P += sz
    else:
        m_seg[key] = SEG_WINDOW
        m_bytes_W += sz

    # Initialize GreedyDual priority
    _set_priority(key, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Update GreedyDual aging variable L to the victim's priority.
      - Remove metadata for the evicted key and adjust segment byte counters.
      - Add to appropriate ghost for adaptive window sizing.
    """
    key = evicted_obj.key

    # Update L to victim priority (non-decreasing aging)
    global m_L
    victim_pri = _priority_for(key, evicted_obj)
    if victim_pri > m_L:
        m_L = victim_pri

    seg = m_seg.pop(key, None)
    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
        _ghost_add(m_ghost_W, key, cache_snapshot.access_count)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
        _ghost_add(m_ghost_M, key, cache_snapshot.access_count)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)
        _ghost_add(m_ghost_M, key, cache_snapshot.access_count)
    else:
        # Unknown segment; place in main ghost
        _ghost_add(m_ghost_M, key, cache_snapshot.access_count)

    m_last_access.pop(key, None)
    m_priority.pop(key, None)

    # No need to touch sketch here; it is aged on adds.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfvvh39mm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpax0t215e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa3zh_abq.pickle

Iteration 49: New subsample score 1.150717 is not better than old score 1.223661, skipping
Iteration 50: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4kuwsj81.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmuzr5qe4.pickle

Iteration 50: Proposed new text for program: # TinyLFU-guided Segmented LRU with Size Awareness (TSLRU-SA)
# Goals:
# - Robust miss-rate reduction using a lightweight, scalable frequency estimator (TinyLFU via Count-Min Sketch).
# - Two segments (probation/protected) to retain items that prove reuse, with LRU-style promotion on hit.
# - Size-aware retention (soft penalty for large objects).
# - Simple recency boost using an adaptive half-life derived from a global EWMA of inter-arrival gaps.
# - Efficient, low-overhead operations to avoid timeouts on large traces (bounded sampling during eviction).

import math

# ----------------------
# Global metadata stores
# ----------------------
# Segments: 0 = probation (new/never-hit), 1 = protected (hit at least once)
m_seg = dict()           # key -> int
m_last_access = dict()   # key -> int (last access time)
m_hits = dict()          # key -> int (#hits while resident)

# Global recency baseline (EWMA of observed gaps on hits)
m_global_mu = 64.0

# Count-Min Sketch for TinyLFU frequency estimation
# Four hashed counters, power-of-two width for fast masking
CMS_WIDTH = 1 << 16                 # 65,536 counters
CMS_MASK = CMS_WIDTH - 1
CMS = [0] * CMS_WIDTH
CMS_ADDS = 0
CMS_RESET_THRESHOLD = CMS_WIDTH * 20  # when exceeded, age (halve) all counters

# Hash salts (fixed)
_HS1 = 0x9E3779B97F4A7C15
_HS2 = 0xC2B2AE3D27D4EB4F
_HS3 = 0x165667B19E3779F9
_HS4 = 0x85EBCA77C2B2AE63

# Size normalization cache: size -> 1/(size^GAMMA)
_size_norm_cache = dict()

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-awareness
GAMMA_SIZE = 0.75

# Segmentation/recency weights
PROTECT_MULT = 1.35       # multiplier if in protected segment
RECENCY_WEIGHT = 1.75     # weight of recency boost
HIT_BONUS = 0.20          # bonus per hit (capped) to the keep value
HIT_BONUS_CAP = 5

# Recency half-life bounds (in access-count units)
HL_MIN = 64.0
HL_MAX = 4096.0
HL_MULT = 4.0             # HL ~ HL_MULT * m_global_mu

# Eviction sampling to avoid timeouts on large caches
EVICT_SCAN_LIMIT = 2048   # scan at most this many items when cache is large

_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _size_norm(sz):
    # 1 / (size^GAMMA)
    v = _size_norm_cache.get(sz)
    if v is None:
        v = 1.0 / (float(sz) ** GAMMA_SIZE)
        _size_norm_cache[sz] = v
    return v

def _half_life():
    hl = HL_MULT * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _recency_boost(age):
    # age in access-count units
    hl = _half_life()
    # Smooth bounded boost in [~0,1]; then weighted
    frac = 1.0 / (1.0 + (float(age) / hl))
    return 1.0 + RECENCY_WEIGHT * frac

def _cms_hashes(key):
    # Produce 4 indices for CMS
    h = hash(key)
    x1 = (h ^ _HS1) & CMS_MASK
    x2 = ((h * _HS2) ^ (h >> 16)) & CMS_MASK
    x3 = ((h ^ (h >> 32) ^ _HS3)) & CMS_MASK
    x4 = ((h * _HS4) + (h >> 24)) & CMS_MASK
    return x1, x2, x3, x4

def _cms_inc(key, delta=1):
    global CMS_ADDS
    if delta <= 0:
        return
    i1, i2, i3, i4 = _cms_hashes(key)
    CMS[i1] += delta
    CMS[i2] += delta
    CMS[i3] += delta
    CMS[i4] += delta
    CMS_ADDS += delta
    if CMS_ADDS >= CMS_RESET_THRESHOLD:
        # Age (halve) all counters to keep the window recent
        for i in range(CMS_WIDTH):
            CMS[i] >>= 1
        CMS_ADDS = 0

def _cms_est(key):
    i1, i2, i3, i4 = _cms_hashes(key)
    return float(min(CMS[i1], CMS[i2], CMS[i3], CMS[i4]))

def _update_global_mu(gap):
    # Update global mean gap with a small beta for stability
    # gap >= 1
    global m_global_mu
    BETA = 0.02
    prev = float(m_global_mu)
    m_global_mu = (1.0 - BETA) * prev + BETA * float(gap)


# ----------------------
# Core scoring
# ----------------------
def _keep_value(key, obj, now):
    # Higher keep_value => better to keep. We evict the smallest.
    sz = _size_of(obj)
    seg = m_seg.get(key, 0)
    la = m_last_access.get(key, None)
    age = (now - la) if la is not None else (10 * _half_life())

    freq = _cms_est(key)
    hits = m_hits.get(key, 0)

    base = freq + HIT_BONUS * min(hits, HIT_BONUS_CAP)
    mult = PROTECT_MULT if seg == 1 else 1.0
    rboost = _recency_boost(age)

    keep = (base + _EPS) * mult * rboost
    keep *= _size_norm(sz)  # size-aware penalty
    return keep, seg, la if la is not None else -1, sz


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim with the smallest keep_value among a sampled set (or all if small).
    keep_value = (TinyLFU_est + HIT_BONUS * capped_hits)
                 * (1 + RECENCY_WEIGHT * recency(age; half-life))
                 * (PROTECT_MULT if protected)
                 / size^GAMMA
    Ties: prefer evicting probationary, then older LRU, then larger size, then key order.
    """
    cache = getattr(cache_snapshot, "cache", None) or {}
    if not cache:
        return None

    now = _now(cache_snapshot)

    n = len(cache)
    # Determine sampling strategy
    if n <= EVICT_SCAN_LIMIT:
        iterator = cache.items()
        stride = 1
        limit = n
    else:
        # Sample approximately EVICT_SCAN_LIMIT items by striding
        stride = max(1, n // EVICT_SCAN_LIMIT)
        iterator = cache.items()
        limit = EVICT_SCAN_LIMIT

    victim_key = None
    victim_keep = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    # Iterate with stride sampling if needed
    idx = 0
    taken = 0
    for k, v in iterator:
        if stride > 1:
            if (idx % stride) != 0:
                idx += 1
                continue
            idx += 1
        # evaluate
        keep, seg, la, sz = _keep_value(k, v, now)

        if victim_key is None:
            victim_key = k
            victim_keep = keep
            victim_seg = seg
            victim_la = la
            victim_sz = sz
        else:
            better = False
            if keep < victim_keep:
                better = True
            elif keep == victim_keep:
                # Prefer evicting probation
                if seg < victim_seg:
                    better = True
                elif seg == victim_seg:
                    # Older LRU
                    if la < victim_la:
                        better = True
                    elif la == victim_la:
                        # Free more space sooner
                        if sz > victim_sz:
                            better = True
                        elif sz == victim_sz and k < victim_key:
                            better = True
            if better:
                victim_key = k
                victim_keep = keep
                victim_seg = seg
                victim_la = la
                victim_sz = sz

        taken += 1
        if taken >= limit:
            break

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    - Increment TinyLFU frequency for this key.
    - Update last access.
    - Promote to protected segment.
    - Update global EWMA of inter-arrival using per-key gap (if available).
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Frequency estimator counts requests (hits contribute)
    _cms_inc(key, 1)

    # Update gap-based global mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1, now - last)
        _update_global_mu(gap)

    m_last_access[key] = now

    # Hits and segment promotion
    m_hits[key] = m_hits.get(key, 0) + 1
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1  # promote to protected


def update_after_insert(cache_snapshot, obj):
    """
    - Count the request in TinyLFU (admission counts requests too).
    - Initialize metadata and start in probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _cms_inc(key, 1)

    # Initialize metadata
    m_last_access[key] = now
    # On insert, hits = 0; stay probation
    m_hits[key] = 0
    m_seg[key] = 0


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    - Clean live metadata of the evicted key.
    - TinyLFU is global and decays automatically; no need for ghost history.
    """
    ekey = evicted_obj.key
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd_zqi1ev.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpota7oup4.pickle

Iteration 50: New subsample score 0.111304 is not better than old score 0.137909, skipping
Iteration 51: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwndwk4at.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyge5hw8j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp22g0xa3_.pickle

Iteration 51: Proposed new text for program: # W-TinyLFU with Size-aware SLRU (WTLFU-SLRU)
# - Single cache with three logical segments tracked by metadata:
#     0) WINDOW (recency buffer): small, pure LRU to absorb bursts.
#     1) PROBATION: main area for items that saw at least one hit or aged out of WINDOW.
#     2) PROTECTED: items with repeated hits; guarded from eviction.
# - Size-aware and frequency-aware victim selection:
#     * In WINDOW: evict LRU (ties: evict larger size first).
#     * In PROBATION: evict item with minimum value = (est_freq+1) / (size^alpha * age),
#         where age = now - last_access + 1 (smaller value => worse).
#     * PROTECTED never directly evicted; oversized PROTECTED demotes LRU to PROBATION.
# - TinyLFU (Count-Min Sketch) estimates long-term frequencies with periodic decay.
# - On miss: insert into WINDOW.
# - On hit: WINDOW -> PROBATION; PROBATION -> PROTECTED; PROTECTED stays, all refresh LRU.
# - Adaptive window sizing (AIMD): gently tunes window fraction based on where hits occur.

# --------------------
# Metadata and globals
# --------------------
SEG_WINDOW = 0
SEG_PROB   = 1
SEG_PROT   = 2

m_seg = dict()           # key -> segment id (0,1,2)
m_last_access = dict()   # key -> int, last access time
m_bytes = [0, 0, 0]      # bytes in [WINDOW, PROB, PROT]

# Sketch
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
SKETCH_DECAY_EVERY = 20000
m_sketch = None
m_sketch_ops = 0
m_inited = False

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 1.0           # size penalty exponent in [0.5, 1.0]; 1.0: linear bytes
EPS = 1e-12

# Adaptive window/protected fractions
m_win_frac = 0.08        # start with 8% of capacity as WINDOW
m_prot_frac = 0.70       # 70% of (capacity - window) as PROTECTED, rest PROBATION

ADAPT_EVERY = 20000      # accesses
ADAPT_STEP = 0.01        # +/- 1% per adjustment
WIN_MIN = 0.02           # 2%
WIN_MAX = 0.20           # 20%

# Hit distribution tracking for adaptation
m_accesses_since_adapt = 0
m_hits_in_window = 0
m_hits_in_nonwindow = 0

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _targets(capacity):
    # Compute desired byte targets for segments
    win_t = int(capacity * m_win_frac)
    win_t = max(0, min(win_t, capacity))
    main = max(0, capacity - win_t)
    prot_t = int(main * m_prot_frac)
    prot_t = max(0, min(prot_t, main))
    prob_t = max(0, capacity - win_t - prot_t)
    return win_t, prob_t, prot_t

def _seg_of(key):
    return m_seg.get(key, SEG_PROB)

def _size_of(obj):
    return getattr(obj, "size", 1)

def _move_segment(key, obj, new_seg):
    old_seg = m_seg.get(key, None)
    if old_seg is not None:
        if 0 <= old_seg <= 2:
            m_bytes[old_seg] -= _size_of(obj)
            if m_bytes[old_seg] < 0:
                m_bytes[old_seg] = 0
    m_seg[key] = new_seg
    m_bytes[new_seg] += _size_of(obj)

def _pick_lru_from_segment(cache_snapshot, seg_id):
    # Returns (key, last_access, size) with minimal last_access in seg_id
    weakest_key = None
    weakest_la = None
    weakest_size = None
    for k, v in cache_snapshot.cache.items():
        if _seg_of(k) != seg_id:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if weakest_key is None:
            weakest_key, weakest_la, weakest_size = k, la, sz
            continue
        if la < weakest_la:
            weakest_key, weakest_la, weakest_size = k, la, sz
        elif la == weakest_la:
            # Among equal recency, evict larger to free more space
            if sz > weakest_size:
                weakest_key, weakest_la, weakest_size = k, la, sz
            elif sz == weakest_size and k < weakest_key:
                weakest_key, weakest_la, weakest_size = k, la, sz
    return weakest_key, weakest_la, weakest_size

def _pick_worst_from_probation(cache_snapshot, now):
    # Evict minimal "value" = (est_freq + 1) / (size^alpha * age)
    worst_k = None
    worst_val = None
    worst_la = None
    worst_sz = None
    for k, v in cache_snapshot.cache.items():
        if _seg_of(k) != SEG_PROB:
            continue
        la = m_last_access.get(k, -1)
        age = max(1, now - la + 1)
        est = _sketch_estimate(k)
        denom = _denom(_size_of(v))
        value = (float(est) + 1.0) / (denom * float(age))
        # Smaller value => worse candidate (more stale per byte and low est freq)
        if worst_k is None or value < worst_val - EPS:
            worst_k, worst_val, worst_la, worst_sz = k, value, la, _size_of(v)
        elif worst_k is not None and abs(value - worst_val) <= EPS:
            # tie-break: older first, then larger
            if la < worst_la or (la == worst_la and (_size_of(v) > worst_sz or (_size_of(v) == worst_sz and k < worst_k))):
                worst_k, worst_val, worst_la, worst_sz = k, value, la, _size_of(v)
    return worst_k

def _rebalance_segments(cache_snapshot):
    # Demote from PROTECTED to PROBATION until within target
    win_t, prob_t, prot_t = _targets(cache_snapshot.capacity)
    # Demote protected overflow
    guard = 0
    while m_bytes[SEG_PROT] > prot_t and guard < 8:
        k, _, _ = _pick_lru_from_segment(cache_snapshot, SEG_PROT)
        if k is None:
            break
        obj = cache_snapshot.cache.get(k)
        if obj is None:
            break
        _move_segment(k, obj, SEG_PROB)
        guard += 1
    # No need to rebalance WINDOW; eviction decision will handle it.

def _adapt_window_fraction():
    # AIMD style: if more hits came from WINDOW, increase; else decrease
    global m_hits_in_window, m_hits_in_nonwindow, m_accesses_since_adapt, m_win_frac
    total_hits = m_hits_in_window + m_hits_in_nonwindow
    if total_hits > 0:
        if m_hits_in_window > m_hits_in_nonwindow:
            m_win_frac = min(WIN_MAX, m_win_frac + ADAPT_STEP)
        else:
            m_win_frac = max(WIN_MIN, m_win_frac - ADAPT_STEP)
    m_hits_in_window = 0
    m_hits_in_nonwindow = 0
    m_accesses_since_adapt = 0

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim key to evict.
    Strategy:
      1) Rebalance: demote PROTECTED overflow to PROBATION.
      2) If WINDOW exceeds target: evict LRU from WINDOW.
      3) Else: evict worst from PROBATION by value; fallback to WINDOW LRU, then PROTECTED LRU.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    _rebalance_segments(cache_snapshot)

    now = cache_snapshot.access_count
    win_t, prob_t, prot_t = _targets(cache_snapshot.capacity)

    # If WINDOW is oversized, trim it first (pure recency eviction)
    if m_bytes[SEG_WINDOW] > win_t:
        k, _, _ = _pick_lru_from_segment(cache_snapshot, SEG_WINDOW)
        if k is not None:
            return k

    # Otherwise, evict from PROBATION based on size- & recency-adjusted value
    victim = _pick_worst_from_probation(cache_snapshot, now)
    if victim is not None:
        return victim

    # Fallbacks
    k_w, _, _ = _pick_lru_from_segment(cache_snapshot, SEG_WINDOW)
    if k_w is not None:
        return k_w

    k_p, _, _ = _pick_lru_from_segment(cache_snapshot, SEG_PROT)
    return k_p


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Refresh last access time.
      - Promote across segments: WINDOW -> PROBATION, PROBATION -> PROTECTED.
      - Track hit distribution for adaptive window sizing.
      - Rebalance segments if PROTECTED overflowed.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    seg = _seg_of(key)
    m_last_access[key] = now

    if seg == SEG_WINDOW:
        _move_segment(key, obj, SEG_PROB)
        # hit originates from window
        global m_hits_in_window
        m_hits_in_window += 1
    elif seg == SEG_PROB:
        _move_segment(key, obj, SEG_PROT)
        global m_hits_in_nonwindow
        m_hits_in_nonwindow += 1
    else:
        # SEG_PROT
        m_hits_in_nonwindow += 1

    _rebalance_segments(cache_snapshot)

    # Adapt window fraction periodically
    global m_accesses_since_adapt
    m_accesses_since_adapt += 1
    if m_accesses_since_adapt >= ADAPT_EVERY:
        _adapt_window_fraction()


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch.
      - Place item into WINDOW and set last access.
      - Rebalance segments (in case PROTECTED is oversized).
      - Periodically adapt window fraction.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # First admission into WINDOW
    _move_segment(key, obj, SEG_WINDOW)

    _rebalance_segments(cache_snapshot)

    global m_accesses_since_adapt
    m_accesses_since_adapt += 1
    if m_accesses_since_adapt >= ADAPT_EVERY:
        _adapt_window_fraction()


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove metadata and subtract bytes in the segment.
    """
    ekey = evicted_obj.key
    seg = m_seg.pop(ekey, None)
    if seg is not None and 0 <= seg <= 2:
        m_bytes[seg] -= _size_of(evicted_obj)
        if m_bytes[seg] < 0:
            m_bytes[seg] = 0
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi3n58t1n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoma0s1so.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl8mlo7_9.pickle

Iteration 51: New subsample score 1.64123 is not better than old score 1.787004, skipping
Iteration 52: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpka17mmsj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfob8jibh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm43eclj0.pickle

Iteration 52: Proposed new text for program: # Improved Adaptive W-TinyLFU with Segmented LRU (AW-TLFU)
# Key changes vs previous version:
# 1) Corrected hit behavior in the Window (W): a hit in W no longer promotes to P. This matches W-TinyLFU/Caffeine
#    where W is a pure recency buffer; items graduate to P only when W overflows on insertion.
# 2) Admission decision is now applied for every capacity eviction: always compare new item score vs P-LRU score.
#    If new wins, evict P-LRU (admit); otherwise evict W-LRU (reject). This yields better filtering of one-hit wonders.
# 3) Protected (S) demotion uses a loop to keep S within its target (handles large objects).
# 4) Window overflow handling after insert uses a loop to restore W to its target (handles large objects).
# 5) Size-awareness smoothened by using an exponent (<1) instead of strict 1/size to reduce overly strong bias
#    against moderately large but frequent objects.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Adaptive window sizing (ARC-inspired, but by fraction of bytes)
m_W_frac = 0.20         # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80           # 80% of main in protected
ADMIT_EPS = 1e-12       # epsilon for float comparisons
EPS = 1e-12

# Size-score smoothing exponent for TinyLFU score. Use <1 to reduce over-penalizing large-but-hot items.
SCORE_SIZE_EXP = 0.85

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    # Compute target bytes for Window and Protected using adaptive window fraction
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)  # non-zero window
    main_target = max(0, cap - w_target)
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    Loop until within target to handle large objects.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    while m_bytes_S > s_target:
        victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
        if victim_key is None:
            return
        v = cache_snapshot.cache.get(victim_key)
        if v is None:
            return
        sz = _size_of(v)
        _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _score_for_key(cache_snapshot, key):
    """
    Size-aware frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate / (size ** SCORE_SIZE_EXP)
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    sz = float(_size_of(v))
    denom = max(1.0, pow(sz, SCORE_SIZE_EXP))
    return est / denom

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    denom = max(1.0, pow(sz, SCORE_SIZE_EXP))
    return est / denom

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision implementing W-TinyLFU admission consistently:
      - Always compare the new object's TinyLFU score against P-LRU's score (size-aware).
        * If new_score > p_score: evict from P (admit).
        * Else: evict from W (reject).
      - If either candidate doesn't exist, fall back gracefully:
        * If P is empty, evict from W.
        * If W is empty, evict from P.
        * If both empty (degenerate), evict from S as last resort, else global LRU.
      - This aligns with Caffeine's admission where the window buffers recency,
        while the probation segment is the choke point controlled by TinyLFU.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_score = _score_for_new(obj)

    # Identify LRU candidates in Window and Probation
    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)

    # If both W and P have candidates, perform admission decision
    if w_lru is not None and p_lru is not None:
        p_score = _score_for_key(cache_snapshot, p_lru)
        if new_score > p_score + ADMIT_EPS:
            return p_lru  # admit new: evict from P
        else:
            return w_lru  # reject new: evict from W

    # Fallbacks if one segment is empty
    if p_lru is not None:
        return p_lru
    if w_lru is not None:
        return w_lru

    # As a last resort, evict from Protected if nothing else is available
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - Behavior per segment:
        * W (Window): do NOT promote to P. W is a pure recency buffer; just refresh timestamp.
        * P (Probation): promote to S (Protected).
        * S (Protected): refresh recency only.
      - Keep S within target via demotion loop.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        # Pure recency refresh; do not promote to P here.
        return
    elif seg == SEG_PROBATION:
        # Promote to Protected on hit
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh only via last access
        return


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Always insert into Window (recency admission).
      - Update sketch and recency.
      - Adapt window target using ghost histories:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - If Window exceeds its target, move its LRU to P (loop until within target).
      - Keep S within its target via demotion loop.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window
    sz = _size_of(obj)
    global m_bytes_W
    m_seg[key] = SEG_WINDOW
    m_bytes_W += sz

    # If Window is above target, move its LRU(s) to Probation until within target
    w_target, _ = _targets(cache_snapshot)
    if m_bytes_W > w_target:
        # Exclude the just-inserted key to favor migrating older items first
        while m_bytes_W > w_target:
            candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
            if candidate is None:
                break
            v = cache_snapshot.cache.get(candidate)
            if v is None:
                break
            csz = _size_of(v)
            _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3c413b0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuz4gs0fu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpod4o_la9.pickle

Iteration 52: New subsample score 0.5662739999999999 is better than old score 0.44446. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp51om6a04.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp76t7lxib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptw61om14.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4c73mpzb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxejtjkbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmtcsg2cw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp952p7nz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw8ob5abk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9_75c418.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq_uamzne.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprye588dw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp61abxgki.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoh9_0wrs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp07i1l3kb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0q9_7js2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3va7o_v_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj6rc1x31.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6f0__tp3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprds_vlv5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl2zbi5q8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe5e4u4qh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy3ldfi0a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkb1h55ii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw49k5y4v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp80q09qbm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0q1b4uau.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplfai94fm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpydedbbto.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc8_t3g8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4ba2cfen.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppnp79uuj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjpg1fax2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5zunb0l8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2vpssh5a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3fbobqj5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptdpeog_z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbzwo0i_n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbatcxrgq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjjo96wvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyv81klim.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwmum53l1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoz11g3vx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk97n1484.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyxxn_kph.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxczdt2a_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn9jz2wky.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoq2j6vgm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa8d79gs4.pickle

Iteration 52: New program is on the linear pareto front
Iteration 52: Full valset score for new program: 0.2509817291666669
Iteration 52: Full train_val score for new program: 0.2509817291666669
Iteration 52: Individual valset scores for new program: [0.455284, 0.442155, 0.446155, 0.402152, 0.458469, 0.432895, 0.285885, 0.498624, 0.504469, 0.531017, 0.133333, 0.330373, 0.082694, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.272227, 0.418879, 0.024694, 0.059961, 0.059961, 0.295962, 0.294355, 0.781529, 0.867192, 0.178197, 0.036364, 0.066059, 0.044028, 0.044028, 0.766678, 0.096491, 0.137688, 0.181922, 0.604843, 0.118081, 0.118658, 0.110568, 0.143056, 0.075658, 0.2, 0.188077, 0.17195, 0.466258, 0.104575]
Iteration 52: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.325915, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.098684, 0.159753, 0.218379, 0.643483, 0.125461, 0.157749, 0.122175, 0.178125, 0.095395, 0.366667, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 52: Full valset pareto front score: 0.27241185416666674
Iteration 52: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {23, 14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23}, {17, 23, 6, 14}, {17, 23, 6, 14}, {17, 14, 23}, {17, 23, 6, 14}, {17, 23, 6, 14}, {11}, {17, 14}, {16}, {17, 14}, {17, 14}, {2}, {3}, {8, 7}, {3}, {17, 14, 23}, {18}, {17, 6, 14}, {14}, {14}, {23}, {17, 6, 14}, {17, 14}, {14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22}, {14}, {17}, {14}, {6}, {3, 5, 6, 7, 8, 17}, {14, 23}, {14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23}, {8}]
Iteration 52: Best valset aggregate score so far: 0.2509817291666669
Iteration 52: Best program as per aggregate score on train_val: 23
Iteration 52: Best program as per aggregate score on valset: 23
Iteration 52: Best score on valset: 0.2509817291666669
Iteration 52: Best score on train_val: 0.2509817291666669
Iteration 52: Linear pareto front program index: 23
Iteration 52: New program candidate index: 23
Iteration 53: Selected program 3 score: 0.23035914583333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7ylu0jby.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0f_9wl2w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbt8zueuy.pickle

Iteration 53: Proposed new text for program: # Segmented GreedyDual + Window-TinyLFU (SGD-W-TLFU)
# - Two segments:
#   * Window (W): small recency window (LRU by last_access), scan-resistant via size cap and ghost-adaptation.
#   * Main (M): size-aware GreedyDual-Size prioritized by H = L + freq/size^alpha, with TinyLFU-based initialization.
# - New items enter W; first hit promotes to M. Very hot items may fast-track to M at insert using TinyLFU estimate.
# - Eviction:
#   * Prefer evicting from W (LRU) when W is over its adaptive byte target or when M has no items.
#   * Otherwise evict the minimum-priority item from M (GDSF).
# - Global clock L advances only on M evictions (GreedyDual invariant).
# - TinyLFU sketch provides:
#   * Admission dampening (threshold) to curb one-hit pollution.
#   * Fast-track promotion to M for items with high estimated frequency.
# - ARC-like ghost histories (for W and M) adapt W's byte fraction: hits on a ghost of W increase W fraction; hits on a
#   ghost of M decrease it. This balances recency vs frequency per workload.

from collections import OrderedDict

# --------------------
# Metadata and globals
# --------------------
m_priority = dict()     # key -> float, eviction priority H (used for Main segment)
m_freq = dict()         # key -> small int, in-cache frequency (hits since (re)admission/promotion)
m_last_access = dict()  # key -> int, last access timestamp
m_seg = dict()          # key -> 'W' (window) or 'M' (main)
m_L = 0.0               # global clock for GreedyDual (applies to Main segment only)
m_inited = False        # sketch initialization guard

# Window tracking (bytes)
m_wbytes = 0            # total bytes currently in Window segment
m_wfraction = 0.10      # adaptive window fraction (bytes / capacity)

# Ghost histories for adaptation (ARC-like)
m_ghostW = OrderedDict()  # keys recently evicted from Window
m_ghostM = OrderedDict()  # keys recently evicted from Main

# TinyLFU sketch (Count-Min Sketch)
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
SIZE_EXP = 0.9               # size penalty exponent in [0.5, 1.0]; 0.9 is often a good tradeoff
NEW_ADMIT_THRESHOLD = 2      # TinyLFU doorkeeper threshold; reduces one-hit pollution
NEW_ITEM_BIAS = 0.0          # extra additive bias at admission (usually 0)
FREQ_CAP = 255               # cap on per-key in-cache frequency
FAST_TRACK_FREQ = 3          # if TinyLFU est - threshold >= this, insert directly into Main
W_MIN_FRAC = 0.05            # min Window fraction of capacity (bytes)
W_MAX_FRAC = 0.30            # max Window fraction of capacity (bytes)
W_ADAPT_STEP = 0.01          # step for adaptive window fraction
GHOST_CAP_KEYS = 65536       # cap on ghost sets (each)

EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(cache_snapshot):
    global m_inited, m_sketch
    if m_inited:
        return
    m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
    m_inited = True

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _denom(size):
    sz = max(1, int(getattr(size, "size", size) if hasattr(size, "size") else size))
    if SIZE_EXP == 1.0:
        return float(sz)
    return pow(float(sz), float(SIZE_EXP))

def _current_priority(key, obj):
    return m_priority.get(key, m_L)

def _ghost_add(ghost, key):
    ghost[key] = True
    ghost.move_to_end(key, last=True)
    # trim oldest
    while len(ghost) > GHOST_CAP_KEYS:
        ghost.popitem(last=False)

def _adjust_window_fraction_on_miss(key):
    # ARC-style adaptation using ghost hits on miss
    global m_wfraction
    if key in m_ghostW:
        m_wfraction = min(W_MAX_FRAC, m_wfraction + W_ADAPT_STEP)
    elif key in m_ghostM:
        m_wfraction = max(W_MIN_FRAC, m_wfraction - W_ADAPT_STEP)
    # Touch ghosts to keep them recent
    if key in m_ghostW:
        _ghost_add(m_ghostW, key)
    if key in m_ghostM:
        _ghost_add(m_ghostM, key)

def _promote_to_main(key, obj, now):
    # Move a Window item into Main with proper initialization
    global m_wbytes
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)
    est = _sketch_estimate(key)
    inwin_f = m_freq.get(key, 0)
    init_f = max(1, max(inwin_f, int(est) - int(NEW_ADMIT_THRESHOLD)))
    init_f = min(init_f, FREQ_CAP)
    m_freq[key] = init_f
    m_priority[key] = float(m_L) + float(init_f) / denom
    m_last_access[key] = now
    if m_seg.get(key) != 'M':
        # adjust window bytes if coming from Window
        if m_seg.get(key) == 'W':
            m_wbytes = max(0, m_wbytes - sz)
        m_seg[key] = 'M'

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Select a victim:
      - Prefer Window LRU if Window bytes exceed target or Main is empty.
      - Otherwise choose min-priority item from Main (GreedyDual).
      - Deterministic tie-breakers:
            LRU among equals -> larger size -> lexicographic key.
    """
    if not cache_snapshot.cache:
        return None

    cap = max(1, int(cache_snapshot.capacity))
    wtarget = int(cap * m_wfraction)

    # Segment presence
    has_main = any(m_seg.get(k) == 'M' for k in cache_snapshot.cache.keys())
    has_win = any(m_seg.get(k) == 'W' for k in cache_snapshot.cache.keys())

    # Decide segment to evict from
    evict_from_window = False
    if has_win and (m_wbytes > wtarget or not has_main):
        evict_from_window = True

    if evict_from_window:
        # LRU in Window
        victim_key = None
        victim_la = None
        victim_size = None
        for k, v in cache_snapshot.cache.items():
            if m_seg.get(k) != 'W':
                continue
            la = m_last_access.get(k, -1)
            sz = getattr(v, "size", 1)
            if victim_key is None:
                victim_key, victim_la, victim_size = k, la, sz
                continue
            if la < victim_la:
                victim_key, victim_la, victim_size = k, la, sz
            elif la == victim_la:
                if sz > victim_size:
                    victim_key, victim_la, victim_size = k, la, sz
                elif sz == victim_size and k < victim_key:
                    victim_key, victim_la, victim_size = k, la, sz
        if victim_key is not None:
            return victim_key
        # fallback to Main if Window empty (shouldn't happen due to has_win check)
        # but keep robust
    # Evict from Main by minimal priority
    weakest_key = None
    weakest_pri = None
    weakest_la = None
    weakest_size = None
    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k) != 'M':
            continue
        pri = _current_priority(k, v)
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)
        if weakest_key is None:
            weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
            continue
        if pri < weakest_pri - EPS:
            weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
            continue
        if abs(pri - weakest_pri) <= EPS:
            if la < weakest_la:
                weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
                continue
            if la == weakest_la:
                if sz > weakest_size:
                    weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
                    continue
                if sz == weakest_size and k < weakest_key:
                    weakest_key, weakest_pri, weakest_la, weakest_size = k, pri, la, sz
    # If no Main items were found, fall back to Window LRU or any
    if weakest_key is not None:
        return weakest_key

    # Final fallback: LRU across all (degenerate case)
    victim_key = None
    victim_la = None
    victim_size = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = getattr(v, "size", 1)
        if victim_key is None or la < victim_la or (la == victim_la and (sz > victim_size or (sz == victim_size and k < victim_key))):
            victim_key, victim_la, victim_size = k, la, sz
    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update TinyLFU sketch.
      - Update last access timestamp.
      - If in Window: increment a small local freq and promote to Main on first hit.
      - If in Main: increment freq (capped) and rejuvenate priority: H = L + freq / size^alpha.
    """
    global m_L

    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    seg = m_seg.get(key, 'W')  # default to W if missing

    if seg == 'W':
        # Window LRU maintenance and promotion on first hit
        f = m_freq.get(key, 0)
        if f < FREQ_CAP:
            f += 1
        m_freq[key] = f
        m_last_access[key] = now
        # Promote on first hit to benefit from frequency in Main
        if f >= 1:
            _promote_to_main(key, obj, now)
    else:
        # Main: TinyLFU-initialized GreedyDual
        f = m_freq.get(key, 0)
        if f < FREQ_CAP:
            f += 1
        m_freq[key] = f
        m_priority[key] = float(m_L) + float(f) / denom
        m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update TinyLFU sketch and adapt Window fraction based on ghost histories.
      - Decide placement:
          * Fast-track to Main if TinyLFU estimate indicates high reuse.
          * Otherwise place in Window (LRU), counted towards window bytes.
      - Initialize metadata accordingly.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    sz = getattr(obj, "size", 1)
    denom = _denom(sz)

    # Adapt window fraction using ghost histories (ARC-like)
    _adjust_window_fraction_on_miss(key)

    est = _sketch_estimate(key)
    raw_f = max(0, int(est) - int(NEW_ADMIT_THRESHOLD))

    # Fast-track to Main if sufficiently hot per TinyLFU
    if raw_f >= FAST_TRACK_FREQ:
        init_f = min(FREQ_CAP, max(1, raw_f + int(NEW_ITEM_BIAS)))
        m_freq[key] = init_f
        m_priority[key] = float(m_L) + float(init_f) / denom
        m_last_access[key] = now
        m_seg[key] = 'M'
        # no window byte impact
    else:
        # Enter Window
        m_seg[key] = 'W'
        m_last_access[key] = now
        m_freq[key] = 0
        # Priority is irrelevant in Window, but set a minimal baseline for ties/fallbacks
        m_priority[key] = float(m_L)  # baseline
        # Count bytes in Window
        global m_wbytes
        m_wbytes += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - If evicted from Main: advance L to victim's priority, then clean metadata.
      - If evicted from Window: do NOT advance L; adjust Window bytes.
      - Record key in appropriate ghost history for adaptation.
      - Clean all metadata for the evicted key.
    """
    global m_L, m_wbytes

    ekey = evicted_obj.key
    eseg = m_seg.get(ekey, None)
    esize = getattr(evicted_obj, "size", 1)

    # Advance L only for Main evictions (GreedyDual invariant)
    if eseg == 'M':
        ev_pri = m_priority.get(ekey, m_L)
        if ev_pri > m_L:
            m_L = float(ev_pri)
        _ghost_add(m_ghostM, ekey)
    else:
        # Window eviction: update window bytes and ghostW
        m_wbytes = max(0, m_wbytes - esize)
        _ghost_add(m_ghostW, ekey)

    # Clean metadata
    m_priority.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkocfsoqy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe4xhodt5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps399wobd.pickle

Iteration 53: New subsample score 1.456924 is not better than old score 1.4667800000000002, skipping
Iteration 54: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpet3fpd7_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc_ym2xzp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmd4atjdo.pickle

Iteration 54: Proposed new text for program: # Adaptive ARC-LRFU with Size Awareness (A2L-S)
# Core ideas:
# - Two segments (ARC-style): T1 = probation (recent), T2 = protected (frequent).
#   A target protected fraction adapts online using ghost feedback (ARC heuristic).
# - Within the chosen segment to evict from, select the victim via a size-aware decayed
#   reuse score (LRFU-like) with an adaptive half-life and a lightweight EWMA reuse predictor.
# - Ghost history captures last-seen segment, score, and reuse timing to warm-start on reinsert.
# - Gentle size-awareness: slightly favors small objects but avoids over-penalizing larger ones.
# - Conservative admission for cold items: start with near-zero retention unless ghost indicates value.

import math

# ----------------------
# Global metadata stores
# ----------------------
# LRFU decayed score (value to keep)
m_score = dict()       # key -> float
m_score_ts = dict()    # key -> int (last time score was updated)

# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()         # key -> int

# Predictive reuse timing (EWMA of inter-arrival)
m_last_access = dict() # key -> int
m_mu = dict()          # key -> float (EWMA of inter-arrival)
m_freq = dict()        # key -> int (hit count)

# Global EWMA mean inter-arrival (for adaptive half-life)
m_global_mu = 64.0

# ARC-style target for protected segment share (fraction of live keys)
m_protect_target_frac = 0.50

# Ghost history (bounded)
g_mu = dict()          # key -> float
g_last = dict()        # key -> int
g_freq = dict()        # key -> int
g_score = dict()       # key -> float
g_seg = dict()         # key -> int (segment at eviction: 0=T1, 1=T2)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Decay half-life (in access-count time units); adapted from m_global_mu
HL_BASE = 256.0
HL_MIN = 16.0
HL_MAX = 16384.0
HL_ADAPT_K = 0.6

# Size-awareness
ALPHA_SIZE_CREDIT = 0.40   # credit per hit = 1 / size^alpha (small items get slightly more credit)
GAMMA_SIZE_EVICT = 0.50    # retain score scaled by 1 / size^gamma when comparing victims

# Segment selection bias (ARC-like)
NONPREF_BIAS = 4.0         # inflate retention for the non-preferred segment when picking a victim
ARC_ADAPT_STEP = 0.08      # how fast the protected target fraction adapts
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# Urgency boost (predicted near-term reuse)
URGENCY_WEIGHT = 1.2       # lower than before to reduce noisy oscillations

# EWMA for reuse predictor
EWMA_BETA = 0.25
GLOBAL_BETA = 0.02

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 3.0

# Numerics
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict oldest ghost entries by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_mu) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)
        g_score.pop(k, None)
        g_seg.pop(k, None)

def _half_life():
    # Adapt the half-life to the global mean gap: longer gaps => longer memory
    hl = HL_BASE + HL_ADAPT_K * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    hl = _half_life()
    return 0.5 ** (float(delta) / float(hl))

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _predicted_delta(key, now):
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    return max(0.0, float(next_t - now))

def _urgency_boost(key, now):
    # Boost retention if predicted next access is soon relative to expectation
    pd = _predicted_delta(key, now)
    if pd >= _INF / 2:
        return 1.0
    mu = m_mu.get(key, None)
    norm = mu if (mu is not None and mu > 0.0) else _half_life()
    frac = 1.0 / (1.0 + (pd / max(1.0, float(norm))))
    return 1.0 + URGENCY_WEIGHT * frac

def _current_decayed_score(key, now):
    base = float(m_score.get(key, 0.0))
    ts = m_score_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _record_ghost_on_evict(evicted_key, now):
    # Always record minimal ghost info
    mu = m_mu.get(evicted_key, None)
    if mu is not None:
        g_mu[evicted_key] = float(mu)
    else:
        g_mu.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 0)
    g_score[evicted_key] = float(_current_decayed_score(evicted_key, now))
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))

def _seed_predictor_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert, with warm-start from ghost if available.
    ARC-style adaptation: adjust protected target based on which ghost list was hit.
    """
    global m_protect_target_frac

    # ARC adaptation using ghost feedback about where the key was last evicted from
    if key in g_seg:
        if g_seg[key] == 0:
            # Ghost hit from probation (B1): favor recency -> shrink protected target
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            # Ghost hit from protected (B2): favor frequency -> grow protected target
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    # Reuse-time predictor (EWMA)
    if key in g_mu:
        m_mu[key] = max(1.0, float(g_mu[key]))
        m_freq[key] = max(0, int(g_freq.get(key, 0)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0

    m_last_access[key] = now

    # Decayed score: conservative admission if cold; warm from ghost if available
    if key in g_score:
        # Warm but conservative (avoid pollution on large reinsert bursts)
        m_score[key] = 0.60 * float(g_score[key])
    else:
        m_score[key] = 0.0
    m_score_ts[key] = now

    # Segment placement: ARC places ghost hits directly into protected
    if START_PROTECTED_FROM_GHOST and key in g_seg:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

def _update_predictors_on_hit(key, now, obj):
    """
    Update reuse-time predictor (per-key + global) and decayed score with quickness bonus.
    """
    # Reuse-time predictor EWMA
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Decayed score update (lazy decay + size-aware credit + quickness bonus)
    cur = _current_decayed_score(key, now)
    credit = _size_credit(obj)

    # Quickness bonus: earlier-than-expected hits are more valuable
    mu = m_mu.get(key, None)
    if mu is not None and last is not None:
        gap = max(1.0, float(now - last))
        earliness = max(0.0, (float(mu) - gap)) / max(float(mu), 1.0)
        # Up to +50% extra credit if hit arrives much earlier than predicted
        credit *= (1.0 + 0.5 * earliness)

    m_score[key] = cur + credit
    m_score_ts[key] = now

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using ARC-style segment preference + LRFU scoring.

    - Compute preferred segment to evict from: whichever (T1 or T2) exceeds its adaptive target.
    - Within that segment, select the key with smallest retention:
        retention = decayed_score(now) / size^GAMMA
                    * urgency_boost(key)
      Lower retention => worse to keep => evict.
    - Apply a bias (NONPREF_BIAS) so that non-preferred segment objects are less likely candidates.
    - Tie-breakers:
        * Prefer evicting from preferred segment
        * Older last access (LRU)
        * Larger size (to free more space quickly)
        * Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Determine current segment composition
    total = len(cache)
    prot_count = 0
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            prot_count += 1
    target_prot = int(round(float(m_protect_target_frac) * float(total)))

    # Evict from whichever segment is above its target
    preferred_seg = 1 if prot_count > target_prot else 0

    victim_key = None
    victim_val = None
    victim_score = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        s_now = _current_decayed_score(k, now)
        sz = _size_of(v)

        # Base "keep" retention
        retention = (s_now + _EPS) / (float(sz) ** GAMMA_SIZE_EVICT)
        retention *= _urgency_boost(k, now)

        seg = m_seg.get(k, 0)

        # Bias away from the non-preferred segment
        if seg != preferred_seg:
            retention *= NONPREF_BIAS

        la = m_last_access.get(k, -1)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_score:
            better = True
        elif retention == victim_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update per-key/global EWMA reuse predictor.
      - Update decayed score with size-aware credit and quickness bonus.
      - Promote to protected segment if currently probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (cold hit)
    if key not in m_score_ts or key not in m_mu or key not in m_last_access:
        _seed_predictor_on_insert(key, now, obj)

    _update_predictors_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style adaptation using ghost feedback to adjust protected target fraction.
      - Initialize per-key predictor and decayed score (warm-start from ghost when available).
      - Place ghost hits directly into protected; cold items start in probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, score, last segment).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_score.pop(ekey, None)
    m_score_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph1cr2fx4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp56936xtg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptwbaumde.pickle

Iteration 54: New subsample score 0.9758049999999999 is better than old score 0.904575. Continue to full eval and add to candidate pool.
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_ccbkj8k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7wr5w5j8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_56m_8_v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmid9s2b2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppn6a_n5g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_afivuix.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps9a9_x4h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp89oaxbu4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa1pvi513.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpywiqcrqp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdaydl49l.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8uy5t6rz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpasjild1w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1o7sfo1h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph0i0bsbd.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfn20x_h3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8yfy5unk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpncegtv8g.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgat0qa0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps2llkuma.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp803lk_f7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp317uxus8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqb35hopm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2xuqmn4e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvkr5nf20.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqxw87unj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgqnugk_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwhjwpu9a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxmjw0uv2.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2qwhrkpa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_klt5qmg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnkhi6uqa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpumhp_3qo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1npu_7fo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbptmp2a2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfvz3t9dh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbi1v32w2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyk_7vvi_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp57gi5p__.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpri1mi2wg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzk_q0oxx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6it5y79c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpev89j_l5.pickle

Iteration 54: Full valset score for new program: 0.2291873541666667
Iteration 54: Full train_val score for new program: 0.2291873541666667
Iteration 54: Individual valset scores for new program: [0.0, 0.426804, 0.434095, 0.393883, 0.438466, 0.434567, 0.253589, 0.374951, 0.509117, 0.531017, 0.133333, 0.300622, 0.0, 0.0, 0.022653, 0.022823, 0.021408, 0.0, 0.024047, 0.265811, 0.428712, 0.0, 0.059961, 0.059961, 0.33212, 0.260081, 0.749752, 0.888242, 0.178197, 0.038636, 0.072893, 0.087816, 0.087816, 0.0, 0.111842, 0.163283, 0.218379, 0.625966, 0.125461, 0.157472, 0.144166, 0.178125, 0.101974, 0.35, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 54: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.163283, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 54: Full valset pareto front score: 0.2735548750000001
Iteration 54: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {24, 23, 14, 7}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {24, 17, 14}, {16}, {17, 14}, {17, 14}, {24}, {3}, {8, 7}, {3}, {24, 17, 14, 23}, {18}, {24, 17, 6, 14}, {24, 14}, {24, 14}, {23}, {24}, {24}, {24, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24}, {14}, {24}, {24, 14}, {24}, {3, 5, 6, 7, 8, 17}, {24, 14, 23}, {24, 14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24}, {24}]
Iteration 54: Best valset aggregate score so far: 0.2509817291666669
Iteration 54: Best program as per aggregate score on train_val: 23
Iteration 54: Best program as per aggregate score on valset: 23
Iteration 54: Best score on valset: 0.2509817291666669
Iteration 54: Best score on train_val: 0.2509817291666669
Iteration 54: Linear pareto front program index: 23
Iteration 54: New program candidate index: 24
Iteration 55: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1xa7crtd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0d9dzcc2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1ia2m40d.pickle

Iteration 55: Proposed new text for program: # Improved policy: SLRU with TinyLFU-style frequency sketch and size-aware retention (SLRU-TLFU-SA)
# Key ideas and changes vs. previous version:
# - Replace complex per-key decayed scores and noisy "urgency" predictor with a simple, robust
#   Count-Min Sketch (CMS) to estimate global popularity (TinyLFU-style), aged over time.
# - Use 3 segments (window -> probation -> protected), like SLRU: insert to window; on first hit
#   promote to probation; on second hit promote to protected. Eviction prefers less-proven segments.
# - Balance segment sizes dynamically by nudging eviction pressure toward segments that exceed
#   target fractions, which reduces scan pollution and keeps hot items protected.
# - Make eviction size-aware by normalizing popularity by size^gamma; large items need more hits.
# - Use simple recency decay via last-access timestamps to avoid hoarding stale but once-popular items.
# - Maintain a bounded ghost of evicted keys' last segments to warm-start reinsert placement.
#
# Entry points:
#   - evict(cache_snapshot, obj)
#   - update_after_hit(cache_snapshot, obj)
#   - update_after_insert(cache_snapshot, obj)
#   - update_after_evict(cache_snapshot, obj, evicted_obj)

import math

# ----------------------
# Global metadata
# ----------------------
# Segments: 0 = window (new), 1 = probation (1+ hit), 2 = protected (2+ hits)
m_seg = dict()           # key -> int
m_last_access = dict()   # key -> int
m_hits = dict()          # key -> int (hit count while resident)

# Ghost history for warm start on reinserts
g_seg = dict()           # key -> last segment when evicted (0/1/2)
g_last = dict()          # key -> int (last seen time)

# Global recency baseline (EWMA of global inter-access gap)
m_global_gap = 64.0
m_global_last = None

# ----------------------
# Count-Min Sketch (TinyLFU-style) for frequency estimation
# ----------------------
CMS_DEPTH = 4
CMS_WIDTH = 8192
CMS_DECAY_INTERVAL = 200000   # in access events
CMS_MAX_COUNT = (1 << 31) - 1

cms = [[0] * CMS_WIDTH for _ in range(CMS_DEPTH)]
cms_seeds = [0x9e3779b97f4a7c15, 0x94d049bb133111eb, 0xbf58476d1ce4e5b9, 0x2545f4914f6cdd1d]
cms_last_decay_at = 0

# ----------------------
# Tunables
# ----------------------
# Segment target fractions (by object count)
WINDOW_FRAC = 0.20
PROTECTED_FRAC = 0.50
BALANCE_ETA = 0.75  # strength of rebalancing pressure

# Size awareness
GAMMA_SIZE_EVICT = 0.70

# Segment base multipliers (higher => more retention)
SEG_MULT = {
    0: 0.65,   # window: easiest to evict
    1: 1.00,   # probation
    2: 1.60,   # protected: keep longer
}

# Recency baseline multiplier relative to global gap
RECENCY_BASE_MULT = 3.0

# EWMA for global gap
GLOBAL_BETA = 0.02

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    # Allow ghost to be at most 4x live items, but not less than 1024
    return max(1024, int(4.0 * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_seg) <= limit:
        return
    # Evict oldest ghost entries by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_seg) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_seg.pop(k, None)
        g_last.pop(k, None)

def _hash_index(key, seed):
    # Use Python's hash with seed mixing; modulus to CMS width
    return (hash((key, seed)) & 0x7FFFFFFF) % CMS_WIDTH

def _cms_estimate(key):
    # Estimate frequency as the minimum across depth
    return min(cms[i][_hash_index(key, cms_seeds[i])] for i in range(CMS_DEPTH))

def _cms_increment(key, now):
    # Conservative update: increment only counters equal to current min to reduce overestimation
    idxs = [_hash_index(key, cms_seeds[i]) for i in range(CMS_DEPTH)]
    vals = [cms[i][idxs[i]] for i in range(CMS_DEPTH)]
    minv = min(vals)
    for i in range(CMS_DEPTH):
        if cms[i][idxs[i]] == minv:
            if cms[i][idxs[i]] < CMS_MAX_COUNT:
                cms[i][idxs[i]] += 1
    _cms_maybe_decay(now)

def _cms_maybe_decay(now):
    global cms_last_decay_at
    if now - cms_last_decay_at >= CMS_DECAY_INTERVAL:
        # Halve all counters to age out old frequency
        for i in range(CMS_DEPTH):
            row = cms[i]
            for j in range(CMS_WIDTH):
                row[j] >>= 1
        cms_last_decay_at = now

def _update_global_gap(now):
    global m_global_gap, m_global_last
    if m_global_last is not None:
        gap = max(1.0, float(now - m_global_last))
        prev = float(m_global_gap)
        m_global_gap = (1.0 - GLOBAL_BETA) * prev + GLOBAL_BETA * gap
    m_global_last = now

def _segment_targets(n):
    if n <= 0:
        return (0, 0, 0)
    w_t = max(1, int(WINDOW_FRAC * n))
    p_t = max(0, int(PROTECTED_FRAC * n))
    if w_t + p_t > n:
        # Adjust down if rounding pushed sum over n
        overflow = w_t + p_t - n
        if p_t >= overflow:
            p_t -= overflow
        else:
            w_t = max(1, w_t - (overflow - p_t))
            p_t = 0
    prob_t = max(0, n - (w_t + p_t))
    return (w_t, prob_t, p_t)

def _segment_balance_factor(seg, count, target):
    # If segment is over target, reduce retention (<1). If under, increase (>1).
    c = max(1.0, float(count))
    t = max(1.0, float(target))
    return (t / c) ** BALANCE_ETA

def _recency_factor(age, recency_base):
    # Simple smooth recency decay; recent accesses yield factor near 1, old ones approach 0.
    return 1.0 / (1.0 + (float(age) / max(1.0, recency_base)))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the object with the smallest retention value:
      retention = (freq_estimate + 1) / size^GAMMA
                  * recency_factor(age)
                  * SEG_MULT[segment]
                  * segment_balance_factor(segment)
    Tie-breakers (in order):
      - Prefer lower segment (window < probation < protected)
      - Older last access (LRU)
      - Larger size
      - Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    n = len(cache)
    # Count segment membership
    cnt = [0, 0, 0]
    for k in cache.keys():
        seg = m_seg.get(k, 1)
        if seg < 0 or seg > 2:
            seg = 1
        cnt[seg] += 1
    target_w, target_prob, target_prot = _segment_targets(n)
    target = [target_w, target_prob, target_prot]
    bal_factor = [
        _segment_balance_factor(0, cnt[0], target[0]) if n > 0 else 1.0,
        _segment_balance_factor(1, cnt[1], target[1]) if n > 0 else 1.0,
        _segment_balance_factor(2, cnt[2], target[2]) if n > 0 else 1.0,
    ]
    rec_base = max(8.0, RECENCY_BASE_MULT * float(m_global_gap))

    victim_key = None
    victim_val = None
    victim_ret = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        seg = m_seg.get(k, 1)
        if seg < 0 or seg > 2:
            seg = 1
        sz = _size_of(v)
        freq = _cms_estimate(k)
        age = now - m_last_access.get(k, now)

        base = (float(freq) + 1.0) / (float(sz) ** GAMMA_SIZE_EVICT)
        rfac = _recency_factor(age, rec_base)
        retention = base * rfac * SEG_MULT.get(seg, 1.0) * bal_factor[seg]

        la = m_last_access.get(k, -1)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_ret = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_ret - _EPS:
            better = True
        elif abs(retention - victim_ret) <= _EPS:
            # tie-breakers
            if seg < victim_seg:
                better = True
            elif seg == victim_seg:
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_ret = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update global gap EWMA and CMS (frequency).
      - Update last access and hit count.
      - Promote through segments: window -> probation -> protected.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _update_global_gap(now)
    _cms_increment(key, now)

    # Initialize defaults if missing (defensive)
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_seg:
        # If unknown, assume probation to avoid over-protection
        m_seg[key] = 1
    m_hits[key] = m_hits.get(key, 0) + 1
    m_last_access[key] = now

    seg = m_seg.get(key, 1)
    if seg == 0:
        # window -> probation on first hit
        m_seg[key] = 1
    elif seg == 1:
        # probation -> protected on second hit
        m_seg[key] = 2
    # if already protected, stay protected


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update global gap EWMA and CMS (this access).
      - Initialize last_access, hits, and segment.
      - Use ghost history and current frequency estimate to decide initial segment:
        * If previously seen (ghost) or freq >= 3: start in probation (1) to avoid re-learning.
        * Else start in window (0).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _update_global_gap(now)
    _cms_increment(key, now)

    m_last_access[key] = now
    m_hits[key] = 0

    freq = _cms_estimate(key)
    glvl = g_seg.get(key, None)

    if glvl is not None or freq >= 3:
        # Warm-start: it has some prior signal, skip the transient window
        # Be conservative: start probation; promotion to protected requires a hit
        m_seg[key] = 1
    else:
        m_seg[key] = 0


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (last segment, last seen time) for warm-starts.
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Record ghost
    g_seg[ekey] = m_seg.get(ekey, 0)
    g_last[ekey] = now
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfi32sck3.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi8lbne0f.pickle

Iteration 55: New subsample score 0.125689 is not better than old score 0.29468099999999997, skipping
Iteration 56: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2eahd68t.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbo3j8l6d.pickle

Iteration 56: Proposed new text for program: # S3-LRFU: Segmented, Sampled, Size-aware LRFU with Ghost Adaptation
# Goals:
# - Strong recency/frequency balance using two segments (probation/protected).
# - Size-aware retention: value density per byte guides eviction.
# - Adaptive target protected share via ghost feedback (ARC heuristic).
# - Low overhead victim selection using segment-aware random sampling (fixes timeouts).
# - Robust reuse-time predictor (EWMA) + lazy LRFU decayed scoring.
# - Bounded, fast ghost history using a min-heap (no O(n log n) sorting on trim).

import math
import random
import heapq

# ----------------------
# Global metadata stores
# ----------------------
# LRFU decayed score
m_score = dict()       # key -> float
m_score_ts = dict()    # key -> int (last time score was updated)

# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()         # key -> int
m_seg_keys = {0: set(), 1: set()}  # live keys per segment (for fast sampling)

# Predictive reuse timing (EWMA of inter-arrival)
m_last_access = dict() # key -> int
m_mu = dict()          # key -> float
m_freq = dict()        # key -> int (hit count per residency)

# Global EWMA mean inter-arrival (for adaptive half-life)
m_global_mu = 64.0

# ARC-style target for protected segment share (fraction of live keys)
m_protect_target_frac = 0.50

# Ghost history (bounded)
g_mu = dict()          # key -> float
g_last = dict()        # key -> int
g_freq = dict()        # key -> int
g_score = dict()       # key -> float
g_seg = dict()         # key -> int (segment at eviction: 0=T1, 1=T2)

# Fast trim support for ghost: min-heap of (last_time, seq, key)
g_heap = []
_g_seq = 0

# ----------------------
# Tunable hyperparameters
# ----------------------
# Decay half-life (in access-count time units); adapted from m_global_mu
HL_BASE = 192.0
HL_MIN = 12.0
HL_MAX = 16384.0
HL_ADAPT_K = 0.7

# Size-awareness
ALPHA_SIZE_CREDIT = 0.45   # hit credit scales by 1 / size^alpha
GAMMA_SIZE_EVICT = 0.60    # retention density ~ score / size^gamma

# Segment selection bias (ARC-like)
NONPREF_BIAS = 3.0         # bias against evicting from non-preferred segment
ARC_ADAPT_STEP = 0.06      # adapt speed of protected share target
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# Urgency boost (predicted near-term reuse)
URGENCY_WEIGHT = 0.8       # balanced; avoids noisy spikes

# EWMA for reuse predictor
EWMA_BETA = 0.25
GLOBAL_BETA = 0.02

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 3.0

# Sampling
SAMPLE_SMALL = 16
SAMPLE_MED = 32
SAMPLE_LARGE = 48
SAMPLE_PREF_FRAC = 0.75     # share of candidates drawn from preferred segment
FULL_SCAN_THRESHOLD = 64    # scan all if cache size <= this

# Numerics
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _half_life():
    hl = HL_BASE + HL_ADAPT_K * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    hl = _half_life()
    return 0.5 ** (float(delta) / float(hl))

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _predicted_delta(key, now):
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    return max(0.0, float(next_t - now))

def _urgency_boost(key, now):
    pd = _predicted_delta(key, now)
    if pd >= _INF / 2:
        return 1.0
    mu = m_mu.get(key, None)
    norm = mu if (mu is not None and mu > 0.0) else _half_life()
    frac = 1.0 / (1.0 + (pd / max(1.0, float(norm))))
    return 1.0 + URGENCY_WEIGHT * frac

def _current_decayed_score(key, now):
    base = float(m_score.get(key, 0.0))
    ts = m_score_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _record_ghost_on_evict(evicted_key, now):
    global g_heap, _g_seq
    # Record minimal ghost info
    mu = m_mu.get(evicted_key, None)
    if mu is not None:
        g_mu[evicted_key] = float(mu)
    else:
        g_mu.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 0)
    g_score[evicted_key] = float(_current_decayed_score(evicted_key, now))
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))
    # Heap push for fast trimming
    heapq.heappush(g_heap, (now, _g_seq, evicted_key))
    _g_seq += 1

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    # Trim using heap: remove oldest by time
    while len(g_last) > limit and g_heap:
        t, _, k = heapq.heappop(g_heap)
        # Validate entry is current
        last = g_last.get(k, None)
        if last is None or last != t:
            continue  # stale heap entry
        # Remove this ghost record
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)
        g_score.pop(k, None)
        g_seg.pop(k, None)

def _place_in_segment(key, seg_new):
    seg_old = m_seg.get(key, None)
    if seg_old is not None and seg_old == seg_new:
        return
    if seg_old is not None:
        m_seg_keys.get(seg_old, set()).discard(key)
    m_seg[key] = int(seg_new)
    m_seg_keys.setdefault(int(seg_new), set()).add(key)

def _seed_predictor_on_insert(key, now, obj):
    global m_protect_target_frac

    # ARC adaptation using ghost feedback about where the key was last evicted from
    if key in g_seg:
        if g_seg[key] == 0:
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    # Reuse-time predictor (EWMA)
    if key in g_mu:
        m_mu[key] = max(1.0, float(g_mu[key]))
        m_freq[key] = max(0, int(g_freq.get(key, 0)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0

    m_last_access[key] = now

    # Decayed score: conservative admission; warm from ghost if available
    if key in g_score:
        m_score[key] = 0.60 * float(g_score[key])
    else:
        m_score[key] = 0.0
    m_score_ts[key] = now

    # Segment placement
    if START_PROTECTED_FROM_GHOST and key in g_seg:
        _place_in_segment(key, 1)
    else:
        _place_in_segment(key, 0)

def _update_predictors_on_hit(key, now, obj):
    # Reuse-time predictor EWMA
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Decayed score update (lazy decay + size-aware credit + quickness bonus)
    cur = _current_decayed_score(key, now)
    credit = _size_credit(obj)

    # Quickness bonus: earlier-than-expected hits are more valuable
    mu = m_mu.get(key, None)
    if mu is not None and last is not None:
        gap = max(1.0, float(now - last))
        earliness = max(0.0, (float(mu) - gap)) / max(float(mu), 1.0)
        credit *= (1.0 + 0.5 * earliness)

    m_score[key] = cur + credit
    m_score_ts[key] = now

    # Promote to protected on first hit (ARC-style)
    if m_seg.get(key, 0) == 0:
        _place_in_segment(key, 1)


def _retention_value(key, obj_val, now, preferred_seg):
    # Compute size-aware retention density with urgency and segment bias
    s_now = _current_decayed_score(key, now)
    sz = _size_of(obj_val)
    retention = (s_now + _EPS) / (float(sz) ** GAMMA_SIZE_EVICT)
    retention *= _urgency_boost(key, now)
    seg = m_seg.get(key, 0)
    if seg != preferred_seg:
        retention *= NONPREF_BIAS
    la = m_last_access.get(key, -1)
    return retention, seg, la, sz

def _pick_candidates(cache_snapshot, preferred_seg):
    # Decide sample size based on cache size
    cache = cache_snapshot.cache
    n = len(cache)
    if n <= FULL_SCAN_THRESHOLD:
        return list(cache.items())

    if n <= 512:
        s = SAMPLE_SMALL
    elif n <= 4096:
        s = SAMPLE_MED
    else:
        s = SAMPLE_LARGE

    # Draw from segments with bias towards preferred
    s_pref = int(round(SAMPLE_PREF_FRAC * s))
    s_other = max(0, s - s_pref)

    cand = []

    # Helper to sample from a segment set
    def sample_from_seg(seg, k):
        if k <= 0:
            return
        keys = m_seg_keys.get(seg, set())
        if not keys:
            return
        if len(keys) <= k:
            for kk in keys:
                if kk in cache:
                    cand.append((kk, cache[kk]))
        else:
            # random.sample can operate on a set
            for kk in random.sample(list(keys), k):
                if kk in cache:
                    cand.append((kk, cache[kk]))

    # Sample preferred segment first
    sample_from_seg(preferred_seg, s_pref)
    # Sample from the other segment
    sample_from_seg(1 - preferred_seg, s_other)

    # Fallback: ensure at least some candidates
    if not cand:
        # take a few arbitrary items
        it = iter(cache.items())
        for _ in range(min(s, n)):
            try:
                cand.append(next(it))
            except StopIteration:
                break
    return cand


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using sampled, segment-aware selection:
      - Compute preferred segment to evict from: whichever (T1 or T2) exceeds its adaptive target.
      - Sample candidates primarily from the preferred segment (and some from the other).
      - For each candidate, compute size-aware LRFU retention with urgency.
      - Evict the candidate with the smallest retention (ties: preferred seg, LRU, larger size, key).
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Determine current segment composition
    total = len(cache)
    prot_count = len(m_seg_keys.get(1, set()))
    if prot_count > total:
        prot_count = 0  # safety
    target_prot = int(round(float(m_protect_target_frac) * float(max(1, total))))

    preferred_seg = 1 if prot_count > target_prot else 0

    # Candidate set
    candidates = _pick_candidates(cache_snapshot, preferred_seg)

    victim_key = None
    victim_val = None
    victim_score = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in candidates:
        retention, seg, la, sz = _retention_value(k, v, now, preferred_seg)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_score:
            better = True
        elif retention == victim_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Initialize metadata if missing.
      - Update per-key/global EWMA reuse predictor.
      - Update decayed score with size-aware credit and quickness bonus.
      - Promote to protected segment if currently probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (cold hit)
    if key not in m_score_ts or key not in m_mu or key not in m_last_access or key not in m_seg:
        _seed_predictor_on_insert(key, now, obj)

    _update_predictors_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style adaptation using ghost feedback to adjust protected target fraction.
      - Initialize per-key predictor and decayed score (warm-start from ghost when available).
      - Place ghost hits directly into protected; cold items start in probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, score, last segment) and push to heap.
      - Trim ghost to bounded size (heap-based, no full sort).
      - Remove all live metadata of evicted key and segment set membership.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_score.pop(ekey, None)
    m_score_ts.pop(ekey, None)

    seg = m_seg.pop(ekey, None)
    if seg is not None:
        m_seg_keys.get(seg, set()).discard(ekey)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8xyechen.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvovtgev3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3maljkvf.pickle

Iteration 56: New subsample score 0.923263 is better than old score 0.45738999999999996. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw7dlue73.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpskatx12v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb6z6xw06.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaqacxpiu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl6sweh2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp2jwaarg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcq8krh_t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_wv9y23a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprl6olxth.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_knsknds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph_f9n8b6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9eqay2hs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6lvjek6m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5mqdaq9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9zibt314.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiemmdbh5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7oiv6ce8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyw57p97y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmi1em_8x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbnxedsv1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzfa17h4c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp82mkfnq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0ymshu9l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbxmzlf87.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphru21k7s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpni0ehaq_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptmxf_b0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpss4d7tud.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplrcky4qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbvh5s0f3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg1_guyyb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptrmrwrhf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5sftsi2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe4hx__jl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpowy8laiv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxv4pa5hd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptvz1x6zb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwbhrzcmn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwrltdt3l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph466ptvm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy6xf55y3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqp04_zsd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfjzqb2hi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8pu3s5sh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjak19yjn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7ekf29y0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvff605ae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6tl72r_h.pickle

Iteration 56: New program is on the linear pareto front
Iteration 56: Full valset score for new program: 0.25787243750000005
Iteration 56: Full train_val score for new program: 0.25787243750000005
Iteration 56: Individual valset scores for new program: [0.460665, 0.431415, 0.440155, 0.396017, 0.449163, 0.444137, 0.251196, 0.468541, 0.526993, 0.531017, 0.133333, 0.309059, 0.066245, 0.0, 0.022229, 0.022401, 0.021004, 0.024318, 0.023907, 0.27406, 0.428712, 0.024988, 0.059317, 0.058672, 0.332118, 0.262097, 0.748759, 0.888242, 0.178197, 0.038636, 0.072893, 0.087816, 0.087816, 0.644316, 0.111842, 0.163283, 0.218379, 0.629057, 0.125461, 0.157472, 0.144166, 0.178125, 0.101974, 0.35, 0.188077, 0.17195, 0.466258, 0.163399]
Iteration 56: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.163283, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 56: Full valset pareto front score: 0.2735548750000001
Iteration 56: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {7, 14, 23, 24, 25}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {24, 17, 14, 25}, {16}, {17, 14}, {17, 14}, {24}, {3}, {8, 7}, {3}, {14, 17, 23, 24, 25}, {18}, {6, 14, 17, 24, 25}, {24, 25, 14}, {24, 25, 14}, {23}, {24, 25}, {24, 25}, {24, 25, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25}, {14}, {24, 25}, {24, 25, 14}, {24, 25}, {3, 5, 6, 7, 8, 17}, {24, 25, 14, 23}, {24, 25, 14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25}, {24}]
Iteration 56: Best valset aggregate score so far: 0.25787243750000005
Iteration 56: Best program as per aggregate score on train_val: 25
Iteration 56: Best program as per aggregate score on valset: 25
Iteration 56: Best score on valset: 0.25787243750000005
Iteration 56: Best score on train_val: 0.25787243750000005
Iteration 56: Linear pareto front program index: 25
Iteration 56: New program candidate index: 25
Iteration 57: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe3gcxk6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9mj_touh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp75hkvv6g.pickle

Iteration 57: Proposed new text for program: # ARC-guided GreedyDual-Size-Frequency (ARC-GDSF)
# Core ideas:
# - Two segments (probation/protected) like SLRU/ARC. Evict from probation unless protected
#   outgrows an adaptive target. Promote to protected on first hit.
# - GreedyDual-Size-Frequency priority for victim choice:
#     priority[key] accumulates credits; items with the smallest priority are evicted first.
#     On each access, priority += credit(size, decayed_freq); global L is set to the last
#     evicted item's priority to age the stack (classic GreedyDual).
# - Size awareness: smaller items accrue more credit (1 / size^alpha).
# - Frequency awareness: per-key decayed frequency (half-life) boosts credit with diminishing returns.
# - ARC-style ghost feedback (where an item was evicted from) adapts the protected fraction target
#   and warm-starts priority/frequency on reinsert.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Segment: 0 = probation, 1 = protected
m_seg = dict()          # key -> int

# GreedyDual priority and global aging base
m_pri = dict()          # key -> float
m_last_access = dict()  # key -> int (LRU tie-breaker)

# Decayed frequency (EWMA with half-life)
m_f = dict()            # key -> float (decayed hit count proxy)
m_f_ts = dict()         # key -> int (last freq update time)

# Global priority aging base (GreedyDual: set to priority of last evicted item)
g_L = 0.0

# ARC-style target for protected share (by count)
m_protect_target_frac = 0.50

# Ghost history (bounded) for ARC adaptation and warm starts
g_seg = dict()          # key -> int (segment at eviction)
g_pri = dict()          # key -> float (priority at eviction)
g_f = dict()            # key -> float (decayed frequency at eviction)
g_last = dict()         # key -> int (last seen time)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-awareness: credit ~ 1 / size^ALPHA_SIZE
ALPHA_SIZE = 0.85

# Frequency boost: credit multiplier ~ 1 + F_WEIGHT * log1p(freq)
F_WEIGHT = 1.2

# Frequency half-life (in access-count units)
FREQ_HL = 512.0

# ARC adaptation
ARC_ADAPT_STEP = 0.08
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_last) <= limit:
        return
    # Evict oldest ghosts by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_last) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_seg.pop(k, None)
        g_pri.pop(k, None)
        g_f.pop(k, None)
        g_last.pop(k, None)

def _decay_factor(delta, hl):
    if delta <= 0:
        return 1.0
    return 0.5 ** (float(delta) / float(hl))

def _current_freq(key, now):
    f = float(m_f.get(key, 0.0))
    ts = m_f_ts.get(key, None)
    if ts is None:
        return f
    delta = int(now - ts)
    if delta > 0 and f > 0.0:
        f *= _decay_factor(delta, FREQ_HL)
    return f

def _credit(size_bytes, freq_val):
    # Size-aware base credit
    base = 1.0 / (float(size_bytes) ** ALPHA_SIZE)
    # Diminishing returns for frequency
    freq_mult = 1.0 + F_WEIGHT * math.log1p(max(0.0, float(freq_val)))
    return base * freq_mult

def _record_ghost_on_evict(evicted_key, now):
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))
    g_pri[evicted_key] = float(m_pri.get(evicted_key, 0.0))
    g_f[evicted_key] = float(_current_freq(evicted_key, now))
    g_last[evicted_key] = now

def _seed_on_insert(key, now, obj):
    global m_protect_target_frac

    # ARC adaptation based on where the key had been evicted from
    if key in g_seg:
        if g_seg[key] == 0:
            # Recently evicted from probation: favor recency -> shrink protected target
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            # Evicted from protected: favor frequency -> grow protected target
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    # Initialize decayed frequency
    if key in g_f:
        m_f[key] = max(0.0, 0.5 * float(g_f[key]))
    else:
        m_f[key] = 0.0
    m_f_ts[key] = now

    # Initialize priority near the current global base with a size-aware credit, warmed by ghost
    size_b = _size_of(obj)
    warm_pri = float(g_pri.get(key, 0.0)) if key in g_pri else 0.0
    # Start above the aging base but not too aggressive
    base = max(float(globals()['g_L']), 0.5 * warm_pri)
    m_pri[key] = base + _credit(size_b, m_f.get(key, 0.0))

    # Segment placement
    if START_PROTECTED_FROM_GHOST and key in g_seg and g_seg[key] == 1:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

    m_last_access[key] = now

def _on_hit_update(key, now, obj):
    # Decay and add one hit
    cur_f = _current_freq(key, now)
    new_f = cur_f + 1.0
    m_f[key] = new_f
    m_f_ts[key] = now
    m_last_access[key] = now

    # Accumulate GreedyDual priority credit
    size_b = _size_of(obj)
    pri_before = max(float(m_pri.get(key, 0.0)), float(globals()['g_L']))
    m_pri[key] = pri_before + _credit(size_b, new_f)

    # Promote to protected on first hit from probation
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Eviction rule (ARC-guided GDSF):
      - Prefer evicting from probation unless protected's share exceeds its adaptive target.
      - Within the chosen segment, evict the key with the smallest GreedyDual priority.
      - Tie-breakers: older last access (LRU), then larger size (frees more), then key order.
      - After choosing a victim, update global aging base g_L to the victim's priority.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Segment composition
    total = len(cache)
    prot_count = sum(1 for k in cache.keys() if m_seg.get(k, 0) == 1)
    target_prot = int(round(float(m_protect_target_frac) * float(total)))

    # Prefer evicting from probation unless protected is oversized
    preferred_seg = 1 if prot_count > target_prot else 0

    # Candidates
    cand_keys = [k for k in cache.keys() if m_seg.get(k, 0) == preferred_seg]
    if not cand_keys:
        cand_keys = list(cache.keys())

    victim_key = None
    victim_pri = None
    victim_la = None
    victim_sz = None
    victim_seg = None

    for k in cand_keys:
        v = cache[k]
        pri = float(m_pri.get(k, 0.0))
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        seg = m_seg.get(k, 0)

        if victim_key is None:
            victim_key = k
            victim_pri = pri
            victim_la = la
            victim_sz = sz
            victim_seg = seg
            continue

        better = False
        if pri < victim_pri - _EPS:
            better = True
        elif abs(pri - victim_pri) <= _EPS:
            # LRU within tie
            if la < victim_la:
                better = True
            elif la == victim_la:
                # Free more space
                if sz > victim_sz:
                    better = True
                elif sz == victim_sz and seg == preferred_seg and victim_seg != preferred_seg:
                    better = True
                elif sz == victim_sz and seg == victim_seg and k < victim_key:
                    better = True

        if better:
            victim_key = k
            victim_pri = pri
            victim_la = la
            victim_sz = sz
            victim_seg = seg

    # Update global aging base to the victim's priority (GreedyDual aging)
    if victim_key is not None:
        globals()['g_L'] = max(float(globals()['g_L']), float(m_pri.get(victim_key, 0.0)))

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update decayed frequency (half-life) and LRU timestamp.
      - Accumulate GreedyDual priority credit (size-aware, frequency-boosted).
      - Promote to protected if currently probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize on cold metadata miss (should be rare)
    if key not in m_pri or key not in m_f_ts or key not in m_last_access:
        _seed_on_insert(key, now, obj)

    _on_hit_update(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style ghost feedback adapts protected fraction.
      - Initialize frequency, priority, and segment (warm-start from ghost when available).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record ghost metadata (segment, priority, decayed frequency).
      - Trim ghost set to a bounded size.
      - Remove live metadata for the evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_seg.pop(ekey, None)
    m_pri.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_f.pop(ekey, None)
    m_f_ts.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46xqvamz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoxticdbv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpluv57l8o.pickle

Iteration 57: New subsample score 0.31814600000000004 is better than old score 0.313354. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpricbo5dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppyo6mxpw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpufic3lf3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqf_oygh_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe320z6x3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy9q1va7l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb63fesu2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc58g8oep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2nwrs3ia.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4d44duaz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpukhjhsu5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpggrphcrf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg9xstpvx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp29c9ekgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi7tqc_uu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd5ti9roc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0l81kxm_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjxvoodnz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfptq__4l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph64mm3i8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl4qd9ksv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpku3i8305.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc_b69462.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmqz9c3gj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0g8riiej.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr0bqx23l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6b_ew9ra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk0nent0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2cpa9u95.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk4nw4lnf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0npqm8fm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvq1prk74.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp399efxp3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi_7fmqw3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbv_r3cju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcskys0mi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpogx2x4gi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkr7pp1a_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpldhtnw0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx4iezp49.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9uevve6u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyi4x57fv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3kcl9ot4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppg98lwdp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpme27plxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2p8hm2y5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpff9zkyg4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp5sor12o.pickle

Iteration 57: Full valset score for new program: 0.21466443749999997
Iteration 57: Full train_val score for new program: 0.21466443749999997
Iteration 57: Individual valset scores for new program: [0.423214, 0.384368, 0.406036, 0.337868, 0.414452, 0.409028, 0.255981, 0.498624, 0.534859, 0.531017, 0.075, 0.305506, 0.040045, 0.0, 0.020671, 0.020569, 0.019793, 0.023334, 0.0225, 0.271311, 0.375615, 0.026164, 0.058672, 0.058672, 0.276064, 0.220766, 0.785501, 0.876604, 0.039832, 0.038636, 0.045558, 0.002765, 0.006915, 0.734003, 0.078947, 0.067961, 0.026022, 0.613086, 0.125461, 0.02412, 0.039096, 0.041403, 0.046053, 0.116667, 0.041854, 0.040673, 0.460123, 0.042484]
Iteration 57: New valset pareto front scores: [0.486386, 0.462935, 0.470941, 0.420201, 0.482377, 0.470541, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.373446, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.366935, 0.853029, 0.890467, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.163283, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 57: Full valset pareto front score: 0.2735548750000001
Iteration 57: Updated valset pareto front programs: [{16}, {16}, {16}, {16}, {16}, {16}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {7, 14, 23, 24, 25}, {7}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {24, 17, 14, 25}, {16}, {17, 14}, {17, 14}, {24}, {3}, {8, 7}, {3}, {14, 17, 23, 24, 25}, {18}, {6, 14, 17, 24, 25}, {24, 25, 14}, {24, 25, 14}, {23}, {24, 25}, {24, 25}, {24, 25, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26}, {14}, {24, 25}, {24, 25, 14}, {24, 25}, {3, 5, 6, 7, 8, 17}, {24, 25, 14, 23}, {24, 25, 14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25}, {24}]
Iteration 57: Best valset aggregate score so far: 0.25787243750000005
Iteration 57: Best program as per aggregate score on train_val: 25
Iteration 57: Best program as per aggregate score on valset: 25
Iteration 57: Best score on valset: 0.25787243750000005
Iteration 57: Best score on train_val: 0.25787243750000005
Iteration 57: Linear pareto front program index: 25
Iteration 57: New program candidate index: 26
Iteration 58: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqpe0aw18.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkk66yp21.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8uyj5w9r.pickle

Iteration 58: Proposed new text for program: # SARC-TF: Size-aware Adaptive Recency with ARC-style Targeting and Tiny Frequency
# Design goals:
# - Segment the cache into two logical parts:
#     T1 (probation: recently seen, never hit) and T2 (protected: seen >= 1 hit).
# - Adapt the target size of T1 vs T2 using ARC-style ghost history feedback (B1/B2).
#   This provides scan resistance and automatic tuning for recency vs frequency.
# - Evict using a size-aware "keep score" that blends recency, light frequency, segment protection,
#   and segment pressure relative to the current target. Lower keep score => evict first.
# - Maintain only simple, robust metadata: last access time, hit count, segment, ghosts.
#   Avoid over-parameterized predictors that can overfit diverse traces.
#
# Notes:
# - We assume evict(cache_snapshot, obj) is called before admitting obj when cache is full.
# - We can't reject admission explicitly; instead, we choose the least valuable live item to evict.
# - Size-awareness: larger objects are penalized using size^GAMMA where GAMMA in [0.7, 0.95].
# - ARC-like adaptation:
#     * B1: ghosts of T1 evictions (recent, not yet frequent)
#     * B2: ghosts of T2 evictions (frequent)
#     * When a miss inserts a key present in B1, increase T1 target p (favor recency).
#       When a miss inserts a key present in B2, decrease p (favor frequency).
# - Segment pressure:
#     If T1 exceeds its target p, reduce keep score of T1 entries (evict more from T1), and vice versa.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_seg = dict()        # key -> int (0=T1 probation, 1=T2 protected)
m_last = dict()       # key -> int (last access time)
m_freq = dict()       # key -> int (hit count)

# Ghost histories with lightweight metadata
# Store last-seen time and size for target adaptation and trimming
g_B1_ts = dict()      # key -> int (ghost of T1)
g_B1_sz = dict()      # key -> int
g_B2_ts = dict()      # key -> int (ghost of T2)
g_B2_sz = dict()      # key -> int

# ARC-style target for T1 (in bytes). Will be initialized lazily.
g_target_p_bytes = None

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size awareness
GAMMA_SIZE_EVICT = 0.85

# Segment protection and frequency weighting
PROTECTED_BOOST = 1.5         # protected entries (T2) get multiplicative keep boost
FREQ_LOG_WEIGHT = 1.0         # multiply log2(1+freq) by this factor
RECENCY_BASE = 1.0            # baseline for recency; keep = RECENCY_BASE / (age+1)

# Segment pressure tuning
PRESSURE_DECAY = 0.7          # intensity of reduction in keep for over-target segments
PRESSURE_GAIN = 0.20          # bonus keep (up to +20%) for under-target segments

# ARC target adjustment
ARC_STEP_FRACTION = 1.0 / 64.0  # minimum step as fraction of capacity bytes
ARC_GHOST_FACTOR = 4.0          # total ghosts capped to 4x live count (by count)
ARC_MIN_P_FRACTION = 0.02       # minimum p as fraction of capacity
ARC_MAX_P_FRACTION = 0.98       # maximum p as fraction of capacity

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _cap_bytes(cache_snapshot):
    # Use byte capacity if available; fallback to number of live entries as pseudo-capacity
    cap = int(getattr(cache_snapshot, "capacity", 0) or 0)
    if cap > 0:
        return cap
    # Fallback: approximate capacity in "units" = live count
    return max(1, len(getattr(cache_snapshot, "cache", {}) or {}))

def _init_target_if_needed(cache_snapshot):
    global g_target_p_bytes
    if g_target_p_bytes is None:
        cap = float(_cap_bytes(cache_snapshot))
        # Start with 20% of capacity for T1; clamp to [min,max]
        p0 = 0.20 * cap
        pmin = ARC_MIN_P_FRACTION * cap
        pmax = ARC_MAX_P_FRACTION * cap
        g_target_p_bytes = max(pmin, min(p0, pmax))

def _ghost_total_count():
    return len(g_B1_ts) + len(g_B2_ts)

def _ghost_trim(cache_snapshot):
    # Trim ghost histories to bounded count (relative to live cache size)
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    limit = max(1024, int(ARC_GHOST_FACTOR * live_cnt))
    extra = _ghost_total_count() - limit
    if extra <= 0:
        return
    # Evict oldest ghosts first (by timestamp), taking evenly from B1 and B2 pools.
    # Build unified list of (key, ts, which)
    all_ghosts = []
    all_ghosts.extend([(k, g_B1_ts[k], 1) for k in g_B1_ts.keys()])
    all_ghosts.extend([(k, g_B2_ts[k], 2) for k in g_B2_ts.keys()])
    all_ghosts.sort(key=lambda x: (x[1], x[0]))  # oldest first
    for i in range(min(extra, len(all_ghosts))):
        k, _, which = all_ghosts[i]
        if which == 1:
            g_B1_ts.pop(k, None)
            g_B1_sz.pop(k, None)
        else:
            g_B2_ts.pop(k, None)
            g_B2_sz.pop(k, None)

def _segment_bytes(cache_snapshot):
    # Compute current bytes in T1 and T2
    cache = cache_snapshot.cache or {}
    t1 = 0
    t2 = 0
    for k, v in cache.items():
        sz = _size_of(v)
        if m_seg.get(k, 0) == 1:
            t2 += sz
        else:
            t1 += sz
    return t1, t2

def _segment_pressure_weight(seg, t1_bytes, t2_bytes, cap_bytes, p_bytes):
    # Determine pressure relative to target
    if seg == 0:
        target = max(1.0, float(p_bytes))
        used = float(t1_bytes)
    else:
        target = max(1.0, float(cap_bytes - p_bytes))
        used = float(t2_bytes)

    if used <= target:
        # Under target: gently protect (increase keep)
        deficit = (target - used) / target  # in [0, 1+]
        return 1.0 + min(PRESSURE_GAIN, PRESSURE_GAIN * deficit)
    else:
        # Over target: reduce keep to shed entries from this segment
        over = (used - target) / target
        # Reduce keep, but not below 50% to avoid overreacting
        return max(0.5, 1.0 - PRESSURE_DECAY * over)

def _keep_score(now, la, freq, seg, size, t1_bytes, t2_bytes, cap_bytes, p_bytes):
    # Recency component: more recent => higher keep
    age = max(0, int(now - la))
    recency = RECENCY_BASE / float(1 + age)  # in (0, 1]

    # Frequency component: light log scaling
    f_weight = 1.0 + FREQ_LOG_WEIGHT * math.log2(1.0 + max(0, int(freq)))

    # Segment base boost
    seg_boost = PROTECTED_BOOST if seg == 1 else 1.0

    # Segment pressure adjustment based on target
    pressure = _segment_pressure_weight(seg, t1_bytes, t2_bytes, cap_bytes, p_bytes)

    # Size normalization: penalize large objects
    size_penalty = float(size) ** GAMMA_SIZE_EVICT

    keep = (recency * f_weight * seg_boost * pressure) / (size_penalty + _EPS)
    return keep

def _arc_adjust_p_on_insert(cache_snapshot, key, obj):
    """
    ARC-style adaptation on a miss (insert):
      - If key in B1 (recent-cold), increase p (grow T1).
      - If key in B2 (frequent), decrease p (grow T2).
    Adjustment is size- and capacity-aware.
    """
    global g_target_p_bytes
    _init_target_if_needed(cache_snapshot)

    cap = float(_cap_bytes(cache_snapshot))
    pmin = ARC_MIN_P_FRACTION * cap
    pmax = ARC_MAX_P_FRACTION * cap
    step_min = ARC_STEP_FRACTION * cap
    size = float(_size_of(obj))

    if key in g_B1_ts:
        delta = max(step_min, size)
        g_target_p_bytes = min(pmax, g_target_p_bytes + delta)
        # Once adapted, remove ghost entry (standard ARC behavior)
        g_B1_ts.pop(key, None)
        g_B1_sz.pop(key, None)
    elif key in g_B2_ts:
        delta = max(step_min, size)
        g_target_p_bytes = max(pmin, g_target_p_bytes - delta)
        g_B2_ts.pop(key, None)
        g_B2_sz.pop(key, None)
    # else: no change

def _record_ghost(evicted_key, evicted_seg, evicted_size, now):
    if evicted_seg == 1:
        g_B2_ts[evicted_key] = now
        g_B2_sz[evicted_key] = int(evicted_size)
    else:
        g_B1_ts[evicted_key] = now
        g_B1_sz[evicted_key] = int(evicted_size)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose the victim with the smallest "keep score":
      keep = recency * log-frequency * segment_boost * segment_pressure / size^GAMMA
    - Segment pressure uses ARC-style target p to decide whether to evict from T1 or T2.
    - Tie-breakers:
        1) Prefer evicting T1 (probation) over T2 (protected)
        2) Older last access (LRU)
        3) Larger size (free more space)
        4) Lexicographic key
    """
    cache = cache_snapshot.cache or {}
    if not cache:
        return None

    _init_target_if_needed(cache_snapshot)
    now = _now(cache_snapshot)
    cap_bytes = float(_cap_bytes(cache_snapshot))
    p_bytes = float(g_target_p_bytes)

    # Compute segment occupancy for pressure
    t1_bytes, t2_bytes = _segment_bytes(cache_snapshot)

    victim_key = None
    victim_keep = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        seg = m_seg.get(k, 0)   # default probation
        la = m_last.get(k, now) # if missing, assume now (just touched)
        freq = m_freq.get(k, 0)
        sz = _size_of(v)

        keep = _keep_score(now, la, freq, seg, sz, t1_bytes, t2_bytes, cap_bytes, p_bytes)

        if victim_key is None:
            victim_key = k
            victim_keep = keep
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if keep < victim_keep - _EPS:
            better = True
        elif abs(keep - victim_keep) <= _EPS:
            # Tie-breakers
            if seg < victim_seg:  # evict T1 before T2
                better = True
            elif seg == victim_seg:
                if la < victim_la:  # older first
                    better = True
                elif la == victim_la:
                    if sz > victim_sz:  # free more space
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key = k
            victim_keep = keep
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update last access time.
      - Increment local frequency.
      - Promote from T1 to T2 on first hit; remain in T2 otherwise.
    """
    now = _now(cache_snapshot)
    key = obj.key

    # Initialize if missing (defensive)
    if key not in m_last:
        m_last[key] = now
        m_freq[key] = 0
        m_seg[key] = 0

    m_last[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Promote to protected (T2) on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - ARC-style adjust target p using ghost feedback (B1/B2).
      - Initialize metadata and start in probation (T1).
      - Remove any ghost record for the inserted key.
    """
    _init_target_if_needed(cache_snapshot)
    now = _now(cache_snapshot)
    key = obj.key

    # ARC-style target adjustment based on ghosts
    _arc_adjust_p_on_insert(cache_snapshot, key, obj)

    # Initialize live metadata
    m_last[key] = now
    # We treat insert as a miss with 0 hits so far; count will grow on subsequent hits
    m_freq[key] = m_freq.get(key, 0)
    m_seg[key] = 0  # probation (T1)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record the evicted key into the appropriate ghost (B1 for T1, B2 for T2).
      - Trim ghost history to a bounded size.
      - Remove all live metadata for the evicted key.
    """
    now = _now(cache_snapshot)
    ekey = evicted_obj.key
    eseg = m_seg.get(ekey, 0)
    esize = _size_of(evicted_obj)

    # Record in ghosts
    _record_ghost(ekey, eseg, esize, now)
    _ghost_trim(cache_snapshot)

    # Clean live metadata
    m_last.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv_fbbta4.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplex4kg6y.pickle

Iteration 58: New subsample score 0.768265 is not better than old score 1.3100869999999998, skipping
Iteration 59: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiv2jfh26.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4uv1_r10.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppibfapqe.pickle

Iteration 59: Proposed new text for program: # Byte-ARC LRFU (B-ARC-LRFU)
# Goals:
# - Lower miss rate across diverse traces by stabilizing decisions and embracing byte-awareness.
# - ARC-style two segments (probation/protected) with ADAPTIVE target sized in BYTES (not counts).
# - Victim selection via stabilized size-aware LRFU score, with minimal noise and strong size bias.
# - Conservative admission (near-zero score), warm-start from ghost, and promotion on first hit.
# - Segment target adapts using both ARC ghost feedback and rolling segment-hit feedback.
# - Ghost bounded by live size and trimmed by last-seen time.

import math

# ----------------------
# Global metadata stores
# ----------------------

# LRFU decayed keep score and its last update time
m_score = dict()       # key -> float
m_score_ts = dict()    # key -> int

# Segment: 0 = probation (recent), 1 = protected (frequent)
m_seg = dict()         # key -> int

# Reuse predictor (EWMA of gaps) and last access time
m_last_access = dict() # key -> int
m_mu = dict()          # key -> float
m_freq = dict()        # key -> int  # lightweight hit counter (no global use other than ghost)

# Global EWMA of mean gap (for adaptive half-life)
m_global_mu = 64.0

# ARC-style protected target (fraction of BYTES)
m_protect_target_frac = 0.50

# Ghost history
g_mu = dict()
g_last = dict()
g_freq = dict()
g_score = dict()
g_seg = dict()

# Rolling segment-hit feedback (to refine protected target)
_hwin_hits_prob = 0
_hwin_hits_prot = 0
_hwin_total = 0

# ----------------------
# Tunable hyperparameters
# ----------------------

# LRFU half-life adapts to global mean gap (in access-count units)
HL_BASE = 128.0
HL_MIN = 16.0
HL_MAX = 16384.0
HL_ADAPT_K = 0.40

# Size awareness
ALPHA_SIZE_CREDIT = 0.55   # hit credit is 1 / size^alpha
GAMMA_SIZE_EVICT = 0.80    # eviction retention scales as 1 / size^gamma

# Segment selection bias (ARC-like)
NONPREF_BIAS = 1.6         # reduce bias to avoid pathological evictions
ARC_ADAPT_STEP = 0.06
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# Reuse predictor EWMA
EWMA_BETA = 0.25
GLOBAL_BETA = 0.02

# Rolling segment-hit feedback window
HIT_FEEDBACK_WINDOW = 256
HIT_FEEDBACK_ALPHA = 0.20  # blending weight for feedback -> target fraction

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 3.0

# Numerics
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict oldest ghosts by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_mu) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)
        g_score.pop(k, None)
        g_seg.pop(k, None)

def _half_life():
    hl = HL_BASE + HL_ADAPT_K * float(m_global_mu)
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return hl

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    return 0.5 ** (float(delta) / float(_half_life()))

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _current_decayed_score(key, now):
    base = float(m_score.get(key, 0.0))
    ts = m_score_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _segment_bytes(cache_snapshot):
    cache = cache_snapshot.cache
    prob_bytes = 0
    prot_bytes = 0
    for k, v in cache.items():
        if m_seg.get(k, 0) == 1:
            prot_bytes += _size_of(v)
        else:
            prob_bytes += _size_of(v)
    return prob_bytes, prot_bytes

def _record_ghost_on_evict(evicted_key, now):
    # Always record minimal ghost info
    mu = m_mu.get(evicted_key, None)
    if mu is not None:
        g_mu[evicted_key] = float(mu)
    else:
        g_mu.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 0)
    g_score[evicted_key] = float(_current_decayed_score(evicted_key, now))
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))

def _seed_predictor_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert, with warm-start from ghost if available.
    Also updates protected target fraction via ARC ghost feedback.
    """
    global m_protect_target_frac

    # ARC-style adaptation driven by ghost list where the key came from
    if key in g_seg:
        if g_seg[key] == 0:
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    # Reuse predictor (EWMA)
    if key in g_mu:
        m_mu[key] = max(1.0, float(g_mu[key]))
        m_freq[key] = max(0, int(g_freq.get(key, 0)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0

    m_last_access[key] = now

    # Decayed score: conservative admission if cold; warm from ghost if available
    if key in g_score:
        m_score[key] = 0.65 * float(g_score[key])
    else:
        m_score[key] = 0.0
    m_score_ts[key] = now

    # Segment placement: if seen in ghost, admit directly to protected; else probation
    if START_PROTECTED_FROM_GHOST and key in g_seg:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

def _update_predictors_on_hit(key, now, obj):
    """
    Update reuse-time predictor (per-key + global) and decayed score (size-aware).
    Promote to protected on first hit.
    Also update rolling segment-hit feedback to refine protected target.
    """
    global m_protect_target_frac, _hwin_hits_prob, _hwin_hits_prot, _hwin_total

    # Reuse predictor EWMA
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Decayed score update (lazy decay + size-aware credit with mild quickness emphasis)
    cur = _current_decayed_score(key, now)
    credit = _size_credit(obj)

    if last is not None:
        gap = max(1.0, float(now - last))
        mu = m_mu.get(key, gap)
        # Early hits get up to +30% credit; conservative to avoid noise
        earliness = max(0.0, (float(mu) - gap)) / max(float(mu), 1.0)
        credit *= (1.0 + 0.3 * earliness)

    m_score[key] = cur + credit
    m_score_ts[key] = now

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1

    # Rolling segment-hit feedback to refine protected target by observed utility
    seg = m_seg.get(key, 0)
    if seg == 1:
        _hwin_hits_prot += 1
    else:
        _hwin_hits_prob += 1
    _hwin_total += 1

    if _hwin_total >= HIT_FEEDBACK_WINDOW:
        share = float(_hwin_hits_prot) / max(1.0, float(_hwin_total))
        target_new = min(PROTECT_FRAC_MAX, max(PROTECT_FRAC_MIN, share))
        m_protect_target_frac = (1.0 - HIT_FEEDBACK_ALPHA) * m_protect_target_frac + HIT_FEEDBACK_ALPHA * target_new
        _hwin_hits_prob = 0
        _hwin_hits_prot = 0
        _hwin_total = 0


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using byte-aware ARC segment preference and size-aware LRFU scoring.

    Steps:
    - Determine preferred segment to evict from by comparing protected BYTES vs adaptive BYTES target.
    - For each resident key, compute a retention score:
        retention = decayed_score(now) / size^GAMMA
      Lower retention => worse to keep => evict.
    - Apply a mild bias so that non-preferred segment entries are slightly less likely candidates.
    - Incorporate "space urgency": if we need to free many bytes for the incoming object,
      prefer evicting larger objects by reducing their retention further.
    - Tie-breakers:
        * Prefer evicting from preferred segment
        * Lower retention
        * Older last access (LRU)
        * Larger size (to free space quickly)
        * Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Byte-aware segment composition
    prob_bytes, prot_bytes = _segment_bytes(cache_snapshot)
    total_bytes = max(1, prob_bytes + prot_bytes)
    target_prot_bytes = float(m_protect_target_frac) * float(total_bytes)
    preferred_seg = 1 if prot_bytes > target_prot_bytes else 0

    # Estimate bytes needed to admit incoming object (for space urgency)
    inc_sz = _size_of(obj)
    need_bytes = max(0, int(cache_snapshot.size + inc_sz - cache_snapshot.capacity))

    victim_key = None
    victim_val = None
    victim_score = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    # Space urgency factor: prefer evicting larger items when we need to free more space
    def _space_urgency(sz):
        if need_bytes <= 0:
            return 1.0
        # If object is larger than needed bytes, give a stronger push (up to ~2x)
        ratio = float(sz) / float(need_bytes)
        return 1.0 + min(1.0, 0.75 * ratio)

    for k, v in cache.items():
        s_now = _current_decayed_score(k, now)
        sz = _size_of(v)

        # Base "keep" retention (stabilized LRFU, size-aware)
        retention = (s_now + _EPS) / (float(sz) ** GAMMA_SIZE_EVICT)

        # Space urgency: lower retention for larger objects when more bytes are needed
        retention /= _space_urgency(sz)

        seg = m_seg.get(k, 0)

        # Bias away from non-preferred segment
        if seg != preferred_seg:
            retention *= NONPREF_BIAS

        la = m_last_access.get(k, -1)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_score:
            better = True
        elif retention == victim_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Initialize missing metadata if needed (defensive for warm-cache starts).
      - Update per-key/global EWMA reuse predictor.
      - Update decayed score with size-aware credit and mild quickness bonus.
      - Promote to protected segment if currently in probation.
      - Update rolling feedback to refine protected target fraction.
    """
    key = obj.key
    now = _now(cache_snapshot)

    if key not in m_score_ts or key not in m_mu or key not in m_last_access:
        _seed_predictor_on_insert(key, now, obj)

    _update_predictors_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style adaptation using ghost feedback to adjust protected target fraction (bytes).
      - Initialize per-key predictor and decayed score (warm-start from ghost when available).
      - Place ghost hits directly into protected; cold items start in probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, score, last segment, last seen).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_score.pop(ekey, None)
    m_score_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcyr1egx7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpunvd4omi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkmt2lnbj.pickle

Iteration 59: New subsample score 0.647742 is not better than old score 0.717357, skipping
Iteration 60: Selected program 14 score: 0.17844416666666663
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8r6k17_g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8a2sfmwb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxyglvphs.pickle

Iteration 60: Proposed new text for program: # GreedyDual-Size with Decayed Frequency and Segmented Protection (GDS-DF+SP)
# Goals:
# - Strong size-awareness without crushing large items: normalize by moving average size.
# - Blend recency and frequency by using GreedyDual aging (global L) and a decayed frequency.
# - Segmented protection: promote on first hit (protected), evict from probation first.
# - Ghost history to warm-start frequency on reinsert.
# - O(n) victim scan with stable, simple tie-breakers.

import math

# ----------------------
# Global metadata stores
# ----------------------
# GreedyDual priority score per key
m_H = dict()            # key -> float (priority)
# Segments: 0 = probation (never hit), 1 = protected (hit at least once)
m_seg = dict()          # key -> int
# Decayed frequency (DF) and last update time
m_dfreq = dict()        # key -> float
m_dfreq_ts = dict()     # key -> int (time when dfreq was last updated)
# Last access time (for LRU tie-breaking)
m_last_access = dict()  # key -> int

# Global GreedyDual aging parameter L
g_L = 0.0

# Moving average of object sizes (for size normalization)
g_avg_size = 4096.0

# Last evicted H to update L in update_after_evict
_last_evicted_H = 0.0

# Ghost history (warm start for decayed frequency)
g_dfreq = dict()        # key -> float (decayed frequency snapshot at eviction)
g_last = dict()         # key -> int (last seen time)


# ----------------------
# Tunable hyperparameters
# ----------------------
# Decayed frequency half-life in "access-count" units
DF_HALF_LIFE = 2048.0

# Frequency weighting on score increment (log-scaled to avoid runaway)
FREQ_BOOST = 0.75

# Warm-start boost on insert when ghost exists
WARM_BOOST = 0.35

# EWMA for average size normalization
AVG_SIZE_BETA = 0.05

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Numerics
_EPS = 1e-12


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _update_avg_size(sz):
    global g_avg_size
    # Update moving average of sizes using EWMA
    g_avg_size = (1.0 - AVG_SIZE_BETA) * float(g_avg_size) + AVG_SIZE_BETA * float(sz)
    if g_avg_size < 1.0:
        g_avg_size = 1.0

def _size_norm(sz):
    # Normalize by moving average size to keep weights well-scaled even for byte-sized inputs
    return max(1.0, float(sz) / float(g_avg_size))

def _weight_for_size(sz):
    # Relative value: prefer smaller objects, but not excessively
    # Weight is in (0, 1], equals 1 at average size.
    return 1.0 / _size_norm(sz)

def _dfreq_now(key, now):
    f = float(m_dfreq.get(key, 0.0))
    ts = m_dfreq_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and f > 0.0:
        f *= 0.5 ** (float(delta) / float(DF_HALF_LIFE))
    return f

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_dfreq) <= limit:
        return
    # Evict oldest ghosts first
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_dfreq) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_dfreq.pop(k, None)
        g_last.pop(k, None)


# ----------------------
# Core updates
# ----------------------
def _touch_on_hit(key, now, obj):
    """
    Update decayed frequency, priority H (GreedyDual), last access, and protection segment on hit.
    """
    global g_L

    sz = _size_of(obj)
    _update_avg_size(sz)

    # Update decayed frequency lazily
    f = _dfreq_now(key, now)
    f += 1.0
    m_dfreq[key] = f
    m_dfreq_ts[key] = now

    # Frequency boost (log-scaled)
    f_boost = 1.0 + FREQ_BOOST * math.log1p(f)

    w = _weight_for_size(sz)

    # GreedyDual refresh: H := max(H, L) + w * f_boost
    old_H = float(m_H.get(key, g_L))
    m_H[key] = max(old_H, g_L) + w * f_boost

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1

    m_last_access[key] = now


def _seed_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert. Start in probation. Warm-start DF from ghost if any.
    """
    global g_L

    sz = _size_of(obj)
    _update_avg_size(sz)

    # Warm-start decayed frequency from ghost (not too aggressively)
    base_f = float(g_dfreq.get(key, 0.0))
    f = 0.5 * base_f if base_f > 0.0 else 0.0
    m_dfreq[key] = f
    m_dfreq_ts[key] = now

    w = _weight_for_size(sz)
    warm_boost = 1.0 + WARM_BOOST * math.log1p(f)

    # On insert, H := L + w * warm_boost (probationary)
    m_H[key] = g_L + w * warm_boost
    m_seg[key] = 0
    m_last_access[key] = now


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose victim by minimal GreedyDual priority H, preferring probationary items.
    Tie-breakers:
      1) Segment: evict probation (0) before protected (1)
      2) Smaller H first
      3) Older last access (LRU)
      4) Larger size (free more space earlier)
      5) Lexicographic key
    """
    global _last_evicted_H

    cache = cache_snapshot.cache
    if not cache:
        _last_evicted_H = 0.0
        return None

    now = _now(cache_snapshot)

    # First pass: probation; Second pass: all
    best_key = None
    best_val = None
    best_tuple = None  # comparison tuple

    def candidate_tuple(k, v):
        seg = int(m_seg.get(k, 0))
        H = float(m_H.get(k, 0.0))
        la = int(m_last_access.get(k, -1))
        sz = _size_of(v)
        # Build tuple to minimize by: (segment, H, last_access, -size, key)
        # Probation seg=0 is preferred over protected seg=1
        return (seg, H, la, -sz, k)

    # Try probation first
    for k, v in cache.items():
        if int(m_seg.get(k, 0)) != 0:
            continue
        tup = candidate_tuple(k, v)
        if best_tuple is None or tup < best_tuple:
            best_tuple = tup
            best_key = k
            best_val = v

    # If no probation candidate, consider all
    if best_key is None:
        for k, v in cache.items():
            tup = candidate_tuple(k, v)
            if best_tuple is None or tup < best_tuple:
                best_tuple = tup
                best_key = k
                best_val = v

    # Record H of victim to update L after eviction
    _last_evicted_H = float(m_H.get(best_key, 0.0))
    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update moving average size
      - Update decayed frequency (EWMA via exponential decay)
      - Refresh GreedyDual priority H with frequency-scaled weight
      - Promote to protected
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure initialized if missing
    if key not in m_H or key not in m_dfreq_ts or key not in m_last_access:
        _seed_on_insert(key, now, obj)

    _touch_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Initialize metadata
      - Start in probation
      - Warm-start from ghost decayed frequency when available
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Advance global aging parameter L to evicted H
      - Store ghost decayed frequency snapshot
      - Trim ghost to a bounded size
      - Remove all live metadata of evicted key
    """
    global g_L

    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Advance L to the evicted priority (GreedyDual aging)
    # This ensures relative priorities decay without per-item work.
    g_L = max(g_L, float(_last_evicted_H))

    # Record ghost DF snapshot (at 'now', decayed)
    df = _dfreq_now(ekey, now)
    if df > _EPS:
        g_dfreq[ekey] = float(df)
        g_last[ekey] = now

    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_H.pop(ekey, None)
    m_seg.pop(ekey, None)
    m_dfreq.pop(ekey, None)
    m_dfreq_ts.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcks4gm0v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_p9ulxq5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4la_b2ci.pickle

Iteration 60: New subsample score 0.15669899999999998 is not better than old score 0.290196, skipping
Iteration 61: Selected program 19 score: 0.24042418749999997
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6f25qwkp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4aq706s4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph5rctra6.pickle

Iteration 61: Proposed new text for program: # Count-based Windowed TinyLFU with Segmented Main (W/P/S), Doorkeeper,
# and ARC-like adaptive window sizing. This version is count-aware (object count
# is the capacity), removing size-based routing/scoring to better match the judge.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment counts (by number of objects, not bytes)
m_cnt_W = 0
m_cnt_P = 0
m_cnt_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Doorkeeper (filters first occurrences to reduce TinyLFU noise)
DOOR_MAX = 32768
m_door = dict()          # key -> last_seen_time (LRU-ish by timestamp)
m_door_ops = 0
DOOR_DECAY_EVERY = 50000 # periodically clear to avoid unbounded bias

# Adaptive window sizing (ARC-inspired, by fraction of entry count)
m_W_frac = 0.25          # start with 25% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Adaptive protected fraction within main
m_S_frac = 0.80          # start with 80% of main in protected
S_FRAC_MIN = 0.50
S_FRAC_MAX = 0.95
S_FRAC_STEP = 0.02
m_s_promotions = 0
m_s_demotions = 0
m_last_s_adjust_access = 0
S_ADAPT_EVERY = 30000

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
ADMIT_EPS = 1e-12
EPS = 1e-12

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    # 64-bit mix for stability
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        # periodic aging (halve all counters)
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _targets(cache_snapshot):
    """
    Compute count-based targets for Window and Protected segments.
    """
    cap = max(1, int(cache_snapshot.capacity))
    # Window target: fraction of capacity, at least 1 and at most capacity
    w_target = max(1, min(cap, int(cap * m_W_frac)))
    main_target = max(0, cap - w_target)
    # Protected target: fraction of main
    s_target = int(main_target * m_S_frac)
    s_target = max(0, min(main_target, s_target))
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) lexicographic key
    """
    best_k = None
    best_la = None
    for k in cache_snapshot.cache.keys():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        if best_k is None or la < best_la or (la == best_la and k < best_k):
            best_k, best_la = k, la
    return best_k

def _promote(k, from_seg, to_seg):
    global m_cnt_W, m_cnt_P, m_cnt_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_cnt_W = max(0, m_cnt_W - 1)
    elif from_seg == SEG_PROBATION:
        m_cnt_P = max(0, m_cnt_P - 1)
    elif from_seg == SEG_PROTECTED:
        m_cnt_S = max(0, m_cnt_S - 1)

    if to_seg == SEG_WINDOW:
        m_cnt_W += 1
    elif to_seg == SEG_PROBATION:
        m_cnt_P += 1
    elif to_seg == SEG_PROTECTED:
        m_cnt_S += 1

    m_seg[k] = to_seg

def _insert_into_segment(key, seg):
    global m_cnt_W, m_cnt_P, m_cnt_S
    m_seg[key] = seg
    if seg == SEG_WINDOW:
        m_cnt_W += 1
    elif seg == SEG_PROBATION:
        m_cnt_P += 1
    elif seg == SEG_PROTECTED:
        m_cnt_S += 1

def _maybe_adjust_S_frac(cache_snapshot):
    global m_s_promotions, m_s_demotions, m_last_s_adjust_access, m_S_frac
    now = cache_snapshot.access_count
    if now - m_last_s_adjust_access < S_ADAPT_EVERY:
        return
    m_last_s_adjust_access = now
    promos = m_s_promotions
    demos = m_s_demotions
    # reset counters
    m_s_promotions = 0
    m_s_demotions = 0
    if promos > demos * 1.10:
        m_S_frac = min(S_FRAC_MAX, m_S_frac + S_FRAC_STEP)
    elif demos > promos * 1.10:
        m_S_frac = max(S_FRAC_MIN, m_S_frac - S_FRAC_STEP)

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_cnt_S, m_s_demotions
    # Demote until under target by a small loop (usually 0 or 1 iterations)
    while m_cnt_S > s_target:
        victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
        if victim_key is None:
            return
        _promote(victim_key, SEG_PROTECTED, SEG_PROBATION)
        m_s_demotions += 1

def _door_should_count(cache_snapshot, key):
    """
    Doorkeeper: returns True if this key should be counted into the TinyLFU sketch now.
    On first sight in a short window, we record it but do not count. On repeat, we count.
    Periodically reset to avoid stale positives.
    """
    global m_door_ops, m_door
    now = cache_snapshot.access_count
    m_door_ops += 1
    if key in m_door:
        # repeat within door horizon -> count
        m_door[key] = now
        # Aging/prune
        if len(m_door) > DOOR_MAX:
            # evict oldest by timestamp
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return True
    else:
        # first occurrence -> record only
        m_door[key] = now
        if len(m_door) > DOOR_MAX:
            oldest_k = None
            oldest_t = None
            for dk, dt in m_door.items():
                if oldest_k is None or dt < oldest_t or (dt == oldest_t and dk < oldest_k):
                    oldest_k, oldest_t = dk, dt
            if oldest_k is not None:
                m_door.pop(oldest_k, None)
        if m_door_ops >= DOOR_DECAY_EVERY:
            m_door.clear()
            m_door_ops = 0
        return False

def _score_for_key(cache_snapshot, key):
    """
    Frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = TinyLFU_estimate
    """
    return _sketch_estimate(key)

def _score_for_new(obj):
    return _sketch_estimate(obj.key)

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

def _p_victim_by_score(cache_snapshot, exclude_key=None):
    """
    Choose a victim in Probation with the lowest TinyLFU score.
    Tie-breakers:
      1) lower score
      2) older last_access
      3) lexicographic key
    """
    best_k = None
    best_score = None
    best_la = None
    for k in cache_snapshot.cache.keys():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != SEG_PROBATION:
            continue
        sc = _score_for_key(cache_snapshot, k)
        la = m_last_access.get(k, -1)
        if best_k is None:
            best_k, best_score, best_la = k, sc, la
            continue
        if sc < best_score - EPS:
            best_k, best_score, best_la = k, sc, la
            continue
        if abs(sc - best_score) <= EPS:
            if la < best_la or (la == best_la and k < best_k):
                best_k, best_score, best_la = k, sc, la
    return best_k

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision (count-based W-TinyLFU):
      - Keep Window close to its target (evict W-LRU if oversized).
      - On predicted Window overflow at insertion:
         * Compare new item's score vs. the worst (by TinyLFU score) victim in Probation.
         * If new wins, evict that P victim (admit); else evict W-LRU (reject to keep W bounded).
      - If no predicted overflow, evict from P (by worst score); if empty, fall back to W-LRU; then S-LRU.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_score = _score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_cnt_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_cnt_W + 1) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)

    if predict_overflow:
        p_victim = _p_victim_by_score(cache_snapshot)
        if p_victim is None:
            # No P items; evict from W if possible, else from S
            if w_lru is not None:
                return w_lru
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            return s_lru
        else:
            # Admission comparison: frequency-aware score
            p_score = _score_for_key(cache_snapshot, p_victim)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict P victim; W-LRU (if any) will be moved to P in update_after_insert
                return p_victim
            else:
                # Reject: evict from W to keep W within bound
                if w_lru is not None:
                    return w_lru
                # If W empty, evict from P anyway
                return p_victim

    # No predicted W overflow: evict from P by worst score if possible; else W; else S.
    p_victim = _p_victim_by_score(cache_snapshot)
    if p_victim is not None:
        return p_victim
    if w_lru is not None:
        return w_lru
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    for k in cache_snapshot.cache.keys():
        la = m_last_access.get(k, -1)
        if best_k is None or la < best_la or (la == best_la and k < best_k):
            best_k, best_la = k, la
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - W: keep in W (recency buffer behavior).
      - P: promote to S.
      - S: refresh recency.
      - Enforce S target by demoting S-LRU to P if needed.
      - Periodically adapt S fraction based on promotions/demotions balance.
    """
    _ensure_sketch(cache_snapshot)

    # On hit, always count into TinyLFU
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing

    if seg == SEG_WINDOW:
        # window is pure recency: keep in W, just refresh timestamp
        pass
    elif seg == SEG_PROBATION:
        # Promote to protected
        _promote(key, SEG_PROBATION, SEG_PROTECTED)
        # Track promotions for S-fraction adaptation
        global m_s_promotions
        m_s_promotions += 1
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency; structure unchanged
        pass

    _maybe_adjust_S_frac(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Doorkeeper: only count into TinyLFU if it is not the first sighting in short horizon.
      - Adaptive window sizing via ghosts:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - Insert into Window (recency admission).
      - If Window exceeds its target, move its LRU to P (keeps W near target).
      - Keep Protected within target by demoting S-LRU to P when needed.
    """
    _ensure_sketch(cache_snapshot)

    # Doorkeeper filtering (reduce sketch noise from one-timers)
    if _door_should_count(cache_snapshot, obj.key):
        _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window (count-based)
    _insert_into_segment(key, SEG_WINDOW)

    # Keep W within its target by moving its LRU to P if needed
    w_target, _ = _targets(cache_snapshot)
    if m_cnt_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is not None:
            _promote(candidate, SEG_WINDOW, SEG_PROBATION)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)

    _maybe_adjust_S_frac(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment counts accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    global m_cnt_W, m_cnt_P, m_cnt_S

    if seg == SEG_WINDOW:
        m_cnt_W = max(0, m_cnt_W - 1)
    elif seg == SEG_PROBATION:
        m_cnt_P = max(0, m_cnt_P - 1)
    elif seg == SEG_PROTECTED:
        m_cnt_S = max(0, m_cnt_S - 1)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
    # Clean doorkeeper entry to avoid stale growth
    m_door.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnszei9hy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwn1drnmi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptlsl4f9o.pickle

Iteration 61: New subsample score 0.240794 is not better than old score 0.243072, skipping
Iteration 62: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppq4ppbh0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph4aiqrn2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7gg9e_e3.pickle

Iteration 62: Proposed new text for program: # ARC+Predict: Adaptive Recency/Frequency with size-awareness and reuse-time prediction
# - Core policy: ARC-style two segments (T1 probation, T2 protected) with adaptive target p.
# - Eviction victim is chosen from T1 or T2 per ARC's rule, then ranked by a size-aware,
#   prediction- and frequency-aware score within that segment.
# - Frequency is lightly decayed by time-since-last-access to resist stale popularity.
# - Ghosts (B1, B2) steer the adaptive balance, while separate ghosts keep reuse-time seeds.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_last_gap = dict()      # key -> float (last observed gap)
m_freq = dict()          # key -> int (hit count)
m_stage = dict()         # key -> int (0=T1 probation, 1=T2 protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Predictor ghost history (for warm starts)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order for trimming

# ARC ghost lists for adaptation
b1_set = dict()          # key -> True if in B1 (ghost of T1)
b2_set = dict()          # key -> True if in B2 (ghost of T2)
b1_stamp = dict()        # key -> int (version counter for B1)
b2_stamp = dict()        # key -> int (version counter for B2)
b1_order = deque()       # LRU order of B1 ghosts: (key, stamp)
b2_order = deque()       # LRU order of B2 ghosts: (key, stamp)

# ARC target for T1 size (by count). Float; bounded to [0, live_count].
arc_p = 0.0

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25            # per-key EWMA learning rate
GLOBAL_BETA = 0.005         # global EWMA learning rate
SIZE_EXP = 1.0              # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.7             # frequency dampening in denominator
DECAY_HALF_LIFE = 1024.0    # accesses for frequency half-life (time-based decay)
DEFAULT_MU_MULT = 8.0       # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_EPS = 0.1           # small epsilon for overdue items (avoid evicting imminents)
GHOST_LIMIT_MIN = 1024      # minimum predictor ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0    # predictor ghost capacity  factor * live cache item count
ARC_GHOST_FACTOR = 1.0      # ARC ghost size  factor * live cache item count
_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _live_count(cache_snapshot):
    return max(1, len(getattr(cache_snapshot, "cache", {}) or {}))

def _predictor_ghost_limit(cache_snapshot):
    live_cnt = _live_count(cache_snapshot)
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _arc_ghost_limit(cache_snapshot):
    # Limit for each of B1 and B2
    live_cnt = _live_count(cache_snapshot)
    return max(64, int(ARC_GHOST_FACTOR * live_cnt))

def _trim_predictor_ghosts(cache_snapshot):
    limit = _predictor_ghost_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _trim_arc_ghosts(cache_snapshot):
    limit = _arc_ghost_limit(cache_snapshot)
    # Trim B1
    while len(b1_set) > limit and b1_order:
        k, s = b1_order.popleft()
        if b1_stamp.get(k, None) == s:
            b1_set.pop(k, None)
            b1_stamp.pop(k, None)
    # Trim B2
    while len(b2_set) > limit and b2_order:
        k, s = b2_order.popleft()
        if b2_stamp.get(k, None) == s:
            b2_set.pop(k, None)
            b2_stamp.pop(k, None)

def _add_b1(key):
    b1_set[key] = True
    s = b1_stamp.get(key, 0) + 1
    b1_stamp[key] = s
    b1_order.append((key, s))

def _add_b2(key):
    b2_set[key] = True
    s = b2_stamp.get(key, 0) + 1
    b2_stamp[key] = s
    b2_order.append((key, s))

def _remove_from_b1(key):
    if key in b1_set:
        b1_set.pop(key, None)
        b1_stamp.pop(key, None)

def _remove_from_b2(key):
    if key in b2_set:
        b2_set.pop(key, None)
        b2_stamp.pop(key, None)

def _effective_freq(key, now):
    # Time-decayed frequency using half-life w.r.t. time since last access
    f = float(m_freq.get(key, 0))
    la = m_last_access.get(key, None)
    if f <= 0.0 or la is None:
        return 0.0
    age = max(0.0, float(now - la))
    if DECAY_HALF_LIFE <= 0:
        return f
    decay = 0.5 ** (age / DECAY_HALF_LIFE)
    return f * decay

def _predicted_delta(key, now):
    """
    Predict non-negative time-to-next-access.
    Uses min(EWMA, last_gap) for responsiveness, overdue -> small epsilon.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    lg = m_last_gap.get(key, None)
    if mu is None or la is None:
        return _INF
    base = float(mu)
    if lg is not None:
        base = min(base, float(lg))
    next_t = float(la) + base
    if now <= next_t:
        return float(next_t - now)
    # overdue: likely imminent, do not penalize
    return OVERDUE_EPS

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from predictor ghosts if available.
    - Else seed from global mu with conservative multiplier.
    - Start in probation (T1) by default; may be overridden by ARC admission.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now
    m_last_gap[key] = m_mu.get(key, float(DEFAULT_MU_MULT * m_global_mu))
    m_stage[key] = 0  # default probation (T1)

def _update_predictor_on_hit(key, now):
    """
    Update predictor on hit:
    - EWMA of inter-arrival time with fixed beta.
    - Update global mu slowly.
    - Increment frequency counter.
    """
    global m_mu, m_last_access, m_freq, m_global_mu, m_last_gap

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_last_gap[key] = gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Defensive: seed if missing
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_last_gap[key] = m_mu[key]

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_predictor_ghost(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _segment_counts(cache_snapshot):
    t1 = 0
    t2 = 0
    for k in (cache_snapshot.cache or {}).keys():
        if m_stage.get(k, 0) >= 1:
            t2 += 1
        else:
            t1 += 1
    return t1, t2

def _bounded_arc_p(cache_snapshot):
    global arc_p
    live_cnt = _live_count(cache_snapshot)
    if arc_p < 0.0:
        arc_p = 0.0
    elif arc_p > float(live_cnt):
        arc_p = float(live_cnt)

def _eviction_score_within_segment(key, obj, now):
    """
    Higher score => evict sooner.
    Combines:
      - age: LRU pressure
      - predicted delta: time to next access (larger => colder)
      - frequency: dampens eviction (decayed)
      - size-aware normalization
    """
    sz = _size_of(obj)
    la = m_last_access.get(key, now)
    age = max(0.0, float(now - la))
    delta = _predicted_delta(key, now)
    eff_freq = _effective_freq(key, now)

    # Numerator prefers cold, old items
    numer = age + delta

    # Denominator penalizes big items and favors frequent ones
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0.0, eff_freq)))

    return numer / denom

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    ARC-driven segment choice, then size/prediction-aware victim within that segment.
    Tie-breakers within equal scores:
      1) Older last access (LRU among equals)
      2) Larger size (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _bounded_arc_p(cache_snapshot)

    # Decide which segment to evict from (ARC rule)
    t1_cnt, t2_cnt = _segment_counts(cache_snapshot)
    target_t1 = int(max(1, math.floor(arc_p)))
    evict_from_t1 = t1_cnt > target_t1

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        stage = m_stage.get(k, 0)
        in_t1 = (stage == 0)
        if evict_from_t1 and not in_t1:
            continue
        if (not evict_from_t1) and in_t1:
            continue

        s = _eviction_score_within_segment(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    # Fallback: if segment empty due to metadata mismatch, choose global worst
    if best_key is None:
        for k, v in cache.items():
            s = _eviction_score_within_segment(k, v, now)
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if best_key is None or s > best_score or (
                s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))
            ):
                best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update predictor (EWMA, last gap, global mu).
    - Increment frequency.
    - Promote to protected (T2) if currently probation (T1).
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Seed if missing (defensive)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Keep stage as-is if already set, else probation
        m_stage[key] = m_stage.get(key, 0)

    _update_predictor_on_hit(key, now)

    # ARC move: T1 -> T2 on hit
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1  # promote to protected (T2)
    else:
        # T2 hit: stay in T2; timestamp already updated by predictor
        pass


def update_after_insert(cache_snapshot, obj):
    """
    After insert (on miss):
    - ARC adaptation via ghost hits:
        * If key in B1: increase p (favor recency), admit into T2.
        * If key in B2: decrease p (favor frequency), admit into T2.
        * Else: admit into T1.
    - Initialize predictor (warm-start from predictor ghosts if available).
    - Remove the key from ARC ghosts if present.
    """
    global arc_p
    key = obj.key
    now = _now(cache_snapshot)

    # ARC adaptation using ghost hits
    in_b1 = key in b1_set
    in_b2 = key in b2_set
    # Live counts for scaling step
    b1n = len(b1_set)
    b2n = len(b2_set)

    if in_b1:
        # Favor recency: increase p
        step = max(1.0, float(b2n) / max(1.0, float(b1n)))
        arc_p += step
        _bounded_arc_p(cache_snapshot)
        _remove_from_b1(key)
        # Reinsert goes to T2 (frequent)
        desired_stage = 1
    elif in_b2:
        # Favor frequency: decrease p
        step = max(1.0, float(b1n) / max(1.0, float(b2n)))
        arc_p -= step
        _bounded_arc_p(cache_snapshot)
        _remove_from_b2(key)
        # Reinsert goes to T2 (frequent)
        desired_stage = 1
    else:
        # New key: goes to T1 (probation)
        desired_stage = 0

    # Seed predictor metadata
    _seed_predictor_on_insert(key, now)
    m_stage[key] = desired_stage


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record predictor ghost (EWMA / freq) for warm-start on future insert.
    - Add the key to ARC ghost list B1 or B2 depending on its resident segment.
    - Trim both predictor and ARC ghost stores to bounded sizes.
    - Remove live metadata for the evicted key.
    """
    ekey = evicted_obj.key

    # Predictor ghost
    _record_predictor_ghost(ekey)

    # ARC ghost: track which segment it was evicted from
    stage = m_stage.get(ekey, 0)
    if stage >= 1:
        _add_b2(ekey)
    else:
        _add_b1(ekey)

    # Trim ghosts
    _trim_predictor_ghosts(cache_snapshot)
    _trim_arc_ghosts(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_last_gap.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgw4rzhl7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzozk7649.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz6ani0bu.pickle

Iteration 62: New subsample score 0.41818299999999997 is better than old score 0.227095. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2umamyyu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpivgipk5v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd4t4x_m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph2n_b7ip.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvcbkbnrd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpewk45nbc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9n24ee54.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjqf4sbrs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpilrlgf7b.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqs9nw2_4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbwn5ve8t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpngty35vc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkepfg075.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0wjjyln9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5zs6a46j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe1114z37.pickle

Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp86oanf2o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm7q1mcr7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp70_cbryy.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu_svjvdr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4_1py9fv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwfqg2jv9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpabbbptt1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptq62ufjd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp_7oct2g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptxjg1rbf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_ea65wf_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvh8l5p5j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp64rr8935.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6ib4ar0e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp06e_mwni.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1rpz1cuo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf4atuv11.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6cepbe6q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkuz120n7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpttpfks37.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbeeqlx3v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9h5ofckp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3751140c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0nfmv_86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkxw0atpt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqpa42dlw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk49jdvbk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp29ijmi2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppc_0f3bx.pickle

Iteration 62: Full valset score for new program: 0.24045458333333325
Iteration 62: Full train_val score for new program: 0.24045458333333325
Iteration 62: Individual valset scores for new program: [0.511623, 0.485699, 0.492394, 0.446608, 0.506659, 0.493601, 0.271531, 0.497247, 0.536289, 0.531017, 0.1, 0.41119, 0.066468, 0.0, 0.021379, 0.020992, 0.0, 0.0, 0.022782, 0.271311, 0.378564, 0.0, 0.058672, 0.058672, 0.272557, 0.331653, 0.77855, 0.891494, 0.06499, 0.038636, 0.045558, 0.032632, 0.036934, 0.754255, 0.078947, 0.067079, 0.020211, 0.641422, 0.125461, 0.073191, 0.07025, 0.049436, 0.052632, 0.233333, 0.035603, 0.067321, 0.466258, 0.130719]
Iteration 62: New valset pareto front scores: [0.511623, 0.485699, 0.492394, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.366935, 0.853029, 0.891494, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.163283, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 62: Full valset pareto front score: 0.2773460000000001
Iteration 62: Updated valset pareto front programs: [{27}, {27}, {27}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}, {7, 14, 23, 24, 25}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {24, 17, 14, 25}, {16}, {17, 14}, {17, 14}, {24}, {3}, {8, 7}, {27}, {14, 17, 23, 24, 25}, {18}, {6, 14, 17, 24, 25}, {24, 25, 14}, {24, 25, 14}, {23}, {24, 25}, {24, 25}, {24, 25, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27}, {14}, {24, 25}, {24, 25, 14}, {24, 25}, {3, 5, 6, 7, 8, 17}, {24, 25, 14, 23}, {24, 25, 14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27}, {24}]
Iteration 62: Best valset aggregate score so far: 0.25787243750000005
Iteration 62: Best program as per aggregate score on train_val: 25
Iteration 62: Best program as per aggregate score on valset: 25
Iteration 62: Best score on valset: 0.25787243750000005
Iteration 62: Best score on train_val: 0.25787243750000005
Iteration 62: Linear pareto front program index: 25
Iteration 62: New program candidate index: 27
Iteration 63: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa14zq6z0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_6whe3mn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnokm4gbe.pickle

Iteration 63: Proposed new text for program: # Size-Aware ARC with Hyperbolic Tail Sampling (SA-ARC-HTS)
# - Retains ARC's adaptive split between "recent" (T1) and "frequent" (T2)
#   using ghost lists B1/B2 and a target T1 size p_bytes (all in bytes).
# - Improves eviction by sampling several LRU-tail candidates from both T1 and T2
#   and selecting the one with the lowest "value density" score:
#       score = ((freq + 1) / size) * (1 / (1 + age / HALF_LIFE)) * segment_bias
#   where age is since last access, favoring eviction of large, old, infrequent items.
# - Preserves O(1) average-time per access; eviction examines a small bounded number
#   of keys from tails (default 6 from T1 and 4 from T2).
# - Maintains per-key last access time and a bounded frequency counter.
# - Ghost lists bounded to at most 2x capacity bytes combined.
#
# This hybrid tends to:
#   - Evict big low-value items earlier (size-aware).
#   - Protect T2 items somewhat more than T1 (segment bias).
#   - Adapt T1/T2 target via ARC's ghost hits, in bytes.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
T1 = OrderedDict()  # key -> None (LRU -> MRU)
T2 = OrderedDict()  # key -> None (LRU -> MRU)

# Ghost segments (keys only)
B1 = OrderedDict()  # key -> None
B2 = OrderedDict()  # key -> None

# Segment assignment for live keys: 0 = T1, 1 = T2
m_seg = dict()      # key -> int

# Last known size for keys (live or ghost)
m_size = dict()     # key -> int

# Frequency counter (bounded, lightweight)
m_freq = dict()     # key -> int
FREQ_CLIP = 64

# Last access time (by access_count from snapshot) for live keys
m_time = dict()     # key -> int

# Byte accounting for segments
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# Target bytes for T1 (adaptation knob)
p_bytes = None

# Ghost sizing
# Keep ghosts bounded by at most 2x capacity bytes (combined B1+B2).
GHOST_BYTES_FACTOR = 2.0

# Eviction sampling parameters
TAIL_SCAN_T1 = 6
TAIL_SCAN_T2 = 4

# Recency decay parameter (in "accesses"); larger = slower decay
HALF_LIFE = 8000

# Segment protection bias: make T2 a bit harder to evict than T1
PROTECT_FACTOR_T2 = 1.25
# Bias to prefer evicting from T1 when T1 exceeds its target, or protect when below
T1_OVERSHOOT_BIAS = 0.85  # reduce score to make T1 candidates more evictable when oversized
T1_UNDERSHOOT_BIAS = 1.15 # increase score to make T1 candidates less evictable when undersized


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _ensure_p(snapshot):
    global p_bytes
    cap = _cap(snapshot)
    if p_bytes is None:
        p_bytes = cap // 2
    else:
        if cap > 0:
            if p_bytes < 0:
                p_bytes = 0
            elif p_bytes > cap:
                p_bytes = cap
        else:
            p_bytes = 0

def _trim_ghosts(snapshot):
    # Ensure B1+B2 total bytes <= factor * capacity
    global b1_bytes, b2_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        # Clear ghosts if capacity is zero
        B1.clear()
        B2.clear()
        b1_bytes = 0
        b2_bytes = 0
        return
    limit = int(GHOST_BYTES_FACTOR * max(1, cap))
    while (b1_bytes + b2_bytes) > limit and (B1 or B2):
        # Prefer shrinking the larger ghost
        target = B1 if b1_bytes >= b2_bytes else B2
        if target:
            k, _ = target.popitem(last=False)
            sz = m_size.get(k, 1)
            if target is B1:
                b1_bytes = max(0, b1_bytes - sz)
            else:
                b2_bytes = max(0, b2_bytes - sz)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _lru_key_of(od):
    if not od:
        return None
    for k in od:
        return k
    return None

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front of the provided OrderedDict if they are not in cache.
    Adjust byte counters accordingly using m_size.
    """
    global t1_bytes, t2_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = _lru_key_of(od)
        if k is None:
            break
        if k not in cache:
            # stale; drop it
            _ = od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                t1_bytes = max(0, t1_bytes - sz)
                m_seg.pop(k, None)
            else:
                t2_bytes = max(0, t2_bytes - sz)
                m_seg.pop(k, None)
            changed = True

def _ensure_tracking_on_hit(snapshot, obj):
    # Ensure we track size, bounded frequency, and last access time
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    m_freq[k] = min(FREQ_CLIP, m_freq.get(k, 0) + 1)
    m_time[k] = _now(snapshot)

def _peek_lru_keys(od, k_max, cache):
    """Return up to k_max keys from the LRU side that are still present in the cache."""
    keys = []
    if not od or k_max <= 0:
        return keys
    count = 0
    for k in od:
        if k in cache:
            keys.append(k)
            count += 1
            if count >= k_max:
                break
    return keys

def _candidate_score(k, seg_id, now):
    """
    Lower scores are more evictable.
    score = ((freq + 1) / size) * (1 / (1 + age / HALF_LIFE)) * segment_bias * t1_target_bias
    """
    global t1_bytes, p_bytes
    sz = max(1, int(m_size.get(k, 1)))
    freq = int(m_freq.get(k, 0))
    last = int(m_time.get(k, 0))
    age = max(0, now - last)

    # Value density: frequency per byte, decayed by recency
    base = (freq + 1.0) / float(sz)
    age_factor = 1.0 / (1.0 + (age / float(HALF_LIFE)))  # in (0,1]

    score = base * age_factor

    # Segment protection bias: protect T2 a bit
    if seg_id == 1:
        score *= PROTECT_FACTOR_T2

    # ARC target bias: if T1 is oversized, make T1 items more evictable; else protect them
    if seg_id == 0:
        if t1_bytes > p_bytes:
            score *= T1_OVERSHOOT_BIAS
        else:
            score *= T1_UNDERSHOOT_BIAS

    return score


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using size-aware ARC with hyperbolic tail sampling:
    - Sample up to TAIL_SCAN_T1 keys from T1's LRU and TAIL_SCAN_T2 from T2's LRU.
    - Compute a size/frequency/recency-based score and evict the lowest-scored key.
    - If structures are empty or out of sync, fall back to any key in cache (prefer largest size).
    """
    global t1_bytes, t2_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_p(cache_snapshot)

    # Defensive cleanup at the heads
    _cleanup_stale_front(cache_snapshot, T1, seg_id=0)
    _cleanup_stale_front(cache_snapshot, T2, seg_id=1)

    now = _now(cache_snapshot)

    # Gather candidates from both segments
    cands = []
    if T1:
        for k in _peek_lru_keys(T1, TAIL_SCAN_T1, cache):
            cands.append((k, 0))
    if T2:
        for k in _peek_lru_keys(T2, TAIL_SCAN_T2, cache):
            cands.append((k, 1))

    # If no internal tracking, fallback to largest object in cache to free bytes faster
    if not cands:
        # Out of sync: pick the largest resident object
        victim = None
        max_sz = -1
        for k, o in cache.items():
            s = _size_of(o)
            if s > max_sz:
                max_sz = s
                victim = k
        return victim

    # Select the most evictable candidate
    best_key = None
    best_score = float("inf")
    for k, seg in cands:
        s = _candidate_score(k, seg, now)
        if s < best_score:
            best_score = s
            best_key = k

    # As a final guard, if somehow best_key vanished, fallback to LRU in biased segment
    if best_key is None:
        # ARC-like fallback
        choose_T1 = False
        if T1 and T2:
            choose_T1 = (t1_bytes > p_bytes)
        elif T1:
            choose_T1 = True
        elif T2:
            choose_T1 = False
        else:
            # pick any
            for k in cache:
                return k
            return None
        if choose_T1:
            k = _lru_key_of(T1)
            return k if k is not None else _lru_key_of(T2)
        else:
            k = _lru_key_of(T2)
            return k if k is not None else _lru_key_of(T1)

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update size, time, and frequency (bounded).
    - If in T1: promote to T2.
    - If in T2: refresh recency.
    - If untracked (rare): insert directly into T2 as it's at least a second touch.
    """
    global t1_bytes, t2_bytes
    k = obj.key
    sz = _size_of(obj)
    _ensure_p(cache_snapshot)
    _ensure_tracking_on_hit(cache_snapshot, obj)

    if k in T1:
        # Promote T1 -> T2
        T1.pop(k, None)
        t1_bytes = max(0, t1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in T2:
        # Refresh recency
        _move_to_mru(T2, k)
        m_seg[k] = 1
    else:
        # Defensive: not tracked but a hit => treat as frequent
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1

    # Ensure size/time are up-to-date
    m_size[k] = sz
    m_time[k] = _now(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - If key is in B1: increase p_bytes and place in T2 (recency helpful).
    - If key is in B2: decrease p_bytes and place in T2 (frequency helpful).
    - Else: place in T1 (probation).
    - Do not increment frequency on insert; hits will do that.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, p_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)

    # Update size/time tracking; leave freq unchanged on cold insert
    m_size[k] = sz
    m_time[k] = now
    m_freq[k] = m_freq.get(k, 0)

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    # Adaptation magnitude scaled to avoid very slow shifts for tiny items
    adapt_step = max(sz, cap // 64)

    if k in B1:
        # Favor recency: grow T1 target; move B1 -> T2
        p_bytes = min(cap, p_bytes + adapt_step)
        if k in B1:
            B1.pop(k, None)
            b1_bytes = max(0, b1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in B2:
        # Favor frequency: shrink T1 target; move B2 -> T2
        p_bytes = max(0, p_bytes - adapt_step)
        if k in B2:
            B2.pop(k, None)
            b2_bytes = max(0, b2_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    else:
        # New key: goes to T1 (probation)
        T1[k] = None
        t1_bytes += sz
        m_seg[k] = 0

    _trim_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting a victim:
    - Move the evicted key to the corresponding ghost list (B1 if from T1, B2 if from T2).
    - Update byte accounting and clean live metadata.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes
    _ensure_p(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)
    # Record last known size/time for ghost usage
    m_size[ekey] = esz
    # Don't carry over last access time for any math, but keep as a weak hint
    # (we won't use m_time[ekey] for ghosts)

    seg = m_seg.get(ekey, None)

    if seg == 0:
        # From T1 -> B1
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        B1[ekey] = None
        b1_bytes += esz
    elif seg == 1:
        # From T2 -> B2
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)
        B2[ekey] = None
        b2_bytes += esz
    else:
        # Unknown segment (defensive): put into B1
        B1[ekey] = None
        b1_bytes += esz
        # Try to remove from T1/T2 if present
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)

    # Clean live metadata
    m_seg.pop(ekey, None)
    # Keep m_size[ekey] for ghost info; keep m_freq as lightweight history

    _trim_ghosts(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsrpgd9mz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc8frt3jf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0hqb7z1q.pickle

Iteration 63: New subsample score 0.892037 is better than old score 0.865541. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpis_s65m0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5nei6866.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1sld0qgg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5if4szmh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeizpkduu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpct0uutxz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy_858476.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcl3y6utt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuz6kurdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpghh985pu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyte9_wdo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaf28miag.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfy7xvgxo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps50k1r64.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_7ijqub1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu4xgbych.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2s_s7svu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpplm3c6hf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps7j49l1i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphnqrz5ra.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphkhaihcw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg1pm_1lz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1waoxnhn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfg_2kpkh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf3nttsxm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc001m5ai.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpie4qm6a5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_jyw7ppc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplmca7oy9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgq6b62zw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpidjgeqem.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptw1j2_0m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw5299oxc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjlhvcjn6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsoyb391_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5x_ujvw1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3lb51kqf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprwidq8hx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5c6o4o6h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuydp84aa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl_tipzly.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2479tlci.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8wycjw7a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9d9umh3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk1lpn28p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa4qbq8fu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvwvxyzu2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp80hd0uvy.pickle

Iteration 63: Full valset score for new program: 0.24792383333333334
Iteration 63: Full train_val score for new program: 0.24792383333333334
Iteration 63: Individual valset scores for new program: [0.509632, 0.484357, 0.493849, 0.443585, 0.50468, 0.491756, 0.272727, 0.478569, 0.540937, 0.531017, 0.091667, 0.35746, 0.040045, 0.0, 0.021237, 0.021133, 0.020062, 0.023615, 0.022782, 0.272227, 0.353982, 0.026556, 0.058672, 0.058672, 0.269971, 0.386089, 0.830189, 0.89235, 0.137167, 0.038636, 0.045558, 0.029555, 0.029983, 0.724983, 0.083333, 0.067961, 0.076688, 0.640392, 0.125461, 0.088994, 0.073305, 0.096091, 0.052632, 0.366667, 0.09217, 0.084993, 0.466258, 0.081699]
Iteration 63: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.89235, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.163283, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 63: Full valset pareto front score: 0.2777931875000001
Iteration 63: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}, {7, 14, 23, 24, 25}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {24, 17, 14, 25}, {16, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {28}, {14, 17, 23, 24, 25}, {18}, {6, 14, 17, 24, 25}, {24, 25, 14}, {24, 25, 14}, {23}, {24, 25}, {24, 25}, {24, 25, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28}, {14}, {24, 25}, {24, 25, 14}, {24, 25}, {3, 5, 6, 7, 8, 17, 28}, {24, 25, 14, 23}, {24, 25, 14, 23}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28}, {24}]
Iteration 63: Best valset aggregate score so far: 0.25787243750000005
Iteration 63: Best program as per aggregate score on train_val: 25
Iteration 63: Best program as per aggregate score on valset: 25
Iteration 63: Best score on valset: 0.25787243750000005
Iteration 63: Best score on train_val: 0.25787243750000005
Iteration 63: Linear pareto front program index: 25
Iteration 63: New program candidate index: 28
Iteration 64: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcomx77ku.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkzoftcr_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmu_0c5y2.pickle

Iteration 64: Proposed new text for program: # GreedyDual-Size-Frequency (GDSF) with lightweight, epoch-based frequency decay
# - Prioritizes items by benefit per byte and ages them using a global "clock" (L).
# - Evicts the item with the smallest priority H = L + (base + alpha*log2(1+freq)) / size.
# - On eviction, L is increased to the evicted item's H, aging the whole cache.
# - Frequencies are lazily decayed by epochs to avoid unbounded growth and drift.
# - All operations are O(log n) amortized using a min-heap (no full scans).
#
# Rationale:
# - Size-aware: cost/size favors retaining small, frequently used items (improves byte efficiency).
# - Frequency-aware: repeated hits increase priority logarithmically (resists scans and bursts).
# - Aging via L prevents stale items from lingering (robust across workloads).
# - Lazy decay keeps history helpful but self-limiting without expensive global passes.

from heapq import heappush, heappop
import math

# ----------------------
# Global state
# ----------------------
# Min-heap of (priority, version, key)
_heap = []

# Latest version per key (to ignore stale heap entries)
m_ver = dict()        # key -> int

# Latest computed priority per key (for updating L on eviction)
m_prio = dict()       # key -> float

# Last known size and lightweight frequency
m_size = dict()       # key -> int
m_freq = dict()       # key -> int (decayed, clipped)
m_freq_epoch = dict() # key -> int (last epoch this key was updated)

# Global aging clock (GreedyDual)
L_clock = 0.0

# Tunables
FREQ_CLIP = 255                   # Upper bound for per-key frequency
ALPHA = 3.0                       # Weight of frequency vs base cost (=1)
DECAY_WINDOW = 20000              # Accesses per epoch for frequency decay
# Note: larger DECAY_WINDOW preserves history longer; smaller decays faster.

# Cached epoch
_global_epoch = 0
_last_epoch_computed_at = -1


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _ensure_epoch(snapshot):
    """
    Compute the current epoch from access_count with caching to avoid recomputation.
    """
    global _global_epoch, _last_epoch_computed_at
    now = _now(snapshot)
    if now != _last_epoch_computed_at:
        _last_epoch_computed_at = now
        _global_epoch = now // max(1, DECAY_WINDOW)

def _decay_key_freq(k):
    """
    Lazy frequency decay: shift frequency right by the number of epochs elapsed for this key.
    Keeps O(1) update per key without scanning all keys.
    """
    e_now = _global_epoch
    e_prev = m_freq_epoch.get(k, e_now)
    d = e_now - e_prev
    if d > 0:
        f = m_freq.get(k, 0)
        # Right-shift decay approximates halving per epoch (fast, integer)
        if f > 0:
            f >>= min(d, 8)  # cap decay steps to avoid over-shifts
        m_freq[k] = f
        m_freq_epoch[k] = e_now
    else:
        # Ensure epoch presence for new keys
        if k not in m_freq_epoch:
            m_freq_epoch[k] = e_now

def _effective_cost_for_key(k):
    """
    Compute the effective 'cost' component from frequency with diminishing returns.
    base cost = 1
    frequency boost = ALPHA * log2(1 + freq)
    """
    f = m_freq.get(k, 0)
    return 1.0 + ALPHA * math.log2(1.0 + float(f))

def _priority_for(k, sz):
    """
    Priority according to GDSF:
      H = L_clock + (effective_cost / size)
    """
    if sz <= 0:
        sz = 1
    return float(L_clock) + (_effective_cost_for_key(k) / float(sz))

def _push_or_update_heap(k, H):
    """
    Insert a new (H, version, k) into the heap by bumping the version.
    Older heap entries for this key will become stale and be ignored later.
    """
    v = m_ver.get(k, 0) + 1
    m_ver[k] = v
    m_prio[k] = H
    heappush(_heap, (H, v, k))

def _evict_candidate_from_heap(cache_snapshot):
    """
    Pop from heap until a valid, in-cache, up-to-date entry is found.
    Return its key, or None if not found.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    while _heap:
        H, v, k = _heap[0]
        # Validate against current cache and version
        # - If key not in cache, it's stale; drop.
        # - If version mismatch, stale; drop.
        if k not in cache:
            heappop(_heap)
            continue
        if m_ver.get(k, None) != v:
            heappop(_heap)
            continue
        # Found a current entry
        return k
    return None


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose the eviction victim as the key with minimal GDSF priority among cached items.
    Falls back to any key if heap/bookkeeping is temporarily out-of-sync.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_epoch(cache_snapshot)

    # Return the best candidate from the heap, skipping stale entries
    k = _evict_candidate_from_heap(cache_snapshot)
    if k is not None:
        return k

    # Fallback: if heap has no valid entries (e.g., cold start), evict any key deterministically
    for k in cache:
        return k
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Increment decayed frequency for the key (with clipping).
    - Recompute its GDSF priority and push a new heap entry (lazy-update versioning).
    """
    _ensure_epoch(cache_snapshot)
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz

    # Decay then increment frequency
    _decay_key_freq(k)
    f = m_freq.get(k, 0)
    if f < FREQ_CLIP:
        m_freq[k] = f + 1

    # Recompute and update priority
    H = _priority_for(k, sz)
    _push_or_update_heap(k, H)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss served, object admitted):
    - Record size and decay existing frequency history if any.
    - Do NOT increment frequency here (it increments on hits).
    - Assign initial GDSF priority and push onto heap.
    """
    _ensure_epoch(cache_snapshot)
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz

    # Apply decay to any existing history for this key
    _decay_key_freq(k)
    # Frequency remains as decayed value (no bump on insert)
    f = m_freq.get(k, 0)
    m_freq[k] = min(f, FREQ_CLIP)

    # Initial priority
    H = _priority_for(k, sz)
    _push_or_update_heap(k, H)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After victim is evicted:
    - Advance global clock L to the priority of the evicted item (GDS aging).
    - Remove its heap bookkeeping (version will be ignored, entry lazily purged later).
    - Keep frequency with decay metadata to preserve lightweight history across evictions.
    """
    global L_clock
    _ensure_epoch(cache_snapshot)

    ekey = evicted_obj.key
    # Bump L to evicted priority if available
    H = m_prio.pop(ekey, None)
    if H is not None:
        if H > L_clock:
            L_clock = H

    # Remove version so any stale heap entries for this key are ignored
    m_ver.pop(ekey, None)
    # Keep m_freq/m_freq_epoch so that history persists but will decay over epochs
    # Keep m_size optional; it's not needed post-eviction, but harmless either way.
    # We'll drop it to reduce memory footprint.
    m_size.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy3xogv27.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9l4it0u7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplwwxaw5c.pickle

Iteration 64: New subsample score 0.42812 is not better than old score 0.488796, skipping
Iteration 65: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphymlzzli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf1wamj5h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz6lney1n.pickle

Iteration 65: Proposed new text for program: # LRB-Tiny2Q: Size-aware, predictive eviction with TinyLFU-informed protection
# Highlights:
# - Learns per-key inter-arrival (EWMA) and uses a soft-deadline hazard: overdue items
#   are penalized more (evicted sooner), while near-deadline items are protected.
# - Integrates a lightweight Count-Min Sketch (TinyLFU) across all requests to estimate
#   key popularity; used for promotion and eviction scoring.
# - 2Q-like segmentation with adaptive protection strength: protected discount adapts
#   from the hit distribution between probation vs protected, and weakens when an item
#   is overdue relative to its own mean gap.
# - Size-aware normalization: evicts large items earlier if utility is similar.
# - Compact ghost history for warm restarts; O(1) amortized trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (in-cache hits count)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical freq or TinyLFU est)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# TinyLFU (Count-Min Sketch) for global popularity estimation
_cms_w = 4096
_cms_d = 4
_cms = None              # list of depth arrays
_cms_seeds = (0x9E3779B1, 0x7F4A7C15, 0x85EBCA77, 0xC2B2AE3D)
_cms_last_decay_time = 0
_CMS_DECAY_SPAN = 200000  # decay roughly every this many accesses by halving all counters


# Adaptive protection EWMA counters
prot_hits_ewma = 1.0
prob_hits_ewma = 1.0

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25            # per-key EWMA learning rate
GLOBAL_BETA = 0.005         # global EWMA learning rate (slow)
SIZE_EXP = 1.0              # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.8             # dampening in denominator
TLFU_DAMP = 0.6             # weight of TinyLFU estimate in frequency term
DEFAULT_MU_MULT = 3.5       # default mu multiplier for cold inserts (scaled by global mu)

# Overdue handling: treat late arrivals as increasingly cold
OVERDUE_MULT = 1.0          # how strongly to penalize overdue (per time unit)
OVERDUE_CAP_MULT = 4.0      # cap overdue delta to this multiple of mu

# TinyLFU promotion and use
PROMOTE_THRESHOLD = 2       # promote to protected when CMS estimate >= this
CMS_DECAY_FACTOR = 0.5      # divide all counters by 2 on decay

# 2Q-like protection: adaptive discounting for protected items
PROT_BASE_DISCOUNT = 0.45   # baseline multiplicative discount for protected eviction score
PROT_MIN = 0.30             # min discount (strongest protection)
PROT_MAX = 0.90             # max discount (weakest protection)
PROT_ADAPT_STRENGTH = 0.6   # how much to adapt by hit-ratio signal
PROT_HAZARD_K = 1.0         # how fast protection decays with overdue hazard

# Ghost capacity bounds
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0

# Seeding penalty for very large objects (discourage unless reused)
SIZE_SEED_PENALTY = 3.0
SIZE_SEED_EXP = 1.0

_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _capacity(cache_snapshot):
    return max(1, int(getattr(cache_snapshot, "capacity", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _cms_init_if_needed():
    global _cms
    if _cms is None:
        _cms = [[0] * _cms_w for _ in range(_cms_d)]

def _cms_idx(key, seed, w):
    # Simple 64-bit mix; ensure stable hashing by combining Python hash with seed.
    h = hash(key) ^ seed
    # Mix bits
    h ^= (h >> 33) & 0xFFFFFFFFFFFFFFFF
    h *= 0xff51afd7ed558ccd & 0xFFFFFFFFFFFFFFFF
    h &= 0xFFFFFFFFFFFFFFFF
    h ^= (h >> 33) & 0xFFFFFFFFFFFFFFFF
    return int(h % w)

def _cms_maybe_decay(now):
    global _cms_last_decay_time
    if now - _cms_last_decay_time >= _CMS_DECAY_SPAN:
        _cms_last_decay_time = now
        # Halve all counters (approximate aging)
        for d in range(_cms_d):
            row = _cms[d]
            for i in range(_cms_w):
                row[i] = row[i] >> 1

def _cms_add(key, now):
    _cms_init_if_needed()
    _cms_maybe_decay(now)
    for d, seed in enumerate(_cms_seeds):
        idx = _cms_idx(key, seed, _cms_w)
        # Saturate at a reasonably large count to avoid overflow
        v = _cms[d][idx] + 1
        _cms[d][idx] = v if v < (1 << 31) else (1 << 31) - 1

def _cms_estimate(key):
    if _cms is None:
        return 0
    est = _INF
    for d, seed in enumerate(_cms_seeds):
        idx = _cms_idx(key, seed, _cms_w)
        v = _cms[d][idx]
        if v < est:
            est = v
    return int(est if est != _INF else 0)

def _protected_discount_dynamic():
    # Adapt discount based on recent hit distribution.
    # If protected hits dominate, strengthen protection (smaller factor).
    # If probation hits dominate, weaken protection (larger factor).
    p = float(prot_hits_ewma)
    q = float(prob_hits_ewma)
    ratio = p / max(1e-6, q)
    # Map ratio -> [-1, 1] smoothly
    bias = math.tanh(math.log(max(ratio, 1e-6)))
    # bias > 0 => more protected hits -> stronger protection (lower factor)
    adj = 1.0 - PROT_ADAPT_STRENGTH * 0.35 * bias
    disc = PROT_BASE_DISCOUNT * adj
    return max(PROT_MIN, min(PROT_MAX, disc))

def _predicted_overdue_delta(key, now):
    """
    Soft-deadline model:
      delta = next_t - now if ahead of predicted next access, else
      delta = min( (now - next_t) * OVERDUE_MULT, OVERDUE_CAP_MULT * mu )
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF, 0.0, 0.0  # delta, mu, hazard
    mu = float(max(1.0, mu))
    next_t = la + mu
    dd = float(next_t - now)
    if dd >= 0.0:
        # Not overdue
        return dd, mu, 0.0
    overdue = -dd
    delta = min(overdue * OVERDUE_MULT, OVERDUE_CAP_MULT * mu)
    # hazard relative to mu (how late)
    hazard = overdue / mu
    return delta, mu, hazard

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    score = (delta_eff / denom) * stage_factor
    denom = size^SIZE_EXP * (1 + FREQ_DAMP * (log1p(in-cache-freq) + TLFU_DAMP*log1p(cms_est)))
    stage_factor:
      - probation: 1.0
      - protected: adaptive discount in [PROT_MIN, PROT_MAX], which decays towards 1 with hazard
    """
    sz = _size_of(obj)
    delta, mu, hazard = _predicted_overdue_delta(key, now)

    # Frequency terms
    f_cache = m_freq.get(key, 1)
    f_cms = _cms_estimate(key)
    freq_term = math.log1p(max(0, f_cache - 1)) + TLFU_DAMP * math.log1p(max(0, f_cms))
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * freq_term)

    base = float(delta) / max(1e-9, denom)

    # Stage-based multiplier
    stage = m_stage.get(key, 0)
    if stage >= 1:
        base_disc = _protected_discount_dynamic()
        # Erode protection as hazard grows: from base_disc at hazard=0 to ~1 at large hazard
        stage_factor = 1.0 - (1.0 - base_disc) * math.exp(-PROT_HAZARD_K * max(0.0, hazard))
        base *= stage_factor

    return base

def _seed_predictor_on_insert(key, now, capacity, obj_size):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse a tempered version
    - Else, seed from global mu with a conservative multiplier scaled by size
    - New items start in probation (stage=0)
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        # Reuse ghosted popularity hint if available
        hint = int(g_freq.get(key, 1))
        m_freq[key] = max(1, hint)
    else:
        size_ratio = float(obj_size) / float(max(1, capacity))
        base_mu = DEFAULT_MU_MULT * float(m_global_mu)
        # Larger objects get a longer initial predicted gap to bias eviction until proven hot
        m_mu[key] = max(1.0, base_mu * (1.0 + SIZE_SEED_PENALTY * (size_ratio ** SIZE_SEED_EXP)))
        m_freq[key] = 1

    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment in-cache frequency.
    """
    global m_mu, m_last_access, m_freq, m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = float(m_mu.get(key, gap))
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold hit without prior metadata (rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    # Keep a tiny popularity hint (TinyLFU estimate is global)
    g_freq[evicted_key] = max(m_freq.get(evicted_key, 1), _cms_estimate(evicted_key))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the highest eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update TinyLFU
    - Update per-key EWMA of inter-arrival time
    - Update last access time and frequency
    - Promote to protected when TinyLFU estimate passes threshold
    - Update adaptive protection EWMAs
    """
    key = obj.key
    now = _now(cache_snapshot)

    # TinyLFU update
    _cms_add(key, now)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now, _capacity(cache_snapshot), _size_of(obj))
        # Do not double count in-cache freq; adjust to 0 prior to hit increment
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)

    # Adaptive stage handling
    stage = m_stage.get(key, 0)
    # Count hit into EWMA buckets
    global prot_hits_ewma, prob_hits_ewma
    if stage >= 1:
        prot_hits_ewma = 0.98 * prot_hits_ewma + 0.02
        prob_hits_ewma = 0.98 * prob_hits_ewma
    else:
        prob_hits_ewma = 0.98 * prob_hits_ewma + 0.02
        prot_hits_ewma = 0.98 * prot_hits_ewma

    # Promotion to protected when deemed popular by TinyLFU (or keep protected)
    if stage == 0:
        if _cms_estimate(key) >= PROMOTE_THRESHOLD:
            m_stage[key] = 1


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Update TinyLFU
    - Initialize per-key predictor (warm-start from ghost if available; else default scaled by size and global mu)
    - Set last access time and start in probation (stage=0)
    """
    key = obj.key
    now = _now(cache_snapshot)
    _cms_add(key, now)
    _seed_predictor_on_insert(key, now, _capacity(cache_snapshot), _size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key
    - Trim ghost store to a bounded size (O(1) amortized)
    - Remove per-key metadata of the evicted key
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoyb2wgs6.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpshzxylij.pickle

Iteration 65: New subsample score 0.880556 is not better than old score 1.129912, skipping
Iteration 66: Selected program 27 score: 0.24045458333333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq8qdi9jw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpst11we14.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkrs_y2g4.pickle

Iteration 66: Proposed new text for program: # GD-SIZE-F-RT: GreedyDual-Size with decayed Frequency and Reuse-Time boost
# - Core policy: GreedyDual-size-frequency (GDSF) priority with a global
#   watermark L. Items carry L_at_touch to embed recency; priority is
#   L_at_touch + (effective_freq + time_boost) / size^alpha.
# - effective_freq is time-decayed to resist stale popularity.
# - time_boost uses a simple reuse-time predictor (EWMA of gaps) to
#   give imminent items extra protection.
# - Ghost history retains per-key EWMA and frequency seeds for warm starts.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_last_gap = dict()      # key -> float (last observed gap)
m_freq = dict()          # key -> int (hit count since (re)insert)
m_Ltouch = dict()        # key -> float (global L at last touch: insert/hit)

# Global EWMA baseline for cold-start of mu
m_global_mu = 64.0

# GreedyDual watermark
gds_L = 0.0

# Predictor ghost history for warm starts
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order for trimming

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25            # per-key EWMA learning rate
GLOBAL_BETA = 0.005         # global EWMA learning rate
SIZE_EXP = 1.0              # size exponent (1.0 = per-byte fairness)
DECAY_HALF_LIFE = 512.0     # accesses for frequency half-life (time-based decay)
TIME_BOOST = 2.0            # extra pseudo-frequency for near-future hits
DEFAULT_MU_MULT = 8.0       # default mu multiplier for cold inserts (scaled by global mu)
GHOST_LIMIT_MIN = 1024      # minimum predictor ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0    # predictor ghost capacity  factor * live cache item count
EPS = 1e-9
_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _live_count(cache_snapshot):
    return max(1, len(getattr(cache_snapshot, "cache", {}) or {}))

def _predictor_ghost_limit(cache_snapshot):
    live_cnt = _live_count(cache_snapshot)
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_predictor_ghosts(cache_snapshot):
    limit = _predictor_ghost_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _effective_freq(key, now):
    # Time-decayed frequency using half-life w.r.t. time since last access
    f = float(m_freq.get(key, 0))
    la = m_last_access.get(key, None)
    if f <= 0.0 or la is None:
        return 0.0
    age = max(0.0, float(now - la))
    if DECAY_HALF_LIFE <= 0:
        return f
    decay = 0.5 ** (age / DECAY_HALF_LIFE)
    return f * decay

def _predicted_delta(key, now):
    """
    Predict non-negative time-to-next-access in access-count units.
    Uses EWMA of gaps, guarded by last observed gap.
    Overdue -> treat as imminent (delta = 0).
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    lg = m_last_gap.get(key, None)
    if mu is None or la is None:
        return _INF
    base = float(mu)
    if lg is not None:
        base = min(base, float(lg))
    next_t = float(la) + base
    if now <= next_t:
        return float(next_t - now)
    # overdue: likely imminent
    return 0.0

def _weight(key, obj, now):
    """
    Size-aware utility weight used by GDSF:
      w = (effective_freq + time_boost) / size^alpha
    where time_boost = TIME_BOOST / (1 + predicted_delta)
    """
    sz = float(_size_of(obj))
    eff_f = _effective_freq(key, now)
    delta = _predicted_delta(key, now)
    tboost = TIME_BOOST / (1.0 + max(0.0, float(delta)))
    denom = sz ** float(SIZE_EXP)
    return (eff_f + tboost) / max(1.0, denom)

def _priority(key, obj, now):
    """
    GreedyDual priority = L_at_touch + weight
    """
    return float(m_Ltouch.get(key, 0.0)) + _weight(key, obj, now)

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from predictor ghosts if available.
    - Else seed from global mu with conservative multiplier.
    """
    # Frequency warm start (clamped to avoid overboost)
    if key in g_freq:
        m_freq[key] = max(1, min(8, int(g_freq.get(key, 1))))
    else:
        m_freq[key] = 1

    # EWMA warm start
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_last_gap[key] = m_mu.get(key, float(DEFAULT_MU_MULT * m_global_mu))

def _update_predictor_on_hit(key, now):
    """
    Update predictor on hit:
    - EWMA of inter-arrival time with fixed beta.
    - Update global mu slowly.
    - Increment frequency counter.
    """
    global m_mu, m_last_access, m_freq, m_global_mu, m_last_gap

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_last_gap[key] = gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Defensive: seed if missing
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_last_gap[key] = m_mu[key]

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_predictor_ghost(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose victim with minimal GreedyDual priority:
      priority(k) = L_at_touch(k) + (effective_freq(k) + time_boost(k)) / size(k)^alpha
    Tie-breakers among equal priorities:
      1) Older last access (LRU)
      2) Larger size (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_pri = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        pri = _priority(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_pri, best_la, best_sz = k, pri, la, sz
            continue

        # GreedyDual evicts the minimum priority
        if pri < best_pri - EPS:
            best_key, best_pri, best_la, best_sz = k, pri, la, sz
            continue

        if abs(pri - best_pri) <= EPS:
            if la < best_la:
                best_key, best_pri, best_la, best_sz = k, pri, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_pri, best_la, best_sz = k, pri, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_pri, best_la, best_sz = k, pri, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update predictor (EWMA, last gap, global mu).
    - Increment frequency.
    - Refresh GreedyDual recency by resetting L_at_touch to current L.
    """
    global gds_L
    key = obj.key
    now = _now(cache_snapshot)

    # Defensive seeding if missing
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)

    _update_predictor_on_hit(key, now)

    # Refresh recency in GreedyDual
    m_Ltouch[key] = float(gds_L)


def update_after_insert(cache_snapshot, obj):
    """
    After insert (on miss):
    - Initialize predictor (warm-start from predictor ghosts if available).
    - Initialize frequency (warm-start, clamped).
    - Set GreedyDual L_at_touch to current L (admission at current watermark).
    """
    global gds_L
    key = obj.key
    now = _now(cache_snapshot)

    # Seed predictor metadata
    _seed_predictor_on_insert(key, now)

    # GreedyDual admission at current watermark
    m_Ltouch[key] = float(gds_L)

    # Drop ghosts for this key if any (optional cleanup)
    if key in g_mu or key in g_freq:
        # Keep the most recent sample; don't force remove to retain history if evicted soon after.
        pass


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Advance GreedyDual watermark L to the evicted key's priority.
    - Record predictor ghost (EWMA / freq) for warm-start on future insert.
    - Trim predictor ghost store to bounded size.
    - Remove live metadata for the evicted key.
    """
    global gds_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Compute priority of the evicted item at eviction time and advance L
    try:
        pri = _priority(ekey, evicted_obj, now)
        if pri > gds_L:
            gds_L = float(pri)
    except Exception:
        # If metadata missing, keep L unchanged
        pass

    # Predictor ghost
    _record_predictor_ghost(ekey)

    # Trim ghosts
    _trim_predictor_ghosts(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_last_gap.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_Ltouch.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpldnupu3a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn80uyh5f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj96cgpb8.pickle

Iteration 66: New subsample score 0.8275110000000001 is not better than old score 0.8331379999999999, skipping
Iteration 67: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp55dcpyss.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbjwjys_k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa29ezits.pickle

Iteration 67: Proposed new text for program: # S3-Blend: Size-aware, Stage-segmented, Self-tuned hybrid eviction
# Goals:
# - Robust across recency-, frequency-, and bursty/predictive patterns
# - Size fairness (per-byte utility)
# - Scan resistance via probation/protected segmentation (2Q-like)
# - Predictive component using EWMA inter-arrival with sensible overdue handling
# - Lightweight ghost history for warm-starts
#
# Eviction score (higher => evict sooner):
#   score = (wP * pred_term + wR * age + cold_boost) /
#           (size^SIZE_EXP * (1 + FREQ_STRENGTH * log1p(effective_freq - 1)))
#           * stage_multiplier
#
# Key design points:
# - Predicted term: time to next access if not overdue; otherwise overdue_excess
#   (so long-overdue items become evictable; slightly overdue are kept).
# - Effective freq: LFU count with hyperbolic time decay (no periodic scans).
# - Stage multipliers: prefer evicting probation items, discount protected ones.
# - Ghost history: seeds mu and freq, and optionally starts as protected if past
#   history suggests reuse (small and previously frequent).

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access "time"/access_count)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (hit count)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys
m_global_mu = 64.0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
# Predictive learning
EWMA_BETA = 0.30           # per-key EWMA learning rate
GLOBAL_BETA = 0.010        # global EWMA learning rate
DEFAULT_MU_MULT = 2.5      # default mu = DEFAULT_MU_MULT * global_mu on cold insert

# Size fairness
SIZE_EXP = 1.05            # >1.0 slightly penalizes whales

# Frequency handling
FREQ_STRENGTH = 0.90       # strength of frequency dampening in denominator
TAU_MULT = 6.0             # effective_freq time-scale tau = TAU_MULT * m_global_mu

# Predicted delta handling
OVERDUE_PENALTY = 1.0      # scale for overdue excess (>=1 => more evictable if overdue)
OVERDUE_CAP_MULT = 8.0     # cap overdue term at this multiple of mu

# Stage segmentation (2Q-like)
STAGE_MULT_PROTECTED = 0.45   # discount for protected items (kept longer)
STAGE_MULT_PROBATION = 1.15   # penalty for probation items (evict first)

# Cold-scan resistance
PROBATION_COLD_AGE_MULT = 6.0  # if probation, freq==1, and age > this * global_mu, add boost
PROBATION_COLD_BOOST = 0.5     # extra additive term to numerator (scaled by age/tau)

# Blend weights for recency vs prediction
W_RECENCY = 1.00
W_PREDICT = 1.20

# Ghost history sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _tau():
    # Time-scale for frequency decay
    return max(8.0, float(m_global_mu) * TAU_MULT)

def _predicted_term(key, now):
    """
    Predictive term:
      - If insufficient data: return large (cold)
      - If next predicted time is in future: delta until next
      - If overdue: overdue_excess = (now - next_predicted) * OVERDUE_PENALTY, capped
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    mu = max(1.0, float(mu))
    gap = float(now - la)
    next_pred = la + mu
    if now <= next_pred:
        return float(next_pred - now)
    overdue_excess = (now - next_pred) * OVERDUE_PENALTY
    # Cap overdue so extremely stale items are strongly evictable but bounded
    return min(overdue_excess, OVERDUE_CAP_MULT * mu)

def _effective_freq(key, age, tau):
    """
    Hyperbolic time-decayed frequency without global scans:
      eff = 1 + (raw_freq - 1) / (1 + age / tau)
    """
    raw = max(1, int(m_freq.get(key, 1)))
    if raw <= 1:
        return 1.0
    return 1.0 + (float(raw - 1) / (1.0 + (float(age) / tau)))

def _stage_multiplier(key):
    st = m_stage.get(key, 0)
    if st >= 1:
        return STAGE_MULT_PROTECTED
    return STAGE_MULT_PROBATION

def _cold_scan_boost(key, age, tau):
    """
    If an item is in probation with only one hit and has aged well beyond typical scale,
    we add a small boost to evict it earlier (scan resistance).
    """
    if m_stage.get(key, 0) == 0 and m_freq.get(key, 1) <= 1:
        if age > PROBATION_COLD_AGE_MULT * float(m_global_mu):
            # Scales with normalized age (age/tau), bounded by 1.0
            norm = min(1.0, float(age) / tau)
            return PROBATION_COLD_BOOST * norm
    return 0.0

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    Score blends:
    - Recency (age)
    - Predictive term (time to next if in future; overdue_excess if overdue)
    - Cold-scan boost for one-timers in probation
    Denominator applies size fairness and frequency benefit with time decay.
    Stage multiplier prefers evicting probation and keeps protected longer.
    """
    la = m_last_access.get(key, None)
    if la is None:
        age = _INF
    else:
        age = max(0.0, float(now - la))

    tau = _tau()
    pred = _predicted_term(key, now)
    cold_boost = _cold_scan_boost(key, age, tau)

    numerator = W_PREDICT * pred + W_RECENCY * age + cold_boost

    sz = float(_size_of(obj))
    eff_freq = _effective_freq(key, age, tau)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_STRENGTH * math.log1p(max(0.0, eff_freq - 1.0)))
    base = numerator / max(1e-9, denom)

    return base * _stage_multiplier(key)

def _seed_predictor_on_insert(key, now, size_hint=None, capacity_hint=None):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered mu and freq.
    - Else, seed from global mu with conservative multiplier.
    - Decide initial stage: usually probation; if ghost shows strong reuse and
      object is not huge, start protected.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        hist_freq = max(1, int(g_freq.get(key, 1)))
        m_freq[key] = hist_freq
        # Start protected only if previously frequent and not too large
        start_protected = False
        if hist_freq >= 3:
            if size_hint is None:
                start_protected = True
            else:
                # Avoid putting very large items directly into protected
                if capacity_hint is None:
                    start_protected = (size_hint <= 1 << 20)  # ~1MB heuristic fallback
                else:
                    start_protected = (size_hint <= 0.10 * float(capacity_hint))
        m_stage[key] = 1 if start_protected else 0
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1
        m_stage[key] = 0  # probation by default

    m_last_access[key] = now

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta
    - Update global mu (slowly)
    - Increment frequency
    - Promote to protected on first hit (2Q-style)
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = float(m_mu.get(key, gap))
        new_mu = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_mu[key] = max(1.0, new_mu)
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold hit without prior metadata
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Promote to protected on first (or any) hit while in probation
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    freq = m_freq.get(evicted_key, None)
    if mu is None or freq is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = int(freq)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the highest eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (frees more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key

def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Ensure per-key metadata exists
    - Update EWMA predictor, global mu, last access, freq
    - Promote to protected on first hit
    """
    key = obj.key
    now = _now(cache_snapshot)

    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(
            key, now, size_hint=_size_of(obj),
            capacity_hint=getattr(cache_snapshot, "capacity", None)
        )
        # Do not increment freq twice; seed sets baseline

    _update_predictor_on_hit(key, now)

def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize metadata using ghost warm-start if available
    - Typically start in probation; may start protected if ghost suggests reuse and size is modest
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(
        key, now, size_hint=_size_of(obj),
        capacity_hint=getattr(cache_snapshot, "capacity", None)
    )

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history
    - Trim ghost store (O(1) amortized)
    - Remove per-key metadata for the evicted key
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv64k_t60.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprzx7hioc.pickle

Killing subprocess...
Iteration 67: New subsample score 0.679945 is not better than old score 0.701041, skipping
Iteration 68: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnzd10tpy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpja6qu9d1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqhacjtxm.pickle

Iteration 68: Proposed new text for program: # ReASAP-SLRU-TLFU: Size-aware SLRU with predictive recency and decayed frequency
# Key ideas
# - Segmented LRU (SLRU): entries start in probation; on first hit, promote to protected.
#   Eviction strictly prefers probation (scan resistant); protected is only tapped when needed.
# - Size-aware: operate per-byte via size exponent.
# - Predictive recency: per-key EWMA of inter-arrival time (mu) to estimate next access time.
#   Overdue items are penalized proportional to lateness (fixed from previous "imminent" bug).
# - Decayed frequency (TinyLFU-like): a simple global epoch applies exponential aging to per-key
#   hit counts to prevent stale frequency from dominating.
# - Compact ghost history: retains last mu/freq to warm-start reinserted keys.
#
# All operations are O(number of cached items) for eviction (a single or two passes),
# O(1) for updates, and O(1) amortized for ghost trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_freq = dict()           # key -> int (raw hit count, aged by epoch lazily)
m_f_epoch = dict()        # key -> int (epoch of last freq normalization)
m_stage = dict()          # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Segment byte counters (best-effort; guarded against drift)
bytes_probation = 0
bytes_protected = 0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()             # key -> float (last known EWMA)
g_freq = dict()           # key -> int (historical hits)
g_stamp = dict()          # key -> int (version counter)
g_order = deque()         # deque of (key, stamp) in LRU order of eviction

# Decayed frequency global epoch
g_epoch = 0
g_next_epoch_at = 4096    # next access_count to advance epoch
EPOCH_LENGTH = 4096       # accesses per epoch
# With DECAY = 0.5 per epoch, we can use integer right-shifts for speed.
DECAY_SHIFT_PER_EPOCH = 1  # 1 => halve per epoch

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25              # per-key EWMA learning rate
GLOBAL_BETA = 0.005           # global EWMA learning rate (slow)
SIZE_EXP = 1.0                # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.7               # dampened influence of frequency via log1p
DEFAULT_MU_MULT = 3.0         # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_PENALTY = 0.75        # factor applied to lateness when overdue (higher => evict sooner)
NEW_ITEM_BONUS = 1.35         # multiplier for eviction score of items with freq==1 (more likely to evict)
STAGE_FACTOR_PROTECTED = 0.5  # discount to eviction score for protected items (used within protected pool)

GHOST_LIMIT_MIN = 1024        # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0      # ghost capacity  factor * live cache item count

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    global g_epoch, g_next_epoch_at
    if now >= g_next_epoch_at:
        # Advance by as many epochs as needed (handles large jumps)
        steps = max(1, (now - g_next_epoch_at) // EPOCH_LENGTH + 1)
        g_epoch += steps
        g_next_epoch_at += steps * EPOCH_LENGTH

def _decayed_freq(key):
    # Return frequency aged to current epoch, without mutating stored value
    raw = m_freq.get(key, 0)
    k_epoch = m_f_epoch.get(key, g_epoch)
    d = g_epoch - k_epoch
    if d <= 0:
        return raw
    # divide by 2^(DECAY_SHIFT_PER_EPOCH * d)
    shift = DECAY_SHIFT_PER_EPOCH * d
    return raw >> shift

def _bump_freq_on_hit(key):
    # Normalize to current epoch then increment
    ke = m_f_epoch.get(key, g_epoch)
    if ke != g_epoch:
        d = g_epoch - ke
        if d > 0:
            shift = DECAY_SHIFT_PER_EPOCH * d
            m_freq[key] = m_freq.get(key, 0) >> shift
        m_f_epoch[key] = g_epoch
    m_freq[key] = m_freq.get(key, 0) + 1

def _predicted_delta_components(key, now):
    """
    Return (effective_delta, is_overdue)
    - If no predictor exists: treated as very far (cold).
    - If overdue: effective_delta grows with lateness (OVERDUE_PENALTY * overdue).
    - Else: time remaining until predicted next access.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return (_INF, False)
    next_t = la + mu
    if now <= next_t:
        return (float(next_t - now), False)
    overdue = float(now - next_t)
    return (OVERDUE_PENALTY * overdue, True)

def _eviction_score_within_stage(key, obj, now, is_protected):
    """
    Higher score => evict sooner among items in the same stage.
    score = (effective_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq_adj))) * stage_factor
    - New/low-frequency items get a boost to the score (more likely to evict).
    - Protected items get a discount (stage_factor) when comparing within protected pool.
    """
    sz = _size_of(obj)
    delta, _ = _predicted_delta_components(key, now)

    # Decayed frequency to avoid stale dominance
    f_eff = max(0, _decayed_freq(key))
    denom_freq = 1.0 + FREQ_DAMP * math.log1p(max(0, f_eff))

    base = delta / ((sz ** SIZE_EXP) * denom_freq)

    # Boost eviction score for items that have never hit (freq ~ 0 or 1)
    raw_freq = m_freq.get(key, 0)
    if raw_freq <= 1:
        base *= NEW_ITEM_BONUS

    # Protected items get discount only within protected pool
    if is_protected:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - Start in probation (stage=0).
    - Initialize frequency with epoch tagging (so it will age).
    """
    global m_global_mu
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        # Initialize decayed freq from ghost, but ensure it ages in current epoch
        ghf = max(0, int(g_freq.get(key, 1)))
        m_freq[key] = ghf
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0  # will be incremented on hit

    m_f_epoch[key] = g_epoch
    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment decayed frequency.
    """
    global m_mu, m_last_access, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold edge case (should be rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    _bump_freq_on_hit(key)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    # Store a small bounded freq (avoid huge integers)
    g_freq[evicted_key] = min(255, max(0, int(m_freq.get(evicted_key, 0))))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _stage_of_key(key):
    return m_stage.get(key, 0)

def _inc_probation_bytes(sz):
    global bytes_probation
    bytes_probation += max(0, int(sz))

def _move_probation_to_protected(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0
    bytes_protected += s

def _dec_probation_bytes(sz):
    global bytes_probation
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0

def _dec_protected_bytes(sz):
    global bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = 0

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object prioritizing the probation segment (SLRU).
    Steps:
      - Advance frequency epoch if needed.
      - If any probation item exists, pick the one with the largest eviction score within probation.
      - Else, pick the protected item with the largest eviction score (with protected discount).
    Tie-breakers within the chosen pool:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # First pass: probation only
    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None
    found_probation = False

    for k, v in cache.items():
        if _stage_of_key(k) != 0:
            continue
        found_probation = True
        s = _eviction_score_within_stage(k, v, now, is_protected=False)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
            best_key, best_score, best_la, best_sz = k, s, la, sz

    if found_probation:
        return best_key

    # Second pass: protected only (if no probation items)
    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        if _stage_of_key(k) != 1:
            continue

        s = _eviction_score_within_stage(k, v, now, is_protected=True)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
            best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch and update per-key EWMA predictor, global mu, and decayed frequency.
    - Promote from probation to protected on first hit (SLRU).
    - Maintain last access and segment byte counters.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Do not mutate byte counters here (unknown stage/bytes consistency)
        # Keep stage=0 from seeding; promotion below will adjust safely with guards.

    _update_predictor_on_hit(key, now)

    # SLRU promotion on first hit
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1
        _move_probation_to_protected(_size_of(obj))


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Start in probation (stage=0).
    - Update last access time and byte counters for probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _seed_predictor_on_insert(key, now)
    _inc_probation_bytes(_size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Adjust segment byte counters.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust segment bytes using last known stage
    st = m_stage.get(ekey, 0)
    if st == 0:
        _dec_probation_bytes(_size_of(evicted_obj))
    else:
        _dec_protected_bytes(_size_of(evicted_obj))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_f_epoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_83yd11j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpamhc5np6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkng6qnan.pickle

Iteration 68: New subsample score 1.5735720000000002 is better than old score 1.471156. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpscxtywzg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcyspf4fz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8vq9qbbu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplzeoivq7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpojzgwc2f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm7lqk94r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp47h3ka0c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi1mke648.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo9qhyor3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8gag40hw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp35gqcf8y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpah90m_v7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpatqwz5i9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg3vk6vpk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmogtbr4e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpybmqjf8s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdgok0ja0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmgazxgn4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvubgesnm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpioxro9qi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsaymsl6k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkhewez2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6rw_eftv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfzq6d2i7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpghi9i2m7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiuze0yrk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr91loiiq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc1qb0pw0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpczd0hwwl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqs8nxy1l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc2mut3bb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp30ek1efg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoiis4xli.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbldsa_vx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpszlvjfb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmwssywpt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph1_x40jz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjrnu5m7m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5r7nog_9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcswgc5hu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9lpi2_w5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpujw5yezh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9zq8aqny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk8m0azgy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy31ck7u1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptzmln9cf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxkpnfw7l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvu0q5bxx.pickle

Iteration 68: New program is on the linear pareto front
Iteration 68: Full valset score for new program: 0.25926008333333345
Iteration 68: Full train_val score for new program: 0.25926008333333345
Iteration 68: Individual valset scores for new program: [0.475463, 0.454705, 0.45973, 0.424735, 0.473926, 0.467197, 0.260766, 0.452025, 0.531284, 0.531017, 0.133333, 0.291297, 0.070115, 0.0, 0.02053, 0.020569, 0.019793, 0.023615, 0.022641, 0.256645, 0.428712, 0.024694, 0.057382, 0.057382, 0.269826, 0.252016, 0.77855, 0.888071, 0.178197, 0.038636, 0.072893, 0.087816, 0.087816, 0.688223, 0.098684, 0.166814, 0.218379, 0.632664, 0.125461, 0.157749, 0.13989, 0.178125, 0.077303, 0.366667, 0.188077, 0.17195, 0.466258, 0.156863]
Iteration 68: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.89235, 0.178197, 0.077273, 0.072893, 0.087816, 0.087816, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178125, 0.101974, 0.366667, 0.188077, 0.17195, 0.466258, 0.166667]
Iteration 68: Full valset pareto front score: 0.2778667500000001
Iteration 68: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {24}, {17, 23, 6, 14}, {6, 14, 17, 23, 24}, {11}, {14, 17, 24, 25, 29}, {16, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {28}, {14, 17, 23, 24, 25, 29}, {18}, {6, 14, 17, 24, 25, 29}, {24, 25, 29, 14}, {24, 25, 29, 14}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29}, {29, 14}, {24, 25}, {24, 25, 29, 14}, {24, 25}, {3, 5, 6, 7, 8, 17, 28, 29}, {14, 23, 24, 25, 29}, {14, 23, 24, 25, 29}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29}, {24}]
Iteration 68: Best valset aggregate score so far: 0.25926008333333345
Iteration 68: Best program as per aggregate score on train_val: 29
Iteration 68: Best program as per aggregate score on valset: 29
Iteration 68: Best score on valset: 0.25926008333333345
Iteration 68: Best score on train_val: 0.25926008333333345
Iteration 68: Linear pareto front program index: 29
Iteration 68: New program candidate index: 29
Iteration 69: Selected program 27 score: 0.24045458333333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwwd3zmyg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj95yvx0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqj9p9vkp.pickle

Iteration 69: Proposed new text for program: # GD-SP: GreedyDual Size + Predicted reuse and decayed Frequency
# - Single global priority (GreedyDual-style) per item:
#     H(key) = L + cost(key, now) / size(key)^SIZE_EXP
#   where L is the last evicted item's priority (inflation), carrying recency.
# - cost combines:
#     * a constant base
#     * a time-decayed frequency term
#     * a predicted imminence term derived from an EWMA of inter-arrival time
# - Evict the item with the smallest H (oldest/least-valuable per byte).
# - On hit or insert, refresh H using current L and updated metadata.
# - Lightweight predictor ghosts store EWMA/frequency for warm starts.
#
# Rationale:
# - GreedyDual gives robust recency and size awareness without segment starvation.
# - Decayed frequency helps retain repeatedly used objects.
# - Predicted imminence (using EWMA inter-arrival) boosts items expected soon.
# - Ghost-based warm start accelerates relearning after reinsert.
#
# This design avoids ARC's segment starvation (which can cause near-zero hit rate
# on some traces) by maintaining a single fair priority ordering and always allowing
# protected items to be evicted when their H falls below the inflation threshold.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_last_gap = dict()      # key -> float (last observed gap)
m_freq = dict()          # key -> int (hit count)
m_priority = dict()      # key -> float (GreedyDual priority H)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Predictor ghost history (for warm starts)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order for trimming

# GreedyDual inflation variable
g_L = 0.0

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25            # per-key EWMA learning rate
GLOBAL_BETA = 0.005         # global EWMA learning rate
SIZE_EXP = 1.0              # size exponent (1.0 = per-byte fairness)
DECAY_HALF_LIFE = 1024.0    # accesses for frequency half-life (time-based decay)
DEFAULT_MU_MULT = 8.0       # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_EPS = 0.1           # small epsilon for overdue items (avoid penalizing imminents)
GHOST_LIMIT_MIN = 1024      # minimum predictor ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0    # predictor ghost capacity  factor * live cache item count

# Cost weights
BASE_W = 1.0
FREQ_W = 2.0
PRED_W = 1.5

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _live_count(cache_snapshot):
    return max(1, len(getattr(cache_snapshot, "cache", {}) or {}))

def _predictor_ghost_limit(cache_snapshot):
    live_cnt = _live_count(cache_snapshot)
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_predictor_ghosts(cache_snapshot):
    limit = _predictor_ghost_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _effective_freq(key, now):
    # Time-decayed frequency using half-life w.r.t. time since last access
    f = float(m_freq.get(key, 0))
    la = m_last_access.get(key, None)
    if f <= 0.0 or la is None:
        return 0.0
    age = max(0.0, float(now - la))
    if DECAY_HALF_LIFE <= 0:
        return f
    decay = 0.5 ** (age / DECAY_HALF_LIFE)
    return f * decay

def _predicted_delta(key, now):
    """
    Predict non-negative time-to-next-access.
    Uses min(EWMA, last_gap) for responsiveness, overdue -> small epsilon.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    lg = m_last_gap.get(key, None)
    if mu is None or la is None:
        return _INF
    base = float(mu)
    if lg is not None:
        base = min(base, float(lg))
    next_t = float(la) + base
    if now <= next_t:
        return float(next_t - now)
    # overdue: likely imminent, do not penalize
    return OVERDUE_EPS

def _predicted_imminence(key, now):
    """
    Map predicted delta to [0, 1], higher means sooner expected reuse.
    Scale by global mu for normalization.
    """
    delta = _predicted_delta(key, now)
    if delta >= _INF / 2:
        return 0.0
    scale = max(1.0, float(m_global_mu))
    return 1.0 / (1.0 + (delta / scale))

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from predictor ghosts if available.
    - Else seed from global mu with conservative multiplier.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now
    m_last_gap[key] = m_mu.get(key, float(DEFAULT_MU_MULT * m_global_mu))

def _update_predictor_on_hit(key, now):
    """
    Update predictor on hit:
    - EWMA of inter-arrival time with fixed beta.
    - Update global mu slowly.
    - Increment frequency counter.
    """
    global m_mu, m_last_access, m_freq, m_global_mu, m_last_gap

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_last_gap[key] = gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Defensive: seed if missing
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_last_gap[key] = m_mu[key]

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _record_predictor_ghost(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _compute_cost(key, obj, now):
    """
    Combined utility of retaining key (before size normalization).
    Higher means more useful to keep.
    """
    eff_freq = _effective_freq(key, now)
    imminence = _predicted_imminence(key, now)
    # Use logarithmic scaling for frequency to avoid domination.
    freq_term = math.log1p(max(0.0, eff_freq))
    return BASE_W + FREQ_W * freq_term + PRED_W * imminence

def _priority_for(key, obj, now):
    """
    GreedyDual priority H = L + cost / size^exp.
    """
    sz = _size_of(obj)
    if sz <= 0:
        sz = 1
    cost = _compute_cost(key, obj, now)
    return float(g_L) + (cost / (sz ** SIZE_EXP))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the item with the smallest GreedyDual priority H.
    Tie-breakers for stability and space reclamation:
      1) Older last access (LRU among equal H)
      2) Larger size (free more space)
      3) Lexicographic key order
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Evaluate priorities and pick the minimum-H victim
    best_key = None
    best_H = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        # Use cached priority if available, else compute on the fly.
        H = m_priority.get(k, None)
        if H is None:
            H = _priority_for(k, v, now)
            m_priority[k] = H

        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_H, best_la, best_sz = k, H, la, sz
            continue

        if H < best_H:
            best_key, best_H, best_la, best_sz = k, H, la, sz
            continue

        if H == best_H:
            if la < best_la:
                best_key, best_H, best_la, best_sz = k, H, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_H, best_la, best_sz = k, H, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_H, best_la, best_sz = k, H, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update predictor (EWMA, last gap, global mu).
    - Increment frequency.
    - Refresh item's GreedyDual priority using current L and updated metadata.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Seed if missing (defensive)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)

    _update_predictor_on_hit(key, now)

    # Refresh GreedyDual priority
    H = _priority_for(key, obj, now)
    m_priority[key] = H


def update_after_insert(cache_snapshot, obj):
    """
    After insert (on miss):
    - Initialize predictor (warm-start from predictor ghosts if available).
    - Initialize GreedyDual priority H using current L and seeded metadata.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _seed_predictor_on_insert(key, now)

    H = _priority_for(key, obj, now)
    m_priority[key] = H

    # Optionally trim ghosts occasionally on insert
    _trim_predictor_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record predictor ghost (EWMA / freq) for warm-start on future insert.
    - Update GreedyDual inflation L to the evicted item's priority.
    - Remove live metadata for the evicted key.
    """
    global g_L

    ekey = evicted_obj.key

    # Determine evicted priority (compute if missing)
    evicted_H = m_priority.get(ekey, None)
    if evicted_H is None:
        now = _now(cache_snapshot)
        evicted_H = _priority_for(ekey, evicted_obj, now)

    # GreedyDual inflation update
    g_L = float(evicted_H)

    # Predictor ghost
    _record_predictor_ghost(ekey)

    # Trim ghosts to bounded size
    _trim_predictor_ghosts(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_last_gap.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_priority.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa5f2oxpm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy81duweu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk2hbewbz.pickle

Iteration 69: New subsample score 0.833924 is not better than old score 0.875811, skipping
Iteration 70: Selected program 29 score: 0.25926008333333345
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9zs5h0l6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprbvvieva.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8k8ud56y.pickle


[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new[0m
LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.

Iteration 70: Exception during reflection/proposal: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.
Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 126, in read
    with map_exceptions(exc_map):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/contextlib.py", line 158, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout: The read operation timed out

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 745, in completion
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 673, in completion
    ) = self.make_sync_openai_chat_completion_request(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py", line 237, in sync_wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 489, in make_sync_openai_chat_completion_request
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 471, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1189, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/openai/_base_client.py", line 1000, in request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/main.py", line 2160, in completion
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/main.py", line 2132, in completion
    response = openai_chat_completions.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/llms/openai/openai.py", line 756, in completion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Request timed out.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/reflective_mutation.py", line 116, in propose
    new_texts = self.propose_new_texts(curr_prog, reflective_dataset, predictor_names_to_update)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/reflective_mutation.py", line 60, in propose_new_texts
    return self.adapter.propose_new_texts(candidate, reflective_dataset, components_to_update)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/caching_program_adapter.py", line 152, in propose_new_texts
    new_texts[name] = OpenEvolveProposalSignature.run(
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/proposer/reflective_mutation/base.py", line 45, in run
    lm_out = lm(full_prompt).strip()
             ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/caching_program_adapter.py", line 33, in _call_lm
    completion = self.litellm.completion(
                 ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/utils.py", line 1381, in wrapper
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/utils.py", line 1250, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/main.py", line 3774, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2328, in exception_type
    raise e
  File "/home/ruiying/miniconda3/envs/gepa/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 305, in exception_type
    raise Timeout(
litellm.exceptions.Timeout: litellm.Timeout: APITimeoutError - Request timed out. Error_str: Request timed out.

Iteration 70: Reflective mutation did not propose a new candidate
Iteration 71: Selected program 24 score: 0.2291873541666667
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp_o712ep.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplyaqubuc.pickle

Iteration 71: Proposed new text for program: # Predictive Size-Aware SLRU (PSA-SLRU)
# Core ideas:
# - Two segments (probation T1, protected T2) like SLRU/ARC: items enter T1, first hit promotes to T2.
# - Eviction preference adapts online (ARC-like) using ghost feedback: grow T2 if T2 ghosts are hit, shrink otherwise.
# - Within the chosen segment, evict the item with the largest predicted next-access time, adjusted by:
#     * frequency dampening (more hits => lower eviction score),
#     * size awareness (favor evicting large objects when under space pressure),
#     * segment bias (prefer evicting from the overfull segment).
# - Lightweight predictors:
#     * Per-key EWMA of inter-arrival time (mu) updated on hits.
#     * Global EWMA of inter-arrival (fallback).
# - Bounded ghost history stores minimal info to warm-start reinserted items and drive adaptation.
# - Computation is O(number of cached items) per eviction, with no expensive decays or sorts.
#
# This design aims to reduce miss rate by approximating Belady's "evict farthest reuse"
# while remaining fast and robust across diverse traces.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()           # key -> int

# Reuse predictor
m_last_access = dict()   # key -> int
m_mu = dict()            # key -> float (EWMA inter-arrival)
m_hits = dict()          # key -> int (hit count)

# Global EWMA mean inter-arrival (for cold-start and stabilization)
m_global_mu = 64.0

# ARC-style target for protected segment share (fraction of live keys)
m_protect_target_frac = 0.50

# Ghost history (bounded)
g_mu = dict()            # key -> float
g_last = dict()          # key -> int
g_hits = dict()          # key -> int
g_seg = dict()           # key -> int (segment at eviction: 0=T1, 1=T2)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Frequency dampening (larger reduces weight of hits)
FREQ_DAMP_LOG = True       # use log1p(hits) if True, else power
FREQ_POW = 0.6             # if FREQ_DAMP_LOG == False, divisor = (1+hits)^FREQ_POW

# Size-awareness
SIZE_GAMMA_BASE = 0.80     # base exponent for size in eviction score
SIZE_GAMMA_PRESSURE = 0.60 # extra size bias under space pressure ~ size(new)/capacity

# Segment selection bias (ARC-like)
NONPREF_BIAS = 2.0         # reduce eviction score for non-preferred segment by this factor
ARC_ADAPT_STEP = 0.06      # how fast the protected target fraction adapts
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# EWMA for reuse predictor
EWMA_BETA = 0.20
GLOBAL_BETA = 0.02

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 2.5

# Numerics
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # Keep ghost bounded by removing the oldest entries by last-seen time.
    # Use O(G) linear scans; invoked once per eviction -> cheap enough and avoids full sorts.
    limit = _ghost_capacity_limit(cache_snapshot)
    glen = len(g_last)
    if glen <= limit:
        return
    to_remove = glen - limit
    # Remove 'to_remove' oldest entries by repeated linear min search (usually 1).
    for _ in range(to_remove):
        oldest_k = None
        oldest_t = None
        for k, t in g_last.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k = k
                oldest_t = t
        if oldest_k is None:
            break
        g_mu.pop(oldest_k, None)
        g_last.pop(oldest_k, None)
        g_hits.pop(oldest_k, None)
        g_seg.pop(oldest_k, None)

def _predicted_delta(key, now):
    mu = m_mu.get(key)
    la = m_last_access.get(key)
    if mu is None or la is None:
        # cold-start: use a generous default
        return DEFAULT_MU_MULT * float(m_global_mu)
    return max(0.0, float(la + mu - now))

def _segment_counts(cache):
    prot = 0
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            prot += 1
    total = len(cache)
    return prot, total

def _dynamic_size_gamma(cache_snapshot, incoming_obj):
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    new_sz = float(_size_of(incoming_obj))
    pressure = min(1.0, new_sz / float(cap))
    return SIZE_GAMMA_BASE + SIZE_GAMMA_PRESSURE * pressure

def _evict_score_for_key(k, v, now, preferred_seg, gamma):
    # Larger evict_score => more attractive to evict
    pd = _predicted_delta(k, now)
    hits = m_hits.get(k, 0)
    if FREQ_DAMP_LOG:
        freq_div = 1.0 + math.log1p(max(0, hits))
    else:
        freq_div = (1.0 + float(max(0, hits))) ** FREQ_POW
    base = pd / max(1e-9, freq_div)

    sz = _size_of(v)
    # Size-aware pressure
    base *= (float(sz) ** gamma)

    seg = m_seg.get(k, 0)
    # Bias away from non-preferred segment
    if seg != preferred_seg:
        base /= NONPREF_BIAS

    la = m_last_access.get(k, -1)
    return base, seg, la, sz


def _record_ghost_on_evict(evicted_key, now):
    # Record minimal ghost info for adaptation and warm-start
    mu = m_mu.get(evicted_key)
    if mu is not None:
        g_mu[evicted_key] = float(mu)
    else:
        g_mu.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_hits[evicted_key] = int(m_hits.get(evicted_key, 0))
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert, warm-starting from ghost if available.
    ARC-style adaptation: adjust protected target based on which ghost list was hit.
    """
    global m_protect_target_frac

    if key in g_seg:
        if g_seg[key] == 0:
            # Ghost hit from probation -> favor recency: shrink protected target
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            # Ghost hit from protected -> favor frequency: grow protected target
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    if key in g_mu:
        m_mu[key] = max(1.0, float(g_mu[key]))
        m_hits[key] = max(0, int(g_hits.get(key, 0)))
        # Place ghost hits into protected if enabled
        if START_PROTECTED_FROM_GHOST and key in g_seg:
            m_seg[key] = 1
        else:
            m_seg[key] = 0
    else:
        # Cold start in probation with a conservative mu
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_hits[key] = 0
        m_seg[key] = 0

    m_last_access[key] = now


def _update_predictors_on_hit(key, now):
    # EWMA reuse predictor per-key
    last = m_last_access.get(key)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using adaptive segment preference + predictive size-aware scoring.

    Steps:
      - Compute preferred segment: whichever (T1 or T2) exceeds its adaptive target.
      - Within that segment, select the key with largest eviction score:
            score = predicted_delta / freq_damp * size^gamma
        and reduce score by NONPREF_BIAS for non-preferred segment keys.
      - gamma increases under high space pressure to prefer larger victims when inserting big objects.
      - Tie-breakers:
          * Prefer evicting from preferred segment
          * Older last access (LRU)
          * Larger size (free more space)
          * Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Segment preference
    prot_count, total = _segment_counts(cache)
    target_prot = int(round(float(m_protect_target_frac) * float(total)))
    preferred_seg = 1 if prot_count > target_prot else 0

    gamma = _dynamic_size_gamma(cache_snapshot, obj)

    victim_key = None
    victim_seg = None
    victim_la = None
    victim_sz = None
    best_score = None

    for k, v in cache.items():
        score, seg, la, sz = _evict_score_for_key(k, v, now, preferred_seg, gamma)

        if victim_key is None:
            victim_key = k
            best_score = score
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if score > best_score:
            better = True
        elif score == best_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key = k
            best_score = score
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Initialize per-key predictor if missing (cold hit).
      - Update per-key/global EWMA reuse predictor and hit count.
      - Promote to protected segment if currently probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)

    _update_predictors_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style adaptation using ghost feedback to adjust protected target fraction.
      - Initialize per-key predictor and placement (warm-start from ghost when available).
      - Cold items start in probation; ghost hits can start in protected.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, last segment, hits, timestamp).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpop1gidkc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpp8q7c8m7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp58h3ljj7.pickle

Iteration 71: New subsample score 0.78661 is better than old score 0.453888. Continue to full eval and add to candidate pool.
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo6b0lzhm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp16f39vtr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph1d1f4k8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx0iw_c4t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy2l0i_aw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl183vdfw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp__aqv2wt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi08l7zb2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfwk1g2bc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphuwszd0w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb300xuaa.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7jkcxakt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0d8c7l9x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiwu4x9nm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplo47tyhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptz1i6mts.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn695iti4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphf2zc0xp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7ye7pqk7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl9d48uke.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpijkg2ky4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyeck0q5z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0t2_e1dx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp041o8kds.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmvcqtp0f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw6cqujxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7nopf7mu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptv_74h6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6z39w02u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplb4ci132.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1ut6xzwr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnwp6ik61.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8bvgn32h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6aha6okw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpro1o0vzi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx98zg6j5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpozrwyhg2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaydw6hbv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjtn8_wjs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpof4h783j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0pcwemnc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdcjoq1rs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm2666z0c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjf0d155a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpugchoc2q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmuodynpx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprzdwof4n.pickle

Iteration 71: Full valset score for new program: 0.23011145833333335
Iteration 71: Full train_val score for new program: 0.23011145833333335
Iteration 71: Individual valset scores for new program: [0.0, 0.369309, 0.36222, 0.344092, 0.36685, 0.37046, 0.253589, 0.367086, 0.486593, 0.531017, 0.125, 0.249112, 0.084481, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.229148, 0.423795, 0.024596, 0.059961, 0.059961, 0.330702, 0.116935, 0.748759, 0.866165, 0.178497, 0.040909, 0.072893, 0.087828, 0.087828, 0.643805, 0.098684, 0.159753, 0.2177, 0.601236, 0.125461, 0.157472, 0.121564, 0.178279, 0.077303, 0.35, 0.188153, 0.17223, 0.466258, 0.133987]
Iteration 71: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.89235, 0.178497, 0.077273, 0.072893, 0.087828, 0.087828, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178279, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 71: Full valset pareto front score: 0.2778841250000001
Iteration 71: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {24}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {28}, {30}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {30}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30}, {29, 14}, {24, 25}, {30}, {24, 25}, {3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30}, {24}]
Iteration 71: Best valset aggregate score so far: 0.25926008333333345
Iteration 71: Best program as per aggregate score on train_val: 29
Iteration 71: Best program as per aggregate score on valset: 29
Iteration 71: Best score on valset: 0.25926008333333345
Iteration 71: Best score on train_val: 0.25926008333333345
Iteration 71: Linear pareto front program index: 29
Iteration 71: New program candidate index: 30
Iteration 72: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxzh0p67y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_gtdvksn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjkecn4xz.pickle

Iteration 72: Proposed new text for program: # HLRFU-2Q: Half-life LRFU with size-awareness, adaptive 2Q protection, and compact ghost history
# Goals:
# - Strong recency+frequency blend via exponential decay (LRFU)
# - Size-aware fairness (per-byte value)
# - 2Q-style probation/protection with adaptive target share to resist scans
# - Ghost history for fast warm starts
# - O(1) amortized ghost trimming
#
# Key ideas:
# - Each item maintains a lazily-decayed score (LRFU): score = score * decay^(delta) + 1 on hit
# - Evict the resident with the smallest "keep value" = score / size^exp, biased by segment
# - Protected items receive a multiplicative bonus (keep longer). The bonus self-adjusts so that
#   the protected share tracks a target fraction (more/fewer items migrate as needed).
# - New (probation) items start with a tiny score; if unseen again, they get evicted fast.
# - Ghost history seeds reinserted items with a decayed prior score to accelerate learning.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_score = dict()       # key -> float (LRFU score at last update)
m_last_time = dict()   # key -> int (access_count at last score update)
m_hits = dict()        # key -> int (observed hit count)
m_stage = dict()       # key -> int (0=probation, 1=protected)

# Segment counters (to adapt protection strength)
live_count = 0
prot_count = 0

# Ghost history (bounded by count, O(1) trim via deque + stamps)
g_score = dict()       # key -> float (decayed score snapshot when evicted)
g_time = dict()        # key -> int (time of ghost record)
g_stamp = dict()       # key -> int (version counter to invalidate deque nodes)
g_order = deque()      # deque of (key, stamp) in LRU order of ghost entries

# ----------------------
# Tunable hyperparameters
# ----------------------
# LRFU decay: score decays by 50% every HALF_LIFE accesses
HALF_LIFE = 256
SIZE_EXP = 1.0                   # size normalization exponent (1.0 = per-byte fairness)
PROTECTED_TARGET_FRAC = 0.7      # desired fraction of items in protected stage
PROTECTED_BASE_BONUS = 3.0       # multiplicative bonus on keep-value for protected items
BONUS_ADJ_MIN = 0.5              # clamp for adaptive bonus adjustment
BONUS_ADJ_MAX = 2.0
NEW_ITEM_BASE_SCORE = 0.05       # initial score for cold inserts (very small)
GHOST_SEED_FACTOR = 0.8          # multiplier applied to decayed ghost score on reinsert
BIG_OBJ_PROMOTE_HITS = 2         # hits required to promote very large objects
BIG_OBJ_FRAC = 0.10              # "very large" if size >= BIG_OBJ_FRAC * capacity

# Ghost bounds
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0         # capacity  factor * live cache item count

# Tie-breaking epsilon
_EPS = 1e-12


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_score) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_score.pop(k, None)
            g_time.pop(k, None)
            g_stamp.pop(k, None)

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    # decay_base = 0.5 ** (1/HALF_LIFE); => pow(0.5, delta / HALF_LIFE)
    return math.pow(0.5, float(delta) / float(HALF_LIFE))

def _effective_score(key, now):
    """Return lazily-decayed score at 'now' without mutating state."""
    s = float(m_score.get(key, 0.0))
    lt = m_last_time.get(key, now)
    if lt != now:
        s *= _decay_factor(now - lt)
    return s

def _adaptive_protected_bonus(cache_snapshot):
    """Adjust protected bonus so protected share tracks target fraction."""
    global live_count, prot_count
    lc = max(1, live_count or len(getattr(cache_snapshot, "cache", {}) or {}))
    pc = max(0, prot_count)
    curr_frac = float(pc) / float(lc) if lc > 0 else 0.0
    if curr_frac <= 0.0:
        adj = BONUS_ADJ_MAX
    else:
        # If we have too many protected items, reduce bonus (<1). If too few, increase (>1).
        adj = math.sqrt(PROTECTED_TARGET_FRAC / max(_EPS, curr_frac))
        adj = max(BONUS_ADJ_MIN, min(BONUS_ADJ_MAX, adj))
    return PROTECTED_BASE_BONUS * adj

def _keep_value(key, obj, now, prot_bonus):
    """Higher keep_value => keep longer. Used to derive eviction score."""
    sz = _size_of(obj)
    base = _effective_score(key, now) / (sz ** SIZE_EXP)
    if m_stage.get(key, 0) >= 1:
        base *= prot_bonus
    return base

def _eviction_score_from_keep(keep_val):
    """Higher score => evict sooner. Map keep_value to eviction score."""
    return 1.0 / max(_EPS, keep_val)

def _should_promote(cache_snapshot, obj, hits_after):
    """Size-aware promotion policy."""
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    sz = _size_of(obj)
    if sz >= BIG_OBJ_FRAC * cap:
        return hits_after >= BIG_OBJ_PROMOTE_HITS
    # Default: promote on first hit
    return hits_after >= 1

def _seed_on_insert(cache_snapshot, key, now, obj):
    """Initialize per-key metadata on insert, using ghost if available."""
    global m_score, m_last_time, m_hits, m_stage, live_count

    # Base initial score
    init_score = NEW_ITEM_BASE_SCORE

    # Warm start from ghost (apply decay since eviction)
    if key in g_score:
        gs = float(g_score[key])
        gt = g_time.get(key, now)
        gs *= _decay_factor(now - gt)
        init_score = max(init_score, GHOST_SEED_FACTOR * gs)

    m_score[key] = init_score
    m_last_time[key] = now
    m_hits[key] = m_hits.get(key, 0)  # preserve if any prior bookkeeping
    # New items start in probation
    if m_stage.get(key, None) is None or m_stage.get(key, 0) not in (0, 1):
        m_stage[key] = 0

    # Update live counters
    live_count = max(0, int(live_count) + 1)

def _update_on_hit(cache_snapshot, key, now, obj):
    """LRFU update on hit + probabilistic promotion."""
    global m_score, m_last_time, m_hits, m_stage, prot_count

    # Lazily decay then add 1
    prev_s = float(m_score.get(key, 0.0))
    lt = m_last_time.get(key, now)
    decayed = prev_s * _decay_factor(now - lt)
    new_s = decayed + 1.0

    m_score[key] = new_s
    m_last_time[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Promotion decision
    if m_stage.get(key, 0) == 0 and _should_promote(cache_snapshot, obj, m_hits[key]):
        m_stage[key] = 1
        prot_count = max(0, int(prot_count) + 1)

def _record_ghost_on_evict(ekey, now):
    """Store decayed score/time for warm-start + LRU stamp for O(1) trimming."""
    if ekey not in m_score:
        return
    decayed = _effective_score(ekey, now)
    g_score[ekey] = float(decayed)
    g_time[ekey] = int(now)
    s = g_stamp.get(ekey, 0) + 1
    g_stamp[ekey] = s
    g_order.append((ekey, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident with the largest eviction score.
    keep_value = decayed_score / size^exp, boosted if protected
    eviction_score = 1 / keep_value
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    prot_bonus = _adaptive_protected_bonus(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        keep = _keep_value(k, v, now, prot_bonus)
        evs = _eviction_score_from_keep(keep)
        la = m_last_time.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, evs, la, sz
            continue

        if evs > best_score:
            best_key, best_score, best_la, best_sz = k, evs, la, sz
            continue

        if evs == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, evs, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, evs, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, evs, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Lazily decay and increment the LRFU score
    - Update last access time and hit count
    - Promote from probation to protected per size-aware rule
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Edge case: hit without prior metadata
    if key not in m_last_time or key not in m_score or key not in m_stage:
        _seed_on_insert(cache_snapshot, key, now, obj)

    _update_on_hit(cache_snapshot, key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize metadata with a tiny score; warm-start from ghost if available
    - Start in probation (stage=0)
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_on_insert(cache_snapshot, key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record decayed score/time to ghost history
    - Trim ghost to bounded size
    - Remove live metadata and adjust counters
    """
    global live_count, prot_count

    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Adjust live counters
    live_count = max(0, int(live_count) - 1)
    if m_stage.get(ekey, 0) >= 1:
        prot_count = max(0, int(prot_count) - 1)

    # Clean live metadata
    m_score.pop(ekey, None)
    m_last_time.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4cjxwf5m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwipvgs5o.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw291xd5a.pickle

Iteration 72: New subsample score 0.132421 is not better than old score 0.20293100000000003, skipping
Iteration 73: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgfaqa3r2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppe_8wc3d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6uon8hvi.pickle

Iteration 73: Proposed new text for program: # ReASAP-TLFU-SLRU: Size-aware, predictive SLRU with TinyLFU-style global frequency and adaptive protection
#
# Design summary:
# - Two-tier SLRU with a quarantine stage:
#    * stage=-1: quarantine (new, weakly admitted; evict-first unless quickly reused)
#    * stage=0: probation
#    * stage=1: protected
# - Global Count-Min Sketch (TinyLFU-style) estimates popularity across hits and misses.
#   Used for admission pressure: quarantine items with low estimated frequency are heavily penalized.
# - Size-aware and mildly predictive:
#    Eviction score uses a mix of predicted time-to-next-access (EWMA gap) and raw age,
#    normalized by size and both resident and global frequency signals.
# - Adaptive protection strength:
#    If a key reappears that was evicted from protected, increase protection bias; decrease otherwise.
# - Compact ghost history with O(1) amortized trimming to speed re-learning and guide protection.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (resident hit count)
m_stage = dict()         # key -> int (-1=quarantine, 0=probation, 1=protected)

# Global EWMA of inter-arrival time (baseline for cold start)
m_global_mu = 64.0

# Adaptive protection discount (multiplier applied to protected eviction score)
# Lower -> more protection. Auto-tunes based on ghost hits.
m_prot_discount = 0.40   # initial discount for protected items

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> int (historical hits while resident)
g_stage = dict()         # key -> int (stage at eviction: -1/0/1)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# TinyLFU-style Count-Min Sketch (global frequency estimator)
# ----------------------
# 4x4096 ~= 16k counters; aging every ~16k updates halves counts.
CM_DEPTH = 4
CM_WIDTH = 4096
_cm = [[0] * CM_WIDTH for _ in range(CM_DEPTH)]
_cm_seeds = [0x27d4eb2d, 0x85ebca6b, 0xc2b2ae35, 0x165667b1]
_cm_updates = 0
CM_AGING_THRESHOLD = CM_DEPTH * CM_WIDTH  # halve ~ every table-worth of updates

def _cm_hash(key, seed):
    # Mix Python's hash with a seed; map to [0, CM_WIDTH)
    h = hash(key)
    x = (h ^ seed) & 0xFFFFFFFFFFFFFFFF
    x ^= (x >> 33) & 0xFFFFFFFFFFFFFFFF
    x *= 0xff51afd7ed558ccd & 0xFFFFFFFFFFFFFFFF
    x &= 0xFFFFFFFFFFFFFFFF
    x ^= (x >> 33) & 0xFFFFFFFFFFFFFFFF
    x *= 0xc4ceb9fe1a85ec53 & 0xFFFFFFFFFFFFFFFF
    x &= 0xFFFFFFFFFFFFFFFF
    x ^= (x >> 33) & 0xFFFFFFFFFFFFFFFF
    return int(x % CM_WIDTH)

def _cm_estimate(key):
    m = None
    for i in range(CM_DEPTH):
        idx = _cm_hash(key, _cm_seeds[i])
        v = _cm[i][idx]
        if m is None or v < m:
            m = v
    return 0 if m is None else m

def _cm_add(key, inc=1):
    # Conservative update: increment only counters equal to current minimum
    global _cm_updates
    cur_min = None
    idxs = []
    for i in range(CM_DEPTH):
        idx = _cm_hash(key, _cm_seeds[i])
        idxs.append(idx)
        v = _cm[i][idx]
        if cur_min is None or v < cur_min:
            cur_min = v
    for i in range(CM_DEPTH):
        if _cm[i][idxs[i]] == cur_min:
            _cm[i][idxs[i]] += inc
    _cm_updates += 1
    if _cm_updates >= CM_AGING_THRESHOLD:
        # Halve all counters (simple exponential aging)
        for d in range(CM_DEPTH):
            row = _cm[d]
            for j in range(CM_WIDTH):
                row[j] >>= 1
        _cm_updates = _cm_updates // 2

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30            # per-key EWMA learning rate
GLOBAL_BETA = 0.010         # global EWMA learning rate (slow)
SIZE_EXP = 1.0              # size exponent (1.0 = per-byte fairness)
FREQ_RESIDENT_BETA = 0.30   # weight for resident frequency (log-compressed)
FREQ_GLOBAL_BETA = 1.20     # weight for global CM estimate (log-compressed)
DELTA_WEIGHT = 1.00         # weight for predicted time-to-next-access
AGE_WEIGHT = 0.50           # weight for raw age
DEFAULT_MU_MULT = 2.5       # default mu multiplier for cold inserts (scaled by global mu)

# Quarantine penalty (bigger -> more aggressive evict of new/low-frequency items)
QUAR_PENALTY = 3.0

# Ghost limits
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0

# Adaptive protection bounds
PROT_MIN = 0.20
PROT_MAX = 0.80
PROT_INC = 1.05   # weaken protection multiplier when recency wins (probation ghost)
PROT_DEC = 0.95   # strengthen protection multiplier when frequency wins (protected ghost)

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stage.pop(k, None)
            g_stamp.pop(k, None)

def _predicted_delta(key, now):
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)
    # Overdue -> imminent
    return 0.0

def _eviction_score(key, obj, now):
    # Numerator: predicted delta + raw age
    la = m_last_access.get(key, now)
    age = max(0.0, float(now - la))
    delta = _predicted_delta(key, now)
    numer = DELTA_WEIGHT * delta + AGE_WEIGHT * age

    # Denominator: size and frequency terms
    sz = _size_of(obj)
    res_freq = m_freq.get(key, 0)
    cm = _cm_estimate(key)

    freq_term = (1.0
                 + FREQ_RESIDENT_BETA * math.log1p(max(0, res_freq))
                 + FREQ_GLOBAL_BETA * math.log1p(1 + cm))

    denom = (sz ** SIZE_EXP) * freq_term
    base = numer / denom if denom > 0 else numer

    # Stage modifiers
    st = m_stage.get(key, 0)
    if st == 1:
        # Protected: apply adaptive discount
        base *= m_prot_discount
    elif st == -1:
        # Quarantine: inflate unless global freq is sizable
        # Reduce penalty as global estimate grows
        base *= (QUAR_PENALTY / (1.0 + 0.5 * math.log1p(1 + cm)))

    return base

def _record_ghost_on_evict(evicted_key):
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    g_stage[evicted_key] = m_stage.get(evicted_key, 0)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _seed_predictor_on_insert(key, now, sz, from_ghost=False):
    # Initialize mu/freq from ghost or default
    if from_ghost and key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1

    m_last_access[key] = now

    # Stage assignment (quarantine for low estimated popularity or very large)
    cm = _cm_estimate(key)
    cap_quarantine = (cm <= 1)
    very_large = False
    # Optional: aggressively quarantine objects larger than half cache capacity (if capacity available)
    # We don't have capacity here, so we only use size heuristic via sz relative to global mu is omitted.

    if (not from_ghost and cap_quarantine) or very_large:
        m_stage[key] = -1
    else:
        # If returning from ghost, start probation; require a hit to become protected
        m_stage[key] = 0

def _update_predictor_on_hit(key, now):
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Rare: hit without prior metadata; seed conservatively
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # SLRU promotions
    st = m_stage.get(key, 0)
    if st == -1:
        m_stage[key] = 0  # quarantine -> probation on first touch
    elif st == 0:
        m_stage[key] = 1  # probation -> protected on hit

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the highest eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = getattr(cache_snapshot, "cache", None) or {}
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key

def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update global frequency sketch (TinyLFU).
    - Update per-key EWMA (gap), last access, and resident frequency.
    - Promote through SLRU: quarantine -> probation -> protected.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _cm_add(key, inc=1)

    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now, _size_of(obj), from_ghost=(key in g_mu))
        # Do not increment resident freq here; _update_predictor_on_hit will do it.

    _update_predictor_on_hit(key, now)

def update_after_insert(cache_snapshot, obj):
    """
    After insert (following a miss):
    - Count this access in the global frequency sketch.
    - Initialize per-key predictor (warm-start from ghost if available).
    - Start new items in quarantine if global popularity is low; else probation.
    - Adapt protection bias based on which ghost list got a hit: protected ghost -> strengthen protection, probation ghost -> weaken.
    """
    global m_prot_discount
    key = obj.key
    now = _now(cache_snapshot)
    _cm_add(key, inc=1)

    from_ghost = key in g_mu
    prior_stage = g_stage.get(key, None)
    if from_ghost and prior_stage is not None:
        # ARC-like adaptation: if a protected ghost reappears, we need more protection (lower multiplier).
        if prior_stage >= 1:
            m_prot_discount = max(PROT_MIN, m_prot_discount * PROT_DEC)
        else:
            m_prot_discount = min(PROT_MAX, m_prot_discount * PROT_INC)

    _seed_predictor_on_insert(key, now, _size_of(obj), from_ghost=from_ghost)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history (mu, resident freq, stage).
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key live metadata.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcf73stdq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0154gfin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpplk3tf7p.pickle

Iteration 73: New subsample score 0.150683 is not better than old score 0.174408, skipping
Iteration 74: Selected program 28 score: 0.24792383333333334
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuvgwrxx_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp47jwobs8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprhmyhti9.pickle

Iteration 74: Proposed new text for program: # Byte-ARC (Adaptive Replacement Cache in Bytes)
# - Two resident segments:
#     T1: "probation" (recent, single-touch)
#     T2: "protected" (frequent, multi-touch)
# - Two non-resident ghost segments:
#     B1: keys evicted from T1
#     B2: keys evicted from T2
# - Adaptation target p_bytes controls desired size of T1 (in bytes), adjusted by ghost hits.
# - Eviction is strict ARC-style:
#     Prefer evicting from T1 when T1 exceeds p_bytes; otherwise evict from T2.
#     Within a segment, evict the LRU key (byte-accurate accounting).
# - Lightweight, lazily-decayed per-key frequency counters (not directly used for eviction,
#   but maintained for potential tie-breaking or future use). This keeps updates O(1).
# - Ghost lists bounded by capacity bytes (B1 + B2 <= capacity), retaining informative history
#   while avoiding excessive memory overhead.
#
# This design improves robustness on scans and bursty workloads over the previous hybrid scorer
# by adhering closely to ARC's eviction choice while being byte-aware throughout.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments (LRU -> MRU)
T1 = OrderedDict()  # probation: key -> None
T2 = OrderedDict()  # protected: key -> None

# Ghost segments (keys only, LRU -> MRU)
B1 = OrderedDict()  # evicted from T1
B2 = OrderedDict()  # evicted from T2

# Segment assignment for live keys: 0 = T1, 1 = T2
m_seg = dict()      # key -> int

# Last known size for keys (live or ghost)
m_size = dict()     # key -> int

# Lightweight frequency with lazy exponential decay
m_freq = dict()         # key -> int (count)
m_freq_epoch = dict()   # key -> int (last epoch when count was updated)
FREQ_CLIP = 255
DECAY_PERIOD = 8192  # accesses per epoch; larger => slower decay

# Last access time (by access_count from snapshot) for live keys
m_time = dict()     # key -> int

# Byte accounting for resident segments and ghosts
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# ARC adaptation target (bytes of T1)
p_bytes = None

# Ghost sizing: keep B1+B2 <= capacity bytes
GHOST_BYTES_FACTOR = 1.0


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _epoch(snapshot):
    n = _now(snapshot)
    return 0 if DECAY_PERIOD <= 0 else int(n // DECAY_PERIOD)

def _ensure_p(snapshot):
    global p_bytes
    cap = _cap(snapshot)
    if p_bytes is None:
        p_bytes = max(0, cap // 2)
    else:
        if cap <= 0:
            p_bytes = 0
        else:
            p_bytes = max(0, min(p_bytes, cap))

def _trim_ghosts(snapshot):
    # Ensure B1+B2 total bytes <= factor * capacity
    global b1_bytes, b2_bytes
    cap = _cap(snapshot)
    limit = int(GHOST_BYTES_FACTOR * max(0, cap))
    if limit <= 0:
        B1.clear()
        B2.clear()
        b1_bytes = 0
        b2_bytes = 0
        return
    while (b1_bytes + b2_bytes) > limit and (B1 or B2):
        # Evict from the larger ghost first
        target = B1 if b1_bytes >= b2_bytes else B2
        if target:
            k, _ = target.popitem(last=False)
            sz = m_size.get(k, 1)
            if target is B1:
                b1_bytes = max(0, b1_bytes - sz)
            else:
                b2_bytes = max(0, b2_bytes - sz)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _lru_existing_key(od, cache):
    # Return the first LRU key that still exists in cache;
    # if leading keys are stale, drop and fix byte accounting.
    global t1_bytes, t2_bytes
    while od:
        # Peek LRU
        for k in od:
            candidate = k
            break
        else:
            return None
        if candidate in cache:
            return candidate
        # stale -> remove and fix accounting
        _ = od.pop(candidate, None)
        sz = m_size.get(candidate, 1)
        if od is T1:
            t1_bytes = max(0, t1_bytes - sz)
            m_seg.pop(candidate, None)
        elif od is T2:
            t2_bytes = max(0, t2_bytes - sz)
            m_seg.pop(candidate, None)
        # continue loop
    return None

def _bump_freq(snapshot, key):
    # Lazy exponential decay: shift right by number of elapsed epochs before incrementing.
    e = _epoch(snapshot)
    last = m_freq_epoch.get(key, e)
    cnt = m_freq.get(key, 0)
    delta = max(0, e - last)
    if delta > 0:
        # At most shift by 8 to avoid large cost
        if delta >= 8:
            cnt = 0
        else:
            cnt = cnt >> delta
    cnt = min(FREQ_CLIP, cnt + 1)
    m_freq[key] = cnt
    m_freq_epoch[key] = e

def _safe_remove_from_resident(key, esz):
    # Remove key from T1/T2 if present, updating bytes accordingly
    global t1_bytes, t2_bytes
    if key in T1:
        T1.pop(key, None)
        t1_bytes = max(0, t1_bytes - esz)
    if key in T2:
        T2.pop(key, None)
        t2_bytes = max(0, t2_bytes - esz)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    ARC eviction in bytes:
    - Prefer evicting from T1 when T1 exceeds target p_bytes; else evict from T2.
    - Within a segment, evict its true LRU key (ensuring the key is still resident).
    - Robust fallback: if chosen segment empty/out-of-sync, try the other; else evict largest object.
    """
    global t1_bytes, t2_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_p(cache_snapshot)

    # Choose segment according to ARC rule
    choose_T1 = False
    if T1 and T2:
        choose_T1 = (t1_bytes > max(0, p_bytes))
    elif T1:
        choose_T1 = True
    elif T2:
        choose_T1 = False
    else:
        # No internal tracking; evict largest for faster space reclamation
        victim = None
        max_sz = -1
        for k, o in cache.items():
            s = _size_of(o)
            if s > max_sz:
                max_sz = s
                victim = k
        return victim

    # Try chosen segment's LRU (ensuring the key still exists)
    if choose_T1:
        k = _lru_existing_key(T1, cache)
        if k is not None:
            return k
        # fallback to T2
        k2 = _lru_existing_key(T2, cache)
        if k2 is not None:
            return k2
    else:
        k = _lru_existing_key(T2, cache)
        if k is not None:
            return k
        # fallback to T1
        k2 = _lru_existing_key(T1, cache)
        if k2 is not None:
            return k2

    # Absolute fallback: largest object in cache
    victim = None
    max_sz = -1
    for k, o in cache.items():
        s = _size_of(o)
        if s > max_sz:
            max_sz = s
            victim = k
    return victim


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update size/time and lazily decayed frequency.
    - If in T1: promote to T2 (protected).
    - If in T2: refresh recency (move to MRU).
    - If not tracked but hit: add to T2 (considered frequent), with byte accounting.
    """
    global t1_bytes, t2_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    m_time[k] = _now(cache_snapshot)
    _bump_freq(cache_snapshot, k)

    # Remove any stale ghosts of the same key (defensive)
    if k in B1:
        B1.pop(k, None)
        global b1_bytes
        b1_bytes = max(0, b1_bytes - sz)
    if k in B2:
        B2.pop(k, None)
        global b2_bytes
        b2_bytes = max(0, b2_bytes - sz)

    if k in T1:
        # Promote T1 -> T2
        T1.pop(k, None)
        t1_bytes = max(0, t1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in T2:
        # Refresh recency
        _move_to_mru(T2, k)
        m_seg[k] = 1
    else:
        # Untracked but a hit (defensive): treat as frequent
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - If key was in B1: increase p_bytes slightly and place in T2 (recency helped).
    - If key was in B2: decrease p_bytes slightly and place in T2 (frequency helped).
    - Else: place in T1 (probation).
    - Maintain byte-accurate counters and trim ghosts to capacity.
    - Do not bump frequency on cold insert; hits will do that.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, p_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)
    cap = _cap(cache_snapshot)

    m_size[k] = sz
    m_time[k] = now
    # Keep existing decayed freq count (cold insert does not add one)

    if cap <= 0:
        return

    # Soft adaptation step (bytes): gentle movement to avoid oscillation
    # Use a small fraction of capacity, bounded by item size.
    step = min(sz, max(1, cap // 64))

    if k in B1:
        # Favor recency: grow T1 target, place into T2
        p_bytes = min(cap, p_bytes + step)
        B1.pop(k, None)
        b1_bytes = max(0, b1_bytes - sz)
        # Place into T2
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in B2:
        # Favor frequency: shrink T1 target, place into T2
        p_bytes = max(0, p_bytes - step)
        B2.pop(k, None)
        b2_bytes = max(0, b2_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    else:
        # New key -> T1 (probation)
        T1[k] = None
        t1_bytes += sz
        m_seg[k] = 0

    _trim_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting one victim:
    - Move evicted key to the appropriate ghost list (B1 if from T1, B2 if from T2).
    - Update resident byte counters and metadata.
    - Keep ghosts bounded to capacity.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes
    _ensure_p(cache_snapshot)

    ekey = getattr(evicted_obj, "key", None)
    if ekey is None:
        return

    esz = _size_of(evicted_obj)
    m_size[ekey] = esz  # remember last known size for ghost accounting

    seg = m_seg.get(ekey, None)

    # Remove from resident segments (if still present)
    _safe_remove_from_resident(ekey, esz)

    if seg == 0:
        # From T1 -> B1
        B1[ekey] = None
        b1_bytes += esz
    elif seg == 1:
        # From T2 -> B2
        B2[ekey] = None
        b2_bytes += esz
    else:
        # Unknown segment: default to B1
        B1[ekey] = None
        b1_bytes += esz

    # Clean live segment assignment (keep size and freq as history)
    m_seg.pop(ekey, None)

    _trim_ghosts(cache_snapshot)
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpru5uzz_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3gmrhny_.pickle

Iteration 74: New subsample score 0.309884 is not better than old score 0.9371590000000001, skipping
Iteration 75: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbw0fut6p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxff3d_t7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp55atm3xq.pickle

Iteration 75: Proposed new text for program: # Learning Size-Aware Reuse with TinyLFU (LSR-TLFU)
# -------------------------------------------------
# Goal: Reduce miss rate by predicting next reuse time (LRB-like), blending with frequency (TinyLFU),
# and evicting by a size-aware farthest-next-use heuristic.
#
# Policy summary:
# - Maintain per-key EWMA of inter-arrival time (gap_avg), updated on hits.
# - Predict next reuse as last_access + gap_avg. If unknown, use a global EWMA gap.
# - Use TinyLFU (Count-Min Sketch) for global frequency estimation with periodic decay.
# - Evict the item with the highest "evict score":
#       score = predicted_distance * (size ** SIZE_EXP) / (1 + ALPHA * freq_est)
#   where predicted_distance = max(1, predicted_next - now). This favors:
#       * Evicting items predicted to be used far in the future (Belady-like).
#       * Evicting larger items sooner (size-aware).
#       * Keeping items with high frequency estimate (TinyLFU).
# - On hit: update sketch, last_access, per-key gap_avg via EWMA, and global average gap.
# - On insert: initialize metadata; treat new items conservatively (no special segments).
# - On evict: clean metadata.
#
# Notes:
# - This design intentionally ignores the prior W/P/S segments; metadata is simplified and global.
# - Ties are broken by larger size, older last access, then key order for determinism.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_gap_avg = dict()       # key -> float, EWMA of inter-arrival time
m_hits = dict()          # key -> int, number of hits observed
m_global_gap = 0.0       # global EWMA of inter-arrival time across all keys

# Keep old names defined for compatibility (not used by policy)
m_seg = dict()
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
# EWMA update for per-key inter-arrival prediction
GAP_ALPHA = 0.30          # weight of new gap in per-key EWMA
GLOBAL_GAP_ALPHA = 0.05   # weight for updating global EWMA gap

# TinyLFU weight in eviction scoring (higher -> keep frequent items more)
ALPHA = 2.0

# Size exponent in scoring; 1.0 strongly penalizes large objects
SIZE_EXP = 1.0

# Default init gap multiplier used when we have no per-key history
INIT_GAP_SCALE = 1.0

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _default_gap(now):
    # Use global EWMA if available; else fall back to a small positive gap
    g = m_global_gap if m_global_gap > 0 else 1.0
    return max(1.0, g * INIT_GAP_SCALE)

def _predicted_distance(k, now):
    la = m_last_access.get(k, 0)
    gap = m_gap_avg.get(k)
    if gap is None:
        gap = _default_gap(now)
    # predicted next time and its distance from now (clamped to >= 1)
    pred_next = la + gap
    return max(1.0, float(pred_next - now))

def _evict_score(k, obj, now):
    # Higher score => worse to keep => evict first
    size = float(_size_of(obj))
    dist = _predicted_distance(k, now)
    est = float(_sketch_estimate(k))
    denom = 1.0 + ALPHA * est
    # Size-aware penalty and frequency-aware protection
    score = (dist * (size ** SIZE_EXP)) / denom
    return score

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Select victim with the highest eviction score:
      score = predicted_distance * size^SIZE_EXP / (1 + ALPHA * freq_est)
    Ties: higher size, older last access, lexicographic key
    """
    if not cache_snapshot.cache:
        return None

    now = cache_snapshot.access_count
    best_k = None
    best_score = None
    best_size = None
    best_la = None

    for k, v in cache_snapshot.cache.items():
        sc = _evict_score(k, v, now)
        sz = _size_of(v)
        la = m_last_access.get(k, -1)

        if best_k is None:
            best_k, best_score, best_size, best_la = k, sc, sz, la
            continue

        # Prefer the highest score
        if sc > best_score:
            best_k, best_score, best_size, best_la = k, sc, sz, la
            continue
        if sc == best_score:
            # Tie-breakers
            if sz > best_size:
                best_k, best_score, best_size, best_la = k, sc, sz, la
                continue
            if sz == best_size:
                if la < best_la:
                    best_k, best_score, best_size, best_la = k, sc, sz, la
                    continue
                if la == best_la and k < best_k:
                    best_k, best_score, best_size, best_la = k, sc, sz, la

    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch.
      - Update per-key EWMA of inter-arrival (gap_avg) using time since last access.
      - Update global EWMA gap.
      - Refresh last access and hit count.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    prev = m_last_access.get(key)

    if prev is not None:
        gap = float(max(1, now - prev))
        old = m_gap_avg.get(key)
        if old is None:
            m_gap_avg[key] = gap
        else:
            m_gap_avg[key] = (1.0 - GAP_ALPHA) * old + GAP_ALPHA * gap
        # Update global EWMA gap
        global m_global_gap
        if m_global_gap <= 0.0:
            m_global_gap = gap
        else:
            m_global_gap = (1.0 - GLOBAL_GAP_ALPHA) * m_global_gap + GLOBAL_GAP_ALPHA * gap
    else:
        # No previous access recorded (should be rare), seed gap with default
        m_gap_avg[key] = _default_gap(now)

    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # We keep m_seg for compatibility but do not use segments; mark as generic
    m_seg[key] = 1  # arbitrary marker


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update sketch.
      - Initialize last access.
      - Initialize gap_avg with global EWMA gap (if available) for faster convergence.
      - Initialize hit counter.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now
    m_hits[key] = 0

    # Seed per-key gap with global average to avoid extreme bias
    init_gap = _default_gap(now)
    m_gap_avg[key] = init_gap

    # Keep m_seg for compatibility but unused by policy
    m_seg[key] = 1


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove metadata for the evicted key.
      - Segment counters are unused by this policy but are cleaned for consistency.
      - Sketch decays elsewhere; no change here.
    """
    key = evicted_obj.key

    # Clean segment metadata if present
    m_seg.pop(key, None)

    # Clean per-key stats
    m_last_access.pop(key, None)
    m_gap_avg.pop(key, None)
    m_hits.pop(key, None)

    # Keep legacy counters consistent (no-ops under this policy)
    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S
    m_bytes_W = max(0, m_bytes_W - 0)
    m_bytes_P = max(0, m_bytes_P - 0)
    m_bytes_S = max(0, m_bytes_S - 0)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiqaodmf8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqp7g8g4j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplav3ivqu.pickle

Iteration 75: New subsample score 0.407164 is better than old score 0.21160299999999999. Continue to full eval and add to candidate pool.
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwfdb6tbr.pickle

Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpir9qmcyv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppdjuiin2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcedj7slb.pickle

Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3rpbn2rf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpupmu8mxj.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2r_eqnme.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaa5n4wv8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2l0bfofy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphw7pwnns.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp010r0nfp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5f0qa88t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6te31z3i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmagjx8uf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46_7jfb6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm55yrixh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0nqcfjqr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqwzt7s68.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa_r6sudv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbnyyhvh7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphuu5kjwk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7jzow7h2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp78myi3qw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqw3ukc9w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbgb7ntiz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg59bfl3p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxnhfep02.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphrb7o7_y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4fc4mwqh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb25nosjm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl7i_rl6i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5loj6jua.pickle

Iteration 75: Full valset score for new program: 0.16435370833333332
Iteration 75: Full train_val score for new program: 0.16435370833333332
Iteration 75: Individual valset scores for new program: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.528536, 0.125, 0.250444, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.235564, 0.422812, 0.0, 0.059961, 0.059961, 0.331363, 0.105847, 0.700099, 0.820298, 0.179096, 0.025, 0.068337, 0.0, 0.087912, 0.652825, 0.083333, 0.154457, 0.202261, 0.589902, 0.121771, 0.157194, 0.120953, 0.178742, 0.095395, 0.316667, 0.182969, 0.17223, 0.466258, 0.143791]
Iteration 75: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.89235, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 75: Full valset pareto front score: 0.2779080000000001
Iteration 75: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {24}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {28}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30}, {29, 14}, {24, 25}, {31}, {24, 25}, {3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31}, {24}]
Iteration 75: Best valset aggregate score so far: 0.25926008333333345
Iteration 75: Best program as per aggregate score on train_val: 29
Iteration 75: Best program as per aggregate score on valset: 29
Iteration 75: Best score on valset: 0.25926008333333345
Iteration 75: Best score on train_val: 0.25926008333333345
Iteration 75: Linear pareto front program index: 29
Iteration 75: New program candidate index: 31
Iteration 76: Selected program 28 score: 0.24792383333333334
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkaej8imh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo5kxt3w7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj43xgw4i.pickle

Iteration 76: Proposed new text for program: # GreedyDual-Size-Frequency ARC (GDSF-ARC)
# - ARC-style two resident segments:
#     T1: probation (recent, 1-touch)
#     T2: protected (frequent, 2+ touches)
#   with ghost lists B1 and B2 to adapt the target size of T1 in bytes (p_bytes).
#
# - Eviction priority uses GreedyDual-Size-Frequency:
#     Each key k maintains an H[k] "priority". Lower H is more evictable.
#     On insert:  H[k] = L + (1 / size)
#     On hit:     H[k] += (1 / size)
#     On eviction of victim v: L := H[v]
#   This biases toward retaining small items, rewards frequency, and ages old items
#   through the global L that increases as victims are evicted.
#
# - Eviction chooses segment first by ARC target in bytes:
#     If T1_bytes >= p_bytes (or T2 empty) evict from T1; else from T2.
#   Within the chosen segment, sample a few keys from its LRU tail and evict the one
#   with the smallest H (tie-break by evicting the larger-bytes key for faster space recovery).
#
# - On hit:
#     If in T1 -> promote to T2 (protected), bump H.
#     If in T2 -> refresh MRU, bump H.
#
# - On insert:
#     If key is in B1: increase p_bytes by its size; place into T2 (as ARC).
#     If key is in B2: decrease p_bytes by its size; place into T2.
#     Else: place into T1 (probation).
#
# - After eviction:
#     Move victim key into B1/B2 (matching its segment), update bytes, and set L to H[victim].
#
# - Ghost lists bounded to at most 2x capacity bytes combined. Byte accounting everywhere.
#
# This hybrid keeps ARC's robustness to scans while using GDSF's strong size/frequency
# prioritization, improving miss rates on mixed, size-skewed workloads.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
T1 = OrderedDict()  # key -> None (LRU -> MRU)
T2 = OrderedDict()  # key -> None (LRU -> MRU)

# Ghost segments (keys only)
B1 = OrderedDict()  # key -> None
B2 = OrderedDict()  # key -> None

# Segment assignment for live keys: 0 = T1, 1 = T2
m_seg = dict()      # key -> int

# Last known size for keys (live or ghost)
m_size = dict()     # key -> int

# GreedyDual-Size-Frequency priority per key
m_H = dict()        # key -> float

# Global aging value for GDSF
L_age = 0.0

# Last access time for live keys (not used in priority, kept for housekeeping)
m_time = dict()     # key -> int

# Byte accounting for segments
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# Target bytes for T1 (ARC's adaptation knob)
p_bytes = None

# Ghost sizing: keep ghosts bounded by at most 2x capacity bytes (combined B1+B2).
GHOST_BYTES_FACTOR = 2.0

# Eviction sampling parameters
TAIL_SCAN_T1 = 10
TAIL_SCAN_T2 = 6


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _ensure_p(snapshot):
    global p_bytes
    cap = _cap(snapshot)
    if p_bytes is None:
        p_bytes = cap // 2
    else:
        if cap > 0:
            if p_bytes < 0:
                p_bytes = 0
            elif p_bytes > cap:
                p_bytes = cap
        else:
            p_bytes = 0

def _trim_ghosts(snapshot):
    # Ensure B1+B2 total bytes <= factor * capacity
    global b1_bytes, b2_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        B1.clear()
        B2.clear()
        b1_bytes = 0
        b2_bytes = 0
        return
    limit = int(GHOST_BYTES_FACTOR * max(1, cap))
    while (b1_bytes + b2_bytes) > limit and (B1 or B2):
        # Prefer shrinking the larger ghost
        target = B1 if b1_bytes >= b2_bytes else B2
        if target:
            k, _ = target.popitem(last=False)
            sz = m_size.get(k, 1)
            if target is B1:
                b1_bytes = max(0, b1_bytes - sz)
            else:
                b2_bytes = max(0, b2_bytes - sz)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _lru_key_of(od):
    if not od:
        return None
    for k in od:
        return k
    return None

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front of the provided OrderedDict if they are not in cache.
    Adjust byte counters accordingly using m_size.
    """
    global t1_bytes, t2_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = _lru_key_of(od)
        if k is None:
            break
        if k not in cache:
            # stale; drop it
            _ = od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                t1_bytes = max(0, t1_bytes - sz)
                m_seg.pop(k, None)
            else:
                t2_bytes = max(0, t2_bytes - sz)
                m_seg.pop(k, None)
            changed = True

def _peek_lru_keys(od, k_max, cache):
    """Return up to k_max keys from the LRU side that are still present in the cache."""
    keys = []
    if not od or k_max <= 0:
        return keys
    count = 0
    for k in od:
        if k in cache:
            keys.append(k)
            count += 1
            if count >= k_max:
                break
    return keys

def _H_of(k):
    """Return the GDSF priority for key k; default to current L + (1/size)."""
    global L_age
    sz = max(1, int(m_size.get(k, 1)))
    return float(m_H.get(k, L_age + (1.0 / float(sz))))

def _record_touch(snapshot, obj, boost_hits=1):
    """Update size, time, and GDSF priority on a reference."""
    global m_H
    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    m_time[k] = _now(snapshot)
    # GDSF-F: increase priority by 1/size per hit (boost_hits times)
    inc = (float(boost_hits) / float(sz))
    m_H[k] = _H_of(k) + inc


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using ARC-targeted segment selection plus GDSF priority:
    - Pick segment: if T1 is at/above its byte target (or T2 empty), evict from T1; else from T2.
    - Sample a handful of keys from the chosen segment's LRU tail.
    - Evict the key with the smallest H (GreedyDual priority). Tie-break by larger size.
    - Fallback: if structures are empty or out of sync, evict the largest object in cache.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_p(cache_snapshot)

    # Defensive cleanup at the heads
    _cleanup_stale_front(cache_snapshot, T1, seg_id=0)
    _cleanup_stale_front(cache_snapshot, T2, seg_id=1)

    # Decide which segment to evict from (ARC in bytes)
    global t1_bytes, t2_bytes, p_bytes
    choose_T1 = False
    if T1 and (t1_bytes >= max(1, int(p_bytes)) or not T2):
        choose_T1 = True
    elif not T1 and T2:
        choose_T1 = False
    elif T1 and T2:
        # If both present but T1 under target, prefer T2; else T1
        choose_T1 = (t1_bytes >= int(p_bytes))

    # Gather candidates
    if choose_T1 and T1:
        cands = [(k, 0) for k in _peek_lru_keys(T1, TAIL_SCAN_T1, cache)]
        # If T1 sample empty due to drift, try T2
        if not cands and T2:
            cands = [(k, 1) for k in _peek_lru_keys(T2, TAIL_SCAN_T2, cache)]
    else:
        cands = [(k, 1) for k in _peek_lru_keys(T2, TAIL_SCAN_T2, cache)]
        if not cands and T1:
            cands = [(k, 0) for k in _peek_lru_keys(T1, TAIL_SCAN_T1, cache)]

    if not cands:
        # Out of sync: evict the largest resident to free space quickly
        victim = None
        max_sz = -1
        for k, o in cache.items():
            s = _size_of(o)
            if s > max_sz:
                max_sz = s
                victim = k
        return victim

    # Select the most evictable candidate by minimal H (tie-break: larger size)
    best_key = None
    best_H = float("inf")
    best_sz = -1
    for k, _seg in cands:
        Hk = _H_of(k)
        szk = max(1, m_size.get(k, 1))
        if Hk < best_H or (Hk == best_H and szk > best_sz):
            best_H = Hk
            best_sz = szk
            best_key = k

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update size/time and bump GDSF priority by 1/size.
    - If in T1: promote to T2 (protected).
    - If in T2: refresh MRU.
    - If untracked but a hit: place into T2 (frequent).
    """
    global t1_bytes, t2_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)

    # Record touch and bump GDSF priority
    _record_touch(cache_snapshot, obj, boost_hits=1)

    if k in T1:
        # Promote T1 -> T2
        T1.pop(k, None)
        t1_bytes = max(0, t1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in T2:
        # Refresh MRU in T2
        _move_to_mru(T2, k)
        m_seg[k] = 1
    else:
        # Defensive: not tracked but hit => treat as frequent
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Maintain size/time. Initialize GDSF priority H = L + 1/size.
    - If key in B1: increase p_bytes by its size and place into T2.
    - If key in B2: decrease p_bytes by its size and place into T2.
    - Else: place into T1 (probation).
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, p_bytes
    _ensure_p(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)
    m_size[k] = sz
    m_time[k] = now

    # Initialize GDSF priority on cold insert
    global L_age, m_H
    m_H[k] = _H_of(k)  # ensures default of L + 1/size

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    if k in B1:
        # Favor recency: grow T1 target by the size of this object; move to T2
        p_bytes = min(cap, int(p_bytes) + sz)
        if k in B1:
            B1.pop(k, None)
            b1_bytes = max(0, b1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in B2:
        # Favor frequency: shrink T1 target; move to T2
        p_bytes = max(0, int(p_bytes) - sz)
        if k in B2:
            B2.pop(k, None)
            b2_bytes = max(0, b2_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    else:
        # New key: goes to T1 (probation)
        T1[k] = None
        t1_bytes += sz
        m_seg[k] = 0

    _trim_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting a victim:
    - Move the evicted key to the corresponding ghost list (B1 if from T1, B2 if from T2).
    - Update byte accounting and clean live metadata.
    - Update GDSF aging value L to the evicted key's H.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, L_age
    _ensure_p(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)

    # Update global aging value to evicted key's priority (GreedyDual)
    ev_H = _H_of(ekey)
    if ev_H > L_age:
        L_age = ev_H

    seg = m_seg.get(ekey, None)

    if seg == 0:
        # From T1 -> B1
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        B1[ekey] = None
        b1_bytes += esz
    elif seg == 1:
        # From T2 -> B2
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)
        B2[ekey] = None
        b2_bytes += esz
    else:
        # Unknown segment (defensive): put into B1; remove from T1/T2 if present
        B1[ekey] = None
        b1_bytes += esz
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)

    # Clean live metadata for evicted key (keep size for ghost accounting)
    m_seg.pop(ekey, None)
    # Keep m_H for informational purposes; it's harmless, but we can drop to save memory:
    # m_H.pop(ekey, None)  # optional: disable if you want to reuse H after reentry

    _trim_ghosts(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8ugkm84j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg1zegxrk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6sfqaaq5.pickle

Iteration 76: New subsample score 0.9049109999999999 is not better than old score 0.961637, skipping
Iteration 77: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdy2c85nl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp81z7t4k_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8axtl2z2.pickle

Iteration 77: Proposed new text for program: # EXP-SLRU-TLFU: Exponential reuse probability + Segmented LRU with decayed LFU and size-awareness
# Goals:
# - Robust across patterns (recency + frequency + size)
# - Scan-resistant via segmented protection and "incoming" window
# - Lightweight metadata; O(1) updates

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time estimate)
m_freq = dict()          # key -> float (raw count, not yet decayed)
m_fepoch = dict()        # key -> int (epoch of last freq update)
m_stage = dict()         # key -> int (-1=incoming, 0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Ghost history (bounded; O(1) trimming)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> float (historical hits, for warm-start)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of ghost eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
# Predictive EWMA of inter-arrival
EWMA_BETA = 0.20         # per-key EWMA learning rate
GLOBAL_BETA = 0.010      # global EWMA learning rate (slow)

# Size-awareness and frequency
SIZE_EXP = 1.0           # 1.0 ~ per-byte fairness; <1 favors many small objects
FREQ_BOOST = 1.2         # multiplicative strength of frequency vs recency
MIN_MU = 1.0             # clamp per-key mu lower bound
MAX_MU_FACTOR = 8.0      # clamp per-key mu upper bound relative to global mu

# Cold-start seeding
DEFAULT_MU_MULT = 1.75   # default multiplier for new keys based on global mu

# Segmented policy boost/penalties (value multipliers; larger => more protected)
VALUE_MULT_INCOMING = 0.25     # stage -1, very easy to evict
VALUE_MULT_PROBATION = 1.0     # stage 0
VALUE_MULT_PROTECTED = 2.5     # stage 1, hard to evict

# Decayed LFU via epochs
DECAY_INTERVAL = 8192     # accesses per decay epoch
DECAY_BASE = 0.5          # per-epoch decay multiplier for frequencies

# Ghost history sizing
GHOST_LIMIT_MIN = 2048
GHOST_LIMIT_FACTOR = 1.5

# Numerical guards
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _epoch(now):
    # Integer epoch used for frequency decay
    return int(now // DECAY_INTERVAL)

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _freq_effective(key, now):
    """Return decayed frequency for key using epoch-based aging."""
    fe = m_fepoch.get(key, None)
    f = float(m_freq.get(key, 0.0))
    if f <= 0.0 or fe is None:
        return 0.0
    ge = _epoch(now)
    de = max(0, ge - fe)
    if de == 0:
        return f
    # Apply exponential decay: f * (DECAY_BASE ** de)
    return f * (DECAY_BASE ** de)

def _mu_eff(key):
    """Return clamped per-key mu; if unknown, return None."""
    return m_mu.get(key, None)

def _global_mu_eff():
    return max(MIN_MU, float(m_global_mu))

def _value_score(key, obj, now):
    """
    Higher value => keep longer. Eviction picks the minimum value.
    value = stage_mult * [ reuse_prob(age, mu) * (1 + FREQ_BOOST * log1p(freq_eff)) ] / (size^SIZE_EXP)
    where reuse_prob ~ exp(-age / mu) with mu clamped.
    """
    sz = float(_size_of(obj))
    la = m_last_access.get(key, None)
    if la is None:
        age = 0.0
    else:
        age = max(0.0, float(now - la))

    mu = _mu_eff(key)
    if mu is None:
        # Cold: seed from ghost/global for valuation only (does not persist state)
        if key in g_mu:
            mu = float(g_mu[key])
        else:
            mu = DEFAULT_MU_MULT * _global_mu_eff()
    # Clamp mu for stability
    mu = max(MIN_MU, min(mu, MAX_MU_FACTOR * _global_mu_eff()))

    # Exponential reuse probability
    reuse_prob = math.exp(-age / (mu + _EPS))

    # Decayed frequency boost
    f_eff = _freq_effective(key, now)
    freq_mult = 1.0 + FREQ_BOOST * math.log1p(max(0.0, f_eff))

    # Stage multiplier
    st = m_stage.get(key, 0)
    if st <= -1:
        stage_mult = VALUE_MULT_INCOMING
    elif st >= 1:
        stage_mult = VALUE_MULT_PROTECTED
    else:
        stage_mult = VALUE_MULT_PROBATION

    # Value per byte^alpha
    denom = max(_EPS, (sz ** float(SIZE_EXP)))
    value = stage_mult * (reuse_prob * freq_mult) / denom
    return value

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Start in 'incoming' stage (-1), with frequency=1 (the triggering access).
    """
    global m_mu, m_last_access, m_freq, m_fepoch, m_stage

    if key in g_mu:
        mu0 = float(g_mu[key])
        fr0 = float(g_freq.get(key, 1.0))
        m_mu[key] = max(MIN_MU, min(mu0, MAX_MU_FACTOR * _global_mu_eff()))
        m_freq[key] = max(1.0, fr0)  # warm-start frequency
    else:
        m_mu[key] = max(MIN_MU, DEFAULT_MU_MULT * _global_mu_eff())
        m_freq[key] = 1.0

    m_last_access[key] = now
    m_fepoch[key] = _epoch(now)
    m_stage[key] = -1  # incoming (very easy to evict until proven hot)

def _update_predictor_on_hit(key, now):
    """
    On hit:
    - Update EWMA of inter-arrival time and global mu.
    - Update decayed LFU frequency (epoch-aware).
    - Promote stage to protected on first hit (or keep protected).
    """
    global m_mu, m_last_access, m_freq, m_fepoch, m_stage, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        # Slow global EWMA
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Safety: seed if missing
        m_mu[key] = max(MIN_MU, DEFAULT_MU_MULT * _global_mu_eff())

    m_last_access[key] = now

    # Decayed LFU update: bring to current epoch and add 1
    ge = _epoch(now)
    fe = m_fepoch.get(key, ge)
    f = float(m_freq.get(key, 0.0))
    de = max(0, ge - fe)
    if de > 0:
        f *= (DECAY_BASE ** de)
    f += 1.0
    m_freq[key] = f
    m_fepoch[key] = ge

    # Promotion policy: any hit promotes to protected
    if m_stage.get(key, -1) < 1:
        m_stage[key] = 1

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = float(m_freq.get(evicted_key, 1.0))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the lowest 'value' score.
    Tie-breakers:
      1) Prefer evicting probation/incoming before protected (via value multipliers).
      2) Older last access first (LRU among near-equals).
      3) Larger size first (free more space).
      4) Lexicographic key order for stability.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Select minimum value
    best_key = None
    best_value = None
    best_stage = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        val = _value_score(k, v, now)
        st = m_stage.get(k, 0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz
            continue

        # Lower value is worse; evict it
        if val < best_value:
            best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz
            continue

        if val == best_value:
            # Prefer evicting less protected stages
            if st < best_stage:
                best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz
                continue
            if st == best_stage:
                if la < best_la:
                    best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz
                    continue
                if la == best_la:
                    if sz > best_sz:
                        best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz
                        continue
                    if sz == best_sz and k < best_key:
                        best_key, best_value, best_stage, best_la, best_sz = k, val, st, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time and global mu.
    - Update decayed LFU counter with epoch-aware aging.
    - Promote to protected segment.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline structures exist (edge cases)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Do not double-count frequency; seed already added 1 on insert
        # Here we consider it another hit, so increment via predictor update
    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Warm-start predictor from ghost if available; else from global mu.
    - Set last access time, start in 'incoming' (-1) to resist scans.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key.
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key metadata of the evicted key.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_fepoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfx0a214z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphr5lyn3h.pickle

Killing subprocess...
Iteration 77: New subsample score 0.30960299999999996 is not better than old score 0.483257, skipping
Iteration 78: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc_ipdhnb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptrjnnwhs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa34s7rcg.pickle

Iteration 78: Proposed new text for program: # GreedyDual-Size (GDS) with simple recency tie-breakers
# Core ideas:
# - Maintain a per-key priority H and a global "age" L (classic GreedyDual).
# - On insert: H = L + 1/size(obj). This gives all new items a fair, size-aware base credit.
# - On hit: H += 1/size(obj). Repeatedly-used items accumulate priority.
# - Evict the key with the smallest H (ties broken by LRU and then by size).
# - On eviction: L = H(evicted). This ages the system so stale items' priorities effectively decay.
#
# Properties:
# - Reduces to LRU when all sizes are equal.
# - Naturally favors small, frequently-used items while still capturing recency via rising L.
# - Very low metadata and computational overhead.
#
# Maintained metadata:
# - m_H: per-key priority H
# - m_last_access: per-key last access time (for LRU tie-breaks)
# - g_L: global age L

# ----------------------
# Global state
# ----------------------
m_H = dict()            # key -> float (priority)
m_last_access = dict()  # key -> int (last access time)
g_L = 0.0               # global age L

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _base_credit(obj):
    # Size-aware credit; small objects get more credit.
    return 1.0 / float(_size_of(obj))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick victim with smallest priority H (GreedyDual-Size).
    Tie-breakers:
      1) Oldest last access (LRU)
      2) Larger size (frees more space if byte capacity matters)
      3) Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # We avoid mutating state here; lazily treat missing H as (L + 1/size).
    victim_key = None
    victim_H = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        sz = _size_of(v)
        h = m_H.get(k, g_L + (1.0 / float(sz)))
        la = m_last_access.get(k, 0)

        if victim_key is None:
            victim_key = k
            victim_H = h
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if h < victim_H:
            better = True
        elif h == victim_H:
            # LRU
            if la < victim_la:
                better = True
            elif la == victim_la:
                # Free more space sooner (if applicable)
                if sz > victim_sz:
                    better = True
                elif sz == victim_sz and k < victim_key:
                    better = True

        if better:
            victim_key = k
            victim_H = h
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increase priority H by size-aware credit.
      - Update last access time.
    """
    key = obj.key
    now = _now(cache_snapshot)
    inc = _base_credit(obj)

    # If key somehow lacks state (e.g., warm start), seed then add hit credit.
    base = m_H.get(key, g_L + inc)
    m_H[key] = base + inc
    m_last_access[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Set H = L + 1/size(obj) to give fair, size-aware initial priority.
      - Set last access time.
    """
    key = obj.key
    now = _now(cache_snapshot)
    m_H[key] = g_L + _base_credit(obj)
    m_last_access[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Set global age L to the evicted key's priority H.
      - Remove evicted key's metadata.
    """
    global g_L
    ekey = evicted_obj.key

    # Update global age to the evicted key's H (GreedyDual rule).
    evicted_H = m_H.get(ekey, g_L)
    if evicted_H > g_L:
        g_L = evicted_H

    # Clean metadata
    m_H.pop(ekey, None)
    m_last_access.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqavbrj2d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppod3n2ym.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnmffzjn6.pickle

Iteration 78: New subsample score 0.111109 is not better than old score 0.181046, skipping
Iteration 79: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprocnbyeq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyy6ywakn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppfndmyt6.pickle

Iteration 79: Proposed new text for program: # Improved: Size-aware W-TinyLFU with admission-by-sample (SLRU main)
# - Three segments by bytes:
#   * Window (W): small recency buffer for new items (handles bursts).
#   * Probation (P): main area for items admitted by TinyLFU.
#   * Protected (S): items with at least two hits; demote S-LRU back to P if S over target.
# - Eviction uses size-aware TinyLFU admission-by-sample:
#   * Compare new object's TinyLFU estimate (size-adjusted) vs the P-LRU's estimate.
#   * If new > victim => "admit": evict from Probation; insert new directly into P (bypass W).
#   * Else "reject": evict from Window; insert new into W.
#   * If preferred segment is empty, fallback to the other; S only as last resort.
# - Recency within segments is maintained via m_last_access timestamps.
# - Size-aware tie-breaker on equal recency: evict larger item first.
# - TinyLFU Count-Min sketch with periodic aging.
#
# Compared to the previous version:
#   * Makes admission decisions explicitly per insertion (pending decision cache).
#   * Uses size-aware frequency comparison for fair treatment of large items.
#   * Only promotes after demonstrated reuse (W->P->S), with S demotion to keep its target.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Admission decision cache: per incoming key, whether to admit to P or reject to W
# 0 = reject (evict from W, insert into W), 1 = admit (evict from P, insert into P)
m_pending_admit = dict()  # key -> 0/1

# --------------------
# Tunable parameters
# --------------------
# Window (recency buffer) fraction of total capacity (bytes)
W_FRAC = 0.20            # Target ~20% window
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80            # 80% of main in protected
# Size-aware TinyLFU exponent: est / size^alpha
SIZE_AWARE_ALPHA = 0.75
EPS = 1e-12

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * W_FRAC)
    main_target = cap - w_target
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _efficiency(est, size):
    # size-aware TinyLFU score
    return est / ((size ** SIZE_AWARE_ALPHA) + EPS)

def _probation_victim_info(cache_snapshot):
    """
    Returns (key, est, size) for P-LRU victim; or (None, 0, 0) if none.
    """
    k = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)
    if k is None:
        return None, 0, 0
    v = cache_snapshot.cache.get(k)
    if v is None:
        return None, 0, 0
    est = _sketch_estimate(k)
    sz = _size_of(v)
    return k, est, sz

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Size-aware W-TinyLFU admission-by-sample:
      - Compare new object's TinyLFU score against P-LRU's score.
      - If new wins (admit): evict from Probation; insert new directly into P.
      - Else (reject): evict from Window; insert new into W.
    Fallbacks:
      - If preferred segment empty, use the other (W or P).
      - If both empty, evict from Protected (S-LRU).
      - Global LRU as last resort.
    Also: keep W near its target when it's oversized.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)

    key_new = obj.key
    size_new = _size_of(obj)
    w_target, _s_target = _targets(cache_snapshot)

    # Decide admission for this insertion, cache it for update_after_insert
    admit = m_pending_admit.get(key_new, None)
    if admit is None:
        # Admission-by-sample vs P-LRU
        new_est = _sketch_estimate(key_new)
        pv_k, pv_est, pv_sz = _probation_victim_info(cache_snapshot)
        if pv_k is None:
            # No P victim: if W has items, reject to W; else admit to P (must free space)
            admit = 0 if m_bytes_W > 0 else 1
        else:
            admit = 1 if _efficiency(new_est, size_new) > _efficiency(pv_est, pv_sz) else 0
        m_pending_admit[key_new] = admit

    # If window is over target, prefer evicting from W to keep it bounded (unless W empty)
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Preferred segment based on admission decision
    prefer_seg = SEG_PROBATION if admit == 1 else SEG_WINDOW

    # Try preferred
    victim = _lru_key_in_segment(cache_snapshot, prefer_seg)
    if victim is not None:
        return victim

    # Fallback to the other main segment
    other_seg = SEG_WINDOW if prefer_seg == SEG_PROBATION else SEG_PROBATION
    victim = _lru_key_in_segment(cache_snapshot, other_seg)
    if victim is not None:
        return victim

    # Last resort: Protected
    victim = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if victim is not None:
        return victim

    # Global LRU fallback (in case of inconsistencies)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch.
      - Update last access time.
      - Promote along W -> P -> S.
      - If S exceeds its byte target, demote S-LRU back to P.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency only
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update sketch and last access.
      - Use any pending admission decision from evict():
           * admit==1 -> place into Probation (bypass W).
           * admit==0 -> place into Window.
        If no pending decision exists (cache wasn't full), perform the same
        admission-by-sample vs P-LRU to decide W vs P.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    sz = _size_of(obj)

    admit = m_pending_admit.pop(key, None)
    if admit is None:
        # Cache wasn't full; still apply admission-by-sample choice
        new_est = _sketch_estimate(key)
        pv_k, pv_est, pv_sz = _probation_victim_info(cache_snapshot)
        if pv_k is None:
            # If no P victim, new goes to W unless W is clearly oversized
            w_target, _ = _targets(cache_snapshot)
            admit = 1 if m_bytes_W <= 0 else (0 if m_bytes_W > w_target else 0)
        else:
            admit = 1 if _efficiency(new_est, sz) > _efficiency(pv_est, pv_sz) else 0

    if admit == 1:
        m_seg[key] = SEG_PROBATION
        global m_bytes_P
        m_bytes_P += sz
    else:
        m_seg[key] = SEG_WINDOW
        global m_bytes_W
        m_bytes_W += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Clear stale pending admission decisions for evicted keys (defensive).
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    m_last_access.pop(key, None)
    # Defensive cleanup; harmless if key not present
    m_pending_admit.pop(key, None)
Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 336
    global m_bytes_W
    ^^^^^^^^^^^^^^^^
SyntaxError: name 'm_bytes_W' is used prior to global declaration

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 336
    global m_bytes_W
    ^^^^^^^^^^^^^^^^
SyntaxError: name 'm_bytes_W' is used prior to global declaration

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 336
    global m_bytes_W
    ^^^^^^^^^^^^^^^^
SyntaxError: name 'm_bytes_W' is used prior to global declaration

Iteration 79: New subsample score 0.0 is not better than old score 0.632146, skipping
Iteration 80: Selected program 30 score: 0.23011145833333335
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpudq4zkzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkzxr6w43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfwio906j.pickle

Iteration 80: Proposed new text for program: # Predictive Size-Aware SLRU with Staleness scoring (PSA-SLRU+S)
# Key improvements over prior version:
# - Fixes anti-LRU behavior: items whose "expected reuse time has passed" are now considered stale
#   and made more evictable via a staleness score = age / EWMA(inter-arrival).
# - Time-aware frequency: frequency contribution decays with idleness, preventing stale heavy hitters
#   from pinning the cache.
# - Scan/pollution resistance: a lightweight recent hit-rate EWMA shrinks the protected segment during scans
#   and increases size-bias under low hit-rate pressure.
# - Keeps ARC-like ghost feedback to adapt the protected share and warm-start reinserts.
# - Size-aware eviction tuned by dynamic gamma depending on object size, capacity, and hit-rate.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()           # key -> int

# Reuse predictor
m_last_access = dict()   # key -> int
m_mu = dict()            # key -> float (EWMA inter-arrival)
m_hits = dict()          # key -> int (total hits)

# Global EWMA mean inter-arrival (for cold-start and stabilization)
m_global_mu = 64.0

# ARC-style target for protected segment share (fraction of live keys)
m_protect_target_frac = 0.50

# Ghost history (bounded)
g_mu = dict()            # key -> float
g_last = dict()          # key -> int
g_hits = dict()          # key -> int
g_seg = dict()           # key -> int (segment at eviction: 0=T1, 1=T2)

# Lightweight signal of recent hit-rate (for scan detection / adaptation)
m_recent_hr = 0.50       # EWMA of instantaneous hit indicator
m_last_tick = 0          # last seen access_count

# ----------------------
# Tunable hyperparameters
# ----------------------
# Frequency dampening (time-aware)
FREQ_DAMP_LOG = True       # use log1p if True; else power
FREQ_POW = 0.6             # if FREQ_DAMP_LOG == False, divisor = (1+eff_hits)^FREQ_POW

# Size-awareness
SIZE_GAMMA_BASE = 0.80     # base exponent for size in eviction score
SIZE_GAMMA_PRESSURE = 0.60 # extra size bias under space pressure ~ size(new)/capacity
SIZE_GAMMA_HR_BOOST = 0.40 # extra boost when recent hit-rate is low (scan-like)

# Segment selection bias (ARC-like)
NONPREF_BIAS = 2.0         # reduce eviction score for non-preferred segment by this factor
ARC_ADAPT_STEP = 0.06      # how fast the protected target fraction adapts on ghost hits
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# EWMA for reuse predictor
EWMA_BETA = 0.20
GLOBAL_BETA = 0.02

# Recent hit-rate EWMA parameters
HR_BETA = 0.05
HR_LOW = 0.20
HR_HIGH = 0.60

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
DEFAULT_MU_MULT = 2.5

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    glen = len(g_last)
    if glen <= limit:
        return
    to_remove = glen - limit
    # Remove 'to_remove' oldest entries by repeated linear min search (usually 1).
    for _ in range(to_remove):
        oldest_k = None
        oldest_t = None
        for k, t in g_last.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k = k
                oldest_t = t
        if oldest_k is None:
            break
        g_mu.pop(oldest_k, None)
        g_last.pop(oldest_k, None)
        g_hits.pop(oldest_k, None)
        g_seg.pop(oldest_k, None)

def _segment_counts(cache):
    prot = 0
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            prot += 1
    total = len(cache)
    return prot, total

def _dynamic_size_gamma(cache_snapshot, incoming_obj):
    # Base + pressure due to incoming size + additional boost when recent hit-rate is low
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    new_sz = float(_size_of(incoming_obj))
    pressure = min(1.0, new_sz / float(cap))
    gamma = SIZE_GAMMA_BASE + SIZE_GAMMA_PRESSURE * pressure
    # If recent hit-rate is low (scan/pollution), be more aggressive against large objects
    if m_recent_hr < HR_LOW:
        gamma += SIZE_GAMMA_HR_BOOST * (HR_LOW - m_recent_hr) / HR_LOW
    return max(0.0, gamma)

def _effective_hits(key, now):
    # Time-decayed hits: older idle items lose frequency influence
    h = float(m_hits.get(key, 0))
    last = m_last_access.get(key, None)
    mu = float(m_mu.get(key, max(1.0, m_global_mu)))
    if last is None:
        return h
    age = max(0.0, float(now - last))
    tau = max(1.0, 2.0 * mu)  # decay horizon proportional to recent inter-arrival
    return h / (1.0 + age / tau)

def _staleness(key, now):
    # staleness = age / mu
    last = m_last_access.get(key, None)
    mu = m_mu.get(key)
    if last is None or mu is None:
        return DEFAULT_MU_MULT * float(m_global_mu)  # cold-start: highly evictable
    age = max(0.0, float(now - last))
    mu = max(1.0, float(mu))
    return age / mu

def _evict_score_for_key(k, v, now, preferred_seg, gamma):
    # Larger evict_score => more attractive to evict
    stale = _staleness(k, now)

    # Time-aware frequency dampening
    eff_hits = _effective_hits(k, now)
    if FREQ_DAMP_LOG:
        freq_div = 1.0 + math.log1p(max(0.0, eff_hits))
    else:
        freq_div = (1.0 + max(0.0, eff_hits)) ** FREQ_POW

    base = stale / max(_EPS, freq_div)

    # Size-aware pressure
    sz = _size_of(v)
    base *= (float(sz) ** gamma)

    seg = m_seg.get(k, 0)
    # Bias away from non-preferred segment
    if seg != preferred_seg:
        base /= NONPREF_BIAS

    la = m_last_access.get(k, -1)
    return base, seg, la, sz


def _record_ghost_on_evict(evicted_key, now):
    # Record minimal ghost info for adaptation and warm-start
    mu = m_mu.get(evicted_key)
    if mu is not None:
        g_mu[evicted_key] = float(mu)
    else:
        g_mu.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_hits[evicted_key] = int(m_hits.get(evicted_key, 0))
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert, warm-starting from ghost if available.
    ARC-style adaptation: adjust protected target based on which ghost list was hit.
    """
    global m_protect_target_frac

    if key in g_seg:
        if g_seg[key] == 0:
            # Ghost hit from probation -> favor recency: shrink protected target
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            # Ghost hit from protected -> favor frequency: grow protected target
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    if key in g_mu:
        m_mu[key] = max(1.0, float(g_mu[key]))
        m_hits[key] = max(0, int(g_hits.get(key, 0)))
        # Place ghost hits into protected if enabled
        if START_PROTECTED_FROM_GHOST and key in g_seg:
            m_seg[key] = 1
        else:
            m_seg[key] = 0
    else:
        # Cold start in probation with a conservative mu; 0 hits
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_hits[key] = 0
        m_seg[key] = 0

    m_last_access[key] = now


def _update_predictors_on_hit(key, now):
    # EWMA reuse predictor per-key
    last = m_last_access.get(key)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * float(prev_mu) + EWMA_BETA * gap
        # Global baseline
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_hits[key] = m_hits.get(key, 0) + 1

    # Promote to protected on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


def _update_recent_hr(is_hit):
    # Update a simple EWMA of recent hit rate (per access)
    global m_recent_hr
    m_recent_hr = (1.0 - HR_BETA) * float(m_recent_hr) + HR_BETA * (1.0 if is_hit else 0.0)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using adaptive segment preference + staleness-based, time-decayed frequency,
    size-aware scoring.

    Steps:
      - Compute preferred segment: whichever (T1 or T2) exceeds its adaptive target.
      - Under scan-like phases (low recent hit-rate), the preferred segment is further biased to probation.
      - Within segments, select key with largest eviction score:
            score = (age / mu) / freq_damp * size^gamma
        and reduce score by NONPREF_BIAS for non-preferred segment keys.
      - gamma increases under high space pressure and during low hit-rate phases.
      - Tie-breakers:
          * Prefer evicting from preferred segment
          * Older last access (LRU)
          * Larger size (free more space)
          * Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Segment preference
    prot_count, total = _segment_counts(cache)
    target_prot = int(round(float(m_protect_target_frac) * float(total)))

    preferred_seg = 1 if prot_count > target_prot else 0
    # During scans (low hit-rate), prefer evicting from probation even more
    if m_recent_hr < HR_LOW:
        preferred_seg = 0

    gamma = _dynamic_size_gamma(cache_snapshot, obj)

    victim_key = None
    victim_seg = None
    victim_la = None
    victim_sz = None
    best_score = None

    for k, v in cache.items():
        score, seg, la, sz = _evict_score_for_key(k, v, now, preferred_seg, gamma)

        if victim_key is None:
            victim_key = k
            best_score = score
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if score > best_score:
            better = True
        elif score == best_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key = k
            best_score = score
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Ensure per-key predictor state exists.
      - Update per-key/global EWMA reuse predictor and hit count.
      - Promote to protected segment if currently probation.
      - Update recent hit-rate EWMA (for scan detection / adaptation).
    """
    key = obj.key
    now = _now(cache_snapshot)

    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)

    _update_predictors_on_hit(key, now)
    _update_recent_hr(is_hit=True)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Update recent hit-rate EWMA with a miss.
      - ARC-style adaptation using ghost feedback to adjust protected target fraction.
      - Initialize per-key predictor and placement (warm-start from ghost when available).
      - Cold items start in probation; ghost hits can start in protected.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _update_recent_hr(is_hit=False)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction of evicted_obj:
      - Record ghost metadata (mu, last segment, hits, timestamp).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeslmelnm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjinpm1z9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgwunsj30.pickle

Iteration 80: New subsample score 1.282566 is not better than old score 1.297498, skipping
Iteration 81: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqzzbeg5i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2hgzotf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt6kqv3yz.pickle

Iteration 81: Proposed new text for program: # Improved cache eviction algorithm: Size-aware Recency-Frequency with Adaptive Window and SLRU (SRF-AW)
# - Architecture remains three segments (Window W, Probation P, Protected S) but with improved scoring and adaptation.
# - Admission/Eviction decisions use a size-normalized TinyLFU score with a recency multiplier.
# - Window size is adapted using ghosts and gentle hit-based hill-climbing.
# - Protected is bounded strictly by target with iterative demotion when needed.
# - Size-aware promotion guard avoids polluting Protected with extremely large objects.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096  # must be power of two for fast masking
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Adaptive window sizing (ARC-inspired, by fraction of bytes)
m_W_frac = 0.20         # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Gentle hit-based hill-climb adjustments (small nudges)
HIT_TUNE_W_INC = 0.005
HIT_TUNE_W_DEC = 0.003

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80           # 80% of main in protected

# Scoring parameters
ADMIT_EPS = 1e-12
EPS = 1e-12
SIZE_BETA = 0.75                 # size exponent for density (est / size^beta)
RECENCY_HALF_LIFE = 4096         # accesses: larger -> slower recency decay
S_PROMOTE_MAX_FRAC = 0.50        # do not promote to S if item size > 50% of capacity

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    try:
        if hasattr(obj_or_size, "size"):
            return max(1, int(obj_or_size.size))
        return max(1, int(obj_or_size))
    except Exception:
        # Defensive: in any weird case, treat as 1
        return 1

def _targets(cache_snapshot):
    # Compute target bytes for Window and Protected using adaptive window fraction
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)  # non-zero window
    main_target = max(0, cap - w_target)
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    Iterate until within target or no candidates remain.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    while m_bytes_S > s_target + EPS:
        victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
        if victim_key is None:
            return
        v = cache_snapshot.cache.get(victim_key)
        if v is None:
            return
        sz = _size_of(v)
        _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _recency_factor(key, now):
    """
    Recency multiplier in (0,1], decays with age.
    """
    last = m_last_access.get(key, now)
    age = max(0, now - last)
    hl = max(1.0, float(RECENCY_HALF_LIFE))
    return 1.0 / (1.0 + (age / hl))

def _est_for_key(cache_snapshot, key):
    return _sketch_estimate(key)

def _keep_score_for_key(cache_snapshot, key):
    """
    Size-aware, recency-weighted keep score for an existing item.
    Higher score means more deserving to keep.
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = float(_est_for_key(cache_snapshot, key))
    sz = float(_size_of(v))
    if sz <= 0:
        sz = 1.0
    density = est / (sz ** SIZE_BETA)
    now = cache_snapshot.access_count
    rec = _recency_factor(key, now)
    return density * rec

def _admit_score_for_new(obj):
    """
    Score for a new candidate object: size-aware frequency score.
    Recency factor is 1.0 at admission time (it is 'now').
    """
    est = float(_sketch_estimate(obj.key))
    sz = float(_size_of(obj))
    if sz <= 0:
        sz = 1.0
    return est / (sz ** SIZE_BETA)

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp (deterministic tie by key)
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

def _choose_between(cache_snapshot, k1, k2):
    """
    Choose the worse key (lower keep score) between two candidates.
    Returns the key to evict; if one is None, returns the other.
    """
    if k1 is None:
        return k2
    if k2 is None:
        return k1
    s1 = _keep_score_for_key(cache_snapshot, k1)
    s2 = _keep_score_for_key(cache_snapshot, k2)
    # Evict the lower keep score. On tie, prefer evicting the larger item, then lexicographic.
    if s1 < s2 - ADMIT_EPS:
        return k1
    if s2 < s1 - ADMIT_EPS:
        return k2
    # tie-breakers
    v1 = cache_snapshot.cache.get(k1)
    v2 = cache_snapshot.cache.get(k2)
    sz1 = _size_of(v1) if v1 is not None else 1
    sz2 = _size_of(v2) if v2 is not None else 1
    if sz1 > sz2:
        return k1
    if sz2 > sz1:
        return k2
    return k1 if k1 < k2 else k2

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction policy:
      - Maintain segment targets: if W is oversized, evict W-LRU.
      - If inserting new would overflow W target, perform TinyLFU-style admission:
          * Compare new's score vs P-LRU's keep score (both size-aware; P-LRU also gets recency factor).
          * If new wins -> evict P-LRU (admit new into W).
          * Else -> evict W-LRU (approximate admission rejection).
      - Otherwise, evict the worse of P-LRU and W-LRU by keep score.
      - Avoid S unless no other options; then use S-LRU as last resort.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)

    new_sz = _size_of(obj)
    new_score = _admit_score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_bytes_W > w_target + EPS:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_bytes_W + new_sz) > w_target + EPS

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)

    if predict_overflow:
        if p_lru is None:
            # No P items to compare against -> evict from W if possible
            if w_lru is not None:
                return w_lru
            # Last resort: evict from S
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            if s_lru is not None:
                return s_lru
            # Fallback global LRU
            return _choose_between(cache_snapshot, None, None)
        else:
            # Admission comparison: new vs P-LRU
            p_keep = _keep_score_for_key(cache_snapshot, p_lru)
            if new_score > p_keep + ADMIT_EPS:
                # Admit: evict P-LRU
                return p_lru
            else:
                # Reject-like: evict from W
                if w_lru is not None:
                    return w_lru
                return p_lru  # if W empty, must evict from P

    # No predicted W overflow: evict the worse of P-LRU and W-LRU
    if p_lru is not None or w_lru is not None:
        return _choose_between(cache_snapshot, p_lru, w_lru)

    # If both empty, consider S as last resort
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - Promotions:
          * W -> P on hit (first confirmation).
          * P -> S on hit (frequency confirmation), unless object is extremely large (guard).
      - Keep S within its target by iterative demotion of S-LRU to P when needed.
      - Gentle window tuning: W hit slightly increases W fraction; P/S hit slightly decreases.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    # Gentle hill-climb on window fraction based on where hits occur
    global m_W_frac
    if seg == SEG_WINDOW:
        m_W_frac = min(W_FRAC_MAX, m_W_frac + HIT_TUNE_W_INC)
        # First confirmation; graduate to P to test persistence
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        m_W_frac = max(W_FRAC_MIN, m_W_frac - HIT_TUNE_W_DEC)
        # Second confirmation; promote to S if not too large
        cap = max(1, int(cache_snapshot.capacity))
        if sz <= int(cap * S_PROMOTE_MAX_FRAC):
            _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        # Keep S under target
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency; keep S under target
        m_W_frac = max(W_FRAC_MIN, m_W_frac - HIT_TUNE_W_DEC)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Record access in sketch and recency.
      - Adaptive window sizing via ghosts (ARC-like):
          * If key hits W-ghost, increase W fraction.
          * If key hits Main-ghost (P/S), decrease W fraction.
      - Insert into W.
      - If Window exceeds its target, iteratively move its LRU(s) to P (keeps W near target).
      - Keep S within target via iterative demotion.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window
    sz = _size_of(obj)
    global m_bytes_W
    m_seg[key] = SEG_WINDOW
    m_bytes_W += sz

    # If Window is above target, move its LRU(s) to Probation (does not change total size)
    w_target, _ = _targets(cache_snapshot)
    # Iterate to avoid leaving W far above target after large inserts
    while m_bytes_W > w_target + EPS:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=key)
        if candidate is None:
            break
        v = cache_snapshot.cache.get(candidate)
        if v is None:
            break
        csz = _size_of(v)
        _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl3c7lmkl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpktnwsgz_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps8o4_9as.pickle

Iteration 81: New subsample score 0.6031239999999999 is better than old score 0.598622. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqd5y7zk2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcy71t_sv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvtoha5f4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2cu53ao5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq86l94fc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqwjoqco8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw_dgx8xu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpglvixwhe.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp49olnc9r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxr8p4yed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwkr8a6ly.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp30ktmdzx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_zripbh9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpimeifsot.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp66_t5l2e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvotgkjg3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpav72z02x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp08bcsbeu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7e3rofqi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwxhspd43.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn_pphrll.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwxq44to9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr_7j9e_j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu5mu0dzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9fr30lup.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpckrn70ib.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_rhyno52.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8ceuuisz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6b1x4am3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc1ibnrrw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb58je7zy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9qt30zbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgyqhcbnd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkrwr2swm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp81gkhtck.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp018apdu_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxaor5vf7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphqucez1x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwesnfk_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxtzau_nn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptrp4nu8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbknj8k7g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1o7ihrol.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwev80mrp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcfw1n9i6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr4ds4i7f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgc3ax0gs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1b7igybh.pickle

Iteration 81: Full valset score for new program: 0.23755431249999992
Iteration 81: Full train_val score for new program: 0.23755431249999992
Iteration 81: Individual valset scores for new program: [0.458674, 0.442622, 0.449973, 0.407309, 0.461037, 0.447192, 0.260766, 0.388911, 0.528781, 0.531017, 0.116667, 0.353464, 0.026424, 0.0, 0.021237, 0.021274, 0.020062, 0.023615, 0.022922, 0.265811, 0.391347, 0.025576, 0.057382, 0.059317, 0.293784, 0.300403, 0.805362, 0.887558, 0.107517, 0.038636, 0.04328, 0.01233, 0.04434, 0.732641, 0.070175, 0.126214, 0.097281, 0.633694, 0.125461, 0.078736, 0.066585, 0.104743, 0.047697, 0.216667, 0.111077, 0.102945, 0.466258, 0.107843]
Iteration 81: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.89235, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 81: Full valset pareto front score: 0.2779080000000001
Iteration 81: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {24}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {28}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32}, {29, 14}, {24, 25}, {31}, {24, 25}, {3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32}, {24}]
Iteration 81: Best valset aggregate score so far: 0.25926008333333345
Iteration 81: Best program as per aggregate score on train_val: 29
Iteration 81: Best program as per aggregate score on valset: 29
Iteration 81: Best score on valset: 0.25926008333333345
Iteration 81: Best score on train_val: 0.25926008333333345
Iteration 81: Linear pareto front program index: 29
Iteration 81: New program candidate index: 32
Iteration 82: Selected program 29 score: 0.25926008333333345
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp08jxl2ae.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqozkjs6m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfhy74vbv.pickle

Iteration 82: Proposed new text for program: # ReASAP-SLRU-TLFU v2: Bounded-SLRU with predictive recency and decayed frequency
# Improvements over previous version:
# - True bounded SLRU: maintain a target protected segment size (by bytes). When protected exceeds
#   its target, demote the coldest protected item back to probation. Eviction draws from the
#   pressured segment (probation by default, protected when over target). This fixes over-protection
#   and adapts to shifting workloads.
# - Safer cold-start prediction: new items use a finite default next-arrival estimate instead of INF.
#   Still evicted aggressively via NEW_ITEM_BONUS, but avoids pathological scoring.
# - Tuned parameters: faster per-key EWMA, stronger overdue penalty, slightly higher new-item bonus,
#   and slightly stronger frequency damping.
# - Ghost reinsert is tempered more: halve historical frequency when warming to avoid stale dominance.
# - Tie-breakers and size fairness retained; frequency uses aging epochs as before.
#
# All operations are O(number of cached items) for eviction/demotion (single pass),
# O(1) for updates, and O(1) amortized for ghost trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_freq = dict()           # key -> int (raw hit count, aged by epoch lazily)
m_f_epoch = dict()        # key -> int (epoch of last freq normalization)
m_stage = dict()          # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Segment byte counters (best-effort; guarded against drift)
bytes_probation = 0
bytes_protected = 0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()             # key -> float (last known EWMA)
g_freq = dict()           # key -> int (historical hits)
g_stamp = dict()          # key -> int (version counter)
g_order = deque()         # deque of (key, stamp) in LRU order of eviction

# Decayed frequency global epoch
g_epoch = 0
g_next_epoch_at = 4096    # next access_count to advance epoch
EPOCH_LENGTH = 4096       # accesses per epoch
# With DECAY = 0.5 per epoch, we can use integer right-shifts for speed.
DECAY_SHIFT_PER_EPOCH = 1  # 1 => halve per epoch

# ----------------------
# Tunable hyperparameters
# ----------------------
# Predictor / frequency
EWMA_BETA = 0.4               # per-key EWMA learning rate (faster adaptation)
GLOBAL_BETA = 0.01            # global EWMA learning rate (slow)
FREQ_DAMP = 0.8               # dampened influence of frequency via log1p
DECAY_SHIFT_PER_EPOCH = 1     # halve per epoch

# Size awareness
SIZE_EXP = 1.0                # size exponent (1.0 = per-byte fairness)

# Recency prediction / cold-start
DEFAULT_MU_MULT = 2.5         # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_PENALTY = 1.25        # factor applied to lateness when overdue (higher => evict sooner)
NEW_ITEM_BONUS = 1.75         # multiplier for eviction score of items with freq==0/1 (more likely to evict)

# SLRU segmenting
PROTECTED_FRACTION = 0.8      # target fraction of bytes for protected segment
STAGE_FACTOR_PROTECTED = 0.6  # discount to eviction score for protected items (within protected pool)

# Ghost history housekeeping
GHOST_LIMIT_MIN = 1024        # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0      # ghost capacity  factor * live cache item count

_INF = 1e12  # finite but effectively "very large" for our scale

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    global g_epoch, g_next_epoch_at
    if now >= g_next_epoch_at:
        # Advance by as many epochs as needed (handles large jumps)
        steps = max(1, (now - g_next_epoch_at) // EPOCH_LENGTH + 1)
        g_epoch += steps
        g_next_epoch_at += steps * EPOCH_LENGTH

def _decayed_freq(key):
    # Return frequency aged to current epoch, without mutating stored value
    raw = m_freq.get(key, 0)
    k_epoch = m_f_epoch.get(key, g_epoch)
    d = g_epoch - k_epoch
    if d <= 0:
        return raw
    # divide by 2^(DECAY_SHIFT_PER_EPOCH * d)
    shift = DECAY_SHIFT_PER_EPOCH * d
    return raw >> shift

def _bump_freq_on_hit(key):
    # Normalize to current epoch then increment
    ke = m_f_epoch.get(key, g_epoch)
    if ke != g_epoch:
        d = g_epoch - ke
        if d > 0:
            shift = DECAY_SHIFT_PER_EPOCH * d
            m_freq[key] = m_freq.get(key, 0) >> shift
        m_f_epoch[key] = g_epoch
    m_freq[key] = m_freq.get(key, 0) + 1

def _protected_target_bytes(cache_snapshot):
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    # Ensure at least some probation space
    target = int(PROTECTED_FRACTION * cap)
    target = min(max(1, target), cap - 1)
    return target

def _maybe_repair_segment_counters(cache_snapshot):
    # If counters drift too much, recompute from current cache and m_stage.
    global bytes_probation, bytes_protected
    total = bytes_probation + bytes_protected
    cache_size = int(getattr(cache_snapshot, "size", 0))
    if cache_size <= 0:
        bytes_probation = 0
        bytes_protected = 0
        return
    # Allow some slack; if off by > 10% of capacity, recompute
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    if abs(total - cache_size) > max(1024, cap // 10):
        bp = 0
        bpr = 0
        cache = getattr(cache_snapshot, "cache", {}) or {}
        for k, v in cache.items():
            if m_stage.get(k, 0) == 1:
                bpr += _size_of(v)
            else:
                bp += _size_of(v)
        bytes_probation = bp
        bytes_protected = bpr

def _predicted_delta_components(key, now):
    """
    Return (effective_delta, is_overdue)
    - If predictor exists: time until next predicted access; if overdue, penalize lateness.
    - If no predictor: use a conservative finite default estimate based on global mu.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if la is None:
        return (_INF, False)
    if mu is None:
        # Finite cold start estimate
        mu_eff = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        next_t = la + mu_eff
        delta = max(1.0, float(next_t - now))
        return (delta, False)

    next_t = la + mu
    if now <= next_t:
        return (float(next_t - now), False)
    overdue = float(now - next_t)
    return (OVERDUE_PENALTY * overdue, True)

def _eviction_score_within_stage(key, obj, now, is_protected):
    """
    Higher score => evict sooner among items in the same stage.
    score = (effective_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq_adj))) * stage_factor
    - New/low-frequency items get a boost to the score (more likely to evict).
    - Protected items get a discount (stage_factor) when comparing within protected pool.
    """
    sz = _size_of(obj)
    delta, _ = _predicted_delta_components(key, now)

    # Decayed frequency to avoid stale dominance
    f_eff = max(0, _decayed_freq(key))
    denom_freq = 1.0 + FREQ_DAMP * math.log1p(float(f_eff))

    base = delta / ((sz ** SIZE_EXP) * denom_freq)

    # Boost eviction score for items that have never or barely hit
    raw_freq = m_freq.get(key, 0)
    if raw_freq <= 1:
        base *= NEW_ITEM_BONUS

    # Protected items get discount only within protected pool comparisons
    if is_protected:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - Start in probation (stage=0).
    - Initialize frequency with epoch tagging (so it will age).
    """
    global m_global_mu
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        # Initialize decayed freq from ghost, but temper it (halve) and ensure current epoch
        ghf = max(0, int(g_freq.get(key, 1)))
        m_freq[key] = max(0, ghf >> 1)
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0  # will be incremented on hit

    m_f_epoch[key] = g_epoch
    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment decayed frequency.
    """
    global m_mu, m_last_access, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold edge case (should be rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    _bump_freq_on_hit(key)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    # Store a small bounded freq (avoid huge integers)
    g_freq[evicted_key] = min(255, max(0, int(m_freq.get(evicted_key, 0))))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _stage_of_key(key):
    return m_stage.get(key, 0)

def _inc_probation_bytes(sz):
    global bytes_probation
    bytes_probation += max(0, int(sz))

def _move_probation_to_protected(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = max(0, bytes_probation - s)
    bytes_protected += s

def _move_protected_to_probation(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = max(0, bytes_protected - s)
    bytes_probation += s

def _dec_probation_bytes(sz):
    global bytes_probation
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0

def _dec_protected_bytes(sz):
    global bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = 0

def _choose_victim_in_stage(cache_snapshot, stage, now, exclude_key=None):
    """
    Return the key of the highest score victim in the requested stage (0=probation,1=protected).
    If exclude_key is provided, skip it.
    """
    cache = getattr(cache_snapshot, "cache", {}) or {}
    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None
    is_protected = (stage == 1)

    for k, v in cache.items():
        if k == exclude_key:
            continue
        if _stage_of_key(k) != stage:
            continue
        s = _eviction_score_within_stage(k, v, now, is_protected=is_protected)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
            best_key, best_score, best_la, best_sz = k, s, la, sz
    return best_key

def _demote_one_protected(cache_snapshot, avoid_key=None):
    """
    Demote the coldest protected item to probation to keep protected within budget.
    Avoid demoting 'avoid_key' if possible.
    """
    now = _now(cache_snapshot)
    victim = _choose_victim_in_stage(cache_snapshot, stage=1, now=now, exclude_key=avoid_key)
    if victim is None:
        return
    cache = getattr(cache_snapshot, "cache", {}) or {}
    vobj = cache.get(victim, None)
    if vobj is None:
        return
    # Demote victim
    m_stage[victim] = 0
    _move_protected_to_probation(_size_of(vobj))

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict a resident object using bounded SLRU with predictive scoring.
    Steps:
      - Advance frequency epoch if needed and repair counters if heavily drifted.
      - Determine the pressured segment:
         * Prefer evicting from probation.
         * If protected is over its target bytes OR probation has no items, prefer protected.
      - Choose the highest-score victim from the chosen segment.
    Tie-breakers within the chosen pool:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)
    _maybe_repair_segment_counters(cache_snapshot)

    # Determine segment pressure
    target_prot = _protected_target_bytes(cache_snapshot)

    # Check existence in each stage
    has_prob = False
    has_prot = False
    for k in cache.keys():
        st = _stage_of_key(k)
        if st == 0:
            has_prob = True
        else:
            has_prot = True
        if has_prob and has_prot:
            break

    # Decide which segment to evict from
    from_protected = False
    if has_prot and (bytes_protected > target_prot or not has_prob):
        from_protected = True

    stage = 1 if from_protected else 0
    victim = _choose_victim_in_stage(cache_snapshot, stage=stage, now=now)
    if victim is not None:
        return victim

    # Fallback if chosen segment empty (should be rare): try the other
    alt_stage = 0 if stage == 1 else 1
    return _choose_victim_in_stage(cache_snapshot, stage=alt_stage, now=now)


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch, update per-key EWMA predictor, global mu, and decayed frequency.
    - Promote from probation to protected on first hit (SLRU).
    - If protected exceeds its target, demote the coldest protected back to probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Keep stage=0 from seeding; promotion below will adjust with guards.

    _update_predictor_on_hit(key, now)

    # SLRU promotion on first hit
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1
        _move_probation_to_protected(_size_of(obj))

        # Bounded protected: demote if over target
        target_prot = _protected_target_bytes(cache_snapshot)
        if bytes_protected > target_prot:
            _demote_one_protected(cache_snapshot, avoid_key=key)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Start in probation (stage=0).
    - Update last access time and byte counters for probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _seed_predictor_on_insert(key, now)
    _inc_probation_bytes(_size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Adjust segment byte counters.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust segment bytes using last known stage
    st = m_stage.get(ekey, 0)
    if st == 0:
        _dec_probation_bytes(_size_of(evicted_obj))
    else:
        _dec_protected_bytes(_size_of(evicted_obj))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_f_epoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb_sooxh1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpms_fodrj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_t3lsdk_.pickle

Iteration 82: New subsample score 0.6388739999999999 is not better than old score 0.6656409999999999, skipping
Iteration 83: Selected program 16 score: 0.23630508333333336
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9fi1nan8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd7nt9p_p.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3kn35rwj.pickle

Iteration 83: Proposed new text for program: # Size-aware SLRU with decayed TinyLFU-style frequency and adaptive probation target
# - Two resident segments (byte-accounted):
#     Q (probation: seen once or (re)inserted) and P (protected: promoted on hit from Q)
#   Both maintained as LRU lists (OrderedDict), MRU at the end.
# - Frequency estimates:
#     Lightweight, decayed counters per key with epoch-based right-shift (no global scans).
# - Adaptive split:
#     q_target_bytes is adjusted online: hits in Q increase q_target (favor recency),
#     hits in P decrease q_target (favor frequency).
# - Eviction:
#     Prefer evicting from Q. When Q grows beyond target, consider P as well.
#     Select a victim from a small LRU sample using a size-aware utility score:
#         score = (est_freq + 1) / size  -> evict the minimum score
#   This avoids keeping large, low-value items.
# - Complexity:
#     O(1) average-time operations, O(sample) on eviction (small constant).

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
Q = OrderedDict()  # probation: key -> None
P = OrderedDict()  # protected: key -> None

# Segment assignment for live keys: 0 = Q, 1 = P
m_seg = dict()      # key -> int

# Last known size for keys (live or recently seen)
m_size = dict()     # key -> int

# Decayed frequency with epoch stamping
m_freq = dict()     # key -> int
m_fepoch = dict()   # key -> int (last epoch when freq was normalized)
FREQ_CLIP = 255

# Byte accounting for segments
q_bytes = 0
p_bytes = 0

# Target bytes for Q (probation)
q_target_bytes = None

# Adaptive knobs
# Sample sizes for eviction candidates
SAMPLE_Q = 5
SAMPLE_P = 2

# Frequency aging: one epoch per this many accesses
# Large enough to be stable, small enough to react. Use 20k by default.
EPOCH_SPAN = 20000


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _epoch(snapshot):
    # Discretize time for frequency decay
    n = _now(snapshot)
    span = max(1000, EPOCH_SPAN)
    return n // span

def _ensure_targets(snapshot):
    global q_target_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        q_target_bytes = 0
        return
    if q_target_bytes is None:
        q_target_bytes = cap // 2
    # clamp to [5%, 95%] of capacity to avoid extremes
    lo = max(0, int(0.05 * cap))
    hi = max(0, int(0.95 * cap))
    if q_target_bytes < lo:
        q_target_bytes = lo
    elif q_target_bytes > hi:
        q_target_bytes = hi

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front (LRU) if they are not in the real cache.
    Adjust byte counters accordingly using m_size.
    """
    global q_bytes, p_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        # peek LRU
        k = None
        for _k in od:
            k = _k
            break
        if k is None:
            break
        if k not in cache:
            od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                q_bytes = max(0, q_bytes - sz)
            else:
                p_bytes = max(0, p_bytes - sz)
            m_seg.pop(k, None)
            changed = True

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _insert_mru(od, key):
    od[key] = None

def _insert_lru(od, key):
    od[key] = None
    od.move_to_end(key, last=False)

def _iter_lru_n(od, n):
    i = 0
    for k in od:
        if i >= n:
            break
        yield k
        i += 1

def _decayed_freq(snapshot, k):
    """
    Return the lazily decayed frequency of k without bumping.
    Update stored value/epoch in-place to avoid global scans.
    """
    e = _epoch(snapshot)
    f = m_freq.get(k, 0)
    fe = m_fepoch.get(k, e)
    if fe is None:
        # If we've never recorded an epoch, set it now.
        m_fepoch[k] = e
        return f
    d = e - fe
    if d > 0:
        shift = min(6, d)  # bound shift to avoid underflow on very large gaps
        f = f >> shift
        m_freq[k] = f
        m_fepoch[k] = e
    return f

def _bump_freq(snapshot, k, inc=1):
    """
    Bump frequency of key k with epoch-aware decay normalization.
    """
    e = _epoch(snapshot)
    f = m_freq.get(k, 0)
    fe = m_fepoch.get(k, e)
    if fe is None:
        fe = e
    d = e - fe
    if d > 0:
        f = f >> min(6, d)
    f = min(FREQ_CLIP, f + max(1, int(inc)))
    m_freq[k] = f
    m_fepoch[k] = e

def _utility(snapshot, k):
    """
    Size-aware utility score for eviction: lower is worse.
    score = (est_freq + 1) / size
    """
    f = _decayed_freq(snapshot, k)
    sz = max(1, m_size.get(k, 1))
    return (float(f) + 1.0) / float(sz)

def _adapt_on_hit(snapshot, seg_id, sz):
    """
    Adjust q_target_bytes based on where hits happen.
    - Hits in Q (probation) -> increase q_target (favor recency)
    - Hits in P (protected) -> decrease q_target (favor frequency)
    Step scaled by size/2 to reflect size pressure.
    """
    global q_target_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        return
    step = max(1, sz // 2)
    if seg_id == 0:
        q_target_bytes = min(cap, q_target_bytes + step)
    else:
        q_target_bytes = max(0, q_target_bytes - step)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using size-aware SLRU rules with sampled utility:
    - Prefer evicting from Q (probation).
    - If Q exceeds its target share, also consider candidates from P.
    - Among sampled candidates, evict the one with minimum utility score.
    """
    global q_bytes, p_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_targets(cache_snapshot)
    _cleanup_stale_front(cache_snapshot, Q, seg_id=0)
    _cleanup_stale_front(cache_snapshot, P, seg_id=1)

    # Build candidate set
    candidates = []
    # Always sample from Q if available
    if Q:
        candidates.extend(list(_iter_lru_n(Q, SAMPLE_Q)))
    # If Q is at/above target or Q is empty, also sample a bit from P
    if not Q or q_bytes >= max(1, q_target_bytes):
        candidates.extend(list(_iter_lru_n(P, SAMPLE_P)))
    # Fallbacks if we somehow have no candidates
    if not candidates:
        # Try any key in cache (defensive)
        for k in cache:
            return k
        return None

    # Choose the minimum-utility candidate
    best_k = None
    best_score = None
    for k in candidates:
        s = _utility(cache_snapshot, k)
        if best_score is None or s < best_score:
            best_score = s
            best_k = k

    if best_k is not None:
        return best_k

    # Final defensive fallback
    for k in cache:
        return k
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Bump decayed frequency.
    - If in Q: promote to P (MRU).
    - If in P: refresh recency (move to MRU).
    - If untracked (defensive): place in P.
    - Adapt q_target based on segment where the hit occurred.
    """
    global q_bytes, p_bytes
    _ensure_targets(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    _bump_freq(cache_snapshot, k, inc=1)

    if k in Q:
        # Hit in probation -> promote to protected
        Q.pop(k, None)
        q_bytes = max(0, q_bytes - sz)
        _insert_mru(P, k)
        p_bytes += sz
        m_seg[k] = 1
        _adapt_on_hit(cache_snapshot, seg_id=0, sz=sz)
    elif k in P:
        # Refresh recency in protected
        _move_to_mru(P, k)
        m_seg[k] = 1
        _adapt_on_hit(cache_snapshot, seg_id=1, sz=sz)
    else:
        # Defensive: not tracked but a hit => ensure it's accounted in protected
        _insert_mru(P, k)
        p_bytes += sz
        m_seg[k] = 1
        _adapt_on_hit(cache_snapshot, seg_id=1, sz=sz)


def update_after_insert(cache_snapshot, obj):
    """
    On insert after a miss:
    - Bump decayed frequency (the access that missed still counts).
    - Place into Q (probation) by default.
      If estimated frequency is already high, place directly into P.
    - For very large, low-frequency items, bias toward quick eviction by inserting at LRU of Q.
    """
    global q_bytes, p_bytes
    _ensure_targets(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz

    # Count this access in the frequency model
    _bump_freq(cache_snapshot, k, inc=1)
    f = _decayed_freq(cache_snapshot, k)

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    # Placement decision
    if f >= 3:
        # High estimated frequency: directly protected
        _insert_mru(P, k)
        p_bytes += sz
        m_seg[k] = 1
    else:
        # Default: probation
        _insert_mru(Q, k)
        q_bytes += sz
        m_seg[k] = 0
        # If very large and still low frequency, bias to quick eviction
        if sz > (cap // 2) and f <= 1:
            # Move to LRU in Q so it becomes a prime eviction candidate
            Q.move_to_end(k, last=False)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After a victim has been evicted by the cache:
    - Remove it from our resident structures and adjust accounting.
    - Keep its frequency/size metadata for future admissions (no ghosts needed).
    - Adapt q_target gently based on which segment provided the victim.
      Evicting from P -> increase q_target (favor recency).
      Evicting from Q -> slight decrease q_target (favor frequency, but gently).
    """
    global q_bytes, p_bytes, q_target_bytes
    _ensure_targets(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)
    # Record last known size
    m_size[ekey] = esz

    seg = m_seg.get(ekey, None)
    if seg == 0:
        if ekey in Q:
            Q.pop(ekey, None)
            q_bytes = max(0, q_bytes - esz)
        # Slightly reduce probation target if we primarily evict from Q
        q_target_bytes = max(0, q_target_bytes - max(1, esz // 4))
    elif seg == 1:
        if ekey in P:
            P.pop(ekey, None)
            p_bytes = max(0, p_bytes - esz)
        # If protected provides victims, capacity is stressing long-term set -> favor recency more
        cap = _cap(cache_snapshot)
        q_target_bytes = min(cap, q_target_bytes + max(1, esz // 2))
    else:
        # Unknown segment (defensive cleanup)
        Q.pop(ekey, None)
        P.pop(ekey, None)
        # No strong adaptation here

    # Clean live metadata of segment assignment
    m_seg.pop(ekey, None)
    # Keep m_freq/m_size for future estimation
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn7jvqe69.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9qokldkf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo5kaofkj.pickle

Iteration 83: New subsample score 0.888686 is better than old score 0.881734. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgy3ewlel.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqo27qd80.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4l8vcyef.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpog9gyzzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqxwfjsow.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn0hks7n6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbb6s6v5m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzmnxhz13.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_wb9xnro.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkytcphv_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdf_aofu3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1kwe76cj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpny496tzo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8edxft7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphnki2_dm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7m505adg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8sftk6lb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsrktij0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0mcfi0he.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgcxonr16.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpff49d1sr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8bagqnxf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyh1dkc0h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1fhmbiga.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxt_qnp9k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc_wji3be.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7qlw47sh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq3i6j7b1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu0r_fn4s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk7bglwtd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnupm5xvw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp312cvmha.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptfcwxsjn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe9rngco6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpchwlwq90.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprri61r4w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy7cwhxf6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph944zhwr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdjjtjy9r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp57rh06dy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsiqxq2jg.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyt2y9j6q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyen9sz8a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5zfrqo63.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxdvwl4xu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg84zpzxh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvlrwoyrv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1nx_do1u.pickle

Iteration 83: Full valset score for new program: 0.23503122916666666
Iteration 83: Full train_val score for new program: 0.23503122916666666
Iteration 83: Individual valset scores for new program: [0.500646, 0.474317, 0.482698, 0.432026, 0.497727, 0.483339, 0.271531, 0.458907, 0.540937, 0.531017, 0.091667, 0.338366, 0.029847, 0.0, 0.021379, 0.021133, 0.020062, 0.023756, 0.022782, 0.272227, 0.351032, 0.026556, 0.058672, 0.058672, 0.271054, 0.359879, 0.85005, 0.893548, 0.039832, 0.038636, 0.045558, 9.6e-05, 3.6e-05, 0.721069, 0.083333, 0.066196, 0.017836, 0.641422, 0.125461, 0.041586, 0.039096, 0.043257, 0.047697, 0.35, 0.040329, 0.040954, 0.466258, 0.04902]
Iteration 83: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.893548, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 83: Full valset pareto front score: 0.27793295833333337
Iteration 83: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {24}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 33, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {33}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33}, {29, 14}, {24, 25}, {31}, {24, 25}, {3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33}, {24}]
Iteration 83: Best valset aggregate score so far: 0.25926008333333345
Iteration 83: Best program as per aggregate score on train_val: 29
Iteration 83: Best program as per aggregate score on valset: 29
Iteration 83: Best score on valset: 0.25926008333333345
Iteration 83: Best score on train_val: 0.25926008333333345
Iteration 83: Linear pareto front program index: 29
Iteration 83: New program candidate index: 33
Iteration 84: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9r1s8lxu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2nzsruy1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxxk8m84_.pickle

Iteration 84: Proposed new text for program: # Size-aware Windowed TinyLFU with Segmented LRU (S-WTLFU)
# - Three segments (by bytes):
#     0 = Window (W): recency buffer for new items.
#     1 = Probation (P): main area for candidates with at least one hit.
#     2 = Protected (S): proven items (>=2 hits). Demoted back to P on overflow.
# - Eviction:
#     * Prefer evicting from W if W exceeds target, else from P.
#     * TinyLFU admission: compare new object's score vs victim's score:
#         score = (estimate + 1) / (size ** ALPHA)
#       If new_score < victim_score, reject admission by evicting the new object itself.
# - Hit processing:
#     * W -> P on first hit, P -> S on second. S stays and refreshes recency.
#     * If S exceeds target bytes, demote its LRU (repeat until within target).
# - Insert:
#     * Insert into W by default (pure recency), optionally bypass directly to P
#       if the TinyLFU estimate is already high (bypass is rare and size-aware).
# - Size-aware tie-breakers: when recency is equal, evict larger object first.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
# Window (recency buffer) fraction of total capacity (bytes)
W_FRAC = 0.20            # 20% window by bytes
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80            # 80% of main in protected
# Size-awareness exponent (0=no size-awareness, 1=full inverse-size weighting)
ALPHA = 0.75
# If estimate >= this, bypass W into P at insert (size-aware doorkeeper)
BYPASS_EST_THRESHOLD = 3
# Tiny epsilon
EPS = 1e-12

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Ensure strictly positive sizes; treat unknown/zero as 1 to avoid div-by-zero.
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * W_FRAC)
    main_target = cap - w_target
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_until_target(cache_snapshot, exclude_key=None):
    """
    If Protected segment exceeds its target size, demote its LRU back to P.
    Repeat until within target.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    while m_bytes_S > s_target + EPS:
        victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
        if victim_key is None:
            break
        v = cache_snapshot.cache.get(victim_key)
        if v is None:
            break
        sz = _size_of(v)
        _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _score_for_item(key, size):
    """
    TinyLFU-based size-aware score. Higher is better.
    """
    est = _sketch_estimate(key)
    # +1 smoothing helps cold starts and avoids zero-score dominance.
    return (est + 1.0) / ((float(size)) ** ALPHA)

def _choose_victim_segment(cache_snapshot):
    """
    Prefer evicting from W if W is above its target; otherwise from P.
    """
    w_target, _ = _targets(cache_snapshot)
    if m_bytes_W > w_target:
        return SEG_WINDOW
    return SEG_PROBATION

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    TinyLFU-size-aware admission and segmented eviction.
    Strategy:
      1) If the incoming object is larger than capacity, reject it immediately.
      2) Choose segment to evict from: W if W > target else P.
      3) Candidate = LRU in chosen segment; fallback to W, then P, then S.
      4) Admission test (size-aware TinyLFU):
           if score(new) < score(candidate): evict new (reject admission)
           else evict candidate
      5) If no candidate found, fallback to global LRU.
    """
    if not cache_snapshot.cache:
        return None

    # If object cannot ever fit, reject immediately to avoid thrashing.
    if _size_of(obj) > max(1, int(cache_snapshot.capacity)):
        return obj.key

    # Pick a candidate according to segment preference.
    prefer_seg = _choose_victim_segment(cache_snapshot)
    candidate = _lru_key_in_segment(cache_snapshot, prefer_seg)
    if candidate is None:
        # Try other segments in order of preference: Window -> Probation -> Protected
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    if candidate is None:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)
    if candidate is None:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)

    # If still none (shouldn't happen), fallback to global LRU.
    if candidate is None:
        best_k = None
        best_la = None
        best_sz = None
        for k, v in cache_snapshot.cache.items():
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if best_k is None or la < best_la or (
                la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
            ):
                best_k, best_la, best_sz = k, la, sz
        # If no candidate but cache not empty, decide based on admission anyway
        if best_k is None:
            return obj.key
        candidate = best_k

    # TinyLFU size-aware admission decision: evict the weaker between (new, candidate)
    v_obj = cache_snapshot.cache.get(candidate)
    if v_obj is None:
        # In case of inconsistency, reject admission to be safe.
        return obj.key

    new_score = _score_for_item(obj.key, _size_of(obj))
    cand_score = _score_for_item(candidate, _size_of(v_obj))

    if new_score + EPS < cand_score:
        # Reject admission by evicting the incoming object.
        return obj.key
    else:
        return candidate


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and last access timestamp.
      - Promote: W -> P (first hit), P -> S (second hit).
      - Keep S within target by demoting S-LRU back to P while over target.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_until_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: just recency refresh via timestamp; segment unchanged.
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update sketch and last access.
      - Place into Window by default to honor recency.
      - Optional size-aware bypass: if estimate is already high (heavy hitter) and
        object is not too large, place directly into Probation.
      - Ensure S is within target (if previous operations or evictions skewed state).
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    est = _sketch_estimate(key)
    sz = _size_of(obj)
    cap = max(1, int(cache_snapshot.capacity))

    global m_bytes_W, m_bytes_P
    # Size-aware doorkeeper: bypass W if quite frequent already and not huge
    # (keeps hot items from re-paying the W->P promotion).
    if est >= BYPASS_EST_THRESHOLD and sz <= cap // 2:
        m_seg[key] = SEG_PROBATION
        m_bytes_P += sz
    else:
        m_seg[key] = SEG_WINDOW
        m_bytes_W += sz

    # Maintain S target proactively (in case of drift due to earlier events).
    _demote_S_until_target(cache_snapshot, exclude_key=None)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove metadata and update segment byte counters.
      - Sketch aging handled on adds; we keep the global estimator (no deletion).
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)
    sz = _size_of(evicted_obj)

    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    m_last_access.pop(key, None)
Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8im_lmgk.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6zud4ujl.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Subprocess stdout: Error in subprocess: CANDID_OBJ_KEY must be in cache
Error saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppbievj6r.pickle

Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 352, in <module>
    result_dict = cache_simulate(trace_path)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 337, in cache_simulate
    cache.get(obj)
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 246, in get
    evicted_cache_object = self.evict(obj)
                           ^^^^^^^^^^^^^^^
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 292, in evict
    raise ValueError("CANDID_OBJ_KEY must be in cache")
ValueError: CANDID_OBJ_KEY must be in cache

Iteration 84: New subsample score 0.0 is not better than old score 0.863436, skipping
Iteration 85: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcifir8u1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa_rpaypf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuza1hd83.pickle

Iteration 85: Proposed new text for program: # Simple SLRU-LRFU (Segmented LRU with decayed frequency) with gentle size-awareness
# Goals:
# - Make eviction decisions simple and robust.
# - Evict from probation (cold segment) first; only evict protected (hot segment) when necessary.
# - Within a segment, choose the lowest "keep" value using an LRFU-like decayed score.
# - Use a mild size bias so small items are slightly favored, but not overly.
# - Keep metadata small and updates fast.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Decayed LRFU score (sum of exponentially decayed hits)
m_score = dict()       # key -> float
m_score_ts = dict()    # key -> int (last time score was updated)

# Segments: 0 = probation (cold), 1 = protected (hot)
m_seg = dict()         # key -> int

# Access metadata
m_last_access = dict() # key -> int
m_freq = dict()        # key -> int (hit count)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Half-life for decay in access-count time units (smaller => more recency bias)
HL = 128.0

# Size awareness (very gentle; keep stable if capacity is object-count)
ALPHA_SIZE_CREDIT = 0.15   # credit per hit = 1 / size^alpha
GAMMA_SIZE_EVICT  = 0.20   # keep_score divided by size^gamma

# Extra bonus for frequency when selecting a victim in protected
FREQ_BONUS_WEIGHT = 0.40   # adds log1p(freq) to keep_score in protected

# Numerical guard
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    return 0.5 ** (float(delta) / float(HL))

def _current_decayed_score(key, now):
    base = float(m_score.get(key, 0.0))
    ts = m_score_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _keep_value(key, now, obj_in_cache, in_protected):
    """
    Higher value => better to keep. Evict the item with the smallest keep_value.
    Base: decayed LRFU score (recency+frequency)
    - Mild size normalization
    - In protected, add a small bonus from log1p(freq) to resist eviction
    """
    s_now = _current_decayed_score(key, now)
    val = s_now

    if in_protected:
        # Small bonus for items that have proven frequency
        val += FREQ_BONUS_WEIGHT * math.log1p(float(m_freq.get(key, 0)))

    # Gentle size normalization so we don't over-penalize large items
    sz = _size_of(obj_in_cache)
    val = val / (float(sz) ** GAMMA_SIZE_EVICT)

    return val


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Victim selection:
    - Prefer evicting from probation segment (cold) if non-empty.
    - Otherwise, evict from protected segment (hot).
    - Within the chosen segment, evict the key with the smallest keep_value.
    Ties: older last access first, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    # Partition keys by segment
    prob_keys = []
    prot_keys = []
    for k in cache.keys():
        if m_seg.get(k, 0) == 1:
            prot_keys.append(k)
        else:
            prob_keys.append(k)

    # Choose segment: probation first to protect hot set
    candidates = prob_keys if len(prob_keys) > 0 else prot_keys
    if not candidates:
        return None

    victim_key = None
    victim_keep = None
    victim_la = None
    victim_sz = None
    in_protected = (candidates is prot_keys)

    for k in candidates:
        v = cache[k]
        keep = _keep_value(k, now, v, in_protected)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if victim_key is None:
            victim_key = k
            victim_keep = keep
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if keep < victim_keep - _EPS:
            better = True
        elif abs(keep - victim_keep) <= _EPS:
            # Tie-breakers: older LRU, then larger size, then lexicographic
            if la < victim_la:
                better = True
            elif la == victim_la:
                if sz > victim_sz:
                    better = True
                elif sz == victim_sz and k < victim_key:
                    better = True

        if better:
            victim_key = k
            victim_keep = keep
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update last access time.
    - Update decayed LRFU score with size-aware credit.
    - Increment frequency.
    - Promote from probation to protected on first hit.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (in case of uninitialized metadata)
    if key not in m_score_ts:
        m_score_ts[key] = now
        m_score[key] = 0.0
    if key not in m_seg:
        m_seg[key] = 0
    if key not in m_freq:
        m_freq[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now

    # LRFU decayed score update
    cur = _current_decayed_score(key, now)
    credit = _size_credit(obj)
    m_score[key] = cur + credit
    m_score_ts[key] = now

    # Frequency and recency
    m_freq[key] = m_freq.get(key, 0) + 1
    m_last_access[key] = now

    # Promotion on first hit
    if m_seg.get(key, 0) == 0:
        m_seg[key] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
    - Initialize metadata.
    - Place new object in probation with zero initial score (require a hit to earn protection).
    """
    key = obj.key
    now = _now(cache_snapshot)

    m_last_access[key] = now
    m_freq[key] = 0
    m_score[key] = 0.0
    m_score_ts[key] = now
    m_seg[key] = 0  # probation


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Remove live metadata of evicted key.
    """
    ekey = evicted_obj.key

    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_score.pop(ekey, None)
    m_score_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu8vk5m6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsakc_7hv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpluz5p0ke.pickle

Iteration 85: New subsample score 0.403354 is not better than old score 0.503897, skipping
Iteration 86: Selected program 27 score: 0.24045458333333325
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvqhygo7q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbr4wtv4a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfw2b_6y8.pickle

Iteration 86: Proposed new text for program: # GDSF+TinyLFU: Size-aware GreedyDual with TinyLFU request estimator
# - Core policy: GreedyDual-Size with dynamic priority (L + value/size).
# - Frequency: TinyLFU-style Count-Min Sketch tracks request frequency for all keys.
# - Size-awareness: value is normalized by size^SIZE_EXP (per-byte fairness).
# - Recency: GreedyDual's global age L rises to the priority of the evicted item.
# - On hit: priority is incremented by value/size, reinforcing popular items.
# - Eviction: removes the minimum-priority item (with stable tie-breakers).

import math
import random

# ----------------------
# Globals (live metadata)
# ----------------------
m_prio = dict()          # key -> float (current GreedyDual priority)
m_last = dict()          # key -> int (last access time for tie-breaking)

# Global GreedyDual age (monotone non-decreasing)
gdsf_L = 0.0

# ----------------------
# TinyLFU Count-Min Sketch
# ----------------------
# Small, fast CMS to estimate request frequencies (hits + misses).
# Periodically halved to provide exponential aging of counts.

CMS_DEPTH = 4
CMS_WIDTH = 8192  # power-of-two is slightly faster; adjust for memory/speed
CMS_RESET_INTERVAL = 100000  # in accesses; halve counters at this cadence

# Initialize CMS tables and hash seeds
cms_tables = [[0] * CMS_WIDTH for _ in range(CMS_DEPTH)]
cms_seeds = [0x9e3779b97f4a7c15 ^ (i * 0x5851f42d4c957f2d) for i in range(CMS_DEPTH)]

def _cms_index(key, seed):
    # Fast, stable index for the sketch (works for str keys)
    h = hash((key, seed))
    # Ensure non-negative and map to width
    return (h & 0x7FFFFFFF) % CMS_WIDTH

def _cms_estimate(key):
    # Min across rows
    est = None
    for d in range(CMS_DEPTH):
        idx = _cms_index(key, cms_seeds[d])
        v = cms_tables[d][idx]
        if est is None or v < est:
            est = v
    return 0 if est is None else est

def _cms_increment(key, inc=1):
    # Conservative update: only rows equal to current min are incremented
    current_min = _cms_estimate(key)
    target = current_min + inc
    for d in range(CMS_DEPTH):
        idx = _cms_index(key, cms_seeds[d])
        if cms_tables[d][idx] == current_min:
            cms_tables[d][idx] = target

def _cms_decay():
    # Halve all counters (integer division) to decay historical frequencies
    for d in range(CMS_DEPTH):
        row = cms_tables[d]
        for i in range(CMS_WIDTH):
            row[i] >>= 1  # fast divide-by-two aging

# ----------------------
# Tunable hyperparameters
# ----------------------
SIZE_EXP = 1.0            # size normalization exponent (1.0 = per-byte fairness)
FREQ_WEIGHT = 2.5         # weight of TinyLFU estimate (log-scaled) in value
RECENCY_BONUS = 0.0       # optional extra per-hit bump (already captured via value); keep 0.0
EPS = 1e-9                # small epsilon for tie-breaking stability

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _value_for(key, sz):
    # TinyLFU frequency estimate (log-scaled to control range)
    f_hat = _cms_estimate(key)
    freq_term = math.log1p(float(f_hat))  # grows slowly
    # Size-aware value: per-byte fairness (size^SIZE_EXP)
    denom = float(sz) ** float(SIZE_EXP)
    return (1.0 + FREQ_WEIGHT * freq_term) / denom

def _maybe_decay_sketch(cache_snapshot):
    # Periodically decay the CMS to keep it fresh and bounded
    now = _now(cache_snapshot)
    if CMS_RESET_INTERVAL > 0 and now > 0 and (now % CMS_RESET_INTERVAL) == 0:
        _cms_decay()

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    GreedyDual eviction: evict the key with the smallest priority.
    Tie-breakers:
      1) Older last access (LRU among equals)
      2) Larger size (free more space)
      3) Lexicographic key for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Find minimum priority key
    best_key = None
    best_pr = None
    best_la = None
    best_sz = None

    for k, v in cache.items():
        pr = m_prio.get(k, gdsf_L)  # default to current age if missing
        la = m_last.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_pr, best_la, best_sz = k, pr, la, sz
            continue

        # Primary: smallest priority first
        if pr < best_pr - EPS:
            best_key, best_pr, best_la, best_sz = k, pr, la, sz
            continue

        if abs(pr - best_pr) <= EPS:
            # Secondary: older last access (LRU among equals)
            if la < best_la:
                best_key, best_pr, best_la, best_sz = k, pr, la, sz
                continue
            if la == best_la:
                # Tertiary: larger size (free up more)
                if sz > best_sz:
                    best_key, best_pr, best_la, best_sz = k, pr, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_pr, best_la, best_sz = k, pr, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Increment TinyLFU request count.
    - Bump GreedyDual priority by value/size.
    - Update last access time.
    """
    global gdsf_L
    _maybe_decay_sketch(cache_snapshot)

    key = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)

    # Count this request in TinyLFU
    _cms_increment(key, 1)

    # Initialize if missing (defensive)
    if key not in m_prio:
        # Cold-initialize at current age
        m_prio[key] = gdsf_L + _value_for(key, sz)

    # GreedyDual bump: add value/size on hit (frequency-aware)
    m_prio[key] += _value_for(key, sz) + (RECENCY_BONUS / float(sz))
    m_last[key] = now


def update_after_insert(cache_snapshot, obj):
    """
    After insert (on miss):
    - Increment TinyLFU request count for this access.
    - Initialize the GreedyDual priority as L + value/size.
    - Record last access time.
    """
    _maybe_decay_sketch(cache_snapshot)

    key = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)

    # Count this request in TinyLFU
    _cms_increment(key, 1)

    # Initialize GreedyDual priority at current age
    m_prio[key] = gdsf_L + _value_for(key, sz)
    m_last[key] = now


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Advance global age L to the evicted item's priority (GreedyDual rule).
    - Remove live metadata for the evicted key.
    """
    global gdsf_L

    ekey = evicted_obj.key
    eprio = m_prio.get(ekey, gdsf_L)

    # Age increases to the priority of the evicted item
    if eprio > gdsf_L:
        gdsf_L = eprio

    # Clean live metadata
    m_prio.pop(ekey, None)
    m_last.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6a9p95al.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp46t0nkr3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpts1vxi3k.pickle

Iteration 86: New subsample score 0.309478 is not better than old score 0.314928, skipping
Iteration 87: Selected program 28 score: 0.24792383333333334
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwh4v6oj5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb1vlh6tk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu6t2_6bj.pickle

Iteration 87: Proposed new text for program: # W-TinyLFU-guided Size-Aware ARC (WTL-ARC)
# - Two resident segments (ARC-style):
#     T1: probation/recent (LRU)
#     T2: protected/frequent (LRU)
#   with adaptive target p_bytes for T1 using ghost lists B1/B2 (in bytes).
# - Eviction uses a TinyLFU frequency estimator and size-aware scoring:
#     score = ((est_freq + base_bias) / (size ** ALPHA)) * recency_decay * segment_bias * t1_target_bias
#   Lower score = more evictable.
#   This favors evicting large, old, and infrequent items.
# - Frequency uses a reset-based TinyLFU (two Count-Min tables with periodic reset)
#   to provide time-decayed counts at O(1) per access, with no per-key state needed.
# - Maintains per-key size and last-access time for recency and accounting.
# - Ghost lists B1/B2 bounded to 2x capacity bytes.
# - Sampling eviction examines a small number of LRU-tail candidates from each segment.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
T1 = OrderedDict()  # probation/recent: key -> None (LRU->MRU)
T2 = OrderedDict()  # protected/frequent: key -> None (LRU->MRU)

# Ghost segments (keys only)
B1 = OrderedDict()  # key -> None (recently evicted from T1)
B2 = OrderedDict()  # key -> None (recently evicted from T2)

# Segment assignment for live keys: 0 = T1, 1 = T2
m_seg = dict()      # key -> int

# Last known size (for live or ghost)
m_size = dict()     # key -> int

# Last access time for live keys (in "access_count")
m_time = dict()     # key -> int

# Byte accounting for segments
t1_bytes = 0
t2_bytes = 0
b1_bytes = 0
b2_bytes = 0

# ARC target bytes for T1
p_bytes = None

# Ghost sizing
GHOST_BYTES_FACTOR = 2.0

# Eviction sampling parameters (LRU tail sampling)
TAIL_SCAN_T1 = 8
TAIL_SCAN_T2 = 8

# Recency decay parameter (by access count)
HALF_LIFE = 12000  # slightly slower decay than before

# Segment bias
PROTECT_FACTOR_T2 = 1.35
T1_OVERSHOOT_BIAS = 0.80
T1_UNDERSHOOT_BIAS = 1.20

# Size sensitivity exponent (penalize large objects superlinearly)
SIZE_ALPHA = 1.20

# ----------------------
# TinyLFU (reset-based Count-Min Sketch)
# ----------------------
CMS_DEPTH = 4
CMS_WIDTH = 2048  # power of two for masking
CMS_MASK = CMS_WIDTH - 1
CMS_MAX_COUNT = 255  # saturate at 8-bit

cms_cur = [[0] * CMS_WIDTH for _ in range(CMS_DEPTH)]
cms_prev = [[0] * CMS_WIDTH for _ in range(CMS_DEPTH)]
cms_seeds = [0x27d4eb2d, 0x85ebca6b, 0xc2b2ae35, 0x165667b1]

# Access-driven reset schedule
last_reset_access = 0
AGE_WINDOW = 50000  # default; adjusted with capacity

def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _ensure_p(snapshot):
    """Initialize target split and TinyLFU reset cadence."""
    global p_bytes, AGE_WINDOW
    cap = _cap(snapshot)
    if p_bytes is None:
        p_bytes = cap // 2
    else:
        if cap > 0:
            if p_bytes < 0:
                p_bytes = 0
            elif p_bytes > cap:
                p_bytes = cap
        else:
            p_bytes = 0
    # Reset cadence scales moderately with capacity (bytes -> accesses heuristic)
    # Keep within sane bounds to avoid too frequent/rare resets.
    AGE_WINDOW = max(20000, min(200000, (cap // 8) + 40000))

def _trim_ghosts(snapshot):
    """Ensure B1+B2 total bytes <= factor * capacity by evicting oldest ghosts."""
    global b1_bytes, b2_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        B1.clear(); B2.clear()
        b1_bytes = 0; b2_bytes = 0
        return
    limit = int(GHOST_BYTES_FACTOR * max(1, cap))
    while (b1_bytes + b2_bytes) > limit and (B1 or B2):
        target = B1 if b1_bytes >= b2_bytes else B2
        if target:
            k, _ = target.popitem(last=False)
            sz = m_size.get(k, 1)
            if target is B1:
                b1_bytes = max(0, b1_bytes - sz)
            else:
                b2_bytes = max(0, b2_bytes - sz)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _lru_key_of(od):
    if not od:
        return None
    for k in od:
        return k
    return None

def _cleanup_stale_front(snapshot, od, seg_id):
    """Remove stale keys (not present in cache) from LRU front of given segment."""
    global t1_bytes, t2_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = _lru_key_of(od)
        if k is None:
            break
        if k not in cache:
            od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                t1_bytes = max(0, t1_bytes - sz)
            else:
                t2_bytes = max(0, t2_bytes - sz)
            m_seg.pop(k, None)
            changed = True

def _peek_lru_keys(od, k_max, cache):
    """Return up to k_max keys from the LRU side that are still present in the cache."""
    keys = []
    if not od or k_max <= 0:
        return keys
    count = 0
    for k in od:
        if k in cache:
            keys.append(k)
            count += 1
            if count >= k_max:
                break
    return keys

# ----------------------
# TinyLFU helpers
# ----------------------
def _idx(hashv, seed):
    return (hashv ^ seed) & CMS_MASK

def _hash_key(key):
    # Use Python's hash; convert to positive 64-bit space
    try:
        h = hash(key)
    except Exception:
        h = hash(str(key))
    return h & 0xFFFFFFFFFFFFFFFF

def _cms_add(key, count=1):
    """Increment TinyLFU for the key (in current epoch), saturating at CMS_MAX_COUNT."""
    h = _hash_key(key)
    for d in range(CMS_DEPTH):
        i = _idx(h, cms_seeds[d])
        v = cms_cur[d][i]
        nv = v + count
        cms_cur[d][i] = CMS_MAX_COUNT if nv > CMS_MAX_COUNT else nv

def _cms_est_from(table, key):
    h = _hash_key(key)
    est = float('inf')
    for d in range(CMS_DEPTH):
        i = _idx(h, cms_seeds[d])
        est = min(est, table[d][i])
    return 0 if est == float('inf') else int(est)

def _cms_est(key):
    """Estimate combining current and previous epochs (0.5 weight for previous)."""
    a = _cms_est_from(cms_cur, key)
    b = _cms_est_from(cms_prev, key)
    return a + (b >> 1)  # a + floor(b/2)

def _cms_maybe_reset(snapshot):
    """Periodically rotate eras to decay old frequencies."""
    global cms_cur, cms_prev, last_reset_access
    now = _now(snapshot)
    # Monitor based on access_count, robust across hits/misses
    if now - last_reset_access >= AGE_WINDOW:
        cms_prev = cms_cur
        cms_cur = [[0] * CMS_WIDTH for _ in range(CMS_DEPTH)]
        last_reset_access = now

# ----------------------
# Scoring
# ----------------------
def _candidate_score(k, seg_id, now):
    """
    Lower scores are more evictable.
    score = ((est_freq + base_bias) / (size ** SIZE_ALPHA))
            * (1 / (1 + age / HALF_LIFE))
            * segment_bias
            * t1_target_bias
    """
    global t1_bytes, p_bytes
    sz = max(1, int(m_size.get(k, 1)))
    last = int(m_time.get(k, 0))
    age = max(0, now - last)

    # TinyLFU estimated freq (decayed, global)
    est = _cms_est(k)

    # Provide a small base bias to not zero-out new items completely.
    base_bias = 0.75 if seg_id == 0 else 1.25  # T2 gets a slightly larger base
    density = (est + base_bias) / (float(sz) ** float(SIZE_ALPHA))

    # Recency decay: older -> more evictable
    age_factor = 1.0 / (1.0 + (age / float(HALF_LIFE)))  # in (0,1]

    score = density * age_factor

    # Segment protection bias
    if seg_id == 1:
        score *= PROTECT_FACTOR_T2

    # ARC target bias for T1
    if seg_id == 0:
        if t1_bytes > p_bytes:
            score *= T1_OVERSHOOT_BIAS
        else:
            score *= T1_UNDERSHOOT_BIAS

    return score

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using TinyLFU-guided size-aware ARC:
    - Sample up to TAIL_SCAN_T1 keys from T1's LRU and TAIL_SCAN_T2 from T2's LRU.
    - Compute a size/frequency/recency-based score; evict the lowest-scored key.
    - If no tracked candidates, fall back to evict the largest resident object.
    """
    global t1_bytes, t2_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_p(cache_snapshot)
    _cms_maybe_reset(cache_snapshot)

    # Defensive cleanup of stale heads
    _cleanup_stale_front(cache_snapshot, T1, seg_id=0)
    _cleanup_stale_front(cache_snapshot, T2, seg_id=1)

    now = _now(cache_snapshot)

    # Gather candidates from both segments' LRU tails
    candidates = []
    if T1:
        for k in _peek_lru_keys(T1, TAIL_SCAN_T1, cache):
            candidates.append((k, 0))
    if T2:
        for k in _peek_lru_keys(T2, TAIL_SCAN_T2, cache):
            candidates.append((k, 1))

    if not candidates:
        # Fallback: evict the largest object (free space quickly)
        victim = None
        max_sz = -1
        for k, o in cache.items():
            s = _size_of(o)
            if s > max_sz:
                max_sz = s
                victim = k
        return victim

    best_key = None
    best_score = float("inf")
    for k, seg in candidates:
        s = _candidate_score(k, seg, now)
        if s < best_score:
            best_score = s
            best_key = k

    if best_key is None:
        # Additional robust fallback: biased-LRU based on current target
        choose_T1 = False
        if T1 and T2:
            choose_T1 = (t1_bytes > p_bytes)
        elif T1:
            choose_T1 = True
        elif T2:
            choose_T1 = False
        if choose_T1:
            k = _lru_key_of(T1)
            return k if k is not None else _lru_key_of(T2)
        else:
            k = _lru_key_of(T2)
            return k if k is not None else _lru_key_of(T1)

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Update TinyLFU, size, last access time.
    - If in T1: promote to T2 (protected).
    - If in T2: refresh recency.
    - If untracked: insert into T2 (defensive, as it is at least second touch).
    """
    global t1_bytes, t2_bytes
    _ensure_p(cache_snapshot)
    _cms_maybe_reset(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)

    # Update TinyLFU
    _cms_add(k, 1)

    # Ensure metadata
    m_size[k] = sz
    m_time[k] = now

    if k in T1:
        # Promote T1 -> T2
        T1.pop(k, None)
        t1_bytes = max(0, t1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in T2:
        _move_to_mru(T2, k)
        m_seg[k] = 1
    else:
        # Not tracked but a hit: treat as frequent
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
    - Update TinyLFU for the requested key.
    - ARC adaptation:
        * If key in B1: increase p_bytes (favor recency) and place in T2.
        * If key in B2: decrease p_bytes (favor frequency) and place in T2.
        * Else: place in T1 (probation).
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes, p_bytes
    _ensure_p(cache_snapshot)
    _cms_maybe_reset(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    now = _now(cache_snapshot)

    # Update TinyLFU on request arrival
    _cms_add(k, 1)

    # Update metadata
    m_size[k] = sz
    m_time[k] = now

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    # Adapt step in bytes: responsive to object size but bounded
    adapt_step = max(sz, cap // 64)

    if k in B1:
        # Favor recency: grow T1 target; move to T2
        p_bytes = min(cap, p_bytes + adapt_step)
        if k in B1:
            B1.pop(k, None)
            b1_bytes = max(0, b1_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    elif k in B2:
        # Favor frequency: shrink T1 target; move to T2
        p_bytes = max(0, p_bytes - adapt_step)
        if k in B2:
            B2.pop(k, None)
            b2_bytes = max(0, b2_bytes - sz)
        T2[k] = None
        t2_bytes += sz
        m_seg[k] = 1
    else:
        # New key: goes to T1 (probation)
        T1[k] = None
        t1_bytes += sz
        m_seg[k] = 0

    _trim_ghosts(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After evicting a victim:
    - Move the evicted key to the corresponding ghost list (B1 if from T1, B2 if from T2).
    - Update byte accounting and live metadata.
    - Keep ghosts bounded.
    """
    global t1_bytes, t2_bytes, b1_bytes, b2_bytes
    _ensure_p(cache_snapshot)
    _cms_maybe_reset(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)

    # Remember last size to account in ghosts
    m_size[ekey] = esz

    seg = m_seg.get(ekey, None)

    if seg == 0:
        # From T1 -> B1
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        B1[ekey] = None
        b1_bytes += esz
    elif seg == 1:
        # From T2 -> B2
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)
        B2[ekey] = None
        b2_bytes += esz
    else:
        # Unknown segment: place into B1 defensively
        if ekey in T1:
            T1.pop(ekey, None)
            t1_bytes = max(0, t1_bytes - esz)
        if ekey in T2:
            T2.pop(ekey, None)
            t2_bytes = max(0, t2_bytes - esz)
        B1[ekey] = None
        b1_bytes += esz

    # Clean live segment map
    m_seg.pop(ekey, None)

    _trim_ghosts(cache_snapshot)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2wxwkvoo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5v6bo9x9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfmwb03dj.pickle

Iteration 87: New subsample score 0.305768 is better than old score 0.302121. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpud0hv7en.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6o07j2dv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnbf_vwcx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnj4_qrir.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4ka7_e7r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptsotk8e3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7ygkhxa2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpedagmbf8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8jf0rwcc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpreoedcmx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmk5hlfu_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuie2i0as.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9_iya3e2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3d_6j7un.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp638_oi07.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm3y3z0tv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqje78ujc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn54dcfbp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6ignwsg0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplcap1yrr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7f85fyln.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9vae7qoq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfkbx92ta.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpljqd5a8_.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuux66til.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvp32p38c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw5y684_u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplfx_p_zv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeh6bi5ny.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpafo9xvrz.pickle

Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuyffkot3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxrwqv0b6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6d_4p8rn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcl3a5rn0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpers2efkv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9ku8trin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpu0qvd5ug.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp__ip6154.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi1jre4ik.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3vhfd88u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4yi5cqyo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph9v3g0dr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt0pih7rc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbgiksakr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptuxj1aov.pickle

Iteration 87: Full valset score for new program: 0.236869625
Iteration 87: Full train_val score for new program: 0.236869625
Iteration 87: Individual valset scores for new program: [0.48983, 0.466729, 0.470881, 0.423046, 0.484623, 0.472328, 0.27512, 0.498624, 0.536646, 0.531017, 0.091667, 0.353464, 0.028135, 0.0, 0.021237, 0.020147, 0.019927, 0.023053, 0.022641, 0.272227, 0.369715, 0.026164, 0.058672, 0.058672, 0.0, 0.367944, 0.815293, 0.885846, 0.109614, 0.040909, 0.050114, 0.0, 0.0, 0.730769, 0.083333, 0.0812, 0.094927, 0.640392, 0.125461, 0.067369, 0.044594, 0.065812, 0.065789, 0.366667, 0.077076, 0.061431, 0.466258, 0.114379]
Iteration 87: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.893548, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 87: Full valset pareto front score: 0.27793295833333337
Iteration 87: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26, 34}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {24}, {6, 14, 17, 23, 30}, {6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 33, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {33}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34}, {29, 14}, {24, 25}, {31}, {24, 25}, {34, 3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34}, {24}]
Iteration 87: Best valset aggregate score so far: 0.25926008333333345
Iteration 87: Best program as per aggregate score on train_val: 29
Iteration 87: Best program as per aggregate score on valset: 29
Iteration 87: Best score on valset: 0.25926008333333345
Iteration 87: Best score on train_val: 0.25926008333333345
Iteration 87: Linear pareto front program index: 29
Iteration 87: New program candidate index: 34
Iteration 88: Selected program 33 score: 0.23503122916666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp796auqqj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpa55dww2t.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprmc7wgag.pickle

Iteration 88: Proposed new text for program: # Windowed, size-aware SLRU with decayed TinyLFU-style frequency,
# ARC-like ghosts, and adaptive window/probation targets.
#
# Design highlights:
# - Three resident segments (byte-accounted):
#     W (window LRU: newest insertions, recency buffer)
#     Q (probation SLRU: seen once or promoted from W)
#     P (protected SLRU: promoted on hit from Q)
#   Each is an LRU list (OrderedDict), MRU at the end.
# - Frequency estimates:
#     Lightweight, lazily decayed per-key counter with epoch-based right-shift.
# - Adaptive split:
#     Two tunables, w_target_bytes and q_target_bytes.
#     Signals:
#       * Hits in W/Q increase their respective targets (favor recency)
#       * Hits in P decrease Q (favor frequency)
#       * Re-references of ghosted keys adjust targets ARC-style:
#           GW hit -> increase window target
#           GQ hit -> increase probation target
#           GP hit -> decrease probation target
# - Eviction:
#     Prefer evicting from W, then Q, rarely P.
#     Select victim using a small LRU sample with size-aware utility:
#         utility = (decayed_freq + 1) / size  -> evict the minimum utility
#     P is protected with a guard: only evict from P if its best utility is
#     substantially worse than Q's.
# - Complexity:
#     O(1) average for updates, O(sample) for eviction (small constants).
#
# This policy aims to capture bursts (W), maintain one-hit-wonders (Q),
# and preserve frequent items (P), while adapting online to workload.

from collections import OrderedDict

# ----------------------
# Global state
# ----------------------
# Resident segments
W = OrderedDict()  # window: key -> None
Q = OrderedDict()  # probation: key -> None
P = OrderedDict()  # protected: key -> None

# Segment assignment for live keys: 0 = W, 1 = Q, 2 = P
m_seg = dict()   # key -> int

# Last known size for keys (live or recently seen)
m_size = dict()  # key -> int

# Decayed frequency with epoch stamping
m_freq = dict()     # key -> int
m_fepoch = dict()   # key -> int (last epoch when freq was normalized)
FREQ_CLIP = 255

# Byte accounting for segments
w_bytes = 0
q_bytes = 0
p_bytes = 0

# Targets
w_target_bytes = None
q_target_bytes = None

# ARC-like ghosts for adaptation (store keys and their last size)
GW = OrderedDict()   # ghost of W
GQ = OrderedDict()   # ghost of Q
GP = OrderedDict()   # ghost of P
gw_bytes = 0
gq_bytes = 0
gp_bytes = 0
# Budget for total ghost bytes (approx), clamped per capacity
GHOST_BUDGET_FRAC = 1.0  # 1x capacity

# Sampling knobs
SAMPLE_W = 4
SAMPLE_Q = 5
SAMPLE_P = 2

# Avoid evicting from P unless clearly better (lower utility) than Q
P_EVICT_GUARD = 0.85  # require P_best_utility < 0.85 * Q_best_utility

# Frequency aging: one epoch per this many accesses (smaller = quicker adaptation)
EPOCH_SPAN = 10000


# ----------------------
# Helpers
# ----------------------
def _size_of(obj):
    try:
        return max(1, int(getattr(obj, "size", 1)))
    except Exception:
        return 1

def _cap(snapshot):
    try:
        return int(getattr(snapshot, "capacity", 0))
    except Exception:
        return 0

def _now(snapshot):
    try:
        return int(getattr(snapshot, "access_count", 0))
    except Exception:
        return 0

def _epoch(snapshot):
    # Discretize time for frequency decay
    n = _now(snapshot)
    span = max(1000, EPOCH_SPAN)
    return n // span

def _ensure_targets(snapshot):
    global w_target_bytes, q_target_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        w_target_bytes = 0
        q_target_bytes = 0
        return
    if w_target_bytes is None:
        # Start window small but non-trivial
        w_target_bytes = max(1, int(0.10 * cap))
    if q_target_bytes is None:
        # Split remaining roughly half probation, half protected
        q_target_bytes = max(1, int(0.40 * cap))
    # Clamp ranges to avoid extremes
    w_lo = max(0, int(0.02 * cap))
    w_hi = max(0, int(0.30 * cap))
    q_lo = max(0, int(0.05 * cap))
    q_hi = max(0, int(0.70 * cap))
    if w_target_bytes < w_lo:
        w_target_bytes = w_lo
    elif w_target_bytes > w_hi:
        w_target_bytes = w_hi
    if q_target_bytes < q_lo:
        q_target_bytes = q_lo
    elif q_target_bytes > q_hi:
        q_target_bytes = q_hi

def _cleanup_stale_front(snapshot, od, seg_id):
    """
    Remove stale keys from the front (LRU) if they are not in the real cache.
    Adjust byte counters accordingly using m_size.
    """
    global w_bytes, q_bytes, p_bytes
    cache = getattr(snapshot, "cache", {}) or {}
    changed = True
    while od and changed:
        changed = False
        k = None
        for _k in od:
            k = _k
            break
        if k is None:
            break
        if k not in cache:
            od.pop(k, None)
            sz = m_size.get(k, 1)
            if seg_id == 0:
                w_bytes = max(0, w_bytes - sz)
            elif seg_id == 1:
                q_bytes = max(0, q_bytes - sz)
            else:
                p_bytes = max(0, p_bytes - sz)
            if m_seg.get(k) == seg_id:
                m_seg.pop(k, None)
            changed = True

def _cleanup_ghosts_budget(snapshot):
    """
    Trim ghost lists to remain within budget in bytes.
    Evict from the oldest among GW, GQ, GP.
    """
    global gw_bytes, gq_bytes, gp_bytes
    cap = _cap(snapshot)
    budget = max(0, int(GHOST_BUDGET_FRAC * cap))
    def _front(od):
        for k in od:
            return k
        return None
    while (gw_bytes + gq_bytes + gp_bytes) > budget:
        # Peek LRU keys
        kw = _front(GW)
        kq = _front(GQ)
        kp = _front(GP)
        # Pick the oldest among the three by insertion order
        # Since we don't store timestamps, just rotate among them to keep balance
        # Prefer trimming the largest list
        lens = [(GW, kw, gw_bytes), (GQ, kq, gq_bytes), (GP, kp, gp_bytes)]
        lens.sort(key=lambda x: x[2], reverse=True)
        od, k, _ = lens[0]
        if k is None:
            # fallback on any non-empty
            for od2 in (GW, GQ, GP):
                for k2 in od2:
                    k = k2
                    od = od2
                    break
                if k is not None:
                    break
        if k is None:
            break
        sz = m_size.get(k, 1)
        od.pop(k, None)
        if od is GW:
            gw_bytes = max(0, gw_bytes - sz)
        elif od is GQ:
            gq_bytes = max(0, gq_bytes - sz)
        else:
            gp_bytes = max(0, gp_bytes - sz)

def _move_to_mru(od, key):
    if key in od:
        od.move_to_end(key, last=True)

def _insert_mru(od, key):
    od[key] = None

def _insert_lru(od, key):
    od[key] = None
    od.move_to_end(key, last=False)

def _iter_lru_n(od, n):
    i = 0
    for k in od:
        if i >= n:
            break
        yield k
        i += 1

def _decayed_freq(snapshot, k):
    """
    Return the lazily decayed frequency of k without bumping.
    Update stored value/epoch in-place to avoid global scans.
    """
    e = _epoch(snapshot)
    f = m_freq.get(k, 0)
    fe = m_fepoch.get(k, e)
    if fe is None:
        m_fepoch[k] = e
        return f
    d = e - fe
    if d > 0:
        shift = min(6, d)
        f = f >> shift
        m_freq[k] = f
        m_fepoch[k] = e
    return f

def _bump_freq(snapshot, k, inc=1):
    """
    Bump frequency of key k with epoch-aware decay normalization.
    """
    e = _epoch(snapshot)
    f = m_freq.get(k, 0)
    fe = m_fepoch.get(k, e)
    if fe is None:
        fe = e
    d = e - fe
    if d > 0:
        f = f >> min(6, d)
    f = min(FREQ_CLIP, f + max(1, int(inc)))
    m_freq[k] = f
    m_fepoch[k] = e

def _utility(snapshot, k):
    """
    Size-aware utility score for eviction: lower is worse.
    score = (est_freq + 1) / size
    """
    f = _decayed_freq(snapshot, k)
    sz = max(1, m_size.get(k, 1))
    return (float(f) + 1.0) / float(sz)

def _adapt_on_hit(snapshot, seg_id, sz):
    """
    Adjust targets based on where hits happen.
    - Hits in W -> increase w_target (favor immediate recency)
    - Hits in Q -> increase q_target (favor probation/recency)
    - Hits in P -> decrease q_target (favor frequency)
    Step scaled modestly by size pressure.
    """
    global w_target_bytes, q_target_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        return
    step = max(1, sz // 4)
    # Window
    if seg_id == 0:
        w_target_bytes = min(cap, w_target_bytes + step)
    # Probation
    elif seg_id == 1:
        q_target_bytes = min(cap, q_target_bytes + step)
    # Protected
    else:
        q_target_bytes = max(0, q_target_bytes - step)

def _ghost_record(snapshot, k, seg_id, sz):
    """
    Add a recently evicted key to the corresponding ghost list.
    """
    global gw_bytes, gq_bytes, gp_bytes
    # Remove from all ghosts to refresh position
    if k in GW:
        GW.pop(k, None)
        gw_bytes = max(0, gw_bytes - m_size.get(k, sz))
    if k in GQ:
        GQ.pop(k, None)
        gq_bytes = max(0, gq_bytes - m_size.get(k, sz))
    if k in GP:
        GP.pop(k, None)
        gp_bytes = max(0, gp_bytes - m_size.get(k, sz))

    if seg_id == 0:
        _insert_mru(GW, k)
        gw_bytes += sz
    elif seg_id == 1:
        _insert_mru(GQ, k)
        gq_bytes += sz
    elif seg_id == 2:
        _insert_mru(GP, k)
        gp_bytes += sz
    _cleanup_ghosts_budget(snapshot)

def _ghost_touch_and_adapt(snapshot, k):
    """
    If the key exists in any ghost, adapt targets ARC-style and remove the ghost entry.
    """
    global w_target_bytes, q_target_bytes, gw_bytes, gq_bytes, gp_bytes
    cap = _cap(snapshot)
    if cap <= 0:
        return
    sz = m_size.get(k, 1)
    step = max(1, sz // 2)
    if k in GW:
        # Recent re-reference of a window-evicted item -> favor larger window
        GW.pop(k, None)
        gw_bytes = max(0, gw_bytes - sz)
        w_target_bytes = min(cap, w_target_bytes + step)
    elif k in GQ:
        # Re-reference of probation-evicted item -> more probation
        GQ.pop(k, None)
        gq_bytes = max(0, gq_bytes - sz)
        q_target_bytes = min(cap, q_target_bytes + step)
    elif k in GP:
        # Re-reference of protected-evicted item -> reduce probation
        GP.pop(k, None)
        gp_bytes = max(0, gp_bytes - sz)
        q_target_bytes = max(0, q_target_bytes - step)


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim using window-SLRU rules with sampled size-aware utility:
    - Prefer evicting from W (window) if it is non-empty or exceeds its target.
    - Otherwise prefer Q (probation).
    - Consider P (protected) only if Q is small or P candidates are clearly worse
      than Q's best (guarded by P_EVICT_GUARD).
    - Evict the minimum-utility candidate among the sampled set.
    """
    global w_bytes, q_bytes, p_bytes
    cache = getattr(cache_snapshot, "cache", {}) or {}
    if not cache:
        return None

    _ensure_targets(cache_snapshot)
    _cleanup_stale_front(cache_snapshot, W, seg_id=0)
    _cleanup_stale_front(cache_snapshot, Q, seg_id=1)
    _cleanup_stale_front(cache_snapshot, P, seg_id=2)

    # Build candidate sets based on targets
    candidates_w = list(_iter_lru_n(W, SAMPLE_W)) if W else []
    candidates_q = list(_iter_lru_n(Q, SAMPLE_Q)) if Q else []
    candidates_p = []

    # Decide when to consider P
    consider_p = False
    if not Q or q_bytes < max(1, q_target_bytes // 2):
        # Q small -> occasionally consider P
        consider_p = True
    if W and w_bytes >= max(1, w_target_bytes):
        # window over target: focus on W
        candidates_q = []
        consider_p = False

    if consider_p and P:
        candidates_p = list(_iter_lru_n(P, SAMPLE_P))

    # If nothing collected somehow, fall back to any key
    if not candidates_w and not candidates_q and not candidates_p:
        for k in cache:
            return k
        return None

    # Evaluate utilities per group
    def best_of(cands):
        best_k, best_score = None, None
        for k in cands:
            s = _utility(cache_snapshot, k)
            if best_score is None or s < best_score:
                best_k, best_score = k, s
        return best_k, best_score

    bw_k, bw_s = best_of(candidates_w) if candidates_w else (None, None)
    bq_k, bq_s = best_of(candidates_q) if candidates_q else (None, None)
    bp_k, bp_s = best_of(candidates_p) if candidates_p else (None, None)

    # Choose victim according to preference ordering and guard
    # 1) Prefer a W victim if available (window buffer)
    if bw_k is not None:
        return bw_k
    # 2) Otherwise prefer Q
    if bq_k is not None and bp_k is None:
        return bq_k
    # 3) If both Q and P are candidates, use guard to protect P
    if bq_k is not None and bp_k is not None:
        # Only evict from P if it's clearly lower-utility than Q
        if bp_s is not None and bq_s is not None and bp_s < (P_EVICT_GUARD * bq_s):
            return bp_k
        else:
            return bq_k
    # 4) If only P has candidates
    if bp_k is not None:
        return bp_k

    # Defensive fallback
    for k in cache:
        return k
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
    - Bump decayed frequency.
    - Promotion path:
        W -> Q (MRU), Q -> P (MRU), P -> refresh MRU.
    - Adapt targets based on segment where the hit occurred.
    """
    global w_bytes, q_bytes, p_bytes
    _ensure_targets(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz
    _bump_freq(cache_snapshot, k, inc=1)

    if k in W:
        # Promote from window to probation
        W.pop(k, None)
        w_bytes = max(0, w_bytes - sz)
        _insert_mru(Q, k)
        q_bytes += sz
        m_seg[k] = 1
        _adapt_on_hit(cache_snapshot, seg_id=0, sz=sz)
    elif k in Q:
        # Promote from probation to protected
        Q.pop(k, None)
        q_bytes = max(0, q_bytes - sz)
        _insert_mru(P, k)
        p_bytes += sz
        m_seg[k] = 2
        _adapt_on_hit(cache_snapshot, seg_id=1, sz=sz)
    elif k in P:
        # Refresh recency in protected
        _move_to_mru(P, k)
        m_seg[k] = 2
        _adapt_on_hit(cache_snapshot, seg_id=2, sz=sz)
    else:
        # Defensive: not tracked but a hit => ensure it's accounted in protected
        _insert_mru(P, k)
        p_bytes += sz
        m_seg[k] = 2
        _adapt_on_hit(cache_snapshot, seg_id=2, sz=sz)


def update_after_insert(cache_snapshot, obj):
    """
    On insert after a miss:
    - Count the access into the frequency model.
    - ARC-style: if obj key is found in ghosts, adapt targets accordingly.
    - Place into W (window) MRU by default to capture recency bursts.
      For very large, low-frequency items, bias toward quick eviction by inserting at W's LRU.
    """
    global w_bytes, q_bytes, p_bytes
    _ensure_targets(cache_snapshot)

    k = obj.key
    sz = _size_of(obj)
    m_size[k] = sz

    # Count this access in the frequency model
    _bump_freq(cache_snapshot, k, inc=1)

    # Ghost-based adaptation (ARC-style)
    _ghost_touch_and_adapt(cache_snapshot, k)

    cap = _cap(cache_snapshot)
    if cap <= 0:
        return

    # Default: place into Window MRU
    _insert_mru(W, k)
    w_bytes += sz
    m_seg[k] = 0

    # If very large and low frequency, bias it to be evicted quickly
    f = _decayed_freq(cache_snapshot, k)
    if sz > (cap // 2) and f <= 1:
        W.move_to_end(k, last=False)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After a victim has been evicted by the cache:
    - Remove it from resident structures and adjust accounting.
    - Record as a ghost for ARC-style adaptation.
    - Adapt targets gently based on which segment provided the victim:
        Evicting from W -> slightly decrease window target.
        Evicting from Q -> slightly decrease probation target.
        Evicting from P -> slightly increase both window and probation (favor recency).
    - Keep frequency/size metadata for future admissions.
    """
    global w_bytes, q_bytes, p_bytes, w_target_bytes, q_target_bytes
    _ensure_targets(cache_snapshot)

    ekey = evicted_obj.key
    esz = _size_of(evicted_obj)
    m_size[ekey] = esz

    seg = m_seg.get(ekey, None)
    # Remove from resident structures
    if seg == 0:
        if ekey in W:
            W.pop(ekey, None)
            w_bytes = max(0, w_bytes - esz)
        # Adaptation: window yielded a victim -> reduce window slightly
        w_target_bytes = max(0, w_target_bytes - max(1, esz // 8))
    elif seg == 1:
        if ekey in Q:
            Q.pop(ekey, None)
            q_bytes = max(0, q_bytes - esz)
        # Probation yielded a victim -> reduce probation slightly
        q_target_bytes = max(0, q_target_bytes - max(1, esz // 8))
    elif seg == 2:
        if ekey in P:
            P.pop(ekey, None)
            p_bytes = max(0, p_bytes - esz)
        # Protected yielded a victim -> favor recency a bit more
        cap = _cap(cache_snapshot)
        bump = max(1, esz // 6)
        w_target_bytes = min(cap, w_target_bytes + bump // 2)
        q_target_bytes = min(cap, q_target_bytes + bump // 2)
    else:
        # Unknown segment (defensive cleanup)
        W.pop(ekey, None)
        Q.pop(ekey, None)
        P.pop(ekey, None)

    # Record into corresponding ghost list for future adaptation
    if seg in (0, 1, 2):
        _ghost_record(cache_snapshot, ekey, seg, esz)

    # Clean live metadata of segment assignment
    m_seg.pop(ekey, None)
    # Keep m_freq/m_size for future estimation (no reset)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfqqxcqfq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphsix81l9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2ni3sr9m.pickle

Iteration 88: New subsample score 1.133721 is not better than old score 1.134236, skipping
Iteration 89: Selected program 11 score: 0.2409417083333333
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3u7g8hza.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp96y74h5l.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpijgltilb.pickle

Iteration 89: Proposed new text for program: # Improved Adaptive W-TinyLFU with Segmented LRU (AW-TinyLFU+)
# Key upgrades vs. previous version:
# 1) Corrected on-hit behavior in Window: hits stay in Window (recency only).
# 2) Added in-cache hit counters to refine TinyLFU scores (combined frequency).
# 3) Tightened Protected demotion and Window balancing (loop until within targets).
# 4) Refined admission/eviction scoring with size-aware and in-cache-aware estimates.
# 5) Minor robustness fixes and deterministic victim selection.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# In-cache micro-counters to refine TinyLFU (saturating small integers)
m_incache_hits = dict()  # key -> small int

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# Adaptive window sizing (ARC-inspired, by bytes)
m_W_frac = 0.20          # start with 20% of capacity as window
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_FRAC_STEP = 0.02

# Ghost histories (key -> last seen time), bounded by count
# GW: recently evicted from Window; GP: recently evicted from Main (P or S)
m_ghost_W = dict()
m_ghost_P = dict()
GHOST_MAX = 8192

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# --------------------
# Tunable parameters
# --------------------
# Protected fraction of the main area (bytes). Main = capacity - window.
S_FRAC = 0.80            # fraction of main reserved for protected
ADMIT_EPS = 1e-12        # epsilon for float comparisons
EPS = 1e-12

# In-cache micro-counter parameters
INCACHE_MAX = 31         # saturating maximum
INCACHE_BONUS = 4.0      # weight of in-cache hits in score

# ----------------
# Helper routines
# ----------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    # Retrieve size from object or integer, clamped to at least 1
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    # Compute target bytes for Window and Protected using adaptive window fraction
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * m_W_frac)
    w_target = max(1, w_target)  # non-zero window
    main_target = max(0, cap - w_target)
    s_target = int(main_target * S_FRAC)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    Loop until within target or no victim.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    while m_bytes_S > s_target:
        victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
        if victim_key is None:
            return
        v = cache_snapshot.cache.get(victim_key)
        if v is None:
            return
        sz = _size_of(v)
        _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _balance_W_if_over_target(cache_snapshot, exclude_key=None):
    """
    If Window exceeds its target, move W-LRU to Probation until within target.
    """
    w_target, _ = _targets(cache_snapshot)
    while m_bytes_W > w_target:
        candidate = _lru_key_in_segment(cache_snapshot, SEG_WINDOW, exclude_key=exclude_key)
        if candidate is None:
            return
        v = cache_snapshot.cache.get(candidate)
        if v is None:
            return
        csz = _size_of(v)
        _promote(candidate, SEG_WINDOW, SEG_PROBATION, csz)

def _score_for_key(cache_snapshot, key):
    """
    Size-aware combined frequency score used for admission/eviction comparisons.
    Higher score means the item deserves to stay more.
    score = (TinyLFU_estimate + INCACHE_BONUS * in_cache_hits) / size
    """
    v = cache_snapshot.cache.get(key)
    if v is None:
        return 0.0
    est = _sketch_estimate(key)
    inc = m_incache_hits.get(key, 0)
    sz = float(_size_of(v))
    return (est + INCACHE_BONUS * inc) / max(1.0, sz)

def _score_for_new(obj):
    est = _sketch_estimate(obj.key)
    sz = float(_size_of(obj))
    return est / max(1.0, sz)

def _ghost_add(ghost_dict, key, now):
    ghost_dict[key] = now
    if len(ghost_dict) > GHOST_MAX:
        # Evict the oldest by timestamp
        oldest_k = None
        oldest_t = None
        for k, t in ghost_dict.items():
            if oldest_k is None or t < oldest_t or (t == oldest_t and k < oldest_k):
                oldest_k, oldest_t = k, t
        if oldest_k is not None:
            ghost_dict.pop(oldest_k, None)

def _ghost_remove_if_present(key):
    if key in m_ghost_W:
        m_ghost_W.pop(key, None)
        return "W"
    if key in m_ghost_P:
        m_ghost_P.pop(key, None)
        return "P"
    return None

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Eviction decision approximating and improving W-TinyLFU admission:
      - If Window is above its target, evict from Window (W-LRU) to restore balance.
      - Else if the new insert would make Window exceed its target, run an admission
        comparison: evict P-LRU if new_score > score(P-LRU); otherwise evict W-LRU.
      - Else (no predicted W overflow), prefer P-LRU; if empty, fallback to W-LRU; then S-LRU.
      - Scoring uses TinyLFU + in-cache hit micro-counters normalized by size.
    """
    if not cache_snapshot.cache:
        return None

    _ensure_sketch(cache_snapshot)
    new_sz = _size_of(obj)
    new_score = _score_for_new(obj)

    w_target, _ = _targets(cache_snapshot)

    # If W is currently oversized, evict from W to restore balance
    if m_bytes_W > w_target:
        victim = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
        if victim is not None:
            return victim

    # Predict whether adding this object to W would overflow W's target
    predict_overflow = (m_bytes_W + new_sz) > w_target

    w_lru = _lru_key_in_segment(cache_snapshot, SEG_WINDOW)
    p_lru = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)

    if predict_overflow:
        if p_lru is None:
            # No P items to compare against -> evict from W if possible
            if w_lru is not None:
                return w_lru
            # Fall back further if W empty
            s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
            if s_lru is not None:
                return s_lru
        else:
            # Admission comparison: new vs P-LRU
            p_score = _score_for_key(cache_snapshot, p_lru)
            if new_score > p_score + ADMIT_EPS:
                # Admit: evict from P; update_after_insert will move W-LRU to P
                return p_lru
            else:
                # Reject: evict from W (keeps main stable against low-value new arrivals)
                if w_lru is not None:
                    return w_lru
                # If W empty for some reason, evict from P
                return p_lru

    # No predicted W overflow: free space from P if possible; else W; else S.
    if p_lru is not None:
        return p_lru
    if w_lru is not None:
        return w_lru
    s_lru = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_lru is not None:
        return s_lru

    # Fallback: global LRU across all segments (metadata inconsistency guard)
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency.
      - In-cache hit counters increment (saturating).
      - Segment transitions:
          * Window (W): stay in W (recency only).
          * Probation (P): promote to Protected (S).
          * Protected (S): refresh recency.
      - Keep S within target by demoting S-LRU to P if needed.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    # bump in-cache micro-counter
    prev = m_incache_hits.get(key, 0)
    if prev < INCACHE_MAX:
        m_incache_hits[key] = prev + 1

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        # Stay in window: recency already updated via m_last_access
        pass
    elif seg == SEG_PROBATION:
        # Promote to S on first hit in main
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency only
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Always insert into Window (recency admission).
      - Update sketch, recency, and initialize in-cache hit counter.
      - Adapt window target using ghost histories:
          * If key was recently evicted from W (GW), increase W fraction.
          * If key was recently evicted from Main (GP), decrease W fraction.
      - If Window exceeds its target, move its LRU to P (loop until within target).
      - Keep S near its target by demoting its LRU to P when needed.
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now
    m_incache_hits[key] = 1  # initialize micro-counter

    # Adaptive window sizing via ghosts
    origin = _ghost_remove_if_present(key)
    global m_W_frac
    if origin == "W":
        m_W_frac = min(W_FRAC_MAX, m_W_frac + W_FRAC_STEP)
    elif origin == "P":
        m_W_frac = max(W_FRAC_MIN, m_W_frac - W_FRAC_STEP)

    # Insert into Window
    sz = _size_of(obj)
    global m_bytes_W
    m_seg[key] = SEG_WINDOW
    m_bytes_W += sz

    # If Window is above target, move its LRU to Probation (does not change total size)
    _balance_W_if_over_target(cache_snapshot, exclude_key=key)

    # Keep Protected within target by demoting S-LRU to P if needed
    _demote_S_if_over_target(cache_snapshot, exclude_key=key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Record the eviction in ghost histories to adapt W fraction.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record into appropriate ghost for ARC-like adaptation
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        _ghost_add(m_ghost_W, key, now)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        _ghost_add(m_ghost_P, key, now)

    m_last_access.pop(key, None)
    m_incache_hits.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4ccqdvsb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpusu39gg7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5wi_j2j5.pickle

Iteration 89: New subsample score 0.630351 is better than old score 0.6191359999999999. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_vkiob3y.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpad24mx7k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe91vyrgy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk4nh_mm1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0rlk7955.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpev1hgaed.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuwm33jyx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6uixhblj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcrv0syn4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfqox_y_d.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmyg4a2ll.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf2o4on6i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9uy_xxog.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq3v55xm9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo8c7h692.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp339b2pyp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplll63f83.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplwzcrhpl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpn_0rqq4f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6fyveawo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpt4uz8l13.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9p0k6puy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmplmla0_m4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpigu6zqz5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyuk_bl07.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxptk4gdb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk4z7v6e6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnkit7pme.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg5xg8af0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgq4lfiu4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptldxb93k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5tr0pxin.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprluewmfl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0hayuj83.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_kbsc84v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5tgy6fqk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw9cigih2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpf37s71gu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9_qkzycz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2jimidkr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp35rn2f21.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnas4kmp2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp81a4aq48.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptqvoasmk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp52z3_at2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpatmr0kvd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphd2mt5e_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr3ernsht.pickle

Iteration 89: Full valset score for new program: 0.24181606250000007
Iteration 89: Full train_val score for new program: 0.24181606250000007
Iteration 89: Individual valset scores for new program: [0.472503, 0.448226, 0.457306, 0.414688, 0.465208, 0.456474, 0.288278, 0.498624, 0.521988, 0.531017, 0.108333, 0.316607, 0.062077, 0.0, 0.022795, 0.022823, 0.021274, 0.02474, 0.024047, 0.272227, 0.393314, 0.024596, 0.059961, 0.059961, 0.294795, 0.290323, 0.743793, 0.871641, 0.119197, 0.036364, 0.066059, 0.054921, 0.054921, 0.755786, 0.083333, 0.123566, 0.09111, 0.599176, 0.118081, 0.079845, 0.086744, 0.118492, 0.077303, 0.15, 0.125181, 0.115568, 0.466258, 0.117647]
Iteration 89: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.893548, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 89: Full valset pareto front score: 0.27793295833333337
Iteration 89: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {35, 19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26, 34, 35}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {24}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 33, 28}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {33}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34}, {29, 14}, {24, 25}, {31}, {24, 25}, {34, 3, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35}, {24}]
Iteration 89: Best valset aggregate score so far: 0.25926008333333345
Iteration 89: Best program as per aggregate score on train_val: 29
Iteration 89: Best program as per aggregate score on valset: 29
Iteration 89: Best score on valset: 0.25926008333333345
Iteration 89: Best score on train_val: 0.25926008333333345
Iteration 89: Linear pareto front program index: 29
Iteration 89: New program candidate index: 35
Iteration 90: Selected program 7 score: 0.24589052083333332
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjqqkalak.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiskqdt0j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp15229_0e.pickle

Iteration 90: Proposed new text for program: # GreedyDual-Size-Frequency with Lateness-Aware Boost (GDSF-LB)
# Core ideas:
# 1) GreedyDual-Size(-Frequency) priority:
#    - Each resident key has a priority H. Evict the key with the smallest H.
#    - Global aging L is set to the priority of the last victim, which naturally ages all items.
#    - On insert/hit, an item's H is raised by a size-normalized "bump".
# 2) Lateness-aware reuse boost:
#    - Use an EWMA of inter-arrival times (mu). Hits that happen "sooner than expected" get a bigger bump,
#      and overdue keys get a smaller bump. This helps recency-sensitive streams.
# 3) Frequency-aware bumping:
#    - Hits apply a mild frequency-based multiplier using log1p(freq).
# 4) Size-aware preference:
#    - Bumps are divided by size^exp so smaller objects are favored, improving object hit rate.
# 5) Ghost history:
#    - Preserve mu and a lightweight frequency for evicted keys to warm-start on re-insert.
# 6) Large-object admission throttling:
#    - Very large objects (relative to capacity) get a smaller initial bump so they are sacrificed first
#      if they cause pressure immediately after insertion.

import math

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (hit count, clipped)
m_h = dict()             # key -> float (GreedyDual priority H)

# Global EWMA of inter-arrival time across all keys (for reasonable initialization)
m_global_mu = 32.0

# Global GreedyDual aging value L (priority of last evicted item)
g_L = 0.0

# Ghost history for warm starts on re-insert
g_mu = dict()            # key -> float (last known EWMA)
g_last = dict()          # key -> int (last time tracked in ghost)
g_freq = dict()          # key -> int (historical hits, optional)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Reuse prediction
EWMA_BETA = 0.30         # per-key EWMA learning rate
GLOBAL_BETA = 0.01       # global EWMA learning rate
DEFAULT_MU_MULT = 1.5    # default mu scaling on cold insert (without ghost)

# Size & frequency
SIZE_EXP = 0.95          # divide bump by size^exp (closer to 1.0 biases toward small objects)
FREQ_CLIP = 64           # cap on per-key frequency
FREQ_BONUS = 0.40        # multiplier for log1p(freq-1) on hit bumps

# Lateness-aware boost on hit
REUSE_BOOST = 1.75       # strength of lateness-aware multiplier on hits
LATE_PENALTY = 2.0       # used in lateness mapping (same spirit as previous implementation)

# Cold admission bumps
COLD_BUMP = 0.35         # base bump for cold insert
GHOST_WARM_MULT = 1.8    # multiplicative bump if there is a ghost entry (warmer start)
LARGE_OBJ_ADMIT_CUTOFF = 0.50  # fraction of capacity considered "very large"
LARGE_OBJ_ADMIT_SCALE = 0.25   # scale down cold bump for very large objects

# Ghost history sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# A very large number
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_mu) <= limit:
        return
    # Evict oldest by g_last
    to_remove = len(g_mu) - limit
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_mu.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)

def _effective_delta_for_score(key, now):
    # Lateness-aware remaining time to next use (>= 0, larger => further in future)
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF
    next_t = la + mu
    raw = float(next_t - now)
    if raw >= 0.0:
        return raw
    # Overdue: penalize proportional to how late it is
    return LATE_PENALTY * (-raw) + 1e-9

def _normed_lateness(key, now):
    # Normalize effective delta by mu, so smaller -> sooner reuse; larger -> far
    mu = m_mu.get(key, None)
    ed = _effective_delta_for_score(key, now)
    if mu is None or mu <= 0:
        return 1.0  # neutral
    return max(0.0, ed / float(mu))

def _update_predictor_on_hit(key, now):
    global m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now
    m_freq[key] = min(FREQ_CLIP, m_freq.get(key, 0) + 1)

def _seed_predictor_on_insert(key, now, warm=False):
    if warm and key in g_mu:
        m_mu[key] = max(1.0, 0.90 * float(g_mu[key]))
        m_freq[key] = max(1, min(FREQ_CLIP, int(1 + 0.5 * g_freq.get(key, 1))))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1
    m_last_access[key] = now

def _size_term(obj):
    return float(_size_of(obj)) ** SIZE_EXP

def _record_ghost_on_evict(evicted_key, now):
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_last[evicted_key] = now
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)

def _hit_bump_value(key, obj, now):
    # Base bump on hit
    freq = max(1, m_freq.get(key, 1))
    freq_mul = 1.0 + FREQ_BONUS * math.log1p(max(0, freq - 1))
    # Lateness-aware multiplier (sooner-than-expected reuse -> larger boost)
    nl = _normed_lateness(key, now)  # ~0 soon, ~1 typical, >1 far/overdue
    reuse_mul = 1.0 + REUSE_BOOST / (1.0 + nl)  # in [1, 1+REUSE_BOOST]
    base = 1.0
    return base * freq_mul * reuse_mul / max(1e-9, _size_term(obj))

def _cold_bump_value(key, obj, cache_snapshot):
    # Cold admission bump; bigger if we have ghost history, smaller if object is very large
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    sz = _size_of(obj)
    large_scale = 1.0
    if sz >= int(LARGE_OBJ_ADMIT_CUTOFF * cap):
        large_scale = LARGE_OBJ_ADMIT_SCALE
    if key in g_mu:
        bump = COLD_BUMP * GHOST_WARM_MULT
    else:
        bump = COLD_BUMP
    return bump * large_scale / max(1e-9, _size_term(obj))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose an eviction victim using GreedyDual priority:
    - Evict the resident with the smallest H (lowest priority).
    - Tie-break by: smaller H, older last access, larger size, then lexicographic key.
    Note: 'obj' is the object to be inserted; we don't directly use it here,
          but its presence lets us bias admission in update_after_insert.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    victim_key = None
    victim_H = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        H = m_h.get(k, 0.0)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if victim_key is None:
            victim_key, victim_H, victim_la, victim_sz = k, H, la, sz
            continue

        # Choose smallest H
        if H < victim_H:
            victim_key, victim_H, victim_la, victim_sz = k, H, la, sz
            continue

        if H == victim_H:
            # Older last access preferred for eviction
            if la < victim_la:
                victim_key, victim_H, victim_la, victim_sz = k, H, la, sz
                continue
            if la == victim_la:
                # Prefer to evict larger object to free more space
                if sz > victim_sz:
                    victim_key, victim_H, victim_la, victim_sz = k, H, la, sz
                    continue
                if sz == victim_sz and k < victim_key:
                    victim_key, victim_H, victim_la, victim_sz = k, H, la, sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update EWMA predictor and last access.
    - Increment freq (bounded).
    - Raise H by a size-normalized, lateness- and frequency-aware bump.
      H := max(H, L) + bump
    """
    global g_L
    key = obj.key
    now = _now(cache_snapshot)

    # Seed if missing (rare cold metadata after rehydration)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now, warm=(key in g_mu))
        # Initialize a modest priority so it can compete immediately
        m_h[key] = max(m_h.get(key, 0.0), g_L)

    _update_predictor_on_hit(key, now)

    bump = _hit_bump_value(key, obj, now)
    m_h[key] = max(m_h.get(key, 0.0), g_L) + bump


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize metadata (warm-start from ghost if present).
    - Initialize priority H := L + cold_bump (size-normalized).
      Large objects get a smaller cold bump so they are evicted first if they cause pressure.
    """
    global g_L
    key = obj.key
    now = _now(cache_snapshot)

    _seed_predictor_on_insert(key, now, warm=(key in g_mu))

    cold_bump = _cold_bump_value(key, obj, cache_snapshot)
    m_h[key] = max(m_h.get(key, 0.0), g_L) + cold_bump


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Set aging L to the evicted key's priority (classic GreedyDual aging).
    - Record ghost history (mu, freq).
    - Trim ghost to bounded size.
    - Clean live metadata for the evicted key.
    """
    global g_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Update global aging L to the evicted priority
    evicted_H = m_h.get(ekey, g_L)
    g_L = float(evicted_H)

    # Record ghost
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_h.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzkol8etf.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpb4ecjxwn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmmum26rm.pickle

Iteration 90: New subsample score 0.790218 is not better than old score 0.9224289999999999, skipping
Iteration 91: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpg125q4kp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp58q5k7zj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx72gf98b.pickle

Iteration 91: Proposed new text for program: # ReACT-2Q: Recency-aware, Adaptive, and size-Conscious with Tiny frequency decay
# Major changes from previous version:
# 1) Fixes "overdue" handling: if predicted next access was missed, staleness increases the score
#    (more likely to evict) instead of making it immune. This corrects pathologies on non-stationary traces.
# 2) Adds time-decayed frequency (TinyLFU-style) per key (m_dfreq), so stale popularity fades.
# 3) Blends predicted next-gap with explicit recency pressure: score ~ (predicted_delta + AGE_BONUS*age)/cost.
#    This yields an LRU fallback when predictions are weak, improving scan resistance.
# 4) Makes 2Q protection dynamic: the protected discount adapts to where hits occur (probation vs protected).
# 5) Retains compact ghost history (mu + dfreq) for better warm restarts.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> int (raw hit count, mostly for stats/ghost)
m_dfreq = dict()         # key -> float (time-decayed frequency weight)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Decayed stage-hit accounting to adapt protection strength
s_hits_prob = 0.0
s_hits_prot = 0.0
s_last_decay_time = 0  # int time at which we last applied decay for s_hits_*

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_dfreq = dict()         # key -> float (last known decayed freq)
g_freq = dict()          # key -> int (historical raw hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30            # per-key EWMA learning rate
GLOBAL_BETA = 0.005         # global EWMA learning rate (slow)
SIZE_EXP = 1.05             # size exponent (>1 penalizes large objects slightly more)
FREQ_DAMP = 0.90            # frequency dampening in denominator
DEFAULT_MU_MULT = 3.0       # default mu multiplier for cold inserts (scaled by global mu)
AGE_BONUS = 0.30            # weight of recency "age" in the numerator
DFREQ_HALF_LIFE = 2000.0    # accesses to halve decayed frequency
HITS_HALF_LIFE = 5000.0     # accesses to halve protected/probation hit tallies for adaptation
OVERDUE_CAP_MULT = 1.0      # cap overdue contribution by mu * this factor
GHOST_LIMIT_MIN = 1024      # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0    # ghost capacity  factor * live cache item count
PROTECTED_DISCOUNT_MIN = 0.25
PROTECTED_DISCOUNT_MAX = 0.65
_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # O(1) amortized trimming using deque with versioning to avoid sorting
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_dfreq.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _decay_factor(dt, half_life):
    if dt <= 0:
        return 1.0
    return math.pow(0.5, float(dt) / float(half_life))

def _update_stage_hit_counters(now, stage):
    # Apply decay to stage-level hit counters and increment appropriate bucket
    global s_hits_prob, s_hits_prot, s_last_decay_time
    dt = now - s_last_decay_time
    if dt < 0:
        dt = 0
    if dt > 0:
        f = _decay_factor(dt, HITS_HALF_LIFE)
        s_hits_prob *= f
        s_hits_prot *= f
        s_last_decay_time = now
    if stage >= 1:
        s_hits_prot += 1.0
    else:
        s_hits_prob += 1.0

def _protected_discount():
    # Adapt the protected discount based on where hits occur
    # If protected receives many more hits than probation, use stronger protection (smaller discount).
    # If probation receives many hits (scan-like), weaken protection to let probation compete.
    r = s_hits_prot / max(1e-6, s_hits_prob)
    # Map log-ratio in [-2,2] to [MAX, MIN]
    x = max(-2.0, min(2.0, math.log(r)))
    # Linear map: x=-2 -> MAX, x= 2 -> MIN
    disc = 0.45 - 0.10 * x  # ranges roughly [0.65,0.25]
    return max(PROTECTED_DISCOUNT_MIN, min(PROTECTED_DISCOUNT_MAX, disc))

def _predicted_delta(key, now):
    """
    Predict non-negative time-to-next-access.
    - If no predictor exists: return a conservative baseline.
    - If overdue (now > la + mu), increase delta with overdue, capped by mu*OVERDUE_CAP_MULT.
      This makes long-overdue items easier to evict.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)
    # Overdue: grow with overdue but cap by mu * OVERDUE_CAP_MULT to avoid runaway
    overdue = float(now - next_t)
    cap = max(1.0, float(mu) * OVERDUE_CAP_MULT)
    return max(1.0, min(cap, overdue))

def _decayed_freq(key, now):
    """
    Return and maintain a time-decayed frequency weight for the key.
    We lazily decay on update sites; for reads here we just return the current value.
    """
    return float(m_dfreq.get(key, 0.0))

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    score = (predicted_delta + AGE_BONUS * age) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(dfreq))) * stage_factor
    with a dynamic discount for protected items.
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    la = m_last_access.get(key, now)
    age = max(0.0, float(now - la))

    dfreq = _decayed_freq(key, now)
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0.0, dfreq)))
    numer = delta + AGE_BONUS * age
    base = numer / denom

    # 2Q-style protection: protected items get a multiplicative discount (reduce score)
    stage = m_stage.get(key, 0)
    if stage >= 1:
        base *= _protected_discount()

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - New items start in probation (stage=0).
    - Start decayed freq from ghost hint or 0.
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.85 * float(g_mu[key]))
        m_dfreq[key] = max(0.0, 0.75 * float(g_dfreq.get(key, 0.0)))
        m_freq[key] = max(1, int(g_freq.get(key, 1)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_dfreq[key] = 0.0
        m_freq[key] = 1

    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Update time-decayed LFU weight (TinyLFU-style).
    - Increment raw frequency.
    - Promote from probation to protected on first hit (2Q).
    - Update adaptive protected-hit counters.
    """
    global m_mu, m_last_access, m_freq, m_global_mu, m_dfreq

    last = m_last_access.get(key, None)
    stage = m_stage.get(key, 0)

    # Update decayed freq with time decay by gap since last access if known
    if last is not None:
        gap = max(1, int(now - last))
        decay = _decay_factor(gap, DFREQ_HALF_LIFE)
        prev_df = m_dfreq.get(key, 0.0)
        m_dfreq[key] = prev_df * decay + 1.0
        # Update EWMA of inter-arrival times
        gapf = float(gap)
        prev_mu = m_mu.get(key, gapf)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gapf
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gapf
    else:
        # Cold hit without prior metadata (rare)
        m_dfreq[key] = m_dfreq.get(key, 0.0) + 1.0
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

    # Promotion to protected on first hit (or keep protected)
    if stage == 0:
        m_stage[key] = 1
        stage = 1

    # Adaptive protection accounting
    _update_stage_hit_counters(now, stage)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_dfreq[evicted_key] = float(m_dfreq.get(evicted_key, 0.0))
    g_freq[evicted_key] = m_freq.get(evicted_key, 1)
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object with the highest eviction score.
    Tie-breakers:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    Uses dynamic protected discount, time-decayed frequency, and recency+prediction blend.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s > best_score:
            best_key, best_score, best_la, best_sz = k, s, la, sz
            continue

        if s == best_score:
            if la < best_la:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if sz == best_sz and k < best_key:
                    best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key EWMA of inter-arrival time.
    - Update last access time and time-decayed frequency.
    - Promote to protected on first hit.
    - Update adaptive protected discount statistics.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Keep dfreq as seeded; raw freq may be zero if new
        m_freq[key] = m_freq.get(key, 0)

    _update_predictor_on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Set last access time and start in probation (stage=0).
    - Initialize decayed frequency from ghost (tempered) or 0.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history (mu + dfreq + raw freq) for the evicted key.
    - Trim ghost store to a bounded size (O(1) amortized).
    - Remove per-key metadata of the evicted key.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_dfreq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd842d5l8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmppevndrm_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqs7j04bs.pickle

Iteration 91: New subsample score 0.981123 is not better than old score 1.1452799999999999, skipping
Iteration 92: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj9cw46fw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwvb007w1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5hot4pve.pickle

Iteration 92: Proposed new text for program: # GreedyDual-Size with Warm Ghosts (GDS-WG)
# Goals:
# - Strong size-awareness via GreedyDual-Size: items carry a priority H = L + credit/size^alpha.
# - L (the "clock") is raised to the evicted item's H, naturally aging older entries.
# - Conservative admission: cold inserts get small credit, warm (recently evicted) items get more.
# - Hits add size-discounted credit, promoting frequently reused items quickly.
# - Light ghost store to warm-start reinserts and resist scans.
# - Simple, fast, and streaming-resistant. No heavy segment or predictor complexity.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Priority (GreedyDual-Size "key height")
m_H = dict()            # key -> float
m_last_access = dict()  # key -> int (last time accessed)
m_freq = dict()         # key -> int (hit count since (re)insert)

# Global GreedyDual "clock"
m_L = 0.0

# Ghost history (bounded)
g_H = dict()            # key -> float (last seen H at eviction)
g_last = dict()         # key -> int (last seen time)
g_freq = dict()         # key -> int (observed hit count while alive)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-discount exponent: higher => stronger preference for small objects
ALPHA_SIZE = 0.90

# Admission/credit constants (per event, before size discount)
COLD_INSERT_CREDIT = 0.12     # credit for a fresh miss insert (low to be conservative)
GHOST_INSERT_CREDIT = 0.95    # base credit when re-admitting something seen in ghost
HIT_CREDIT = 1.00             # credit added on every hit

# Ghost warm-start scaling and decay
GHOST_WARM_SCALE = 0.90       # scale previous H upon re-admission
GHOST_HALF_LIFE = 512.0       # in access-count units, for warming decay

# Probation and recency effects in eviction scoring
PROBATION_DISCOUNT = 0.35     # items with zero post-insert hits get their keep-score reduced by this
RECENCY_BONUS = 2.0           # boosts keep-score for recently used (age-aware)

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Numerics
_EPS = 1e-12

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_H) <= limit:
        return
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_H) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_H.pop(k, None)
        g_last.pop(k, None)
        g_freq.pop(k, None)

def _size_discount(sz):
    return 1.0 / (float(sz) ** ALPHA_SIZE)

def _ghost_decay(delta):
    if delta <= 0:
        return 1.0
    return 0.5 ** (float(delta) / float(GHOST_HALF_LIFE))

def _init_priority_on_insert(key, now, obj):
    """
    Initialize priority H on insert.
    - Start from L + cold credit/size.
    - If we have ghost history, warm-start:
        * Option A: base on ghost's last H (decayed) scaled by GHOST_WARM_SCALE.
        * Option B: give a larger admission credit.
      Take the maximum of cold base and warm options.
    """
    global m_L
    sz = _size_of(obj)
    base = float(m_L) + COLD_INSERT_CREDIT * _size_discount(sz)

    if key in g_H:
        # Warm from ghost
        dt = abs(int(now - g_last.get(key, now)))
        warm_from_H = float(g_H.get(key, 0.0)) * GHOST_WARM_SCALE * _ghost_decay(dt)
        warm_from_credit = float(m_L) + GHOST_INSERT_CREDIT * _size_discount(sz)
        H0 = max(base, warm_from_H, warm_from_credit)
        m_freq[key] = max(0, int(g_freq.get(key, 0)))
    else:
        H0 = base
        m_freq[key] = 0

    m_H[key] = float(H0)
    m_last_access[key] = now

def _bump_on_hit(key, now, obj):
    """
    Increase priority on hit by size-discounted HIT_CREDIT.
    """
    sz = _size_of(obj)
    cur = float(m_H.get(key, 0.0))
    m_H[key] = cur + HIT_CREDIT * _size_discount(sz)
    m_last_access[key] = now
    m_freq[key] = m_freq.get(key, 0) + 1

def _keep_score_for_eviction(key, val, now):
    """
    Compute a "keep score" for eviction ranking.
    Lower score => worse to keep => more likely to evict.
    Components:
      - Priority H (from GDS).
      - Recency bonus for items that have at least one hit post-insert.
      - Probation discount for items with zero hits (more willing to evict new/cold).
    """
    H = float(m_H.get(key, 0.0))
    freq = int(m_freq.get(key, 0))
    if freq <= 0:
        # New/cold entries get discounted to make them easier victims (scan protection)
        keep = H * max(0.0, 1.0 - PROBATION_DISCOUNT)
    else:
        # Give recently-hit items a recency favor
        la = int(m_last_access.get(key, now))
        age = max(0, int(now - la))
        keep = H * (1.0 + RECENCY_BONUS / (1.0 + float(age)))
    return keep

def _record_ghost_on_evict(evicted_key, now):
    g_H[evicted_key] = float(m_H.get(evicted_key, 0.0))
    g_last[evicted_key] = now
    g_freq[evicted_key] = int(m_freq.get(evicted_key, 0))


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick the victim with the smallest keep-score:
      keep-score = H * f(recency, probation)
    Ties: prefer the one with older last access, then larger size, then lexicographic key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)

    victim_key = None
    victim_val = None
    victim_score = None
    victim_la = None
    victim_sz = None

    for k, v in cache.items():
        keep = _keep_score_for_eviction(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_score = keep
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if keep < victim_score - _EPS:
            better = True
        elif abs(keep - victim_score) <= _EPS:
            # Tie-breakers
            if la < victim_la:
                better = True
            elif la == victim_la:
                if sz > victim_sz:
                    better = True
                elif sz == victim_sz and k < victim_key:
                    better = True

        if better:
            victim_key, victim_val = k, v
            victim_score = keep
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Increase priority by size-discounted credit.
      - Update last access and hit count.
    """
    key = obj.key
    now = _now(cache_snapshot)

    if key not in m_H or key not in m_last_access:
        # In case metadata was missing, initialize conservatively
        _init_priority_on_insert(key, now, obj)

    _bump_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert (after a miss):
      - Initialize priority with GreedyDual-Size logic (warm from ghost if available).
      - Record last access time; hit count starts from ghost's remembered freq or 0.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _init_priority_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Set global L to victim's priority H (standard GreedyDual aging).
      - Record ghost metadata: last H, time, and hit count.
      - Trim ghost size.
      - Remove live metadata of evicted key.
    """
    global m_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Advance the GreedyDual clock to the victim's height
    victim_H = float(m_H.get(ekey, 0.0))
    if victim_H > m_L:
        m_L = victim_H

    # Record minimal ghost info
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_H.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcstxa1ob.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmph3pn94eu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpre659ob9.pickle

Iteration 92: New subsample score 1.3118150000000002 is better than old score 1.1751810000000003. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbuywg9e4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6wcrvdh6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptd4b2mzu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp33uw9cdr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0lxaax86.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphblmt27v.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6wrja3zz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprp_ronhk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1vqzyoos.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq455ryrq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy8e7t4zh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyas022xv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptene_ud5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp48vscj2m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfc_d_vsz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpl_yjv6mp.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphd2s2948.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiu6zpwmt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcevkcx8i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp58ozrig8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj8rwn3sb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp43kpunsv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4_hp67d_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpugccsmx0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpeevfgp2z.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4gat8e6a.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpc9k5gyw2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpz4az7t_m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp42jba08s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnwlfs2p9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8e7dlw2_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3q_y84qs.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpi764lv9f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk0d16don.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbk3qbpyz.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbk0wltp2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6thz40oh.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9xkyu9lk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuwl4u7kb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6xjs_1zr.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvo2zq3sy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsqrnc31h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbdlramcv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_fv3hi55.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvqe0hp3s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8_jg_kvu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbj3f5eou.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpk7x88695.pickle

Iteration 92: Full valset score for new program: 0.23944256249999996
Iteration 92: Full train_val score for new program: 0.23944256249999996
Iteration 92: Individual valset scores for new program: [0.496718, 0.471107, 0.47991, 0.429448, 0.491095, 0.482417, 0.271531, 0.498624, 0.540937, 0.531017, 0.075, 0.369005, 0.040045, 0.0, 0.021379, 0.021274, 0.020197, 0.023615, 0.022782, 0.272227, 0.362832, 0.026556, 0.057382, 0.057382, 0.269774, 0.334677, 0.818272, 0.892692, 0.038335, 0.038636, 0.04328, 0.007003, 0.0206, 0.750681, 0.078947, 0.067961, 0.025853, 0.641937, 0.125461, 0.084558, 0.058033, 0.070601, 0.049342, 0.366667, 0.04132, 0.071248, 0.466258, 0.068627]
Iteration 92: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.893548, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 92: Full valset pareto front score: 0.27793295833333337
Iteration 92: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {35, 19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26, 34, 35, 36}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {24}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 33, 28, 36}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {33}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36}, {29, 14}, {24, 25}, {31}, {24, 25}, {34, 3, 36, 5, 6, 7, 8, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36}, {24}]
Iteration 92: Best valset aggregate score so far: 0.25926008333333345
Iteration 92: Best program as per aggregate score on train_val: 29
Iteration 92: Best program as per aggregate score on valset: 29
Iteration 92: Best score on valset: 0.25926008333333345
Iteration 92: Best score on train_val: 0.25926008333333345
Iteration 92: Linear pareto front program index: 29
Iteration 92: New program candidate index: 36
Iteration 93: Selected program 24 score: 0.2291873541666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpah6_0fkj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp995s0_6j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaaxo1tke.pickle

Iteration 93: Proposed new text for program: # Size-aware SLRU with decayed frequency (SA-SLRU-DF)
# Summary:
# - Two segments (SLRU): 0 = probation (recent), 1 = protected (frequent).
# - Segment preference is based on BYTES, not object count.
# - Within a segment, victim is the key with the lowest decayed frequency per byte.
#   score(key) = decayed_freq(now) / size^GAMMA
#   where decayed_freq is an exponential decay of accumulated size-aware credits.
# - On hit: add size-aware credit, decay lazily, and promote from probation to protected.
# - Adaptive protected share using ghost feedback (ARC-like), but in BYTES.
# - Ghost stores (last segment, decayed_freq) to warm-start on reinsert and adapt share.
# - Simple, robust, low-noise predictors; no speculative urgency/reuse-time prediction.

import math

# ----------------------
# Global metadata stores
# ----------------------
# Decayed frequency credit (value to keep)
m_f = dict()          # key -> float
m_f_ts = dict()       # key -> int (last time m_f was updated)

# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()        # key -> int
m_hits = dict()       # key -> int (hit count since insert)

# Last access time
m_last_access = dict()  # key -> int

# Ghost history (bounded)
g_f = dict()          # key -> float (decayed frequency at eviction)
g_last = dict()       # key -> int (last seen time)
g_seg = dict()        # key -> int (segment at eviction: 0=T1, 1=T2)

# ARC-style target for protected BYTES share
m_protect_target_frac = 0.50

# ----------------------
# Tunable hyperparameters
# ----------------------
# Exponential decay half-life (in access-count units)
HL = 256.0
HL_MIN = 16.0
HL_MAX = 16384.0

# Size-awareness
ALPHA_SIZE_CREDIT = 0.70   # credit per hit = 1 / size^alpha
GAMMA_SIZE_EVICT = 1.00    # divide keep-score by size^gamma

# Segment selection bias
NONPREF_BIAS = 3.0         # inflate retention for the non-preferred segment
ARC_ADAPT_STEP = 0.06
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# Ghost sizing
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0

# Defaults
WARM_START_FACTOR = 0.70   # fraction of ghost frequency to seed on reinsert
COLD_SEED = 0.0            # frequency seed for completely cold items

# Numerics
_EPS = 1e-9
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    # Bound ghost by a factor of live keys; keep it simple and safe
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    if len(g_last) <= limit:
        return
    # Evict oldest ghost entries by last-seen time
    items = sorted(g_last.items(), key=lambda kv: (kv[1], kv[0]))
    to_remove = len(g_last) - limit
    for i in range(min(to_remove, len(items))):
        k, _ = items[i]
        g_f.pop(k, None)
        g_last.pop(k, None)
        g_seg.pop(k, None)

def _half_life():
    # Keep within reasonable bounds
    hl = HL
    if hl < HL_MIN:
        hl = HL_MIN
    elif hl > HL_MAX:
        hl = HL_MAX
    return float(hl)

def _decay_factor(delta):
    if delta <= 0:
        return 1.0
    hl = _half_life()
    return 0.5 ** (float(delta) / hl)

def _current_decayed_freq(key, now):
    base = float(m_f.get(key, 0.0))
    ts = m_f_ts.get(key, None)
    if ts is None:
        return 0.0
    delta = int(now - ts)
    if delta > 0 and base > 0.0:
        base *= _decay_factor(delta)
    return base

def _size_credit(obj):
    return 1.0 / (float(_size_of(obj)) ** ALPHA_SIZE_CREDIT)

def _record_ghost_on_evict(evicted_key, now):
    # Record minimal ghost info
    df = _current_decayed_freq(evicted_key, now)
    if df > 0.0:
        g_f[evicted_key] = float(df)
    else:
        g_f.pop(evicted_key, None)
    g_last[evicted_key] = now
    g_seg[evicted_key] = int(m_seg.get(evicted_key, 0))

def _seed_on_insert(key, now, obj):
    """
    Initialize per-key metadata on insert, warm-starting from ghost if available.
    Also adjust protected target fraction based on ghost's last segment (ARC heuristic).
    """
    global m_protect_target_frac

    # Adapt protected fraction from ghost feedback (B1 vs B2)
    if key in g_seg:
        if g_seg[key] == 0:
            # Ghost from probation (recency-biased hit): reduce protected share
            m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
        else:
            # Ghost from protected (frequency-biased hit): increase protected share
            m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)

    # Seed decayed frequency
    if key in g_f:
        m_f[key] = WARM_START_FACTOR * float(g_f[key])
    else:
        m_f[key] = float(COLD_SEED)
    m_f_ts[key] = now

    # Hits since insert (for promotion logic)
    m_hits[key] = 0

    # Last access time
    m_last_access[key] = now

    # Segment placement: ghost from protected -> start protected (like ARC)
    if START_PROTECTED_FROM_GHOST and key in g_seg and g_seg[key] == 1:
        m_seg[key] = 1
    else:
        m_seg[key] = 0

def _update_on_hit(key, now, obj):
    """
    Update lazy-decayed frequency with size-aware credit and manage promotion.
    """
    cur = _current_decayed_freq(key, now)
    credit = _size_credit(obj)

    # Accumulate and stamp time
    m_f[key] = cur + credit
    m_f_ts[key] = now

    # Update hit counters and recency
    m_hits[key] = m_hits.get(key, 0) + 1
    m_last_access[key] = now

    # Promote to protected after first hit (post-insert)
    if m_seg.get(key, 0) == 0 and m_hits.get(key, 0) >= 1:
        m_seg[key] = 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Pick a victim using BYTES-based SLRU + decayed frequency per byte.

    Steps:
      - Compute current protected bytes and target protected bytes (adaptive).
      - Preferred segment to evict from is whichever exceeds its target in BYTES.
      - Within all items, compute retention score:
          retention = decayed_freq(now) / size^GAMMA
        Multiply by NONPREF_BIAS for items not in the preferred segment.
        Lower retention means worse to keep -> evict.
      - Tie-breakers:
          * Prefer evicting from preferred segment
          * Older last access (LRU)
          * Larger size (to free space faster)
          * Lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))

    # Compute segment bytes
    prot_bytes = 0
    total_bytes = 0
    for k, v in cache.items():
        sz = _size_of(v)
        total_bytes += sz
        if m_seg.get(k, 0) == 1:
            prot_bytes += sz

    target_prot_bytes = int(round(float(m_protect_target_frac) * float(total_bytes)))
    preferred_seg = 1 if prot_bytes > target_prot_bytes else 0

    # Candidate selection
    victim_key = None
    victim_val = None
    victim_score = None
    victim_seg = None
    victim_la = None
    victim_sz = None

    # Dynamic bias tweak for very large incoming objects to avoid thrashing protected
    incoming_sz = _size_of(obj) if obj is not None else 1
    dynamic_bias = NONPREF_BIAS
    if incoming_sz >= 0.10 * cap:
        # Strengthen bias to stick to the preferred segment when inserting large items
        dynamic_bias *= 1.25

    for k, v in cache.items():
        sz = _size_of(v)
        seg = m_seg.get(k, 0)
        la = m_last_access.get(k, -1)

        s_now = _current_decayed_freq(k, now)
        # Base retention (higher => better to keep)
        retention = (s_now + _EPS) / (float(sz) ** GAMMA_SIZE_EVICT)

        # Penalize non-preferred segment to avoid cross-segment pollution
        if seg != preferred_seg:
            retention *= dynamic_bias

        if victim_key is None:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz
            continue

        better = False
        if retention < victim_score:
            better = True
        elif retention == victim_score:
            # Prefer evicting from preferred segment
            if seg == preferred_seg and victim_seg != preferred_seg:
                better = True
            elif seg == victim_seg:
                # LRU
                if la < victim_la:
                    better = True
                elif la == victim_la:
                    # Free more space sooner
                    if sz > victim_sz:
                        better = True
                    elif sz == victim_sz and k < victim_key:
                        better = True

        if better:
            victim_key, victim_val = k, v
            victim_score = retention
            victim_seg = seg
            victim_la = la
            victim_sz = sz

    return victim_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Lazy-decay and add size-aware credit to frequency.
      - Update last access and promote from probation to protected on first hit.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Initialize if missing (cold hit or metadata reset)
    if key not in m_f_ts or key not in m_last_access or key not in m_seg:
        _seed_on_insert(key, now, obj)

    _update_on_hit(key, now, obj)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-style adaptation using ghost feedback to adjust protected target fraction (BYTES).
      - Initialize decayed frequency (warm-start from ghost if available).
      - Start in probation unless ghost indicates protected.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_on_insert(key, now, obj)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record ghost metadata (decayed frequency and last segment).
      - Trim ghost to bounded size.
      - Remove all live metadata of evicted key.
    """
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Record the segment before cleaning live metadata
    _record_ghost_on_evict(ekey, now)
    _trim_ghost(cache_snapshot)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_f.pop(ekey, None)
    m_f_ts.pop(ekey, None)
    m_seg.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpphh3r_vt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpsu44ab3s.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxc1xw06n.pickle

Iteration 93: New subsample score 0.578078 is not better than old score 0.719815, skipping
Iteration 94: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp26b0ltey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxd257w5w.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6903g_qd.pickle

Iteration 94: Proposed new text for program: # Adaptive Size-Aware W-TinyLFU with ARC-style window tuning (ASAW-TLFU)
# - Three segments by bytes:
#   * Window (W): small recency buffer for new items (short-term bursts).
#   * Probation (P): main area for items with one confirmed hit.
#   * Protected (S): items with two or more hits; shielded, but can be demoted if oversized.
# - Eviction preference is adaptive and size-aware:
#   * Keep W near its dynamically tuned target (ARC-like ghost feedback).
#   * Within a segment, choose the victim by a composite "keep score":
#       keep_score = seg_bonus * (freq + 1) * recency_weight / size^alpha
#     Lower keep_score => better eviction candidate.
#     This blends TinyLFU frequency, LRU recency, and size-awareness.
# - TinyLFU Count-Min sketch provides global frequency estimates (decayed).
# - Admission:
#   * Doorkeeper: high-estimate items bypass W into P.
#   * Size-aware bypass: very large items require a higher estimate to bypass W.
# - ARC-style adaptive window:
#   * Track recent evictions as "ghosts". If a miss is to a W-ghost, increase W size;
#     if its to a P/S-ghost, decrease W size. This tunes W for recency vs frequency.
# - Promotions on hits:
#   * W -> P on first hit, P -> S on second hit, S stays in S (recency refreshed).
#   * If S exceeds its target bytes, demote its LRU back to P.
# - Additional guard:
#   * If S exceeds its target when choosing a victim, demote S-LRU first to maintain balance.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()  # key -> int, last access time (access_count)
m_seg = dict()          # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None         # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# ARC-style adaptive window tuning (ghosts)
GHOST_MAX = 8192
m_ghost_W = dict()      # key -> last seen time (from W evictions)
m_ghost_M = dict()      # key -> last seen time (from P or S evictions)
ADAPT_STEP = 0.02
MIN_W_FRAC = 0.05
MAX_W_FRAC = 0.50

# --------------------
# Tunable parameters
# --------------------
# Start with a moderate window; adapt at runtime.
m_w_frac = 0.20           # dynamic window fraction of total capacity (bytes)
# Protected fraction of the main area (bytes). Main = capacity - window.
m_s_frac = 0.80           # protected portion of main
# TinyLFU admission threshold. est >= ADMIT_EST_THRESHOLD bypasses W into P (plus size-aware bump).
ADMIT_EST_THRESHOLD = 2
# Size-aware exponent in keep score (higher -> more size penalization)
SIZE_ALPHA = 0.9
# Recency half-life-ish factor: larger -> slower recency decay; affects recency_weight = 1/(1+age/RH)
RECENCY_HALF = 1000

EPS = 1e-12

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# ---------------
# Helper routines
# ---------------
def _ensure_sketch(_cache_snapshot):
    global m_sketch
    if m_sketch is None:
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _targets(cache_snapshot):
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * max(MIN_W_FRAC, min(MAX_W_FRAC, m_w_frac)))
    main_target = cap - w_target
    s_target = int(main_target * m_s_frac)
    return w_target, s_target

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Find LRU key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

def _prune_ghosts(ghost_map):
    """
    Keep ghost maps within GHOST_MAX using oldest-first removal.
    """
    if len(ghost_map) <= GHOST_MAX:
        return
    # Remove oldest entries until within cap
    # (iterate multiple times rather than sorting for simplicity)
    to_remove = len(ghost_map) - GHOST_MAX
    for _ in range(to_remove):
        oldest_k = None
        oldest_t = None
        for k, t in ghost_map.items():
            if oldest_k is None or t < oldest_t:
                oldest_k, oldest_t = k, t
        if oldest_k is None:
            break
        ghost_map.pop(oldest_k, None)

def _adapt_window_fraction_on_miss(cache_snapshot, key):
    """
    ARC-style adaptation using ghost hits:
      - if key was evicted from W, increase window (favor recency).
      - if key was evicted from P/S, decrease window (favor frequency).
    """
    global m_w_frac
    now = cache_snapshot.access_count
    hit_W = key in m_ghost_W
    hit_M = key in m_ghost_M

    if hit_W and not hit_M:
        m_w_frac = min(MAX_W_FRAC, m_w_frac + ADAPT_STEP)
        m_ghost_W.pop(key, None)
    elif hit_M and not hit_W:
        m_w_frac = max(MIN_W_FRAC, m_w_frac - ADAPT_STEP)
        m_ghost_M.pop(key, None)
    elif hit_W and hit_M:
        # If in both, slight nudge toward balanced
        if m_w_frac < 0.25:
            m_w_frac = min(MAX_W_FRAC, m_w_frac + ADAPT_STEP / 2)
        else:
            m_w_frac = max(MIN_W_FRAC, m_w_frac - ADAPT_STEP / 2)
        m_ghost_W.pop(key, None)
        m_ghost_M.pop(key, None)

    # Age ghosts; keep maps bounded
    _prune_ghosts(m_ghost_W)
    _prune_ghosts(m_ghost_M)

def _recency_weight(now, last_access):
    age = max(0, now - (last_access if last_access is not None else 0))
    # 1 / (1 + age / RECENCY_HALF) provides a soft recency decay
    return 1.0 / (1.0 + (age / float(max(1, RECENCY_HALF))))

def _keep_score_for_key(cache_snapshot, key, obj, seg_value):
    """
    Composite keep score: higher => prefer to keep; lower => evict sooner.
    keep_score = seg_bonus * (freq + 1) * recency_weight / size^alpha
    """
    now = cache_snapshot.access_count
    la = m_last_access.get(key, None)
    rec = _recency_weight(now, la)

    freq = _sketch_estimate(key)
    size = _size_of(obj)
    size_term = pow(float(size), max(0.1, float(SIZE_ALPHA)))

    # Segment bonus: prefer keeping multi-hit items more than fresh ones
    # S > P > W
    if seg_value == SEG_PROTECTED:
        seg_bonus = 3.0
    elif seg_value == SEG_PROBATION:
        seg_bonus = 1.6
    else:
        seg_bonus = 1.0

    # The +1 guards cold-start; higher freq boosts survival; recency boosts as well.
    keep_score = (seg_bonus * (float(freq) + 1.0) * rec) / max(1e-9, size_term)
    return keep_score

def _best_victim_in_segment(cache_snapshot, seg_value):
    """
    Select the victim in a segment by the lowest keep score.
    Tie-breakers:
      1) lower keep_score
      2) older last access (LRU)
      3) larger size
      4) lexicographic key
    """
    best_k = None
    best_score = None
    best_la = None
    best_sz = None

    for k, v in cache_snapshot.cache.items():
        if m_seg.get(k) != seg_value:
            continue
        seg = seg_value
        score = _keep_score_for_key(cache_snapshot, k, v, seg)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_score, best_la, best_sz = k, score, la, sz
            continue
        if score < best_score - 1e-15:
            best_k, best_score, best_la, best_sz = k, score, la, sz
            continue
        if abs(score - best_score) <= 1e-15:
            if la < best_la:
                best_k, best_score, best_la, best_sz = k, score, la, sz
                continue
            if la == best_la:
                if sz > best_sz:
                    best_k, best_score, best_la, best_sz = k, score, la, sz
                    continue
                if sz == best_sz and k < best_k:
                    best_k, best_score, best_la, best_sz = k, score, la, sz
    return best_k

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict a victim guided by segment pressures and a size-aware TinyLFU+LRU keep-score.
    Priority:
      - If S exceeds target, demote S-LRU to P (helps keep balance).
      - If Window exceeds its target, evict from Window (lowest keep-score).
      - Else, evict from Probation (lowest keep-score).
      - If P empty, fall back to W; if both empty, evict from S (lowest keep-score).
      - Final fallback: global LRU across all segments.
    """
    if not cache_snapshot.cache:
        return None

    # Proactively maintain S target by demoting its LRU to P if oversized.
    _demote_S_if_over_target(cache_snapshot)

    w_target, _ = _targets(cache_snapshot)
    # Prefer evicting from W if it's above target; else from P.
    prefer_seg = SEG_WINDOW if m_bytes_W > w_target else SEG_PROBATION

    # Try preferred segment using keep-score victim selection
    victim = _best_victim_in_segment(cache_snapshot, prefer_seg)
    if victim is not None:
        return victim

    # Fallbacks
    victim = _best_victim_in_segment(cache_snapshot, SEG_WINDOW)
    if victim is not None:
        return victim

    victim = _best_victim_in_segment(cache_snapshot, SEG_PROTECTED)
    if victim is not None:
        return victim

    # Global LRU fallback for robustness
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and last access.
      - Promote: W -> P -> S on successive confirmations.
      - If S exceeds its byte target, demote S-LRU back to P (excluding the just-hit key).
    """
    _ensure_sketch(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
    else:
        # SEG_PROTECTED: refresh recency only
        pass


def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - ARC-style adaptation using ghost info.
      - Update sketch, last access.
      - Admission: TinyLFU doorkeeper with size-aware threshold
        (very large objects require a higher estimate to bypass W).
    """
    _ensure_sketch(cache_snapshot)
    key = obj.key

    # ARC-style adaptive window using ghost feedback
    _adapt_window_fraction_on_miss(cache_snapshot, key)

    # Update sketch and metadata
    _sketch_add(key, 1)
    now = cache_snapshot.access_count
    m_last_access[key] = now

    est = _sketch_estimate(key)
    sz = _size_of(obj)

    # Size-aware doorkeeper: large items need higher estimate to bypass W.
    capacity = max(1, int(cache_snapshot.capacity))
    large_ratio = float(sz) / float(capacity)
    # Demand stronger evidence if the item is large
    size_bump = 0
    if large_ratio >= 0.25:
        size_bump = 2
    elif large_ratio >= 0.10:
        size_bump = 1

    bypass_threshold = ADMIT_EST_THRESHOLD + size_bump

    global m_bytes_W, m_bytes_P
    if est >= bypass_threshold:
        m_seg[key] = SEG_PROBATION
        m_bytes_P += sz
    else:
        m_seg[key] = SEG_WINDOW
        m_bytes_W += sz


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Add to ghost list for ARC-style adaptation.
    """
    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    # Record ghost for adaptive window tuning
    now = cache_snapshot.access_count
    if seg == SEG_WINDOW:
        m_ghost_W[key] = now
        # keep ghost maps bounded
        _prune_ghosts(m_ghost_W)
    elif seg in (SEG_PROBATION, SEG_PROTECTED):
        m_ghost_M[key] = now
        _prune_ghosts(m_ghost_M)

    m_last_access.pop(key, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp00c5dxjv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqqap_c_1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr708t_z2.pickle

Iteration 94: New subsample score 0.982656 is better than old score 0.935538. Continue to full eval and add to candidate pool.
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphlqxz7gt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd0akeuct.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzrl5ch1k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzpo2d8pi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnq8ax6x3.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbin0g85f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4rs9dhzc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqx181tmx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpffuspfdn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpgdp4w_l_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpux3o58kk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmi7eckxl.pickle

Killing subprocess...
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpud4o3r8u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbytjnpvx.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp38no4kh0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp43xb7n7_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd44yfmnv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5j7m3_ey.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpksbdvguj.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdj14s1vn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp59kjqskc.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4vuxjovk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpouc907pv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp08o8_pei.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoqxxa9iu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoh0x9ovl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqq0ygxcj.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpods1eapc.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpcfsofm8e.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv5y1iscb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm2hryj99.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmptjtb2w3u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphorwfqeb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd9q74fig.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuueavgn5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpup8oumgn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjfebud81.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpnkzssoir.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5ylavsgq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6sjetkgk.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpotp9x14n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpv73ybq2i.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpfuozwl_6.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpo75avd1v.pickle

Iteration 94: Full valset score for new program: 0.2388286458333333
Iteration 94: Full train_val score for new program: 0.2388286458333333
Iteration 94: Individual valset scores for new program: [0.467822, 0.439937, 0.446034, 0.409087, 0.463122, 0.453476, 0.260766, 0.495674, 0.540937, 0.531017, 0.091667, 0.364565, 0.0, 0.0, 0.020105, 0.020147, 0.019523, 0.023194, 0.0225, 0.273144, 0.369715, 0.0, 0.057382, 0.057382, 0.282308, 0.313508, 0.796425, 0.887558, 0.080563, 0.054545, 0.05467, 0.0, 0.021978, 0.737577, 0.085526, 0.088261, 0.090855, 0.640907, 0.125461, 0.072359, 0.065974, 0.073073, 0.060855, 0.366667, 0.104292, 0.075456, 0.466258, 0.091503]
Iteration 94: New valset pareto front scores: [0.511623, 0.485699, 0.493849, 0.446608, 0.506659, 0.493601, 0.288278, 0.498624, 0.541294, 0.531017, 0.133333, 0.41119, 0.084853, 0.0, 0.022795, 0.022823, 0.021408, 0.02474, 0.024047, 0.27681, 0.428712, 0.026556, 0.060606, 0.060606, 0.33212, 0.386089, 0.853029, 0.893548, 0.179096, 0.077273, 0.072893, 0.087828, 0.087912, 0.766678, 0.111842, 0.166814, 0.218379, 0.643483, 0.125461, 0.157749, 0.144166, 0.178742, 0.101974, 0.366667, 0.188153, 0.17223, 0.466258, 0.166667]
Iteration 94: Full valset pareto front score: 0.27793295833333337
Iteration 94: Updated valset pareto front programs: [{27}, {27}, {28}, {27}, {27}, {27}, {35, 19}, {0, 1, 4, 5, 7, 8, 9, 10, 11, 12, 13, 15, 16, 18, 19, 20, 22, 23, 26, 34, 35, 36}, {16}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37}, {7, 14, 23, 24, 25, 29}, {27}, {17, 6}, {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {24}, {35, 6, 14, 17, 23, 30}, {35, 6, 14, 17, 23, 24, 30}, {11}, {14, 17, 24, 25, 29}, {16, 33, 28, 36}, {17, 14}, {17, 14}, {24}, {28}, {8, 7}, {33}, {31}, {18}, {6, 14, 17, 24, 25, 29, 30}, {30}, {31}, {23}, {24, 25}, {29}, {24, 25, 29, 14}, {5}, {1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 14, 15, 16, 17, 18, 20, 21, 22, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 36, 37}, {29, 14}, {24, 25}, {31}, {24, 25}, {34, 3, 36, 5, 6, 7, 8, 37, 17, 28, 29}, {30}, {30, 31}, {2, 3, 4, 5, 6, 8, 9, 11, 14, 17, 19, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37}, {24}]
Iteration 94: Best valset aggregate score so far: 0.25926008333333345
Iteration 94: Best program as per aggregate score on train_val: 29
Iteration 94: Best program as per aggregate score on valset: 29
Iteration 94: Best score on valset: 0.25926008333333345
Iteration 94: Best score on train_val: 0.25926008333333345
Iteration 94: Linear pareto front program index: 29
Iteration 94: New program candidate index: 37
Iteration 95: Selected program 17 score: 0.24124691666666667
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp28g_qx7u.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_mb4o8uo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2kma2_a7.pickle

Iteration 95: Proposed new text for program: # ReSLRU-ASAP: Size-aware SLRU with adaptive protected fraction and robust time prediction
# Highlights:
# - SLRU-style segmentation: probation (Qp) and protected (Qr) queues with lazy upkeep.
# - Adaptive protected fraction (AIMD): grows on promotions, shrinks when protected must evict.
# - Robust prediction: EWMA inter-arrival with overdue penalty (not pinning overdue items).
# - Size-aware, frequency-aware eviction scoring with time-decayed frequency.
# - Compact ghost history to warm-start predictors; O(1) amortized trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()   # key -> int (last access time)
m_mu = dict()            # key -> float (EWMA inter-arrival time)
m_freq = dict()          # key -> float (time-decayed hit count)
m_stage = dict()         # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# SLRU queues (store keys; lazy duplicates allowed)
qP = deque()             # probation (MRU at left, LRU at right)
qR = deque()             # protected (MRU at left, LRU at right)
bytes_qP = 0
bytes_qR = 0

# Adaptive protected fraction (0..1), target fraction of capacity for Qr by bytes
m_prot_frac = 0.60

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()            # key -> float (last known EWMA)
g_freq = dict()          # key -> float (historical decayed hits)
g_stamp = dict()         # key -> int (version counter)
g_order = deque()        # deque of (key, stamp) in LRU order of eviction

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30           # per-key EWMA learning rate
GLOBAL_BETA = 0.01         # global EWMA learning rate (slow)
SIZE_EXP = 1.0             # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.60           # frequency dampening in denominator
DEFAULT_MU_MULT = 2.0      # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_CAP = 6.0          # max multiple of mu used as overdue penalty
PROT_FRAC_STEP = 0.03      # additive change to protected fraction (AIMD)
STAGE_FACTOR_PROTECTED = 0.25  # multiplicative discount to eviction score for protected items
GHOST_LIMIT_MIN = 1024     # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0   # ghost capacity  factor * live cache item count
CANDIDATE_SAMPLE = 8       # max candidates sampled from queue tail for eviction
FREQ_TAU_MULT = 8.0        # frequency exponential decay horizon multiplier
_INF = 1e30                # very large number for "infinite" predicted delay

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _predicted_delta(key, now):
    """
    Predict a non-negative, capped time-to-next-access.
    - Cold (no predictor): treat as very large.
    - Not overdue: delta = next_t - now.
    - Overdue: apply a penalty proportional to lateness, capped to OVERDUE_CAP * mu.
      This makes long-overdue items more evictable (prevents pinning).
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return _INF

    mu = float(mu)
    next_t = la + mu
    if now <= next_t:
        return float(next_t - now)

    # Overdue: lateness ratio relative to mu
    lateness_ratio = (now - next_t) / max(1.0, mu)
    # Effective delta grows with lateness but is capped
    return float(min((1.0 + max(0.0, lateness_ratio)) * mu, OVERDUE_CAP * mu))

def _eviction_score(key, obj, now):
    """
    Higher score => evict sooner.
    score = (predicted_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq))) * stage_factor
    """
    sz = _size_of(obj)
    delta = _predicted_delta(key, now)
    freq = float(m_freq.get(key, 1.0))
    denom = (sz ** SIZE_EXP) * (1.0 + FREQ_DAMP * math.log1p(max(0.0, freq)))
    base = delta / denom

    # Protected items get additional protection
    if m_stage.get(key, 0) >= 1:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - Warm-start from ghost if available; temper the values.
    - Else, seed from global mu with a conservative multiplier.
    - Start in probation (stage=0).
    """
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        m_freq[key] = max(0.5, 0.9 * float(g_freq.get(key, 1.0)))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1.0

    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Time-decayed frequency to avoid stale accumulation.
    - Update last access time.
    """
    global m_mu, m_last_access, m_freq, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = float(m_mu.get(key, gap))
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap

        # global mu
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap

        # decayed frequency
        tau = max(8.0, FREQ_TAU_MULT * float(m_global_mu))
        decay = math.exp(-gap / tau)
        prev_f = float(m_freq.get(key, 0.0))
        m_freq[key] = prev_f * decay + 1.0
    else:
        # cold edge case
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 1.0

    m_last_access[key] = now

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = float(m_freq.get(evicted_key, 1.0))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _target_protected_bytes(cache_snapshot):
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    return int(min(max(0.10, m_prot_frac), 0.90) * cap)

def _demote_one_from_protected(cache_snapshot):
    """
    Demote a valid LRU key from protected (Qr) to probation (Qp).
    Lazy cleanup: skip missing/invalid keys.
    """
    global bytes_qP, bytes_qR
    cache = cache_snapshot.cache

    while qR:
        k = qR.pop()
        if k not in cache:
            continue
        if m_stage.get(k, 1) != 1:
            continue
        obj = cache[k]
        sz = _size_of(obj)
        # Demote
        m_stage[k] = 0
        qP.appendleft(k)
        bytes_qR = max(0, bytes_qR - sz)
        bytes_qP += sz
        return True
    return False

def _enforce_protected_budget(cache_snapshot):
    target = _target_protected_bytes(cache_snapshot)
    # Demote until protected fits budget
    while bytes_qR > target:
        if not _demote_one_from_protected(cache_snapshot):
            break

def _add_to_probation(cache_snapshot, key):
    """
    Insert key to MRU of probation, adjust bytes.
    """
    global bytes_qP
    obj = cache_snapshot.cache.get(key, None)
    if obj is None:
        return
    sz = _size_of(obj)
    qP.appendleft(key)
    bytes_qP += sz
    m_stage[key] = 0

def _promote_to_protected(cache_snapshot, key):
    """
    Promote key to MRU of protected. Adjust bytes and target fraction (AIMD increase).
    """
    global bytes_qP, bytes_qR, m_prot_frac
    cache = cache_snapshot.cache
    obj = cache.get(key, None)
    if obj is None:
        return
    sz = _size_of(obj)

    # Adjust bytes (may have been in probation)
    if m_stage.get(key, 0) == 0:
        bytes_qP = max(0, bytes_qP - sz)
    # Promote
    m_stage[key] = 1
    qR.appendleft(key)
    bytes_qR += sz

    # AIMD: increase fraction on promotion (bounded)
    m_prot_frac = min(0.90, m_prot_frac + PROT_FRAC_STEP)

def _touch_stage(cache_snapshot, key):
    """
    Move key to MRU position of its current stage (lazy duplicates are fine).
    """
    stage = m_stage.get(key, 0)
    if stage == 0:
        qP.appendleft(key)
    else:
        qR.appendleft(key)

def _sample_candidates(Q, desired_stage, cache, now, max_sample):
    """
    Sample up to max_sample valid candidates from the right (LRU end) of Q.
    We pop temporarily, remember, then push back to restore order (with duplicates allowed).
    """
    candidates = []
    popped = []
    while Q and len(candidates) < max_sample:
        k = Q.pop()
        popped.append(k)
        if (k in cache) and (m_stage.get(k, -1) == desired_stage):
            candidates.append(k)
    # restore
    for x in reversed(popped):
        Q.append(x)
    return candidates

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict using SLRU segmentation with size/time-aware scoring:
    - Prefer victims from probation (Qp). If empty, fall back to protected (Qr).
    - From the LRU end, sample up to CANDIDATE_SAMPLE valid keys and pick the one
      with the highest eviction score. Tie-breakers:
        1) Older last access first (LRU among equals)
        2) Larger size first (free more space)
        3) Lexicographic key order for stability
    - Before choosing, enforce protected budget via demotions.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    # Enforce protected budget before eviction
    _enforce_protected_budget(cache_snapshot)

    now = _now(cache_snapshot)

    # Helper to choose best from a list of keys
    def choose_best(keys):
        best_key = None
        best_score = -1.0
        best_la = None
        best_sz = None
        for k in keys:
            v = cache.get(k, None)
            if v is None:
                continue
            s = _eviction_score(k, v, now)
            la = m_last_access.get(k, -1)
            sz = _size_of(v)

            if best_key is None:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue

            if s > best_score:
                best_key, best_score, best_la, best_sz = k, s, la, sz
                continue

            if s == best_score:
                if la < best_la:
                    best_key, best_score, best_la, best_sz = k, s, la, sz
                    continue
                if la == best_la:
                    if sz > best_sz:
                        best_key, best_score, best_la, best_sz = k, s, la, sz
                        continue
                    if sz == best_sz and k < best_key:
                        best_key, best_score, best_la, best_sz = k, s, la, sz
        return best_key

    # Try from probation
    cand = _sample_candidates(qP, 0, cache, now, CANDIDATE_SAMPLE)
    victim = choose_best(cand)
    if victim is not None:
        return victim

    # Fall back to protected if probation empty
    cand = _sample_candidates(qR, 1, cache, now, CANDIDATE_SAMPLE)
    victim = choose_best(cand)
    if victim is not None:
        # Shrink protected fraction a bit if we had to evict from protected
        # (AIMD decrease will be applied after actual eviction in update_after_evict)
        return victim

    # Fallback: full scan (rare)
    all_keys = list(cache.keys())
    victim = None
    best_score = -1.0
    best_la = None
    best_sz = None
    for k in all_keys:
        v = cache[k]
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if victim is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < victim))))):
            victim, best_score, best_la, best_sz = k, s, la, sz
    return victim


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Update per-key predictor (EWMA + decayed frequency).
    - Move to MRU of its stage; promote probation -> protected on first hit.
    - Enforce protected budget (may demote LRU protected to probation).
    - Adjust protected fraction upwards on promotion (AIMD).
    """
    global bytes_qP, bytes_qR, m_prot_frac

    key = obj.key
    now = _now(cache_snapshot)

    # Ensure metadata exists
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Place into probation if somehow missing from queues
        _add_to_probation(cache_snapshot, key)

    _update_predictor_on_hit(key, now)

    stage = m_stage.get(key, 0)
    if stage == 0:
        # Promote on first hit
        _promote_to_protected(cache_snapshot, key)
    else:
        # Touch protected
        _touch_stage(cache_snapshot, key)

    # Enforce protected budget post-promotion
    _enforce_protected_budget(cache_snapshot)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Place in probation MRU.
    - No change to protected budget here (handled on hits and evict).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _seed_predictor_on_insert(key, now)
    _add_to_probation(cache_snapshot, key)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Update queue byte accounting and adapt protected fraction if evicted from protected.
    """
    global bytes_qP, bytes_qR, m_prot_frac

    ekey = evicted_obj.key
    esize = _size_of(evicted_obj)
    estage = m_stage.get(ekey, 0)

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust bytes accounting based on last known stage
    if estage == 0:
        bytes_qP = max(0, bytes_qP - esize)
    else:
        bytes_qR = max(0, bytes_qR - esize)
        # AIMD: decrease protected fraction if protected eviction occurred
        m_prot_frac = max(0.10, m_prot_frac - PROT_FRAC_STEP)

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1s_htvr5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpyhlbf9ii.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd63inedm.pickle

Iteration 95: New subsample score 0.430942 is not better than old score 0.433095, skipping
Iteration 96: Selected program 5 score: 0.23314141666666657
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp68mj1n3m.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8wn9c_xi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp7gvo01xy.pickle

Iteration 96: Proposed new text for program: # Adaptive Windowed Segmented LRU with TinyLFU-guided, size-aware victim selection (AWSLRU-TLFU)
# Key ideas:
# - Three segments (by bytes):
#   * Window (W): recency buffer for new items and short bursts.
#   * Probation (P): main area for items with at least one confirmation.
#   * Protected (S): items with at least two confirmations; demoted back to P when S exceeds target.
# - Eviction uses TinyLFU estimates and size-aware scoring to choose among the oldest K candidates
#   from W and P, instead of blindly evicting a single segment LRU.
# - Admission on insert uses TinyLFU "compare" against P-LRU to decide bypass into P vs W.
# - Window size adapts online: more hits in W grow the window; hits in P/S shrink it.
# - TinyLFU (Count-Min Sketch) with periodic decay captures long-term popularity, cheaply.
#
# Expected benefits vs baseline:
# - Lower miss rate due to LFU-per-byte scoring of victims across segments.
# - Better handling of bursty vs steady workloads via adaptive window size.
# - Size-awareness avoids keeping large, cold objects at the expense of many small, warm ones.

# --------------------
# Metadata and globals
# --------------------
m_last_access = dict()   # key -> int, last access time (access_count)
m_seg = dict()           # key -> segment: 0=Window, 1=Probation, 2=Protected

# Segment byte counters (kept consistent with m_seg)
m_bytes_W = 0
m_bytes_P = 0
m_bytes_S = 0

# TinyLFU sketch (Count-Min Sketch) for global frequency estimation
SKETCH_DEPTH = 4
SKETCH_WIDTH = 4096
m_sketch = None          # list[depth][width]
m_sketch_ops = 0
SKETCH_DECAY_EVERY = 20000

# --------------------
# Tunable parameters
# --------------------
# Dynamic window fraction (of total capacity). Adjusts online.
W_FRAC_DEFAULT = 0.20
W_FRAC_MIN = 0.05
W_FRAC_MAX = 0.50
W_ADAPT_STEP_UP = 0.01
W_ADAPT_STEP_DOWN = 0.01

# Protected fraction (of the main area). Main = capacity - window.
S_FRAC_DEFAULT = 0.80

# Victim sampling: how many oldest candidates to consider per segment
K_CANDIDATES = 5

# TinyLFU admission smoothing and thresholding
ADMIT_COMPARE_SMOOTH = 0.5  # smoothing to avoid overconfident zeros

# Size-aware scoring exponent: score ~ est / (size^alpha). alpha=1 favors many small objects.
SIZE_ALPHA = 1.0

# When W exceeds target by HEADROOM, force evict from W.
W_HEADROOM_FRAC = 0.03

EPS = 1e-12

# Segment constants
SEG_WINDOW = 0
SEG_PROBATION = 1
SEG_PROTECTED = 2

# State guards/reset
m_seen_capacity = None
m_w_frac = W_FRAC_DEFAULT
m_s_frac = S_FRAC_DEFAULT

# ---------------
# Helper routines
# ---------------
def _size_of(obj_or_size):
    return max(1, int(getattr(obj_or_size, "size", obj_or_size) if hasattr(obj_or_size, "size") else obj_or_size))

def _ensure_state(cache_snapshot):
    """
    Initialize or reset metadata if capacity changes (new trace/run) or sketch not built.
    """
    global m_seen_capacity, m_sketch, m_sketch_ops
    global m_last_access, m_seg, m_bytes_W, m_bytes_P, m_bytes_S
    global m_w_frac, m_s_frac

    if (m_seen_capacity is None) or (m_seen_capacity != int(cache_snapshot.capacity)) or (m_sketch is None):
        m_seen_capacity = int(cache_snapshot.capacity)

        # Reset metadata
        m_last_access = {}
        m_seg = {}
        m_bytes_W = 0
        m_bytes_P = 0
        m_bytes_S = 0

        # Reset sketch
        m_sketch = [[0] * SKETCH_WIDTH for _ in range(SKETCH_DEPTH)]
        m_sketch_ops = 0

        # Reset adaptive fractions
        m_w_frac = W_FRAC_DEFAULT
        m_s_frac = S_FRAC_DEFAULT

def _hash_idx(key, i):
    h = hash(key)
    seed = 0x9E3779B97F4A7C15
    x = (h ^ (seed * (i + 1))) & 0xFFFFFFFFFFFFFFFF
    return x & (SKETCH_WIDTH - 1)

def _sketch_add(key, delta=1):
    global m_sketch_ops
    if m_sketch is None:
        return
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        c = m_sketch[i][idx]
        m_sketch[i][idx] = min(65535, c + delta)
    m_sketch_ops += 1
    if m_sketch_ops >= SKETCH_DECAY_EVERY:
        for i in range(SKETCH_DEPTH):
            row = m_sketch[i]
            for j in range(SKETCH_WIDTH):
                row[j] = row[j] >> 1
        m_sketch_ops = 0

def _sketch_estimate(key):
    if m_sketch is None:
        return 0
    est = None
    for i in range(SKETCH_DEPTH):
        idx = _hash_idx(key, i)
        v = m_sketch[i][idx]
        est = v if est is None else min(est, v)
    return 0 if est is None else est

def _targets(cache_snapshot):
    """
    Dynamic targets based on adaptive window fraction and protected fraction inside main.
    """
    cap = max(1, int(cache_snapshot.capacity))
    w_target = int(cap * max(W_FRAC_MIN, min(W_FRAC_MAX, m_w_frac)))
    main_target = cap - w_target
    s_target = int(main_target * max(0.0, min(0.95, m_s_frac)))
    return w_target, s_target

def _promote(k, from_seg, to_seg, size):
    global m_bytes_W, m_bytes_P, m_bytes_S
    if from_seg == to_seg:
        return
    if from_seg == SEG_WINDOW:
        m_bytes_W -= size
    elif from_seg == SEG_PROBATION:
        m_bytes_P -= size
    elif from_seg == SEG_PROTECTED:
        m_bytes_S -= size

    if to_seg == SEG_WINDOW:
        m_bytes_W += size
    elif to_seg == SEG_PROBATION:
        m_bytes_P += size
    elif to_seg == SEG_PROTECTED:
        m_bytes_S += size

    m_seg[k] = to_seg

def _lru_key_in_segment(cache_snapshot, seg_value, exclude_key=None):
    """
    Single LRU (oldest) key in a segment with deterministic tie-breakers:
      1) smallest last_access (oldest)
      2) larger size (free more space)
      3) lexicographic key
    """
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        if exclude_key is not None and k == exclude_key:
            continue
        if m_seg.get(k) != seg_value:
            continue
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la < best_la:
            best_k, best_la, best_sz = k, la, sz
            continue
        if la == best_la:
            if sz > best_sz:
                best_k, best_la, best_sz = k, la, sz
                continue
            if sz == best_sz and k < best_k:
                best_k, best_la, best_sz = k, la, sz
    return best_k

def _k_lru_candidates_in_segment(cache_snapshot, seg_value, k, exclude_key=None):
    """
    Return up to k oldest candidates in a segment (ties broken by larger size, then key).
    """
    cands = []
    for kkey, v in cache_snapshot.cache.items():
        if exclude_key is not None and kkey == exclude_key:
            continue
        if m_seg.get(kkey) != seg_value:
            continue
        la = m_last_access.get(kkey, -1)
        sz = _size_of(v)
        cands.append((la, -sz, kkey))
    if not cands:
        return []
    # Sort by oldest (smallest la), then larger size (-sz), then lexicographic key
    cands.sort(key=lambda t: (t[0], t[1], t[2]))
    # Return keys (and keep tuple if needed)
    return [t[2] for t in cands[:max(1, int(k))]]

def _score_for_eviction(key, obj):
    """
    Lower score means a better eviction victim.
    Score ~ (TinyLFU estimate + smoothing) / (size^alpha).
    """
    est = _sketch_estimate(key)
    sz = float(_size_of(obj))
    denom = max(EPS, sz ** max(0.0, float(SIZE_ALPHA)))
    return (est + ADMIT_COMPARE_SMOOTH) / denom

def _demote_S_if_over_target(cache_snapshot, exclude_key=None):
    """
    If protected segment exceeds its target, demote its LRU (excluding exclude_key) to P.
    """
    _, s_target = _targets(cache_snapshot)
    global m_bytes_S
    if m_bytes_S <= s_target:
        return
    victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED, exclude_key=exclude_key)
    if victim_key is None:
        return
    v = cache_snapshot.cache.get(victim_key)
    if v is None:
        return
    sz = _size_of(v)
    _promote(victim_key, SEG_PROTECTED, SEG_PROBATION, sz)

# -------------------------
# Core cache policy methods
# -------------------------
def evict(cache_snapshot, obj):
    """
    Evict with TinyLFU + size-aware scoring across segments:
      - If Window exceeds target + headroom, evict from W (choose best among K oldest).
      - Else, consider K oldest from P and K oldest from W; evict the one with lowest score.
      - If W and P empty, evict from S (oldest), as last resort.
    Tie-breakers on equal score:
      1) older last access
      2) larger size
      3) lexicographic key
    """
    _ensure_state(cache_snapshot)
    if not cache_snapshot.cache:
        return None

    w_target, _ = _targets(cache_snapshot)
    headroom = int(cache_snapshot.capacity * W_HEADROOM_FRAC)

    # Helper to choose best victim from a set of candidates by score and deterministic ties.
    def choose_best(keys):
        best_k = None
        best_score = None
        best_la = None
        best_sz = None
        for k in keys:
            v = cache_snapshot.cache.get(k)
            if v is None:
                continue
            sc = _score_for_eviction(k, v)
            la = m_last_access.get(k, -1)
            sz = _size_of(v)
            if (best_k is None or sc < best_score or
               (abs(sc - best_score) <= 1e-9 and (la < best_la or
               (la == best_la and (sz > best_sz or (sz == best_sz and k < best_k)))))):
                best_k, best_score, best_la, best_sz = k, sc, la, sz
        return best_k

    # Force evict from W if it's significantly over its target
    if m_bytes_W > w_target + headroom:
        w_cands = _k_lru_candidates_in_segment(cache_snapshot, SEG_WINDOW, K_CANDIDATES)
        victim = choose_best(w_cands)
        if victim is not None:
            return victim

    # Otherwise, consider both W and P candidates and pick the lowest score
    p_cands = _k_lru_candidates_in_segment(cache_snapshot, SEG_PROBATION, K_CANDIDATES)
    w_cands = _k_lru_candidates_in_segment(cache_snapshot, SEG_WINDOW, K_CANDIDATES)

    if p_cands or w_cands:
        victim = choose_best(p_cands + w_cands)
        if victim is not None:
            return victim

    # Fallback to S (protected) if both W and P are empty
    s_cand = _lru_key_in_segment(cache_snapshot, SEG_PROTECTED)
    if s_cand is not None:
        return s_cand

    # Last-resort global LRU
    best_k = None
    best_la = None
    best_sz = None
    for k, v in cache_snapshot.cache.items():
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_k is None or la < best_la or (
            la == best_la and (sz > best_sz or (sz == best_sz and k < best_k))
        ):
            best_k, best_la, best_sz = k, la, sz
    return best_k

def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update sketch and recency timestamp.
      - Promote: W -> P on first hit; P -> S on second; S stays in S.
      - If S exceeds target, demote its LRU to P.
      - Adapt window size: hits in W increase W (more burstiness), hits in P/S decrease W.
    """
    _ensure_state(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    seg = m_seg.get(key, SEG_PROBATION)  # default to P if missing
    sz = _size_of(obj)

    if seg == SEG_WINDOW:
        _promote(key, SEG_WINDOW, SEG_PROBATION, sz)
        # Adapt: hit in W suggests more recency needed
        global m_w_frac
        m_w_frac = min(W_FRAC_MAX, m_w_frac + W_ADAPT_STEP_UP)
    elif seg == SEG_PROBATION:
        _promote(key, SEG_PROBATION, SEG_PROTECTED, sz)
        _demote_S_if_over_target(cache_snapshot, exclude_key=key)
        # Adapt: hit in P suggests favoring main (shrink W slightly)
        global m_w_frac
        m_w_frac = max(W_FRAC_MIN, m_w_frac - W_ADAPT_STEP_DOWN)
    else:
        # SEG_PROTECTED: refresh recency
        # Adapt: sustained hits in S indicate stable popularity; shrink W a bit.
        global m_w_frac
        m_w_frac = max(W_FRAC_MIN, m_w_frac - W_ADAPT_STEP_DOWN)

def update_after_insert(cache_snapshot, obj):
    """
    On insert (miss):
      - Update sketch and recency timestamp.
      - Admission: compare TinyLFU(est/size) of new item with P-LRU; if higher, bypass W into P.
        Otherwise insert into W. Always ensure a minimum window by bytes.
      - Adapt: insertion into W nudges W up; bypass into P nudges W down.
    """
    _ensure_state(cache_snapshot)
    _sketch_add(obj.key, 1)

    key = obj.key
    now = cache_snapshot.access_count
    m_last_access[key] = now

    est_new = _sketch_estimate(key)
    sz_new = _size_of(obj)

    # Minimum window guard: keep at least W_FRAC_MIN
    w_target, _ = _targets(cache_snapshot)
    min_window_bytes = int(cache_snapshot.capacity * W_FRAC_MIN)

    # P-LRU candidate to compare against
    p_victim_key = _lru_key_in_segment(cache_snapshot, SEG_PROBATION)
    admit_to_P = False
    if p_victim_key is not None:
        p_victim_obj = cache_snapshot.cache.get(p_victim_key)
        if p_victim_obj is not None:
            est_p = _sketch_estimate(p_victim_key)
            sz_p = _size_of(p_victim_obj)
            score_new = (est_new + ADMIT_COMPARE_SMOOTH) / max(EPS, float(sz_new) ** SIZE_ALPHA)
            score_p   = (est_p   + ADMIT_COMPARE_SMOOTH) / max(EPS, float(sz_p)  ** SIZE_ALPHA)
            # Admit to P if new is at least as promising as P's LRU (lower score evicts worse).
            admit_to_P = score_new >= score_p

    global m_bytes_W, m_bytes_P, m_w_frac
    if admit_to_P and (m_bytes_W >= min_window_bytes):
        # Bypass window into P
        m_seg[key] = SEG_PROBATION
        m_bytes_P += sz_new
        # Adapt: stronger main preference
        m_w_frac = max(W_FRAC_MIN, m_w_frac - W_ADAPT_STEP_DOWN)
    else:
        # Insert into window
        m_seg[key] = SEG_WINDOW
        m_bytes_W += sz_new
        # Adapt: recent-biased insertion
        m_w_frac = min(W_FRAC_MAX, m_w_frac + W_ADAPT_STEP_UP)

def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Remove all metadata for the evicted key.
      - Update segment byte counters accordingly.
      - Optional small adaptation: if we evicted from W, slightly shrink W (to avoid W overflow).
    """
    _ensure_state(cache_snapshot)

    key = evicted_obj.key
    seg = m_seg.pop(key, None)

    sz = _size_of(evicted_obj)
    global m_bytes_W, m_bytes_P, m_bytes_S, m_w_frac

    if seg == SEG_WINDOW:
        m_bytes_W = max(0, m_bytes_W - sz)
        # Slightly shrink W if we are pushing too many evictions from W
        m_w_frac = max(W_FRAC_MIN, m_w_frac - (W_ADAPT_STEP_DOWN * 0.5))
    elif seg == SEG_PROBATION:
        m_bytes_P = max(0, m_bytes_P - sz)
    elif seg == SEG_PROTECTED:
        m_bytes_S = max(0, m_bytes_S - sz)

    m_last_access.pop(key, None)
Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 346
    global m_w_frac
    ^^^^^^^^^^^^^^^
SyntaxError: name 'm_w_frac' is used prior to global declaration

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 346
    global m_w_frac
    ^^^^^^^^^^^^^^^
SyntaxError: name 'm_w_frac' is used prior to global declaration

Subprocess stdout: 
Subprocess stderr: Traceback (most recent call last):
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/cache_simulate.py", line 4, in <module>
    from gepa.adapters.caching_adapter_gpt5_2 import My
  File "/home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/My.py", line 346
    global m_w_frac
    ^^^^^^^^^^^^^^^
SyntaxError: name 'm_w_frac' is used prior to global declaration

Iteration 96: New subsample score 0.0 is not better than old score 0.409199, skipping
Iteration 97: Selected program 30 score: 0.23011145833333335
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3sot81g4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprvurl9fo.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpubgsmwxi.pickle

Iteration 97: Proposed new text for program: # Adaptive Replacement Cache (ARC) with bounded ghosts
# - Strong, proven policy that adapts online between recency (LRU) and frequency (LFU-like via re-references).
# - Two resident lists: T1 (recent), T2 (frequent) and two ghost lists: B1 (recency history), B2 (frequency history).
# - p (target T1 size) is adapted using ghost hits:
#       * Hit in B1 => increase p (favor recency, grow T1)
#       * Hit in B2 => decrease p (favor frequency, grow T2)
# - Eviction chooses LRU from T1 or T2 via ARC's Replace() rule.
# - Uses object-count capacity intuitively; robust even if the underlying system enforces byte capacity.
#
# This implementation maintains only light metadata and focuses on correctness and adaptation,
# which typically yields lower miss rates across diverse workloads.

from collections import OrderedDict

# Resident sets (in-cache)
T1 = OrderedDict()  # recent (single-reference) keys, MRU at end
T2 = OrderedDict()  # frequent (re-referenced) keys, MRU at end

# Ghost sets (out-of-cache history)
B1 = OrderedDict()  # recency history
B2 = OrderedDict()  # frequency history

# ARC target for T1 size (in number of objects)
p_arc = 0

# Estimated cache capacity (object count). We learn it as the max observed live size.
C_est = 0

# Optional: lightweight last-access timestamps for tie-break or fallback
_last_access = dict()  # key -> int (access_count)


def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))


def _ensure_estimated_capacity(cache_snapshot):
    # Update our estimate of the cache capacity by object count.
    # We use the maximum observed live object count as a stable estimate.
    global C_est
    live = len(getattr(cache_snapshot, "cache", {}) or {})
    if live > C_est:
        C_est = live
    if C_est <= 0:
        C_est = max(1, live)


def _move_to_mru(odict, key):
    if key in odict:
        odict.move_to_end(key, last=True)
    else:
        odict[key] = None  # placeholder; OrderedDict tracks recency


def _pop_lru(odict):
    # Pop and return the LRU key if any; else return None
    if not odict:
        return None
    k, _ = odict.popitem(last=False)
    return k


def _trim_ghosts():
    # ARC bounds total ghost size to 2 * capacity (by object count).
    # Use our capacity estimate; if small, still ensures bounded memory.
    limit = max(2 * max(1, C_est), 1)
    # Trim oldest from the longer ghost list first to keep balance
    while (len(B1) + len(B2)) > limit:
        if len(B1) >= len(B2):
            _pop_lru(B1)
        else:
            _pop_lru(B2)


def _replace(incoming_key=None):
    """
    ARC Replace(): decide whether to evict from T1 or T2.
    Rule:
      - Evict from T1 if |T1| >= 1 and (|T1| > p or (incoming_key in B2 and |T1| == p)), else from T2.
    """
    t1_sz = len(T1)
    if t1_sz >= 1 and (t1_sz > p_arc or (incoming_key in B2 and t1_sz == p_arc)):
        # Evict LRU from T1 -> move to B1
        victim = _pop_lru(T1)
        if victim is not None:
            _move_to_mru(B1, victim)
            # Keep ghosts bounded
            _trim_ghosts()
            return victim
    # Else evict from T2 -> move to B2
    victim = _pop_lru(T2)
    if victim is not None:
        _move_to_mru(B2, victim)
        _trim_ghosts()
        return victim
    # Fallback: if both empty (shouldn't happen), return None
    return None


def _sync_on_eviction(evicted_key):
    # Ensure resident lists are in sync when the external system evicts a key.
    if evicted_key in T1:
        # Already moved to B1 in evict(); nothing to do here.
        T1.pop(evicted_key, None)
    elif evicted_key in T2:
        T2.pop(evicted_key, None)
    # Clean any lingering last-access record
    _last_access.pop(evicted_key, None)


def _add_on_insert(key, now):
    """
    ARC on cache insert:
      - If key in B1: increase p, move to T2 (frequent)
      - Else if key in B2: decrease p, move to T2 (frequent)
      - Else: insert into T1 (recent)
    """
    global p_arc

    if key in B1:
        # Adapt: favor recency, grow T1
        inc = max(1, len(B2) // max(1, len(B1)))
        p_arc = min(max(0, C_est), p_arc + inc)
        B1.pop(key, None)
        _move_to_mru(T2, key)
    elif key in B2:
        # Adapt: favor frequency, grow T2 (shrink T1)
        dec = max(1, len(B1) // max(1, len(B2)))
        p_arc = max(0, p_arc - dec)
        B2.pop(key, None)
        _move_to_mru(T2, key)
    else:
        # Cold insert -> T1
        _move_to_mru(T1, key)

    _last_access[key] = now
    _trim_ghosts()


def _on_hit(key, now):
    """
    ARC on hit:
      - If hit in T1: promote to T2 (frequent)
      - If hit in T2: move to MRU (maintain recency)
    """
    if key in T1:
        T1.pop(key, None)
        _move_to_mru(T2, key)
    elif key in T2:
        _move_to_mru(T2, key)
    else:
        # Should not happen (hit implies resident), but guard:
        _move_to_mru(T2, key)
    _last_access[key] = now


def evict(cache_snapshot, obj):
    """
    Pick a victim using ARC Replace() rule.
    Steps:
      - Estimate capacity by object count (C_est) from current live size.
      - Choose eviction segment via Replace() using incoming key's ghost membership.
      - Return selected victim key (LRU of chosen segment).
      - If metadata missing or inconsistent, fall back to global LRU based on _last_access or any key.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    _ensure_estimated_capacity(cache_snapshot)
    incoming_key = getattr(obj, "key", None)

    # Ensure that our resident lists contain only live keys. This should be synchronized via updates,
    # but in case of mismatch, drop stale entries from the fronts.
    # We do a light cleanup (not full scan) by popping from fronts if they are not live.
    for od in (T1, T2):
        while od:
            k = next(iter(od))
            if k in cache:
                break
            od.popitem(last=False)

    # Choose victim via ARC Replace() behavior
    victim = _replace(incoming_key)
    if victim is not None and victim in cache:
        return victim

    # Fallback 1: evict from whichever resident list has a live LRU
    for od in (T1, T2):
        k = _pop_lru(od)
        if k is not None and k in cache:
            # Move evicted to its ghost
            if od is T1:
                _move_to_mru(B1, k)
            else:
                _move_to_mru(B2, k)
            _trim_ghosts()
            return k

    # Fallback 2: global LRU by last_access among live keys
    now = _now(cache_snapshot)
    oldest_k = None
    oldest_t = now + 1
    for k in cache.keys():
        t = _last_access.get(k, -1)
        if t < oldest_t:
            oldest_t = t
            oldest_k = k
    if oldest_k is not None:
        # Add to appropriate ghost (best-effort: treat as T1 eviction)
        _move_to_mru(B1, oldest_k)
        _trim_ghosts()
        return oldest_k

    # Final fallback: arbitrary key
    for k in cache.keys():
        return k
    return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update capacity estimate.
      - Promote/move within ARC resident lists.
      - Refresh last-access time.
    """
    now = _now(cache_snapshot)
    _ensure_estimated_capacity(cache_snapshot)

    key = obj.key
    _on_hit(key, now)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - Update capacity estimate.
      - ARC placement with ghost-driven adaptation.
      - New or unseen keys go to T1; ghost hits go to T2 and update p.
    """
    now = _now(cache_snapshot)
    _ensure_estimated_capacity(cache_snapshot)

    key = obj.key
    _add_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After the system evicts a victim:
      - Move evicted resident key into the appropriate ghost list (B1 if from T1, else B2).
      - Keep ghosts bounded.
      - Drop resident and last-access metadata for the evicted key.
    """
    _ensure_estimated_capacity(cache_snapshot)

    ekey = evicted_obj.key

    # If evict() ran, we already moved the victim into B1/B2 there.
    # But to be robust, ensure consistency here as well.
    if ekey in T1:
        T1.pop(ekey, None)
        _move_to_mru(B1, ekey)
    elif ekey in T2:
        T2.pop(ekey, None)
        _move_to_mru(B2, ekey)
    else:
        # If it's not tracked in resident lists (rare), place in recency ghost by default
        _move_to_mru(B1, ekey)

    _last_access.pop(ekey, None)
    _trim_ghosts()

    # Final cleanup to remove any ghost entries that correspond to keys still resident
    # (can happen in rare race/mismatch situations).
    for ghost in (B1, B2):
        # A light clean: if MRU ghost is still live, drop it; avoid O(n) scans.
        if ghost:
            gk = next(reversed(ghost))
            if gk in cache_snapshot.cache:
                ghost.pop(gk, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmprcmuwvw2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1s_7v7ug.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpddbp5_po.pickle

Iteration 97: New subsample score 0.223826 is not better than old score 0.474788, skipping
Iteration 98: Selected program 29 score: 0.25926008333333345
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5jbi9rwi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1utns5tm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1hfz1yph.pickle

Iteration 98: Proposed new text for program: # A-SLRU+Predict: Adaptive-byte SLRU with predictive recency, decayed frequency, and admission-aware eviction
# Key upgrades vs previous:
# 1) Adaptive protected/ probation byte quota (A-SLRU by bytes):
#    - Maintain an EMA of where hits occur (probation vs protected).
#    - Target protected byte share = clamp(0.2..0.85, 0.2 + 0.6 * EMA(protected hit share)).
#    - Eviction chooses from the segment that exceeds its byte budget, preventing probation starvation.
# 2) Global victim selection when segments are near-balanced:
#    - Not strictly evict probation first. We evict the globally worst item, with a mild protected bias.
# 3) Improved time-to-next-score:
#    - If predicted next access is in the future: larger slack => more evictable.
#    - If overdue: small grace window keeps it, but highly overdue (relative to mu) becomes evictable.
# 4) Admission-aware threshold using the incoming object's predicted utility:
#    - Prefer to evict items with score >= incoming score; fallback to global worst if all residents look better.
# 5) Stronger (but decaying) frequency and better cold-start for new/ghosted keys.
#
# Complexity:
# - evict(): O(n) single pass to find worst probation and worst protected; constant-time decisions.
# - updates: O(1).
#
# Notes:
# - Size-aware via size^SIZE_EXP in denominator (per-byte fairness).
# - Decayed frequency via epochs with fast right-shift halving.
# - Compact ghost for warm starts with O(1) maintenance.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_freq = dict()           # key -> int (raw hit count, aged by epoch lazily)
m_f_epoch = dict()        # key -> int (epoch of last freq normalization)
m_stage = dict()          # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Segment byte counters (best-effort)
bytes_probation = 0
bytes_protected = 0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()             # key -> float (last known EWMA)
g_freq = dict()           # key -> int (historical hits)
g_stamp = dict()          # key -> int (version counter)
g_order = deque()         # deque of (key, stamp) in LRU order of eviction

# Decayed frequency global epoch
g_epoch = 0
g_next_epoch_at = 4096    # next access_count to advance epoch
EPOCH_LENGTH = 4096       # accesses per epoch
DECAY_SHIFT_PER_EPOCH = 1  # halve per epoch

# ----------------------
# Adaptive protected share (EMA)
# ----------------------
protected_hit_share_ema = 0.5  # start neutral
EMA_ALPHA = 0.05               # hit source EMA smoothing (per hit)
PROTECTED_MIN_FRAC = 0.20
PROTECTED_MAX_FRAC = 0.85

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30              # per-key EWMA learning rate
GLOBAL_BETA = 0.010           # global EWMA learning rate (slow but responsive)
SIZE_EXP = 1.0                # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.90              # dampened influence of frequency via log1p
DEFAULT_MU_MULT = 1.50        # default mu multiplier for cold inserts (scaled by global mu)

# Overdue handling: grace portion of mu; beyond grace, lateness becomes evictable
LATE_GRACE_FRAC = 0.50        # fraction of mu as grace when overdue
LATE_PENALTY = 0.60           # penalty factor applied to lateness beyond grace

NEW_ITEM_BONUS = 1.20         # modest boost to eviction score of rarely-hit items (freq<=1)

# Stage bias multipliers applied to eviction scores
PROBATION_STAGE_BIAS = 1.00
PROTECTED_STAGE_BIAS = 0.70   # protected items are harder to evict, but not immune

GHOST_LIMIT_MIN = 1024        # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0      # ghost capacity  factor * live cache item count

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    global g_epoch, g_next_epoch_at
    if now >= g_next_epoch_at:
        steps = max(1, (now - g_next_epoch_at) // EPOCH_LENGTH + 1)
        g_epoch += steps
        g_next_epoch_at += steps * EPOCH_LENGTH

def _decayed_freq(key):
    raw = m_freq.get(key, 0)
    ke = m_f_epoch.get(key, g_epoch)
    d = g_epoch - ke
    if d <= 0:
        return raw
    shift = DECAY_SHIFT_PER_EPOCH * d
    return raw >> shift

def _bump_freq_on_hit(key):
    ke = m_f_epoch.get(key, g_epoch)
    if ke != g_epoch:
        d = g_epoch - ke
        if d > 0:
            shift = DECAY_SHIFT_PER_EPOCH * d
            m_freq[key] = m_freq.get(key, 0) >> shift
        m_f_epoch[key] = g_epoch
    m_freq[key] = m_freq.get(key, 0) + 1

def _predicted_slack(key, now):
    """
    Return predicted slack (next_t - now) and mu (for grace logic).
    If missing predictor, return large slack and mu.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return (_INF, 1.0)
    next_t = la + mu
    return (float(next_t - now), max(1.0, float(mu)))

def _time_badness_from_slack(slack, mu):
    """
    Map slack to an eviction 'badness' (higher -> evict sooner).
    - If slack >= 0 (next access in future): badness grows with slack.
    - If overdue (slack < 0): small grace keeps badness ~ 0; beyond grace it grows.
    """
    if slack >= 0:
        return slack
    overdue = -slack
    grace = LATE_GRACE_FRAC * mu
    if overdue <= grace:
        return 0.0
    return LATE_PENALTY * (overdue - grace)

def _stage_of_key(key):
    return m_stage.get(key, 0)

def _inc_probation_bytes(sz):
    global bytes_probation
    bytes_probation += max(0, int(sz))

def _move_probation_to_protected(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0
    bytes_protected += s

def _dec_probation_bytes(sz):
    global bytes_probation
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0

def _dec_protected_bytes(sz):
    global bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = 0

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - Start in probation (stage=0).
    - Initialize frequency with epoch tagging (so it will age).
    """
    global m_global_mu
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        ghf = max(0, int(g_freq.get(key, 1)))
        m_freq[key] = ghf
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0  # will be incremented on hit
    m_f_epoch[key] = g_epoch
    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment decayed frequency.
    """
    global m_mu, m_last_access, m_global_mu
    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now
    _bump_freq_on_hit(key)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = min(255, max(0, int(m_freq.get(evicted_key, 0))))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _protected_target_bytes(cache_snapshot):
    """
    Compute current target bytes for the protected segment based on EMA of hit source.
    target_frac = clamp(0.2..0.85, 0.2 + 0.6*ema)
    """
    frac = 0.2 + 0.6 * float(protected_hit_share_ema)
    frac = min(PROTECTED_MAX_FRAC, max(PROTECTED_MIN_FRAC, frac))
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    return int(frac * cap)

def _eviction_score(key, obj, now):
    """
    Compute eviction badness score (higher => evict sooner), stage-biased and size/freq aware.
    """
    slack, mu = _predicted_slack(key, now)
    tbad = _time_badness_from_slack(slack, mu)

    sz = _size_of(obj)
    f_eff = max(0, _decayed_freq(key))
    denom_freq = 1.0 + FREQ_DAMP * math.log1p(max(0, f_eff))
    size_term = (sz ** SIZE_EXP)

    base = tbad / (size_term * denom_freq)

    # Modest boost to items that have not demonstrated reuse yet
    raw_freq = m_freq.get(key, 0)
    if raw_freq <= 1:
        base *= NEW_ITEM_BONUS

    # Stage bias
    st = _stage_of_key(key)
    if st == 1:
        base *= PROTECTED_STAGE_BIAS
    else:
        base *= PROBATION_STAGE_BIAS

    return float(base)

def _estimate_incoming_score(obj, now):
    """
    Estimate eviction score for the incoming object (as if it were resident in probation).
    Used as an admission-aware threshold; we prefer evicting items with score >= this.
    """
    key = obj.key
    sz = _size_of(obj)

    # Estimate mu and freq from ghost, else from global baseline
    if key in g_mu:
        mu = max(1.0, 0.9 * float(g_mu.get(key, 1.0)))
        ghf = max(0, int(g_freq.get(key, 1)))
    else:
        mu = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        ghf = 0

    # Treat "slack" as mu (we just inserted; next in ~mu)
    slack = mu
    tbad = _time_badness_from_slack(slack, mu)

    denom_freq = 1.0 + FREQ_DAMP * math.log1p(max(0, ghf))
    size_term = (sz ** SIZE_EXP)
    base = (tbad / (size_term * denom_freq)) * NEW_ITEM_BONUS * PROBATION_STAGE_BIAS
    return float(base)

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Choose a victim with adaptive segment budgeting and admission-aware thresholding.
    Steps:
      - Advance frequency epoch if needed.
      - Compute target protected bytes from EMA of hit source.
      - Scan cache once, tracking the worst in probation and the worst in protected.
      - Prefer evicting from any segment that exceeds its byte budget.
      - Otherwise pick the globally worse score, with admission-aware threshold: try to evict
        an item whose score >= incoming_score; if none, evict the global worst.
    Tie-breakers within a pool:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # Admission-aware threshold from incoming object
    incoming_score = _estimate_incoming_score(obj, now) if obj is not None else -1.0

    # Track worst in each segment
    worst_prob_k = None
    worst_prob_s = -1.0
    worst_prob_la = None
    worst_prob_sz = None

    worst_prot_k = None
    worst_prot_s = -1.0
    worst_prot_la = None
    worst_prot_sz = None

    for k, v in cache.items():
        st = _stage_of_key(k)
        s = _eviction_score(k, v, now)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if st == 0:
            if (worst_prob_k is None or s > worst_prob_s or
               (s == worst_prob_s and (la < worst_prob_la or (la == worst_prob_la and (sz > worst_prob_sz or (sz == worst_prob_sz and k < worst_prob_k)))))):
                worst_prob_k, worst_prob_s, worst_prob_la, worst_prob_sz = k, s, la, sz
        else:
            if (worst_prot_k is None or s > worst_prot_s or
               (s == worst_prot_s and (la < worst_prot_la or (la == worst_prot_la and (sz > worst_prot_sz or (sz == worst_prot_sz and k < worst_prot_k)))))):
                worst_prot_k, worst_prot_s, worst_prot_la, worst_prot_sz = k, s, la, sz

    # If only one segment has items
    if worst_prob_k is None and worst_prot_k is None:
        return None
    if worst_prob_k is None:
        return worst_prot_k
    if worst_prot_k is None:
        return worst_prob_k

    # Adaptive budgeting: prefer evicting from segment exceeding its target share
    target_prot_bytes = _protected_target_bytes(cache_snapshot)
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    target_prob_bytes = max(0, cap - target_prot_bytes)

    # Bring segments back under their byte budgets if they exceed them
    if bytes_protected > target_prot_bytes:
        # Protected too large: evict from protected
        return worst_prot_k
    if bytes_probation > target_prob_bytes:
        # Probation too large: evict from probation
        return worst_prob_k

    # Near-balanced: choose based on scores, with admission-aware threshold
    # Try to evict item whose score >= incoming score; else fallback to global worst
    p_ok = worst_prob_s >= incoming_score
    r_ok = worst_prot_s >= incoming_score

    if p_ok and not r_ok:
        return worst_prob_k
    if r_ok and not p_ok:
        return worst_prot_k

    # If both pass (or both fail), choose the worse score
    if worst_prob_s > worst_prot_s:
        return worst_prob_k
    if worst_prot_s > worst_prob_s:
        return worst_prot_k

    # Exact tie: break by older last access, then larger size, then key
    if worst_prob_la != worst_prot_la:
        return worst_prob_k if worst_prob_la < worst_prot_la else worst_prot_k
    if worst_prob_sz != worst_prot_sz:
        return worst_prob_k if worst_prob_sz > worst_prot_sz else worst_prot_k
    return worst_prob_k if worst_prob_k < worst_prot_k else worst_prot_k


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch and update per-key EWMA predictor, global mu, and decayed frequency.
    - Update adaptive protected share EMA from the segment the hit occurred in (pre-promotion).
    - Promote from probation to protected on first hit (SLRU by bytes), updating byte counters.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # Cold hit edge case: seed if missing
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Keep stage=0 by default; promotion below will adjust as needed.

    # Record hit source before promotion for EMA
    st_before = m_stage.get(key, 0)

    _update_predictor_on_hit(key, now)

    # Adaptive hit-source EMA
    global protected_hit_share_ema
    hit_is_protected = 1.0 if st_before == 1 else 0.0
    protected_hit_share_ema = (1.0 - EMA_ALPHA) * protected_hit_share_ema + EMA_ALPHA * hit_is_protected

    # SLRU promotion on first hit
    if st_before == 0:
        m_stage[key] = 1
        _move_probation_to_protected(_size_of(obj))


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor and frequency (ghost warm-start if available).
    - Start in probation (stage=0).
    - Update last access time and probation byte counters.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _seed_predictor_on_insert(key, now)
    _inc_probation_bytes(_size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Adjust segment byte counters based on last known stage.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust segment bytes using last known stage
    st = m_stage.get(ekey, 0)
    if st == 0:
        _dec_probation_bytes(_size_of(evicted_obj))
    else:
        _dec_protected_bytes(_size_of(evicted_obj))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_f_epoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdwwhrrud.pickle

Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjplai7j3.pickle

Iteration 98: New subsample score 0.564413 is not better than old score 0.5705020000000001, skipping
Iteration 99: Selected program 30 score: 0.23011145833333335
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp2apo6t8c.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpczxt664v.pickle

Iteration 99: Proposed new text for program: # Fast Adaptive Segmented Predictive (FASP) Eviction
# Key goals:
# - Avoid timeouts: O(log n) updates, O(k log n) eviction with small k via heaps and lazy deletion.
# - Strong hit rate: hybrid of SLRU (recency) + simple reuse predictor (EWMA) + size-aware eviction.
# - ARC-style adaptation using a bounded O(1) ghost implemented with a deque (no costly scans).
#
# Design summary:
# - Two segments (probation T1, protected T2). Items enter T1; first hit promotes to T2.
# - Maintain per-segment min-heaps on recency stamp (LRU at heap top). Use lazy deletion for stale heap entries.
# - Eviction samples a small number (k) of the oldest valid candidates from both segments and picks the maximum
#   size-aware predictive score with ARC-like bias toward the preferred segment.
# - Size-aware scoring: score = predicted_delta / freq_damp * size^gamma,
#   where gamma increases with incoming object size relative to capacity.
# - Predictive reuse: per-key EWMA of inter-arrival time; global EWMA as fallback.
# - ARC adaptation: adjust target protected fraction based on ghost segment of reinserted key.
# - Bounded ghost: O(1) push/pop using deque; store segment of eviction to drive adaptation. No O(G) scans.

import math
import heapq
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()            # key -> int

# Recency stamp for LRU and heaps
m_stamp = dict()          # key -> int (last access/insertion time)

# Reuse predictor
m_last_access = dict()    # key -> int
m_mu = dict()             # key -> float (EWMA inter-arrival)
m_hits = dict()           # key -> int (hit count)

# Global EWMA baseline for inter-arrival
m_global_mu = 64.0

# ARC-style target protected fraction
m_protect_target_frac = 0.50

# Counters to avoid scanning cache
m_count_T1 = 0
m_count_T2 = 0

# Per-segment heaps: (stamp, seq, key). Lazy deletion via comparing stamp and segment.
heap_T1 = []
heap_T2 = []
_seq = 0  # tie-breaker sequence for heap entries

# Bounded ghost history for ARC-like adaptation
GHOST_MAX = 10000
g_deque = deque()         # order of ghost keys (FIFO)
g_seg = dict()            # key -> segment at eviction (0/1)

# ----------------------
# Tunable hyperparameters
# ----------------------
# Frequency dampening (larger reduces weight of hits)
FREQ_DAMP_LOG = True       # use log1p(hits) if True, else power
FREQ_POW = 0.6             # if FREQ_DAMP_LOG == False, divisor = (1+hits)^FREQ_POW

# Size-awareness
SIZE_GAMMA_BASE = 0.80     # base exponent for size in eviction score
SIZE_GAMMA_PRESSURE = 0.60 # extra size bias under space pressure ~ size(new)/capacity

# Segment selection bias (ARC-like)
NONPREF_BIAS = 2.0         # reduce eviction score for non-preferred segment by this factor
ARC_ADAPT_STEP = 0.06      # how fast the protected target fraction adapts
PROTECT_FRAC_MIN = 0.10
PROTECT_FRAC_MAX = 0.90
START_PROTECTED_FROM_GHOST = True

# EWMA for reuse predictor
EWMA_BETA = 0.20
GLOBAL_BETA = 0.02

# Defaults
DEFAULT_MU_MULT = 2.5

# Eviction sampling
CAND_K_PREF = 8            # candidates from preferred segment
CAND_K_NONPREF = 4         # candidates from non-preferred segment

# Numerics
_INF = 1e30


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _dynamic_size_gamma(cache_snapshot, incoming_obj):
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    new_sz = float(_size_of(incoming_obj))
    pressure = min(1.0, new_sz / float(cap))
    return SIZE_GAMMA_BASE + SIZE_GAMMA_PRESSURE * pressure

def _predicted_delta(key, now):
    mu = m_mu.get(key)
    la = m_last_access.get(key)
    if mu is None or la is None:
        return DEFAULT_MU_MULT * float(m_global_mu)
    return max(0.0, float(la + mu - now))

def _freq_divisor(hits):
    if FREQ_DAMP_LOG:
        return 1.0 + math.log1p(max(0, hits))
    return (1.0 + float(max(0, hits))) ** FREQ_POW

def _preferred_segment():
    total = m_count_T1 + m_count_T2
    if total <= 0:
        return 0
    target_prot = int(round(float(m_protect_target_frac) * float(total)))
    # If protected segment is over the target, prefer evicting from protected
    return 1 if m_count_T2 > target_prot else 0

def _push_heap(seg, key, stamp):
    global _seq
    _seq += 1
    entry = (int(stamp), _seq, key)
    if seg == 0:
        heapq.heappush(heap_T1, entry)
    else:
        heapq.heappush(heap_T2, entry)

def _validate_heap_entry(entry, seg_expected):
    stamp, _, key = entry
    # key must be live, in expected segment, and stamp must match current
    return (key in m_seg and
            m_seg.get(key, -1) == seg_expected and
            m_stamp.get(key, -2) == stamp)

def _pop_valid_candidates(heap, seg_id, max_k, cache_obj_map):
    # Pop up to max_k valid candidates (oldest first). Re-push non-chosen later.
    out = []
    popped = []
    while heap and len(out) < max_k:
        entry = heapq.heappop(heap)
        popped.append(entry)
        if _validate_heap_entry(entry, seg_id):
            # Ensure key still exists in cache
            key = entry[2]
            if key in cache_obj_map:
                out.append(entry)
    # We'll re-push all popped entries to keep heap complete; the evictor
    # will skip re-pushing the chosen victim's entry.
    return out, popped

def _compute_score_for_key(k, v, now, preferred_seg, gamma):
    pd = _predicted_delta(k, now)
    hits = m_hits.get(k, 0)
    base = pd / max(1e-9, _freq_divisor(hits))
    sz = _size_of(v)
    base *= (float(sz) ** gamma)
    seg = m_seg.get(k, 0)
    if seg != preferred_seg:
        base /= NONPREF_BIAS
    stamp = m_stamp.get(k, -1)
    return base, seg, stamp, sz

def _record_ghost(ekey, seg):
    # O(1) bounded ghost with deque
    if ekey in g_seg:
        # Move to back to reflect recency
        try:
            # Remove existing occurrence in deque
            # This is O(n); to keep O(1), we simply leave it and update mapping,
            # but that can grow duplicates. Instead, we avoid duplicates by not re-adding
            # a key already in ghost.
            pass
        except Exception:
            pass
        # Do nothing: keep the older entry since we already have mapping.
    else:
        g_seg[ekey] = int(seg)
        g_deque.append(ekey)
        if len(g_deque) > GHOST_MAX:
            old = g_deque.popleft()
            g_seg.pop(old, None)

def _adapt_on_insert_from_ghost(key):
    global m_protect_target_frac
    seg = g_seg.get(key)
    if seg is None:
        return
    if seg == 0:
        m_protect_target_frac = max(PROTECT_FRAC_MIN, m_protect_target_frac - ARC_ADAPT_STEP)
    else:
        m_protect_target_frac = min(PROTECT_FRAC_MAX, m_protect_target_frac + ARC_ADAPT_STEP)


# ----------------------
# Predictor updates
# ----------------------
def _seed_on_insert(key, now):
    """
    Initialize per-key metadata on insert, optionally starting in protected if
    the key is in the ghost (ARC-style warm start).
    """
    global m_count_T1, m_count_T2

    start_seg = 1 if (START_PROTECTED_FROM_GHOST and key in g_seg and g_seg.get(key, 0) == 1) else 0

    m_seg[key] = start_seg
    m_stamp[key] = now
    m_last_access[key] = now
    # Cold-start mu with global baseline
    m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_hits[key] = 0

    if start_seg == 0:
        m_count_T1 += 1
    else:
        m_count_T2 += 1

    _push_heap(start_seg, key, now)


def _update_predictor_on_hit(key, now):
    # Update per-key/global EWMA of inter-arrival
    last = m_last_access.get(key)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        global_prev = float(m_global_mu)
        globals()['m_global_mu'] = (1.0 - GLOBAL_BETA) * global_prev + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_last_access[key] = now
    # Increment hits
    m_hits[key] = m_hits.get(key, 0) + 1


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Select a victim by sampling a small number of oldest candidates from each segment heap
    and choosing the one with the highest size-aware predictive score. Bias toward the
    preferred segment determined by adaptive target of protected fraction.
    """
    cache_map = cache_snapshot.cache
    if not cache_map:
        return None

    now = _now(cache_snapshot)
    preferred = _preferred_segment()
    gamma = _dynamic_size_gamma(cache_snapshot, obj)

    # Collect candidates
    cand_pref, popped_pref = _pop_valid_candidates(
        heap_T2 if preferred == 1 else heap_T1,
        preferred,
        CAND_K_PREF,
        cache_map
    )
    nonpref = 0 if preferred == 1 else 1
    cand_nonpref, popped_nonpref = _pop_valid_candidates(
        heap_T2 if nonpref == 1 else heap_T1,
        nonpref,
        CAND_K_NONPREF,
        cache_map
    )

    candidates = cand_pref + cand_nonpref

    # If no candidates (rare, due to stale entries), do a minimal fallback:
    if not candidates:
        # Fallback: scan a few keys to pick oldest LRU as victim quickly
        victim_key = None
        victim_stamp = None
        for i, (k, v) in enumerate(cache_map.items()):
            if k not in m_stamp:
                # Seed missing metadata lazily
                m_seg[k] = 0
                m_stamp[k] = now
                m_last_access[k] = now
                m_mu[k] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
                m_hits[k] = 0
                _push_heap(0, k, now)
            st = m_stamp.get(k, now)
            if victim_key is None or st < victim_stamp:
                victim_key = k
                victim_stamp = st
            if i >= 32:
                break
        # Re-push popped entries (if any)
        for e in popped_pref:
            heap = heap_T2 if preferred == 1 else heap_T1
            heapq.heappush(heap, e)
        for e in popped_nonpref:
            heap = heap_T2 if nonpref == 1 else heap_T1
            heapq.heappush(heap, e)
        return victim_key

    # Evaluate scores
    best_key = None
    best_score = None
    best_seg = None
    best_stamp = None
    best_sz = None

    for (stamp, _, k) in candidates:
        v = cache_map.get(k)
        if v is None:
            continue
        score, seg, st, sz = _compute_score_for_key(k, v, now, preferred, gamma)
        if best_key is None or score > best_score:
            best_key, best_score = k, score
            best_seg, best_stamp, best_sz = seg, st, sz
        elif score == best_score:
            # Tie-breakers: prefer evicting from preferred segment, then older stamp, then larger size, then key order
            if best_seg != preferred and seg == preferred:
                best_key, best_score = k, score
                best_seg, best_stamp, best_sz = seg, st, sz
            elif seg == best_seg:
                if st < best_stamp or (st == best_stamp and (sz > best_sz or (sz == best_sz and k < best_key))):
                    best_key, best_score = k, score
                    best_seg, best_stamp, best_sz = seg, st, sz

    # Re-push all popped entries except the chosen victim's tuple
    for e in popped_pref:
        if e[2] != best_key:
            heap = heap_T2 if preferred == 1 else heap_T1
            heapq.heappush(heap, e)
    for e in popped_nonpref:
        if e[2] != best_key:
            heap = heap_T2 if nonpref == 1 else heap_T1
            heapq.heappush(heap, e)

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Ensure metadata exists (in case of cold inconsistencies).
      - Update EWMA predictor and global baseline.
      - Update recency stamp and push to appropriate segment heap.
      - Promote to protected on first hit from probation.
    """
    key = obj.key
    now = _now(cache_snapshot)

    # Seed if missing
    if key not in m_seg or key not in m_stamp:
        _seed_on_insert(key, now)

    _update_predictor_on_hit(key, now)

    # Update recency and heap
    m_stamp[key] = now
    seg = m_seg.get(key, 0)

    # Promote to protected on first hit if currently probation
    if seg == 0:
        m_seg[key] = 1
        # Update counts
        global m_count_T1, m_count_T2
        m_count_T1 = max(0, m_count_T1 - 1)
        m_count_T2 += 1
        seg = 1

    _push_heap(seg, key, now)


def update_after_insert(cache_snapshot, obj):
    """
    On insert:
      - ARC-like adaptation based on ghost membership.
      - Initialize per-key metadata; ghost hits can start in protected.
    """
    key = obj.key
    now = _now(cache_snapshot)

    _adapt_on_insert_from_ghost(key)

    # Initialize metadata and placement
    _seed_on_insert(key, now)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Record ghost segment for adaptation (bounded O(1) structure).
      - Remove live metadata and adjust counters.
    """
    ekey = evicted_obj.key

    # Record ghost with segment at eviction
    ev_seg = m_seg.get(ekey, 0)
    _record_ghost(ekey, ev_seg)

    # Clean live metadata and counters
    if m_seg.get(ekey, 0) == 0:
        globals()['m_count_T1'] = max(0, m_count_T1 - 1)
    else:
        globals()['m_count_T2'] = max(0, m_count_T2 - 1)

    m_seg.pop(ekey, None)
    m_stamp.pop(ekey, None)
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_hits.pop(ekey, None)
Killing subprocess...
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe1_2p2uy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp5tjjkg8x.pickle

Iteration 99: New subsample score 0.222301 is not better than old score 0.9596089999999999, skipping
Iteration 100: Selected program 18 score: 0.22947741666666666
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbjpvcqj1.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpx0mqh462.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmi2nef0v.pickle

Iteration 100: Proposed new text for program: # Byte-aware GreedyDual-Size with Segmented Protection and ARC-style Adaptation (bGDS-SP-ARC)
# Key improvements vs. previous version:
# 1) Byte-aware segmenting/eviction: choose segment to evict by bytes, not item counts.
# 2) Stronger size-awareness: credits are normalized by size^ALPHA_SIZE (ALPHA_SIZE ~ 0.9).
# 3) Scan resistance: low insert credit; higher credit only after hits; large objects require an extra hit before protection.
# 4) ARC-style adaptation kept but tuned, ghosts trimmed amortized O(1).
# 5) Lightweight metadata; O(n) victim scan per eviction with simple tie-breakers.

# ----------------------
# Global metadata
# ----------------------
# GDS priorities
m_h = dict()           # key -> float (priority)
GDS_L = 0.0            # global aging value

# Segments: 0 = probation (T1), 1 = protected (T2)
m_seg = dict()         # key -> int

# Lightweight stats
m_last_access = dict() # key -> int (for LRU-ish tie-breaks)
m_hits = dict()        # key -> int (hit count since (re)insert)
m_size = dict()        # key -> int (cached size at last touch/insert)

# Ghost histories for ARC-style adaptation
b1_set = dict()        # key -> last time seen (probation ghosts)
b2_set = dict()        # key -> last time seen (protected ghosts)
b1_q = []              # append-only queue of keys in B1 insertion order
b2_q = []              # append-only queue of keys in B2 insertion order
b1_head = 0            # pop pointer for B1
b2_head = 0            # pop pointer for B2

# Target protected fraction (ARC-style). Adapted on ghost hits.
p_ratio = 0.60         # in [0, 1], initial bias to protected

# ----------------------
# Tunable hyperparameters
# ----------------------
# Size-aware normalization exponent (higher => stronger penalty for large objects)
ALPHA_SIZE = 0.9

# Credits
CREDIT_INSERT = 0.05       # very small: scans don't get large initial priority
CREDIT_HIT = 1.4           # significant boost on hit (size-normalized)
CREDIT_GHOST_B1 = 1.3      # boost if returning from probation ghost
CREDIT_GHOST_B2 = 2.5      # stronger boost if returning from protected ghost

# ARC-style adaptation step on ghost hit
ADAPT_STEP = 0.08
P_MIN = 0.05
P_MAX = 0.90

# Ghost sizing (amortized O(1) trimming)
GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 4.0   # total ghost entries ~ 4x live item count

# Promotion thresholds based on object size (bytes fraction of capacity)
PROMOTE_HITS_SMALL = 1     # required hits to move from probation->protected if small
PROMOTE_HITS_LARGE = 2     # if large

# Numerics
_EPS = 1e-9


# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _cap(cache_snapshot):
    return max(1, int(getattr(cache_snapshot, "capacity", 1)))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _credit_for_size(sz, base_credit):
    return float(base_credit) / (float(sz) ** ALPHA_SIZE)

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _ghost_budgets(cache_snapshot):
    total = _ghost_capacity_limit(cache_snapshot)
    b2_budget = int(round(p_ratio * total))
    b2_budget = min(max(1, b2_budget), total - 1)
    b1_budget = total - b2_budget
    return b1_budget, b2_budget

def _trim_ghost(cache_snapshot):
    # Trim B1 and B2 to their budgets using amortized O(1) pop from append-only queues.
    global b1_head, b2_head
    b1_budget, b2_budget = _ghost_budgets(cache_snapshot)

    # B1 trimming
    while len(b1_set) > b1_budget:
        while b1_head < len(b1_q) and b1_q[b1_head] not in b1_set:
            b1_head += 1
        if b1_head >= len(b1_q):
            break
        k = b1_q[b1_head]
        b1_head += 1
        b1_set.pop(k, None)

    # B2 trimming
    while len(b2_set) > b2_budget:
        while b2_head < len(b2_q) and b2_q[b2_head] not in b2_set:
            b2_head += 1
        if b2_head >= len(b2_q):
            break
        k = b2_q[b2_head]
        b2_head += 1
        b2_set.pop(k, None)

def _record_ghost_on_evict(evicted_key, now, was_protected):
    if was_protected:
        b2_set[evicted_key] = now
        b2_q.append(evicted_key)
    else:
        b1_set[evicted_key] = now
        b1_q.append(evicted_key)

def _maybe_adapt_on_insert(key):
    global p_ratio
    # ARC-style adaptation:
    # B1 hit (recency) => favor probation (decrease p_ratio)
    # B2 hit (frequency) => favor protected (increase p_ratio)
    if key in b1_set and key not in b2_set:
        p_ratio = max(P_MIN, p_ratio - ADAPT_STEP)
    elif key in b2_set:
        p_ratio = min(P_MAX, p_ratio + ADAPT_STEP)

def _init_if_missing_on_live(key, now, obj):
    if key not in m_h:
        m_h[key] = float(GDS_L)
    if key not in m_seg:
        m_seg[key] = 0
    if key not in m_last_access:
        m_last_access[key] = now
    if key not in m_hits:
        m_hits[key] = 0
    m_size[key] = _size_of(obj)

def _promotion_hits_required(cache_snapshot, sz):
    # Require more evidence (2 hits) to protect very large objects.
    cap = _cap(cache_snapshot)
    # Thresholds: small < 1% cap; large >= 5% cap
    small_thr = max(1, int(0.01 * cap))
    large_thr = max(1, int(0.05 * cap))
    if sz >= large_thr:
        return PROMOTE_HITS_LARGE
    elif sz <= small_thr:
        return PROMOTE_HITS_SMALL
    else:
        return PROMOTE_HITS_SMALL  # middle class: still 1 hit


# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict by segment that exceeds its target BYTES:
      - target protected bytes = round(p_ratio * total_bytes)
      - if protected_bytes > target => evict from protected; else from probation (if any)
    Within a segment, choose the item with the lowest GDS priority h.
    Tie-breakers:
      - older last access (LRU-ish)
      - larger size (frees more space)
      - lexicographic key
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    total_bytes = int(getattr(cache_snapshot, "size", 0))
    prot_bytes = 0

    best0_key = None  # probation
    best0_val = None
    best0_h = None
    best0_la = None
    best0_sz = None

    best1_key = None  # protected
    best1_val = None
    best1_h = None
    best1_la = None
    best1_sz = None

    now = _now(cache_snapshot)

    for k, v in cache.items():
        seg = m_seg.get(k, 0)
        h = m_h.get(k, GDS_L)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if seg == 1:
            prot_bytes += sz

        if seg == 0:
            if best0_key is None:
                best0_key, best0_val, best0_h, best0_la, best0_sz = k, v, h, la, sz
            else:
                better = False
                if h < best0_h - _EPS:
                    better = True
                elif abs(h - best0_h) <= _EPS:
                    if la < best0_la:
                        better = True
                    elif la == best0_la:
                        if sz > best0_sz:
                            better = True
                        elif sz == best0_sz and k < best0_key:
                            better = True
                if better:
                    best0_key, best0_val, best0_h, best0_la, best0_sz = k, v, h, la, sz
        else:
            if best1_key is None:
                best1_key, best1_val, best1_h, best1_la, best1_sz = k, v, h, la, sz
            else:
                better = False
                if h < best1_h - _EPS:
                    better = True
                elif abs(h - best1_h) <= _EPS:
                    if la < best1_la:
                        better = True
                    elif la == best1_la:
                        if sz > best1_sz:
                            better = True
                        elif sz == best1_sz and k < best1_key:
                            better = True
                if better:
                    best1_key, best1_val, best1_h, best1_la, best1_sz = k, v, h, la, sz

    target_prot_bytes = int(round(p_ratio * max(1, total_bytes)))

    # Decide which segment to evict from (byte-aware)
    if prot_bytes > target_prot_bytes and best1_key is not None:
        return best1_key
    elif best0_key is not None:
        return best0_key
    elif best1_key is not None:
        return best1_key
    else:
        return None


def update_after_hit(cache_snapshot, obj):
    """
    On hit:
      - Update last access, size snapshot and hit count.
      - Promote to protected if probation and enough evidence (size-aware threshold).
      - Increase priority via size-aware credit: h = max(h, L) + credit(size).
    """
    key = obj.key
    now = _now(cache_snapshot)
    _init_if_missing_on_live(key, now, obj)

    # Update stats
    m_last_access[key] = now
    m_size[key] = _size_of(obj)

    # Decide promotion: require more hits for large objects
    sz = m_size[key]
    required_hits = _promotion_hits_required(cache_snapshot, sz)
    prev_hits = m_hits.get(key, 0)
    m_hits[key] = prev_hits + 1

    if m_seg.get(key, 0) == 0 and (prev_hits + 1) >= required_hits:
        m_seg[key] = 1  # promote to protected

    # Update GDS priority (size-normalized)
    incr = _credit_for_size(sz, CREDIT_HIT)
    cur = m_h.get(key, GDS_L)
    if cur < GDS_L:
        cur = GDS_L
    m_h[key] = cur + incr


def update_after_insert(cache_snapshot, obj):
    """
    On insert (post-admission by the framework):
      - Adjust ARC target p_ratio if key is in ghosts (B1/B2).
      - Initialize stats.
      - Start in protected only if returning from B2 (frequent); otherwise probation.
      - Initialize GDS priority with global age plus size-aware credit (tiny for cold, boosted if from ghosts).
      - Remove the key from ghosts to prevent duplicate ghost entries.
    """
    key = obj.key
    now = _now(cache_snapshot)
    sz = _size_of(obj)

    # ARC-style target adaptation
    _maybe_adapt_on_insert(key)

    from_b2 = key in b2_set
    from_b1 = key in b1_set

    # Remove from ghosts
    if from_b1:
        b1_set.pop(key, None)
    if from_b2:
        b2_set.pop(key, None)

    # Initialize stats
    m_last_access[key] = now
    m_hits[key] = 0
    m_size[key] = sz

    # Segment on insert
    if from_b2:
        m_seg[key] = 1   # frequent return -> protect immediately
    else:
        m_seg[key] = 0   # default probation

    # Initialize GDS priority
    base_credit = CREDIT_INSERT
    if from_b2:
        base_credit *= CREDIT_GHOST_B2
    elif from_b1:
        base_credit *= CREDIT_GHOST_B1

    m_h[key] = max(float(GDS_L), m_h.get(key, float(GDS_L))) + _credit_for_size(sz, base_credit)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
      - Update global age L to the evicted item's priority (GDS aging).
      - Record the evicted key in the appropriate ghost (B1 for probation, B2 for protected).
      - Trim ghosts to budget (amortized O(1)).
      - Cleanup live metadata of the evicted key.
    """
    global GDS_L
    ekey = evicted_obj.key
    now = _now(cache_snapshot)

    # Segment at eviction
    was_protected = (m_seg.get(ekey, 0) == 1)

    # Update GDS global age to victim's priority
    ev_h = m_h.get(ekey, float(GDS_L))
    GDS_L = float(ev_h)

    # Record in ghost history and trim
    _record_ghost_on_evict(ekey, now, was_protected)
    _trim_ghost(cache_snapshot)

    # Cleanup metadata
    m_last_access.pop(ekey, None)
    m_hits.pop(ekey, None)
    m_h.pop(ekey, None)
    m_seg.pop(ekey, None)
    m_size.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp0watbwy0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpuk5yglno.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp8cn1l00t.pickle

Iteration 100: New subsample score 0.43007300000000004 is not better than old score 0.613638, skipping
Iteration 101: Selected program 29 score: 0.25926008333333345
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpepo_kcj_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpq7otqjtd.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpildd7xhb.pickle

Iteration 101: Proposed new text for program: # ARC-inspired Size-aware SLRU with predictive recency, decayed frequency, and adaptive protected quota
#
# What changed vs. the previous version:
# - Adaptive protected/probation balance (ARC-like): keep a target probation size (bytes) that
#   moves up on B1 ghost hits (recency-heavy) and moves down on B2 ghost hits (frequency-heavy).
#   Eviction compares the best candidate in probation and protected with a dynamic bias, and can
#   evict from protected even when probation exists if protected candidates are clearly worse or
#   when probation is under its target.
# - Safer treatment of newly inserted/just-hit items via a short "young guard": extremely recent
#   accesses get a reduced eviction score to avoid ping-pong eviction on scans with short reuse.
# - Less aggressive cold-start mu: default inter-arrival estimate uses ~1.0x global mu (was 3x),
#   which helps keep recent items long enough to observe reuse.
# - Size-aware promotion: large objects require more evidence (hits) before moving to protected;
#   promotion threshold scales with obj.size/capacity to reduce cache pollution by large objects.
# - ARC-like ghost tracking with stage tagging (B1/B2). On inserting an item found in either ghost,
#   adapt the probation target and admit it directly into protected (like ARCs move-to-T2).
# - Improved protected pressure: if protected is bloated or has highly overdue entries, eviction
#   can select protected victim even when probation exists.
# - Slightly stronger overdue penalty and size exponent; slightly weaker bias against low-frequency
#   items to improve recency sensitivity on web-like traces.
#
# Complexity:
# - Eviction scans all live items once to find best probation/protected candidates (O(n)).
# - Updates are O(1).
# - Ghost trimming is O(1) amortized; also bounds the ghost order deque.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_freq = dict()           # key -> int (raw hit count, aged by epoch lazily)
m_f_epoch = dict()        # key -> int (epoch of last freq normalization)
m_stage = dict()          # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Segment byte counters (best-effort; guarded against drift)
bytes_probation = 0
bytes_protected = 0

# Ghost history (bounded by count, O(1) trim); ARC-like with stage tag
g_mu = dict()             # key -> float (last known EWMA)
g_freq = dict()           # key -> int (historical hits)
g_stage = dict()          # key -> int (0 from probation/T1 => B1, 1 from protected/T2 => B2)
g_stamp = dict()          # key -> int (version counter)
g_order = deque()         # deque of (key, stamp) in LRU order of eviction

# Decayed frequency global epoch (TinyLFU-like aging)
g_epoch = 0
g_next_epoch_at = 4096
EPOCH_LENGTH = 4096
DECAY_SHIFT_PER_EPOCH = 1  # halve per epoch

# Adaptive probation target (bytes). Initialized lazily on first use.
target_probation_bytes = 0

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.30              # per-key EWMA learning rate (faster adaptation)
GLOBAL_BETA = 0.01            # global EWMA learning rate (slow)
SIZE_EXP = 1.15               # size exponent (slight penalty to larger objects)
FREQ_DAMP = 0.6               # dampened influence of frequency via log1p
DEFAULT_MU_MULT = 1.0         # default mu multiplier for cold inserts
OVERDUE_PENALTY = 1.2         # penalty factor for lateness (overdue items evicted sooner)
NEW_ITEM_BONUS = 0.95         # <=1.0: give very new items a small break
STAGE_FACTOR_PROTECTED = 0.7  # discount for protected items when ranking within protected pool

# Young-guard to avoid immediate eviction of recently accessed items
YOUNG_AGE1 = 2
YOUNG_MULT1 = 0.35
YOUNG_AGE2 = 5
YOUNG_MULT2 = 0.7

# Adaptive target step bounds (as bytes; scaled per capacity on use)
PROBATION_ADAPT_STEP_FRAC = 1.0 / 8.0  # step ~ capacity/8 (clamped by obj.size)

GHOST_LIMIT_MIN = 1024
GHOST_LIMIT_FACTOR = 2.0
GHOST_ORDER_FACTOR = 8.0  # bound g_order length to ~8x ghost limit

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    # Trim by entry count
    limit = _ghost_capacity_limit(cache_snapshot)
    # Remove oldest until within limit
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stage.pop(k, None)
            g_stamp.pop(k, None)
    # Also cap order deque length to prevent unbounded growth
    max_order = int(GHOST_ORDER_FACTOR * max(1, limit))
    while len(g_order) > max_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            # If still present and stamp matches, remove; else ignore
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stage.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    global g_epoch, g_next_epoch_at
    if now >= g_next_epoch_at:
        steps = max(1, (now - g_next_epoch_at) // EPOCH_LENGTH + 1)
        g_epoch += steps
        g_next_epoch_at += steps * EPOCH_LENGTH

def _decayed_freq(key):
    raw = m_freq.get(key, 0)
    k_epoch = m_f_epoch.get(key, g_epoch)
    d = g_epoch - k_epoch
    if d <= 0:
        return raw
    shift = DECAY_SHIFT_PER_EPOCH * d
    return raw >> shift

def _bump_freq_on_hit(key):
    ke = m_f_epoch.get(key, g_epoch)
    if ke != g_epoch:
        d = g_epoch - ke
        if d > 0:
            shift = DECAY_SHIFT_PER_EPOCH * d
            m_freq[key] = m_freq.get(key, 0) >> shift
        m_f_epoch[key] = g_epoch
    m_freq[key] = m_freq.get(key, 0) + 1

def _predicted_delta_components(key, now):
    """
    Return (effective_delta, is_overdue)
    - If predictor exists: use mu and last_access to compute time until predicted next access.
      If overdue: effective_delta grows with lateness (OVERDUE_PENALTY * overdue).
    - If predictor missing: treat as very far.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return (_INF, False)
    next_t = la + mu
    if now <= next_t:
        return (float(next_t - now), False)
    overdue = float(now - next_t)
    return (OVERDUE_PENALTY * overdue, True)

def _young_guard_multiplier(age):
    if age <= YOUNG_AGE1:
        return YOUNG_MULT1
    if age <= YOUNG_AGE2:
        return YOUNG_MULT2
    return 1.0

def _eviction_score_within_stage(key, obj, now, is_protected):
    """
    Higher score => evict sooner among items in the same stage.
    score = guard(age) * (effective_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq_adj))) * stage_factor
    - Slightly lower score for very new items (freq <= 1).
    - Protected items get discount when comparing within protected pool.
    """
    sz = _size_of(obj)
    delta, _ = _predicted_delta_components(key, now)

    # Decayed frequency
    f_eff = max(0, _decayed_freq(key))
    denom_freq = 1.0 + FREQ_DAMP * math.log1p(max(0, f_eff))

    base = delta / ((sz ** SIZE_EXP) * denom_freq)

    # Very new items: small break
    if m_freq.get(key, 0) <= 1:
        base *= NEW_ITEM_BONUS

    # Young-guard by age since last access
    la = m_last_access.get(key, None)
    if la is not None:
        age = max(0, now - la)
        base *= _young_guard_multiplier(age)

    if is_protected:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key with stage tag (B1/B2).
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    g_freq[evicted_key] = min(255, max(0, int(m_freq.get(evicted_key, 0))))
    g_stage[evicted_key] = int(m_stage.get(evicted_key, 0))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _stage_of_key(key):
    return m_stage.get(key, 0)

def _inc_probation_bytes(sz):
    global bytes_probation
    bytes_probation += max(0, int(sz))

def _move_probation_to_protected(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0
    bytes_protected += s

def _dec_probation_bytes(sz):
    global bytes_probation
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0

def _dec_protected_bytes(sz):
    global bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = 0

def _init_target_probation_if_needed(cache_snapshot):
    global target_probation_bytes
    if target_probation_bytes <= 0:
        cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
        # Start with 50% capacity to probation (T1), 50% protected (T2)
        target_probation_bytes = cap // 2

def _adjust_target_probation(cache_snapshot, delta_bytes):
    """
    Adapt target probation bytes (ARC-like p adaptation).
    Positive delta grows probation; negative grows protected.
    """
    global target_probation_bytes
    _init_target_probation_if_needed(cache_snapshot)
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    target_probation_bytes = min(cap, max(0, target_probation_bytes + int(delta_bytes)))

def _seed_predictor_on_insert(key, now, mu_hint=None, freq_hint=0, stage=0):
    """
    Initialize per-key metadata on insert:
    - Use hints if provided (ghost warm start).
    - Else, seed from global mu with a conservative multiplier.
    - Initialize frequency with epoch tagging (so it will age).
    - Set stage (0 probation default; 1 protected if ghost hit as in ARC).
    """
    global m_global_mu
    if mu_hint is not None:
        m_mu[key] = max(1.0, float(mu_hint))
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
    m_freq[key] = max(0, int(freq_hint))
    m_f_epoch[key] = g_epoch
    m_last_access[key] = now
    m_stage[key] = 0 if stage not in (0, 1) else stage

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment decayed frequency.
    """
    global m_mu, m_last_access, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    _bump_freq_on_hit(key)

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object.
    Strategy:
      - ARC-inspired: maintain a target probation size (bytes) and allow eviction from protected
        even when probation exists if protected candidates are clearly worse or probation is
        under target. Otherwise prefer probation (scan-resistant).
      - Within each pool, choose the item with the largest eviction score (higher => evict sooner).
      - Ties: older last access first, then larger size, then lexicographic key for stability.
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)
    _init_target_probation_if_needed(cache_snapshot)

    # Find best in probation
    best_p_key = None
    best_p_score = -1.0
    best_p_la = None
    best_p_sz = None
    for k, v in cache.items():
        if _stage_of_key(k) != 0:
            continue
        s = _eviction_score_within_stage(k, v, now, is_protected=False)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_p_key is None or s > best_p_score or \
           (s == best_p_score and (la < best_p_la or (la == best_p_la and (sz > best_p_sz or (sz == best_p_sz and k < best_p_key))))):
            best_p_key, best_p_score, best_p_la, best_p_sz = k, s, la, sz

    # Find best in protected
    best_t_key = None
    best_t_score = -1.0
    best_t_la = None
    best_t_sz = None
    for k, v in cache.items():
        if _stage_of_key(k) != 1:
            continue
        s = _eviction_score_within_stage(k, v, now, is_protected=True)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)
        if best_t_key is None or s > best_t_score or \
           (s == best_t_score and (la < best_t_la or (la == best_t_la and (sz > best_t_sz or (sz == best_t_sz and k < best_t_key))))):
            best_t_key, best_t_score, best_t_la, best_t_sz = k, s, la, sz

    # If no probation items, must evict from protected
    if best_p_key is None:
        return best_t_key
    # If no protected items, evict from probation
    if best_t_key is None:
        return best_p_key

    # Both candidates exist. Decide based on adaptive target and dynamic bias.
    # If probation is above its target, prefer evict from probation.
    global bytes_probation, bytes_protected, target_probation_bytes
    T = max(1, int(target_probation_bytes))
    p_over = bytes_probation - T

    if p_over > 0:
        # Over target => pressure probation
        return best_p_key

    # Under target: allow protected eviction if its candidate is sufficiently worse.
    # Dynamic bias grows preference for probation when probation is far under target.
    fill_ratio = bytes_probation / float(T)  # in [0,1]
    if fill_ratio < 0.5:
        bias = 1.6
    elif fill_ratio < 0.8:
        bias = 1.4
    else:
        bias = 1.2

    if best_t_score > bias * best_p_score:
        return best_t_key
    else:
        return best_p_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch and update per-key EWMA predictor, global mu, and decayed frequency.
    - Size-aware SLRU promotion: promote to protected when sufficient evidence:
        threshold = 1 + floor(size / (capacity * 0.02))  (min 1)
      i.e., larger items require more hits before promotion.
    - Maintain last access and segment byte counters.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)
    _init_target_probation_if_needed(cache_snapshot)

    # Ensure baseline structures exist (cold edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)

    _update_predictor_on_hit(key, now)

    # Size-aware promotion policy
    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    sz = _size_of(obj)
    # Require more hits for larger objects (at least 1)
    base = max(1, int(sz / max(1, int(0.02 * cap))))  # 0.02*cap chunk size
    promote_threshold = 1 + base  # hits required
    current_freq = max(1, m_freq.get(key, 1))  # after _bump_freq_on_hit, freq>=1

    if m_stage.get(key, 0) == 0 and current_freq >= promote_threshold:
        m_stage[key] = 1
        _move_probation_to_protected(sz)


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - ARC-inspired adaptation:
      * On ghost hit in B1 (probation ghost): increase target probation bytes and insert into protected.
      * On ghost hit in B2 (protected ghost): decrease target probation bytes and insert into protected.
    - Default stage is probation if no useful ghost info.
    - Update last access time and segment byte counters accordingly.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)
    _init_target_probation_if_needed(cache_snapshot)

    cap = max(1, int(getattr(cache_snapshot, "capacity", 1)))
    step = max(1, int(cap * PROBATION_ADAPT_STEP_FRAC))
    sz = _size_of(obj)

    # Determine if this key exists in ghost and adjust target accordingly (ARC)
    g_st = g_stage.get(key, None)
    mu_hint = g_mu.get(key, None)
    freq_hint = g_freq.get(key, 0)
    start_stage = 0  # probation by default

    if g_st is not None:
        # Adapt probation target
        if g_st == 0:  # B1 ghost hit -> grow probation target (recency)
            _adjust_target_probation(cache_snapshot, min(sz, step))
        else:          # B2 ghost hit -> shrink probation target (frequency)
            _adjust_target_probation(cache_snapshot, -min(sz, step))
        # On a ghost hit (either B1 or B2), ARC moves it to T2 (protected)
        start_stage = 1

        # Remove from ghost (invalidate)
        g_mu.pop(key, None)
        g_freq.pop(key, None)
        g_stage.pop(key, None)
        g_stamp.pop(key, None)
        # We leave the stale record in g_order; bounded by _trim_ghost.

    _seed_predictor_on_insert(key, now, mu_hint=mu_hint, freq_hint=freq_hint, stage=start_stage)

    if start_stage == 1:
        # Directly protected admission (ARC on ghost hit)
        global bytes_protected
        bytes_protected += sz
    else:
        _inc_probation_bytes(sz)

    _trim_ghost(cache_snapshot)


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key with stage tag and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Adjust segment byte counters.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust segment bytes using last known stage
    st = m_stage.get(ekey, 0)
    if st == 0:
        _dec_probation_bytes(_size_of(evicted_obj))
    else:
        _dec_protected_bytes(_size_of(evicted_obj))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_f_epoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpoidfu226.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqd0yb9pi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp74he4ilm.pickle

Iteration 101: New subsample score 0.42318900000000004 is not better than old score 0.508735, skipping
Best program from optimization: # ReASAP-SLRU-TLFU: Size-aware SLRU with predictive recency and decayed frequency
# Key ideas
# - Segmented LRU (SLRU): entries start in probation; on first hit, promote to protected.
#   Eviction strictly prefers probation (scan resistant); protected is only tapped when needed.
# - Size-aware: operate per-byte via size exponent.
# - Predictive recency: per-key EWMA of inter-arrival time (mu) to estimate next access time.
#   Overdue items are penalized proportional to lateness (fixed from previous "imminent" bug).
# - Decayed frequency (TinyLFU-like): a simple global epoch applies exponential aging to per-key
#   hit counts to prevent stale frequency from dominating.
# - Compact ghost history: retains last mu/freq to warm-start reinserted keys.
#
# All operations are O(number of cached items) for eviction (a single or two passes),
# O(1) for updates, and O(1) amortized for ghost trimming.

import math
from collections import deque

# ----------------------
# Global metadata stores
# ----------------------
m_last_access = dict()    # key -> int (last access time)
m_mu = dict()             # key -> float (EWMA inter-arrival time)
m_freq = dict()           # key -> int (raw hit count, aged by epoch lazily)
m_f_epoch = dict()        # key -> int (epoch of last freq normalization)
m_stage = dict()          # key -> int (0=probation, 1=protected)

# Global EWMA of inter-arrival time across all keys (baseline for cold start)
m_global_mu = 64.0

# Segment byte counters (best-effort; guarded against drift)
bytes_probation = 0
bytes_protected = 0

# Ghost history (bounded by count, O(1) trim)
g_mu = dict()             # key -> float (last known EWMA)
g_freq = dict()           # key -> int (historical hits)
g_stamp = dict()          # key -> int (version counter)
g_order = deque()         # deque of (key, stamp) in LRU order of eviction

# Decayed frequency global epoch
g_epoch = 0
g_next_epoch_at = 4096    # next access_count to advance epoch
EPOCH_LENGTH = 4096       # accesses per epoch
# With DECAY = 0.5 per epoch, we can use integer right-shifts for speed.
DECAY_SHIFT_PER_EPOCH = 1  # 1 => halve per epoch

# ----------------------
# Tunable hyperparameters
# ----------------------
EWMA_BETA = 0.25              # per-key EWMA learning rate
GLOBAL_BETA = 0.005           # global EWMA learning rate (slow)
SIZE_EXP = 1.0                # size exponent (1.0 = per-byte fairness)
FREQ_DAMP = 0.7               # dampened influence of frequency via log1p
DEFAULT_MU_MULT = 3.0         # default mu multiplier for cold inserts (scaled by global mu)
OVERDUE_PENALTY = 0.75        # factor applied to lateness when overdue (higher => evict sooner)
NEW_ITEM_BONUS = 1.35         # multiplier for eviction score of items with freq==1 (more likely to evict)
STAGE_FACTOR_PROTECTED = 0.5  # discount to eviction score for protected items (used within protected pool)

GHOST_LIMIT_MIN = 1024        # minimum ghost capacity (by count)
GHOST_LIMIT_FACTOR = 2.0      # ghost capacity  factor * live cache item count

_INF = 1e30

# ----------------------
# Helpers
# ----------------------
def _now(cache_snapshot):
    return int(getattr(cache_snapshot, "access_count", 0))

def _size_of(obj):
    return max(1, int(getattr(obj, "size", 1)))

def _ghost_capacity_limit(cache_snapshot):
    live_cnt = max(1, len(getattr(cache_snapshot, "cache", {}) or {}))
    return max(GHOST_LIMIT_MIN, int(GHOST_LIMIT_FACTOR * live_cnt))

def _trim_ghost(cache_snapshot):
    limit = _ghost_capacity_limit(cache_snapshot)
    while len(g_mu) > limit and g_order:
        k, s = g_order.popleft()
        if g_stamp.get(k, None) == s:
            g_mu.pop(k, None)
            g_freq.pop(k, None)
            g_stamp.pop(k, None)

def _maybe_advance_epoch(now):
    global g_epoch, g_next_epoch_at
    if now >= g_next_epoch_at:
        # Advance by as many epochs as needed (handles large jumps)
        steps = max(1, (now - g_next_epoch_at) // EPOCH_LENGTH + 1)
        g_epoch += steps
        g_next_epoch_at += steps * EPOCH_LENGTH

def _decayed_freq(key):
    # Return frequency aged to current epoch, without mutating stored value
    raw = m_freq.get(key, 0)
    k_epoch = m_f_epoch.get(key, g_epoch)
    d = g_epoch - k_epoch
    if d <= 0:
        return raw
    # divide by 2^(DECAY_SHIFT_PER_EPOCH * d)
    shift = DECAY_SHIFT_PER_EPOCH * d
    return raw >> shift

def _bump_freq_on_hit(key):
    # Normalize to current epoch then increment
    ke = m_f_epoch.get(key, g_epoch)
    if ke != g_epoch:
        d = g_epoch - ke
        if d > 0:
            shift = DECAY_SHIFT_PER_EPOCH * d
            m_freq[key] = m_freq.get(key, 0) >> shift
        m_f_epoch[key] = g_epoch
    m_freq[key] = m_freq.get(key, 0) + 1

def _predicted_delta_components(key, now):
    """
    Return (effective_delta, is_overdue)
    - If no predictor exists: treated as very far (cold).
    - If overdue: effective_delta grows with lateness (OVERDUE_PENALTY * overdue).
    - Else: time remaining until predicted next access.
    """
    mu = m_mu.get(key, None)
    la = m_last_access.get(key, None)
    if mu is None or la is None:
        return (_INF, False)
    next_t = la + mu
    if now <= next_t:
        return (float(next_t - now), False)
    overdue = float(now - next_t)
    return (OVERDUE_PENALTY * overdue, True)

def _eviction_score_within_stage(key, obj, now, is_protected):
    """
    Higher score => evict sooner among items in the same stage.
    score = (effective_delta) / (size^SIZE_EXP * (1 + FREQ_DAMP*log1p(freq_adj))) * stage_factor
    - New/low-frequency items get a boost to the score (more likely to evict).
    - Protected items get a discount (stage_factor) when comparing within protected pool.
    """
    sz = _size_of(obj)
    delta, _ = _predicted_delta_components(key, now)

    # Decayed frequency to avoid stale dominance
    f_eff = max(0, _decayed_freq(key))
    denom_freq = 1.0 + FREQ_DAMP * math.log1p(max(0, f_eff))

    base = delta / ((sz ** SIZE_EXP) * denom_freq)

    # Boost eviction score for items that have never hit (freq ~ 0 or 1)
    raw_freq = m_freq.get(key, 0)
    if raw_freq <= 1:
        base *= NEW_ITEM_BONUS

    # Protected items get discount only within protected pool
    if is_protected:
        base *= STAGE_FACTOR_PROTECTED

    return base

def _seed_predictor_on_insert(key, now):
    """
    Initialize per-key metadata on insert:
    - If ghost history exists, reuse tempered version of it.
    - Else, seed from global mu with a conservative multiplier.
    - Start in probation (stage=0).
    - Initialize frequency with epoch tagging (so it will age).
    """
    global m_global_mu
    if key in g_mu:
        m_mu[key] = max(1.0, 0.9 * float(g_mu[key]))
        # Initialize decayed freq from ghost, but ensure it ages in current epoch
        ghf = max(0, int(g_freq.get(key, 1)))
        m_freq[key] = ghf
    else:
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))
        m_freq[key] = 0  # will be incremented on hit

    m_f_epoch[key] = g_epoch
    m_last_access[key] = now
    m_stage[key] = 0  # probation

def _update_predictor_on_hit(key, now):
    """
    Update per-key predictor on hit:
    - EWMA of inter-arrival times with fixed beta.
    - Update global mu slowly.
    - Increment decayed frequency.
    """
    global m_mu, m_last_access, m_global_mu

    last = m_last_access.get(key, None)
    if last is not None:
        gap = max(1.0, float(now - last))
        prev_mu = m_mu.get(key, gap)
        m_mu[key] = (1.0 - EWMA_BETA) * prev_mu + EWMA_BETA * gap
        m_global_mu = (1.0 - GLOBAL_BETA) * float(m_global_mu) + GLOBAL_BETA * gap
    else:
        # Cold edge case (should be rare)
        m_mu[key] = max(1.0, DEFAULT_MU_MULT * float(m_global_mu))

    m_last_access[key] = now
    _bump_freq_on_hit(key)

def _record_ghost_on_evict(evicted_key):
    """
    Store compact history for evicted key to accelerate relearning on reinsert.
    O(1) append with versioning; trimming deferred to _trim_ghost.
    """
    mu = m_mu.get(evicted_key, None)
    if mu is None:
        return
    g_mu[evicted_key] = float(mu)
    # Store a small bounded freq (avoid huge integers)
    g_freq[evicted_key] = min(255, max(0, int(m_freq.get(evicted_key, 0))))
    s = g_stamp.get(evicted_key, 0) + 1
    g_stamp[evicted_key] = s
    g_order.append((evicted_key, s))

def _stage_of_key(key):
    return m_stage.get(key, 0)

def _inc_probation_bytes(sz):
    global bytes_probation
    bytes_probation += max(0, int(sz))

def _move_probation_to_protected(sz):
    global bytes_probation, bytes_protected
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0
    bytes_protected += s

def _dec_probation_bytes(sz):
    global bytes_probation
    s = max(0, int(sz))
    if bytes_probation >= s:
        bytes_probation -= s
    else:
        bytes_probation = 0

def _dec_protected_bytes(sz):
    global bytes_protected
    s = max(0, int(sz))
    if bytes_protected >= s:
        bytes_protected -= s
    else:
        bytes_protected = 0

# ----------------------
# Policy entry points
# ----------------------
def evict(cache_snapshot, obj):
    """
    Evict the resident object prioritizing the probation segment (SLRU).
    Steps:
      - Advance frequency epoch if needed.
      - If any probation item exists, pick the one with the largest eviction score within probation.
      - Else, pick the protected item with the largest eviction score (with protected discount).
    Tie-breakers within the chosen pool:
      1) Older last access first (LRU among equals)
      2) Larger size first (free more space)
      3) Lexicographic key order for stability
    """
    cache = cache_snapshot.cache
    if not cache:
        return None

    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # First pass: probation only
    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None
    found_probation = False

    for k, v in cache.items():
        if _stage_of_key(k) != 0:
            continue
        found_probation = True
        s = _eviction_score_within_stage(k, v, now, is_protected=False)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
            best_key, best_score, best_la, best_sz = k, s, la, sz

    if found_probation:
        return best_key

    # Second pass: protected only (if no probation items)
    best_key = None
    best_score = -1.0
    best_la = None
    best_sz = None

    for k, v in cache.items():
        if _stage_of_key(k) != 1:
            continue

        s = _eviction_score_within_stage(k, v, now, is_protected=True)
        la = m_last_access.get(k, -1)
        sz = _size_of(v)

        if best_key is None or s > best_score or \
           (s == best_score and (la < best_la or (la == best_la and (sz > best_sz or (sz == best_sz and k < best_key))))):
            best_key, best_score, best_la, best_sz = k, s, la, sz

    return best_key


def update_after_hit(cache_snapshot, obj):
    """
    After a hit:
    - Advance epoch and update per-key EWMA predictor, global mu, and decayed frequency.
    - Promote from probation to protected on first hit (SLRU).
    - Maintain last access and segment byte counters.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    # Ensure baseline structures exist (cold hit edge case)
    if key not in m_last_access or key not in m_mu:
        _seed_predictor_on_insert(key, now)
        # Do not mutate byte counters here (unknown stage/bytes consistency)
        # Keep stage=0 from seeding; promotion below will adjust safely with guards.

    _update_predictor_on_hit(key, now)

    # SLRU promotion on first hit
    if m_stage.get(key, 0) == 0:
        m_stage[key] = 1
        _move_probation_to_protected(_size_of(obj))


def update_after_insert(cache_snapshot, obj):
    """
    After insert:
    - Initialize per-key predictor:
      * Warm-start from ghost if available; otherwise use default scaled by global mu.
    - Start in probation (stage=0).
    - Update last access time and byte counters for probation.
    """
    key = obj.key
    now = _now(cache_snapshot)
    _maybe_advance_epoch(now)

    _seed_predictor_on_insert(key, now)
    _inc_probation_bytes(_size_of(obj))


def update_after_evict(cache_snapshot, obj, evicted_obj):
    """
    After eviction:
    - Record compact ghost history for the evicted key and trim ghost store.
    - Remove per-key metadata of the evicted key.
    - Adjust segment byte counters.
    """
    ekey = evicted_obj.key

    _record_ghost_on_evict(ekey)
    _trim_ghost(cache_snapshot)

    # Adjust segment bytes using last known stage
    st = m_stage.get(ekey, 0)
    if st == 0:
        _dec_probation_bytes(_size_of(evicted_obj))
    else:
        _dec_protected_bytes(_size_of(evicted_obj))

    # Clean live metadata
    m_last_access.pop(ekey, None)
    m_mu.pop(ekey, None)
    m_freq.pop(ekey, None)
    m_f_epoch.pop(ekey, None)
    m_stage.pop(ekey, None)
Best program validation score: 0.25926008333333345
Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp_w0z6srb.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpd1goxuit.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpw0tw2xyl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpiu2d267h.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3on_tlod.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpryjkz7pq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpdb2awsvq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkstk2n9f.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmphqd6xhs9.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpxfbqqkm4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1sf4o7xu.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy8jpzthm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpj7697mxi.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpbhrpwah8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpqqm55vzt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp73pcpxgw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpylvdjd3j.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpclu9j14x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpur1hztc8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaqvh40fy.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp6mepuwt2.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp3nvovkkt.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpwjvouuju.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpm9wk3lk0.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpot0k4ddm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp9avrxcvm.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr55m_46n.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4gqba23r.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmew9hqbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpzbpwfsjn.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpr90z1n0g.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjr0lmxf5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpaei7mp0k.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpodw6svq4.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1o9gir41.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp77csrzpw.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpifmqglb8.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpvjn0awvv.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpkjluxs0x.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpy2zjunro.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpjv4q18r_.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp1lqr063q.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpe4kmb6ic.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp4r3n31_7.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmps8dn_nz5.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpztzl4frq.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmp45gd0mbl.pickle

Subprocess stdout: Results saved to /home/ruiying/ADRS-Dev/gepa/src/gepa/adapters/caching_adapter_gpt5_2/tmp/tmpmv6cy_gc.pickle

Optimized program test score (avg over test): 0.25926008333333345
